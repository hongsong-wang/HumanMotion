<!DOCTYPE html>
<html>
<head>
<title>Paper collected by Wang</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<style type="text/css">


/* BODY
=============================================================================*/

body {
  font-family: Helvetica, arial, freesans, clean, sans-serif;
  font-size: 14px;
  line-height: 1.6;
  color: #333;
  background-color: #fff;
  padding: 10px 20px 10px 20px;
  max-width: 960px;
  margin: 0 auto;
}

span#pid {
  color:red;
  
}
span#filename{
  font-style: oblique;
}

span#title{
  font-family: Times New Roman, freesans, clean, sans-serif;
  font-style: italic;
  font-size: 20px;
  border:1px solid #B50;
}
span#abs{
  font-family: Times New Roman, freesans, clean, sans-serif;
  font-style: oblique;
  font-size: 18px;
}
</style>
</head>
<body><div id='title' style='font-size:1.3em; font-weight:bold;'>arXiv Papers of VLA-RAIL: A Real-Time Asynchronous Inference Linker for VLA Models and Robots</div><br>
<div id='section'>Paperid: <span id='pid'>1, <a href='https://arxiv.org/pdf/2512.24100.pdf' target='_blank'>https://arxiv.org/pdf/2512.24100.pdf</a></span>   <span><a href='https://chenhaoqcdyq.github.io/LMR/' target='_blank'>  GitHub</a></span> <span><a href='https://chenhaoqcdyq.github.io/LMR/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yijie Qian, Juncheng Wang, Yuxiang Feng, Chao Xu, Wang Lu, Yang Liu, Baigui Sun, Yiqiang Chen, Yong Liu, Shujun Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.24100">Think Before You Move: Latent Motion Reasoning for Text-to-Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current state-of-the-art paradigms predominantly treat Text-to-Motion (T2M) generation as a direct translation problem, mapping symbolic language directly to continuous poses. While effective for simple actions, this System 1 approach faces a fundamental theoretical bottleneck we identify as the Semantic-Kinematic Impedance Mismatch: the inherent difficulty of grounding semantically dense, discrete linguistic intent into kinematically dense, high-frequency motion data in a single shot. In this paper, we argue that the solution lies in an architectural shift towards Latent System 2 Reasoning. Drawing inspiration from Hierarchical Motor Control in cognitive science, we propose Latent Motion Reasoning (LMR) that reformulates generation as a two-stage Think-then-Act decision process. Central to LMR is a novel Dual-Granularity Tokenizer that disentangles motion into two distinct manifolds: a compressed, semantically rich Reasoning Latent for planning global topology, and a high-frequency Execution Latent for preserving physical fidelity. By forcing the model to autoregressively reason (plan the coarse trajectory) before it moves (instantiates the frames), we effectively bridge the ineffability gap between language and physics. We demonstrate LMR's versatility by implementing it for two representative baselines: T2M-GPT (discrete) and MotionStreamer (continuous). Extensive experiments show that LMR yields non-trivial improvements in both semantic alignment and physical plausibility, validating that the optimal substrate for motion planning is not natural language, but a learned, motion-aligned concept space. Codes and demos can be found in \hyperlink{https://chenhaoqcdyq.github.io/LMR/}{https://chenhaoqcdyq.github.io/LMR/}
<div id='section'>Paperid: <span id='pid'>2, <a href='https://arxiv.org/pdf/2512.23464.pdf' target='_blank'>https://arxiv.org/pdf/2512.23464.pdf</a></span>   <span><a href='https://github.com/Tencent-Hunyuan/HY-Motion-1.0' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxin Wen, Qing Shuai, Di Kang, Jing Li, Cheng Wen, Yue Qian, Ningxin Jiao, Changhai Chen, Weijie Chen, Yiran Wang, Jinkun Guo, Dongyue An, Han Liu, Yanyu Tong, Chao Zhang, Qing Guo, Juan Chen, Qiao Zhang, Youyi Zhang, Zihao Yao, Cheng Zhang, Hong Duan, Xiaoping Wu, Qi Chen, Fei Cheng, Liang Dong, Peng He, Hao Zhang, Jiaxin Lin, Chao Zhang, Zhongyi Fan, Yifan Li, Zhichao Hu, Yuhong Liu, Linus, Jie Jiang, Xiaolong Li, Linchao Bao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.23464">HY-Motion 1.0: Scaling Flow Matching Models for Text-To-Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present HY-Motion 1.0, a series of state-of-the-art, large-scale, motion generation models capable of generating 3D human motions from textual descriptions. HY-Motion 1.0 represents the first successful attempt to scale up Diffusion Transformer (DiT)-based flow matching models to the billion-parameter scale within the motion generation domain, delivering instruction-following capabilities that significantly outperform current open-source benchmarks. Uniquely, we introduce a comprehensive, full-stage training paradigm -- including large-scale pretraining on over 3,000 hours of motion data, high-quality fine-tuning on 400 hours of curated data, and reinforcement learning from both human feedback and reward models -- to ensure precise alignment with the text instruction and high motion quality. This framework is supported by our meticulous data processing pipeline, which performs rigorous motion cleaning and captioning. Consequently, our model achieves the most extensive coverage, spanning over 200 motion categories across 6 major classes. We release HY-Motion 1.0 to the open-source community to foster future research and accelerate the transition of 3D human motion generation models towards commercial maturity.
<div id='section'>Paperid: <span id='pid'>3, <a href='https://arxiv.org/pdf/2512.22688.pdf' target='_blank'>https://arxiv.org/pdf/2512.22688.pdf</a></span>   <span><a href='https://github.com/Johnathan-Xie/arfm-motion-prediction' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Johnathan Xie, Stefan Stojanov, Cristobal Eyzaguirre, Daniel L. K. Yamins, Jiajun Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.22688">Autoregressive Flow Matching for Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion prediction has been studied in different contexts with models trained on narrow distributions and applied to downstream tasks in human motion prediction and robotics. Simultaneously, recent efforts in scaling video prediction have demonstrated impressive visual realism, yet they struggle to accurately model complex motions despite massive scale. Inspired by the scaling of video generation, we develop autoregressive flow matching (ARFM), a new method for probabilistic modeling of sequential continuous data and train it on diverse video datasets to generate future point track locations over long horizons. To evaluate our model, we develop benchmarks for evaluating the ability of motion prediction models to predict human and robot motion. Our model is able to predict complex motions, and we demonstrate that conditioning robot action prediction and human motion prediction on predicted future tracks can significantly improve downstream task performance. Code and models publicly available at: https://github.com/Johnathan-Xie/arfm-motion-prediction.
<div id='section'>Paperid: <span id='pid'>4, <a href='https://arxiv.org/pdf/2512.22324.pdf' target='_blank'>https://arxiv.org/pdf/2512.22324.pdf</a></span>   <span><a href='https://jiro-zhang.github.io/DeMoGen/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianrong Zhang, Hehe Fan, Yi Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.22324">DeMoGen: Towards Decompositional Human Motion Generation with Energy-Based Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motions are compositional: complex behaviors can be described as combinations of simpler primitives. However, existing approaches primarily focus on forward modeling, e.g., learning holistic mappings from text to motion or composing a complex motion from a set of motion concepts. In this paper, we consider the inverse perspective: decomposing a holistic motion into semantically meaningful sub-components. We propose DeMoGen, a compositional training paradigm for decompositional learning that employs an energy-based diffusion model. This energy formulation directly captures the composed distribution of multiple motion concepts, enabling the model to discover them without relying on ground-truth motions for individual concepts. Within this paradigm, we introduce three training variants to encourage a decompositional understanding of motion: 1. DeMoGen-Exp explicitly trains on decomposed text prompts; 2. DeMoGen-OSS performs orthogonal self-supervised decomposition; 3. DeMoGen-SC enforces semantic consistency between original and decomposed text embeddings. These variants enable our approach to disentangle reusable motion primitives from complex motion sequences. We also demonstrate that the decomposed motion concepts can be flexibly recombined to generate diverse and novel motions, generalizing beyond the training distribution. Additionally, we construct a text-decomposed dataset to support compositional training, serving as an extended resource to facilitate text-to-motion generation and motion composition.
<div id='section'>Paperid: <span id='pid'>5, <a href='https://arxiv.org/pdf/2512.21736.pdf' target='_blank'>https://arxiv.org/pdf/2512.21736.pdf</a></span>   <span><a href='https://humanaigc.github.io/sync_anyone_demo_page/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xindi Zhang, Dechao Meng, Steven Xiao, Qi Wang, Peng Zhang, Bang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.21736">SyncAnyone: Implicit Disentanglement via Progressive Self-Correction for Lip-Syncing in the wild</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>High-quality AI-powered video dubbing demands precise audio-lip synchronization, high-fidelity visual generation, and faithful preservation of identity and background. Most existing methods rely on a mask-based training strategy, where the mouth region is masked in talking-head videos, and the model learns to synthesize lip movements from corrupted inputs and target audios. While this facilitates lip-sync accuracy, it disrupts spatiotemporal context, impairing performance on dynamic facial motions and causing instability in facial structure and background consistency. To overcome this limitation, we propose SyncAnyone, a novel two-stage learning framework that achieves accurate motion modeling and high visual fidelity simultaneously. In Stage 1, we train a diffusion-based video transformer for masked mouth inpainting, leveraging its strong spatiotemporal modeling to generate accurate, audio-driven lip movements. However, due to input corruption, minor artifacts may arise in the surrounding facial regions and the background. In Stage 2, we develop a mask-free tuning pipeline to address mask-induced artifacts. Specifically, on the basis of the Stage 1 model, we develop a data generation pipeline that creates pseudo-paired training samples by synthesizing lip-synced videos from the source video and random sampled audio. We further tune the stage 2 model on this synthetic data, achieving precise lip editing and better background consistency. Extensive experiments show that our method achieves state-of-the-art results in visual quality, temporal coherence, and identity preservation under in-the wild lip-syncing scenarios.
<div id='section'>Paperid: <span id='pid'>6, <a href='https://arxiv.org/pdf/2512.21707.pdf' target='_blank'>https://arxiv.org/pdf/2512.21707.pdf</a></span>   <span><a href='https://github.com/alanyz106/ST-MoE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zheng Yin, Chengjian Li, Xiangbo Shu, Meiqi Cao, Rui Yan, Jinhui Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.21707">Spatiotemporal-Untrammelled Mixture of Experts for Multi-Person Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Comprehensively and flexibly capturing the complex spatio-temporal dependencies of human motion is critical for multi-person motion prediction. Existing methods grapple with two primary limitations: i) Inflexible spatiotemporal representation due to reliance on positional encodings for capturing spatiotemporal information. ii) High computational costs stemming from the quadratic time complexity of conventional attention mechanisms. To overcome these limitations, we propose the Spatiotemporal-Untrammelled Mixture of Experts (ST-MoE), which flexibly explores complex spatio-temporal dependencies in human motion and significantly reduces computational cost. To adaptively mine complex spatio-temporal patterns from human motion, our model incorporates four distinct types of spatiotemporal experts, each specializing in capturing different spatial or temporal dependencies. To reduce the potential computational overhead while integrating multiple experts, we introduce bidirectional spatiotemporal Mamba as experts, each sharing bidirectional temporal and spatial Mamba in distinct combinations to achieve model efficiency and parameter economy. Extensive experiments on four multi-person benchmark datasets demonstrate that our approach not only outperforms state-of-art in accuracy but also reduces model parameter by 41.38% and achieves a 3.6x speedup in training. The code is available at https://github.com/alanyz106/ST-MoE.
<div id='section'>Paperid: <span id='pid'>7, <a href='https://arxiv.org/pdf/2512.20000.pdf' target='_blank'>https://arxiv.org/pdf/2512.20000.pdf</a></span>   <span><a href='https://github.com/yishaohan/MIVA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenhao Li, Shaohan Yi, Zheng Liu, Leonartinus Gao, Minh Ngoc Le, Ambrose Ling, Zhuoran Wang, Md Amirul Islam, Zhixiang Chi, Yuanhao Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.20000">Few-Shot-Based Modular Image-to-Video Adapter for Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion models (DMs) have recently achieved impressive photorealism in image and video generation. However, their application to image animation remains limited, even when trained on large-scale datasets. Two primary challenges contribute to this: the high dimensionality of video signals leads to a scarcity of training data, causing DMs to favor memorization over prompt compliance when generating motion; moreover, DMs struggle to generalize to novel motion patterns not present in the training set, and fine-tuning them to learn such patterns, especially using limited training data, is still under-explored. To address these limitations, we propose Modular Image-to-Video Adapter (MIVA), a lightweight sub-network attachable to a pre-trained DM, each designed to capture a single motion pattern and scalable via parallelization. MIVAs can be efficiently trained on approximately ten samples using a single consumer-grade GPU. At inference time, users can specify motion by selecting one or multiple MIVAs, eliminating the need for prompt engineering. Extensive experiments demonstrate that MIVA enables more precise motion control while maintaining, or even surpassing, the generation quality of models trained on significantly larger datasets.
<div id='section'>Paperid: <span id='pid'>8, <a href='https://arxiv.org/pdf/2512.19283.pdf' target='_blank'>https://arxiv.org/pdf/2512.19283.pdf</a></span>   <span><a href='https://kyungwoncho.github.io/HaMoS/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kyungwon Cho, Hanbyul Joo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.19283">Hand-Aware Egocentric Motion Reconstruction with Sequence-Level Context</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Egocentric vision systems are becoming widely available, creating new opportunities for human-computer interaction. A core challenge is estimating the wearer's full-body motion from first-person videos, which is crucial for understanding human behavior. However, this task is difficult since most body parts are invisible from the egocentric view. Prior approaches mainly rely on head trajectories, leading to ambiguity, or assume continuously tracked hands, which is unrealistic for lightweight egocentric devices. In this work, we present HaMoS, the first hand-aware, sequence-level diffusion framework that directly conditions on both head trajectory and intermittently visible hand cues caused by field-of-view limitations and occlusions, as in real-world egocentric devices. To overcome the lack of datasets pairing diverse camera views with human motion, we introduce a novel augmentation method that models such real-world conditions. We also demonstrate that sequence-level contexts such as body shape and field-of-view are crucial for accurate motion reconstruction, and thus employ local attention to infer long sequences efficiently. Experiments on public benchmarks show that our method achieves state-of-the-art accuracy and temporal smoothness, demonstrating a practical step toward reliable in-the-wild egocentric 3D motion understanding.
<div id='section'>Paperid: <span id='pid'>9, <a href='https://arxiv.org/pdf/2512.19159.pdf' target='_blank'>https://arxiv.org/pdf/2512.19159.pdf</a></span>   <span><a href='https://OmniMoGen.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wendong Bu, Kaihang Pan, Yuze Lin, Jiacheng Li, Kai Shen, Wenqiao Zhang, Juncheng Li, Jun Xiao, Siliang Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.19159">OmniMoGen: Unifying Human Motion Generation via Learning from Interleaved Text-Motion Instructions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models (LLMs) have unified diverse linguistic tasks within a single framework, yet such unification remains unexplored in human motion generation. Existing methods are confined to isolated tasks, limiting flexibility for free-form and omni-objective generation. To address this, we propose OmniMoGen, a unified framework that enables versatile motion generation through interleaved text-motion instructions. Built upon a concise RVQ-VAE and transformer architecture, OmniMoGen supports end-to-end instruction-driven motion generation. We construct X2Mo, a large-scale dataset of over 137K interleaved text-motion instructions, and introduce AnyContext, a benchmark for evaluating interleaved motion generation. Experiments show that OmniMoGen achieves state-of-the-art performance on text-to-motion, motion editing, and AnyContext, exhibiting emerging capabilities such as compositional editing, self-reflective generation, and knowledge-informed generation. These results mark a step toward the next intelligent motion generation. Project Page: https://OmniMoGen.github.io/.
<div id='section'>Paperid: <span id='pid'>10, <a href='https://arxiv.org/pdf/2512.16791.pdf' target='_blank'>https://arxiv.org/pdf/2512.16791.pdf</a></span>   <span><a href='https://kaka-1314.github.io/KineST/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuting Zhao, Zeyu Xiao, Xinrong Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.16791">KineST: A Kinematics-guided Spatiotemporal State Space Model for Human Motion Tracking from Sparse Signals</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Full-body motion tracking plays an essential role in AR/VR applications, bridging physical and virtual interactions. However, it is challenging to reconstruct realistic and diverse full-body poses based on sparse signals obtained by head-mounted displays, which are the main devices in AR/VR scenarios. Existing methods for pose reconstruction often incur high computational costs or rely on separately modeling spatial and temporal dependencies, making it difficult to balance accuracy, temporal coherence, and efficiency. To address this problem, we propose KineST, a novel kinematics-guided state space model, which effectively extracts spatiotemporal dependencies while integrating local and global pose perception. The innovation comes from two core ideas. Firstly, in order to better capture intricate joint relationships, the scanning strategy within the State Space Duality framework is reformulated into kinematics-guided bidirectional scanning, which embeds kinematic priors. Secondly, a mixed spatiotemporal representation learning approach is employed to tightly couple spatial and temporal contexts, balancing accuracy and smoothness. Additionally, a geometric angular velocity loss is introduced to impose physically meaningful constraints on rotational variations for further improving motion stability. Extensive experiments demonstrate that KineST has superior performance in both accuracy and temporal consistency within a lightweight framework. Project page: https://kaka-1314.github.io/KineST/
<div id='section'>Paperid: <span id='pid'>11, <a href='https://arxiv.org/pdf/2512.16456.pdf' target='_blank'>https://arxiv.org/pdf/2512.16456.pdf</a></span>   <span><a href='https://masashi-hatano.github.io/prime-and-reach/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Masashi Hatano, Saptarshi Sinha, Jacob Chalk, Wei-Hong Li, Hideo Saito, Dima Damen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.16456">Prime and Reach: Synthesising Body Motion for Gaze-Primed Object Reach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion generation is a challenging task that aims to create realistic motion imitating natural human behaviour. We focus on the well-studied behaviour of priming an object/location for pick up or put down -- that is, the spotting of an object/location from a distance, known as gaze priming, followed by the motion of approaching and reaching the target location. To that end, we curate, for the first time, 23.7K gaze-primed human motion sequences for reaching target object locations from five publicly available datasets, i.e., HD-EPIC, MoGaze, HOT3D, ADT, and GIMO. We pre-train a text-conditioned diffusion-based motion generation model, then fine-tune it conditioned on goal pose or location, on our curated sequences. Importantly, we evaluate the ability of the generated motion to imitate natural human movement through several metrics, including the 'Reach Success' and a newly introduced 'Prime Success' metric. On the largest dataset, HD-EPIC, our model achieves 60% prime success and 89% reach success when conditioned on the goal object location.
<div id='section'>Paperid: <span id='pid'>12, <a href='https://arxiv.org/pdf/2512.14696.pdf' target='_blank'>https://arxiv.org/pdf/2512.14696.pdf</a></span>   <span><a href='https://crisp-real2sim.github.io/CRISP-Real2Sim/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zihan Wang, Jiashun Wang, Jeff Tan, Yiwen Zhao, Jessica Hodgins, Shubham Tulsiani, Deva Ramanan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.14696">CRISP: Contact-Guided Real2Sim from Monocular Video with Planar Scene Primitives</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce CRISP, a method that recovers simulatable human motion and scene geometry from monocular video. Prior work on joint human-scene reconstruction relies on data-driven priors and joint optimization with no physics in the loop, or recovers noisy geometry with artifacts that cause motion tracking policies with scene interactions to fail. In contrast, our key insight is to recover convex, clean, and simulation-ready geometry by fitting planar primitives to a point cloud reconstruction of the scene, via a simple clustering pipeline over depth, normals, and flow. To reconstruct scene geometry that might be occluded during interactions, we make use of human-scene contact modeling (e.g., we use human posture to reconstruct the occluded seat of a chair). Finally, we ensure that human and scene reconstructions are physically-plausible by using them to drive a humanoid controller via reinforcement learning. Our approach reduces motion tracking failure rates from 55.2\% to 6.9\% on human-centric video benchmarks (EMDB, PROX), while delivering a 43\% faster RL simulation throughput. We further validate it on in-the-wild videos including casually-captured videos, Internet videos, and even Sora-generated videos. This demonstrates CRISP's ability to generate physically-valid human motion and interaction environments at scale, greatly advancing real-to-sim applications for robotics and AR/VR.
<div id='section'>Paperid: <span id='pid'>13, <a href='https://arxiv.org/pdf/2512.13840.pdf' target='_blank'>https://arxiv.org/pdf/2512.13840.pdf</a></span>   <span><a href='https://hynann.github.io/molingo/MoLingo.html' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yannan He, Garvita Tiwari, Xiaohan Zhang, Pankaj Bora, Tolga Birdal, Jan Eric Lenssen, Gerard Pons-Moll
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.13840">MoLingo: Motion-Language Alignment for Text-to-Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce MoLingo, a text-to-motion (T2M) model that generates realistic, lifelike human motion by denoising in a continuous latent space. Recent works perform latent space diffusion, either on the whole latent at once or auto-regressively over multiple latents. In this paper, we study how to make diffusion on continuous motion latents work best. We focus on two questions: (1) how to build a semantically aligned latent space so diffusion becomes more effective, and (2) how to best inject text conditioning so the motion follows the description closely. We propose a semantic-aligned motion encoder trained with frame-level text labels so that latents with similar text meaning stay close, which makes the latent space more diffusion-friendly. We also compare single-token conditioning with a multi-token cross-attention scheme and find that cross-attention gives better motion realism and text-motion alignment. With semantically aligned latents, auto-regressive generation, and cross-attention text conditioning, our model sets a new state of the art in human motion generation on standard metrics and in a user study. We will release our code and models for further research and downstream usage.
<div id='section'>Paperid: <span id='pid'>14, <a href='https://arxiv.org/pdf/2512.13247.pdf' target='_blank'>https://arxiv.org/pdf/2512.13247.pdf</a></span>   <span><a href='https://foivospar.github.io/STARCaster/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Foivos Paraperas Papantoniou, Stathis Galanakis, Rolandos Alexandros Potamias, Bernhard Kainz, Stefanos Zafeiriou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.13247">STARCaster: Spatio-Temporal AutoRegressive Video Diffusion for Identity- and View-Aware Talking Portraits</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents STARCaster, an identity-aware spatio-temporal video diffusion model that addresses both speech-driven portrait animation and free-viewpoint talking portrait synthesis, given an identity embedding or reference image, within a unified framework. Existing 2D speech-to-video diffusion models depend heavily on reference guidance, leading to limited motion diversity. At the same time, 3D-aware animation typically relies on inversion through pre-trained tri-plane generators, which often leads to imperfect reconstructions and identity drift. We rethink reference- and geometry-based paradigms in two ways. First, we deviate from strict reference conditioning at pre-training by introducing softer identity constraints. Second, we address 3D awareness implicitly within the 2D video domain by leveraging the inherent multi-view nature of video data. STARCaster adopts a compositional approach progressing from ID-aware motion modeling, to audio-visual synchronization via lip reading-based supervision, and finally to novel view animation through temporal-to-spatial adaptation. To overcome the scarcity of 4D audio-visual data, we propose a decoupled learning approach in which view consistency and temporal coherence are trained independently. A self-forcing training scheme enables the model to learn from longer temporal contexts than those generated at inference, mitigating the overly static animations common in existing autoregressive approaches. Comprehensive evaluations demonstrate that STARCaster generalizes effectively across tasks and identities, consistently surpassing prior approaches in different benchmarks.
<div id='section'>Paperid: <span id='pid'>15, <a href='https://arxiv.org/pdf/2512.12703.pdf' target='_blank'>https://arxiv.org/pdf/2512.12703.pdf</a></span>   <span><a href='https://boyuaner.github.io/ropar-main/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Boyuan Li, Sipeng Zheng, Bin Cao, Ruihua Song, Zongqing Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.12703">Robust Motion Generation using Part-level Reliable Data from Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Extracting human motion from large-scale web videos offers a scalable solution to the data scarcity issue in character animation. However, some human parts in many video frames cannot be seen due to off-screen captures or occlusions. It brings a dilemma: discarding the data missing any part limits scale and diversity, while retaining it compromises data quality and model performance. To address this problem, we propose leveraging credible part-level data extracted from videos to enhance motion generation via a robust part-aware masked autoregression model. First, we decompose a human body into five parts and detect the parts clearly seen in a video frame as "credible". Second, the credible parts are encoded into latent tokens by our proposed part-aware variational autoencoder. Third, we propose a robust part-level masked generation model to predict masked credible parts, while ignoring those noisy parts. In addition, we contribute K700-M, a challenging new benchmark comprising approximately 200k real-world motion sequences, for evaluation. Experimental results indicate that our method successfully outperforms baselines on both clean and noisy datasets in terms of motion quality, semantic consistency and diversity. Project page: https://boyuaner.github.io/ropar-main/
<div id='section'>Paperid: <span id='pid'>16, <a href='https://arxiv.org/pdf/2512.11872.pdf' target='_blank'>https://arxiv.org/pdf/2512.11872.pdf</a></span>   <span><a href='https://github.com/fudan-generative-vision/WAM-Diff' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingwang Xu, Jiahao Cui, Feipeng Cai, Hanlin Shang, Zhihao Zhu, Shan Luan, Yifang Xu, Neng Zhang, Yaoyi Li, Jia Cai, Siyu Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.11872">WAM-Diff: A Masked Diffusion VLA Framework with MoE and Online Reinforcement Learning for Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>End-to-end autonomous driving systems based on vision-language-action (VLA) models integrate multimodal sensor inputs and language instructions to generate planning and control signals. While autoregressive large language models and continuous diffusion policies are prevalent, the potential of discrete masked diffusion for trajectory generation remains largely unexplored. This paper presents WAM-Diff, a VLA framework that employs masked diffusion to iteratively refine a discrete sequence representing future ego-trajectories. Our approach features three key innovations: a systematic adaptation of masked diffusion for autonomous driving that supports flexible, non-causal decoding orders; scalable model capacity via a sparse MoE architecture trained jointly on motion prediction and driving-oriented visual question answering (VQA); and online reinforcement learning using Group Sequence Policy Optimization (GSPO) to optimize sequence-level driving rewards. Remarkably, our model achieves 91.0 PDMS on NAVSIM-v1 and 89.7 EPDMS on NAVSIM-v2, demonstrating the effectiveness of masked diffusion for autonomous driving. The approach provides a promising alternative to autoregressive and diffusion-based policies, supporting scenario-aware decoding strategies for trajectory generation. The code for this paper will be released publicly at: https://github.com/fudan-generative-vision/WAM-Diff
<div id='section'>Paperid: <span id='pid'>17, <a href='https://arxiv.org/pdf/2512.11654.pdf' target='_blank'>https://arxiv.org/pdf/2512.11654.pdf</a></span>   <span><a href='https://lucazzola.github.io/publications/kinemic' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Luca Cazzola, Ahed Alboody
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.11654">Kinetic Mining in Context: Few-Shot Action Synthesis via Text-to-Motion Distillation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The acquisition cost for large, annotated motion datasets remains a critical bottleneck for skeletal-based Human Activity Recognition (HAR). Although Text-to-Motion (T2M) generative models offer a compelling, scalable source of synthetic data, their training objectives, which emphasize general artistic motion, and dataset structures fundamentally differ from HAR's requirements for kinematically precise, class-discriminative actions. This disparity creates a significant domain gap, making generalist T2M models ill-equipped for generating motions suitable for HAR classifiers. To address this challenge, we propose KineMIC (Kinetic Mining In Context), a transfer learning framework for few-shot action synthesis. KineMIC adapts a T2M diffusion model to an HAR domain by hypothesizing that semantic correspondences in the text encoding space can provide soft supervision for kinematic distillation. We operationalize this via a kinetic mining strategy that leverages CLIP text embeddings to establish correspondences between sparse HAR labels and T2M source data. This process guides fine-tuning, transforming the generalist T2M backbone into a specialized few-shot Action-to-Motion generator. We validate KineMIC using HumanML3D as the source T2M dataset and a subset of NTU RGB+D 120 as the target HAR domain, randomly selecting just 10 samples per action class. Our approach generates significantly more coherent motions, providing a robust data augmentation source that delivers a +23.1% accuracy points improvement. Animated illustrations and supplementary materials are available at (https://lucazzola.github.io/publications/kinemic).
<div id='section'>Paperid: <span id='pid'>18, <a href='https://arxiv.org/pdf/2512.11321.pdf' target='_blank'>https://arxiv.org/pdf/2512.11321.pdf</a></span>   <span><a href='https://github.com/wjc12345123/KeyframeFace' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingchao Wu, Zejian Kang, Haibo Liu, Yuanchen Fei, Xiangru Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.11321">KeyframeFace: From Text to Expressive Facial Keyframes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating dynamic 3D facial animation from natural language requires understanding both temporally structured semantics and fine-grained expression changes. Existing datasets and methods mainly focus on speech-driven animation or unstructured expression sequences and therefore lack the semantic grounding and temporal structures needed for expressive human performance generation. In this work, we introduce KeyframeFace, a large-scale multimodal dataset designed for text-to-animation research through keyframe-level supervision. KeyframeFace provides 2,100 expressive scripts paired with monocular videos, per-frame ARKit coefficients, contextual backgrounds, complex emotions, manually defined keyframes, and multi-perspective annotations based on ARKit coefficients and images via Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs). Beyond the dataset, we propose the first text-to-animation framework that explicitly leverages LLM priors for interpretable facial motion synthesis. This design aligns the semantic understanding capabilities of LLMs with the interpretable structure of ARKit's coefficients, enabling high-fidelity expressive animation. KeyframeFace and our LLM-based framework together establish a new foundation for interpretable, keyframe-guided, and context-aware text-to-animation. Code and data are available at https://github.com/wjc12345123/KeyframeFace.
<div id='section'>Paperid: <span id='pid'>19, <a href='https://arxiv.org/pdf/2512.10730.pdf' target='_blank'>https://arxiv.org/pdf/2512.10730.pdf</a></span>   <span><a href='https://github.com/HumanMLLM/IRG-MotionLLM/tree/main' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuan-Ming Li, Qize Yang, Nan Lei, Shenghao Fu, Ling-An Zeng, Jian-Fang Hu, Xihan Wei, Wei-Shi Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.10730">IRG-MotionLLM: Interleaving Motion Generation, Assessment and Refinement for Text-to-Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in motion-aware large language models have shown remarkable promise for unifying motion understanding and generation tasks. However, these models typically treat understanding and generation separately, limiting the mutual benefits that could arise from interactive feedback between tasks. In this work, we reveal that motion assessment and refinement tasks act as crucial bridges to enable bidirectional knowledge flow between understanding and generation. Leveraging this insight, we propose Interleaved Reasoning for Motion Generation (IRMoGen), a novel paradigm that tightly couples motion generation with assessment and refinement through iterative text-motion dialogue. To realize this, we introduce IRG-MotionLLM, the first model that seamlessly interleaves motion generation, assessment, and refinement to improve generation performance. IRG-MotionLLM is developed progressively with a novel three-stage training scheme, initializing and subsequently enhancing native IRMoGen capabilities. To facilitate this development, we construct an automated data engine to synthesize interleaved reasoning annotations from existing text-motion datasets. Extensive experiments demonstrate that: (i) Assessment and refinement tasks significantly improve text-motion alignment; (ii) Interleaving motion generation, assessment, and refinement steps yields consistent performance gains across training stages; and (iii) IRG-MotionLLM clearly outperforms the baseline model and achieves advanced performance on standard text-to-motion generation benchmarks. Cross-evaluator testing further validates its effectiveness. Code & Data: https://github.com/HumanMLLM/IRG-MotionLLM/tree/main.
<div id='section'>Paperid: <span id='pid'>20, <a href='https://arxiv.org/pdf/2512.10284.pdf' target='_blank'>https://arxiv.org/pdf/2512.10284.pdf</a></span>   <span><a href='https://github.com/elainew728/motion-edit/' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/elainew728/motion-edit/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yixin Wan, Lei Ke, Wenhao Yu, Kai-Wei Chang, Dong Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.10284">MotionEdit: Benchmarking and Learning Motion-Centric Image Editing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce MotionEdit, a novel dataset for motion-centric image editing-the task of modifying subject actions and interactions while preserving identity, structure, and physical plausibility. Unlike existing image editing datasets that focus on static appearance changes or contain only sparse, low-quality motion edits, MotionEdit provides high-fidelity image pairs depicting realistic motion transformations extracted and verified from continuous videos. This new task is not only scientifically challenging but also practically significant, powering downstream applications such as frame-controlled video synthesis and animation. To evaluate model performance on the novel task, we introduce MotionEdit-Bench, a benchmark that challenges models on motion-centric edits and measures model performance with generative, discriminative, and preference-based metrics. Benchmark results reveal that motion editing remains highly challenging for existing state-of-the-art diffusion-based editing models. To address this gap, we propose MotionNFT (Motion-guided Negative-aware Fine Tuning), a post-training framework that computes motion alignment rewards based on how well the motion flow between input and model-edited images matches the ground-truth motion, guiding models toward accurate motion transformations. Extensive experiments on FLUX.1 Kontext and Qwen-Image-Edit show that MotionNFT consistently improves editing quality and motion fidelity of both base models on the motion editing task without sacrificing general editing ability, demonstrating its effectiveness. Our code is at https://github.com/elainew728/motion-edit/.
<div id='section'>Paperid: <span id='pid'>21, <a href='https://arxiv.org/pdf/2512.09270.pdf' target='_blank'>https://arxiv.org/pdf/2512.09270.pdf</a></span>   <span><a href='https://cmlab-korea.github.io/MoRel/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sangwoon Kwak, Weeyoung Kwon, Jun Young Jeong, Geonho Kim, Won-Sik Cheong, Jihyong Oh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.09270">MoRel: Long-Range Flicker-Free 4D Motion Modeling via Anchor Relay-based Bidirectional Blending with Hierarchical Densification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in 4D Gaussian Splatting (4DGS) have extended the high-speed rendering capability of 3D Gaussian Splatting (3DGS) into the temporal domain, enabling real-time rendering of dynamic scenes. However, one of the major remaining challenges lies in modeling long-range motion-contained dynamic videos, where a naive extension of existing methods leads to severe memory explosion, temporal flickering, and failure to handle appearing or disappearing occlusions over time. To address these challenges, we propose a novel 4DGS framework characterized by an Anchor Relay-based Bidirectional Blending (ARBB) mechanism, named MoRel, which enables temporally consistent and memory-efficient modeling of long-range dynamic scenes. Our method progressively constructs locally canonical anchor spaces at key-frame time index and models inter-frame deformations at the anchor level, enhancing temporal coherence. By learning bidirectional deformations between KfA and adaptively blending them through learnable opacity control, our approach mitigates temporal discontinuities and flickering artifacts. We further introduce a Feature-variance-guided Hierarchical Densification (FHD) scheme that effectively densifies KfA's while keeping rendering quality, based on an assigned level of feature-variance. To effectively evaluate our model's capability to handle real-world long-range 4D motion, we newly compose long-range 4D motion-contained dataset, called SelfCap$_{\text{LR}}$. It has larger average dynamic motion magnitude, captured at spatially wider spaces, compared to previous dynamic video datasets. Overall, our MoRel achieves temporally coherent and flicker-free long-range 4D reconstruction while maintaining bounded memory usage, demonstrating both scalability and efficiency in dynamic Gaussian-based representations.
<div id='section'>Paperid: <span id='pid'>22, <a href='https://arxiv.org/pdf/2512.08765.pdf' target='_blank'>https://arxiv.org/pdf/2512.08765.pdf</a></span>   <span><a href='https://github.com/ali-vilab/Wan-Move' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruihang Chu, Yefei He, Zhekai Chen, Shiwei Zhang, Xiaogang Xu, Bin Xia, Dingdong Wang, Hongwei Yi, Xihui Liu, Hengshuang Zhao, Yu Liu, Yingya Zhang, Yujiu Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.08765">Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present Wan-Move, a simple and scalable framework that brings motion control to video generative models. Existing motion-controllable methods typically suffer from coarse control granularity and limited scalability, leaving their outputs insufficient for practical use. We narrow this gap by achieving precise and high-quality motion control. Our core idea is to directly make the original condition features motion-aware for guiding video synthesis. To this end, we first represent object motions with dense point trajectories, allowing fine-grained control over the scene. We then project these trajectories into latent space and propagate the first frame's features along each trajectory, producing an aligned spatiotemporal feature map that tells how each scene element should move. This feature map serves as the updated latent condition, which is naturally integrated into the off-the-shelf image-to-video model, e.g., Wan-I2V-14B, as motion guidance without any architecture change. It removes the need for auxiliary motion encoders and makes fine-tuning base models easily scalable. Through scaled training, Wan-Move generates 5-second, 480p videos whose motion controllability rivals Kling 1.5 Pro's commercial Motion Brush, as indicated by user studies. To support comprehensive evaluation, we further design MoveBench, a rigorously curated benchmark featuring diverse content categories and hybrid-verified annotations. It is distinguished by larger data volume, longer video durations, and high-quality motion annotations. Extensive experiments on MoveBench and the public dataset consistently show Wan-Move's superior motion quality. Code, models, and benchmark data are made publicly available.
<div id='section'>Paperid: <span id='pid'>23, <a href='https://arxiv.org/pdf/2512.08500.pdf' target='_blank'>https://arxiv.org/pdf/2512.08500.pdf</a></span>   <span><a href='https://jiann-li.github.io/mimic2dm/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianan Li, Xiao Chen, Tao Huang, Tien-Tsin Wong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.08500">Learning to Control Physically-simulated 3D Characters via Generating and Mimicking 2D Motions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video data is more cost-effective than motion capture data for learning 3D character motion controllers, yet synthesizing realistic and diverse behaviors directly from videos remains challenging. Previous approaches typically rely on off-the-shelf motion reconstruction techniques to obtain 3D trajectories for physics-based imitation. These reconstruction methods struggle with generalizability, as they either require 3D training data (potentially scarce) or fail to produce physically plausible poses, hindering their application to challenging scenarios like human-object interaction (HOI) or non-human characters. We tackle this challenge by introducing Mimic2DM, a novel motion imitation framework that learns the control policy directly and solely from widely available 2D keypoint trajectories extracted from videos. By minimizing the reprojection error, we train a general single-view 2D motion tracking policy capable of following arbitrary 2D reference motions in physics simulation, using only 2D motion data. The policy, when trained on diverse 2D motions captured from different or slightly different viewpoints, can further acquire 3D motion tracking capabilities by aggregating multiple views. Moreover, we develop a transformer-based autoregressive 2D motion generator and integrate it into a hierarchical control framework, where the generator produces high-quality 2D reference trajectories to guide the tracking policy. We show that the proposed approach is versatile and can effectively learn to synthesize physically plausible and diverse motions across a range of domains, including dancing, soccer dribbling, and animal movements, without any reliance on explicit 3D motion data. Project Website: https://jiann-li.github.io/mimic2dm/
<div id='section'>Paperid: <span id='pid'>24, <a href='https://arxiv.org/pdf/2512.04694.pdf' target='_blank'>https://arxiv.org/pdf/2512.04694.pdf</a></span>   <span><a href='https://github.com/brsylmz23/TimesNet-Gen' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Baris Yilmaz, Bevan Deniz Cilgin, Erdem Akagündüz, Salih Tileylioglu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.04694">TimesNet-Gen: Deep Learning-based Site Specific Strong Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Effective earthquake risk reduction relies on accurate site-specific evaluations. This requires models that can represent the influence of local site conditions on ground motion characteristics. In this context, data driven approaches that learn site controlled signatures from recorded ground motions offer a promising direction. We address strong ground motion generation from time-domain accelerometer records and introduce the TimesNet-Gen, a time-domain conditional generator. The approach uses a station specific latent bottleneck. We evaluate generation by comparing HVSR curves and fundamental site-frequency $f_0$ distributions between real and generated records per station, and summarize station specificity with a score based on the $f_0$ distribution confusion matrices. TimesNet-Gen achieves strong station-wise alignment and compares favorably with a spectrogram-based conditional VAE baseline for site-specific strong motion synthesis. Our codes are available via https://github.com/brsylmz23/TimesNet-Gen.
<div id='section'>Paperid: <span id='pid'>25, <a href='https://arxiv.org/pdf/2512.03918.pdf' target='_blank'>https://arxiv.org/pdf/2512.03918.pdf</a></span>   <span><a href='https://carlyx.github.io/UniMo/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Youxin Pang, Yong Zhang, Ruizhi Shao, Xiang Deng, Feng Gao, Xu Xiaoming, Xiaoming Wei, Yebin Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.03918">UniMo: Unifying 2D Video and 3D Human Motion with an Autoregressive Framework</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose UniMo, an innovative autoregressive model for joint modeling of 2D human videos and 3D human motions within a unified framework, enabling simultaneous generation and understanding of these two modalities for the first time. Current methods predominantly focus on generating one modality given another as the condition or integrating either of them with other modalities such as text and audio. Unifying 2D videos and 3D motions for simultaneous optimization and generation remains largely unexplored, presenting significant challenges due to their substantial structural and distributional differences. Inspired by the LLM's ability to unify different modalities, our method models videos and 3D motions as a unified tokens sequence, utilizing separate embedding layers to mitigate distribution gaps. Additionally, we devise a sequence modeling strategy that integrates two distinct tasks within a single framework, proving the effectiveness of unified modeling. Moreover, to efficiently align with visual tokens and preserve 3D spatial information, we design a novel 3D motion tokenizer with a temporal expansion strategy, using a single VQ-VAE to produce quantized motion tokens. It features multiple expert decoders that handle body shapes, translation, global orientation, and body poses for reliable 3D motion reconstruction. Extensive experiments demonstrate that our method simultaneously generates corresponding videos and motions while performing accurate motion capture. This work taps into the capacity of LLMs to fuse diverse data types, paving the way for integrating human-centric information into existing models and potentially enabling multimodal, controllable joint modeling of humans, objects, and scenes.
<div id='section'>Paperid: <span id='pid'>26, <a href='https://arxiv.org/pdf/2512.03756.pdf' target='_blank'>https://arxiv.org/pdf/2512.03756.pdf</a></span>   <span><a href='https://github.com/KIT-MRT/future-motion' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Marlon Steiner, Royden Wagner, Ömer Sahin Tas, Christoph Stiller
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.03756">Prediction-Driven Motion Planning: Route Integration Strategies in Attention-Based Prediction Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Combining motion prediction and motion planning offers a promising framework for enhancing interactions between automated vehicles and other traffic participants. However, this introduces challenges in conditioning predictions on navigation goals and ensuring stable, kinematically feasible trajectories. Addressing the former challenge, this paper investigates the extension of attention-based motion prediction models with navigation information. By integrating the ego vehicle's intended route and goal pose into the model architecture, we bridge the gap between multi-agent motion prediction and goal-based motion planning. We propose and evaluate several architectural navigation integration strategies to our model on the nuPlan dataset. Our results demonstrate the potential of prediction-driven motion planning, highlighting how navigation information can enhance both prediction and planning tasks. Our implementation is at: https://github.com/KIT-MRT/future-motion.
<div id='section'>Paperid: <span id='pid'>27, <a href='https://arxiv.org/pdf/2512.03619.pdf' target='_blank'>https://arxiv.org/pdf/2512.03619.pdf</a></span>   <span><a href='https://cyberiada.github.io/LAMP/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Muhammed Burak Kizil, Enes Sanli, Niloy J. Mitra, Erkut Erdem, Aykut Erdem, Duygu Ceylan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.03619">LAMP: Language-Assisted Motion Planning for Controllable Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video generation has achieved remarkable progress in visual fidelity and controllability, enabling conditioning on text, layout, or motion. Among these, motion control - specifying object dynamics and camera trajectories - is essential for composing complex, cinematic scenes, yet existing interfaces remain limited. We introduce LAMP that leverages large language models (LLMs) as motion planners to translate natural language descriptions into explicit 3D trajectories for dynamic objects and (relatively defined) cameras. LAMP defines a motion domain-specific language (DSL), inspired by cinematography conventions. By harnessing program synthesis capabilities of LLMs, LAMP generates structured motion programs from natural language, which are deterministically mapped to 3D trajectories. We construct a large-scale procedural dataset pairing natural text descriptions with corresponding motion programs and 3D trajectories. Experiments demonstrate LAMP's improved performance in motion controllability and alignment with user intent compared to state-of-the-art alternatives establishing the first framework for generating both object and camera motions directly from natural language specifications. Code, models and data are available on our project page.
<div id='section'>Paperid: <span id='pid'>28, <a href='https://arxiv.org/pdf/2512.03520.pdf' target='_blank'>https://arxiv.org/pdf/2512.03520.pdf</a></span>   <span><a href='https://shandaai.github.io/FloodDiffusion/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiyi Cai, Yuhan Wu, Kunhang Li, You Zhou, Bo Zheng, Haiyang Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.03520">FloodDiffusion: Tailored Diffusion Forcing for Streaming Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present FloodDiffusion, a new framework for text-driven, streaming human motion generation. Given time-varying text prompts, FloodDiffusion generates text-aligned, seamless motion sequences with real-time latency. Unlike existing methods that rely on chunk-by-chunk or auto-regressive model with diffusion head, we adopt a diffusion forcing framework to model this time-series generation task under time-varying control events. We find that a straightforward implementation of vanilla diffusion forcing (as proposed for video models) fails to model real motion distributions. We demonstrate that to guarantee modeling the output distribution, the vanilla diffusion forcing must be tailored to: (i) train with a bi-directional attention instead of casual attention; (ii) implement a lower triangular time scheduler instead of a random one; (iii) utilize a continues time-varying way to introduce text conditioning. With these improvements, we demonstrate in the first time that the diffusion forcing-based framework achieves state-of-the-art performance on the streaming motion generation task, reaching an FID of 0.057 on the HumanML3D benchmark. Models, code, and weights are available. https://shandaai.github.io/FloodDiffusion/
<div id='section'>Paperid: <span id='pid'>29, <a href='https://arxiv.org/pdf/2512.03041.pdf' target='_blank'>https://arxiv.org/pdf/2512.03041.pdf</a></span>   <span><a href='https://qinghew.github.io/MultiShotMaster' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qinghe Wang, Xiaoyu Shi, Baolu Li, Weikang Bian, Quande Liu, Huchuan Lu, Xintao Wang, Pengfei Wan, Kun Gai, Xu Jia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.03041">MultiShotMaster: A Controllable Multi-Shot Video Generation Framework</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current video generation techniques excel at single-shot clips but struggle to produce narrative multi-shot videos, which require flexible shot arrangement, coherent narrative, and controllability beyond text prompts. To tackle these challenges, we propose MultiShotMaster, a framework for highly controllable multi-shot video generation. We extend a pretrained single-shot model by integrating two novel variants of RoPE. First, we introduce Multi-Shot Narrative RoPE, which applies explicit phase shift at shot transitions, enabling flexible shot arrangement while preserving the temporal narrative order. Second, we design Spatiotemporal Position-Aware RoPE to incorporate reference tokens and grounding signals, enabling spatiotemporal-grounded reference injection. In addition, to overcome data scarcity, we establish an automated data annotation pipeline to extract multi-shot videos, captions, cross-shot grounding signals and reference images. Our framework leverages the intrinsic architectural properties to support multi-shot video generation, featuring text-driven inter-shot consistency, customized subject with motion control, and background-driven customized scene. Both shot count and duration are flexibly configurable. Extensive experiments demonstrate the superior performance and outstanding controllability of our framework.
<div id='section'>Paperid: <span id='pid'>30, <a href='https://arxiv.org/pdf/2512.02492.pdf' target='_blank'>https://arxiv.org/pdf/2512.02492.pdf</a></span>   <span><a href='https://giantailab.github.io/YingVideo-MV/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiahui Chen, Weida Wang, Runhua Shi, Huan Yang, Chaofan Ding, Zihao Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.02492">YingVideo-MV: Music-Driven Multi-Stage Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While diffusion model for audio-driven avatar video generation have achieved notable process in synthesizing long sequences with natural audio-visual synchronization and identity consistency, the generation of music-performance videos with camera motions remains largely unexplored. We present YingVideo-MV, the first cascaded framework for music-driven long-video generation. Our approach integrates audio semantic analysis, an interpretable shot planning module (MV-Director), temporal-aware diffusion Transformer architectures, and long-sequence consistency modeling to enable automatic synthesis of high-quality music performance videos from audio signals. We construct a large-scale Music-in-the-Wild Dataset by collecting web data to support the achievement of diverse, high-quality results. Observing that existing long-video generation methods lack explicit camera motion control, we introduce a camera adapter module that embeds camera poses into latent noise. To enhance continulity between clips during long-sequence inference, we further propose a time-aware dynamic window range strategy that adaptively adjust denoising ranges based on audio embedding. Comprehensive benchmark tests demonstrate that YingVideo-MV achieves outstanding performance in generating coherent and expressive music videos, and enables precise music-motion-camera synchronization. More videos are available in our project page: https://giantailab.github.io/YingVideo-MV/ .
<div id='section'>Paperid: <span id='pid'>31, <a href='https://arxiv.org/pdf/2512.00832.pdf' target='_blank'>https://arxiv.org/pdf/2512.00832.pdf</a></span>   <span><a href='https://github.com/chengzhag/PanFlow' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/chengzhag/PanFlow' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Cheng Zhang, Hanwen Liang, Donny Y. Chen, Qianyi Wu, Konstantinos N. Plataniotis, Camilo Cruz Gambardella, Jianfei Cai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.00832">PanFlow: Decoupled Motion Control for Panoramic Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Panoramic video generation has attracted growing attention due to its applications in virtual reality and immersive media. However, existing methods lack explicit motion control and struggle to generate scenes with large and complex motions. We propose PanFlow, a novel approach that exploits the spherical nature of panoramas to decouple the highly dynamic camera rotation from the input optical flow condition, enabling more precise control over large and dynamic motions. We further introduce a spherical noise warping strategy to promote loop consistency in motion across panorama boundaries. To support effective training, we curate a large-scale, motion-rich panoramic video dataset with frame-level pose and flow annotations. We also showcase the effectiveness of our method in various applications, including motion transfer and video editing. Extensive experiments demonstrate that PanFlow significantly outperforms prior methods in motion fidelity, visual quality, and temporal coherence. Our code, dataset, and models are available at https://github.com/chengzhag/PanFlow.
<div id='section'>Paperid: <span id='pid'>32, <a href='https://arxiv.org/pdf/2511.20640.pdf' target='_blank'>https://arxiv.org/pdf/2511.20640.pdf</a></span>   <span><a href='https://ryanndagreat.github.io/MotionV2V' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ryan Burgert, Charles Herrmann, Forrester Cole, Michael S Ryoo, Neal Wadhwa, Andrey Voynov, Nataniel Ruiz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.20640">MotionV2V: Editing Motion in a Video</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While generative video models have achieved remarkable fidelity and consistency, applying these capabilities to video editing remains a complex challenge. Recent research has explored motion controllability as a means to enhance text-to-video generation or image animation; however, we identify precise motion control as a promising yet under-explored paradigm for editing existing videos. In this work, we propose modifying video motion by directly editing sparse trajectories extracted from the input. We term the deviation between input and output trajectories a "motion edit" and demonstrate that this representation, when coupled with a generative backbone, enables powerful video editing capabilities. To achieve this, we introduce a pipeline for generating "motion counterfactuals", video pairs that share identical content but distinct motion, and we fine-tune a motion-conditioned video diffusion architecture on this dataset. Our approach allows for edits that start at any timestamp and propagate naturally. In a four-way head-to-head user study, our model achieves over 65 percent preference against prior work. Please see our project page: https://ryanndagreat.github.io/MotionV2V
<div id='section'>Paperid: <span id='pid'>33, <a href='https://arxiv.org/pdf/2511.20446.pdf' target='_blank'>https://arxiv.org/pdf/2511.20446.pdf</a></span>   <span><a href='https://tlb-miss.github.io/hhoi/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jeonghyeon Na, Sangwon Baik, Inhee Lee, Junyoung Lee, Hanbyul Joo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.20446">Learning to Generate Human-Human-Object Interactions from Textual Descriptions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The way humans interact with each other, including interpersonal distances, spatial configuration, and motion, varies significantly across different situations. To enable machines to understand such complex, context-dependent behaviors, it is essential to model multiple people in relation to the surrounding scene context. In this paper, we present a novel research problem to model the correlations between two people engaged in a shared interaction involving an object. We refer to this formulation as Human-Human-Object Interactions (HHOIs). To overcome the lack of dedicated datasets for HHOIs, we present a newly captured HHOIs dataset and a method to synthesize HHOI data by leveraging image generative models. As an intermediary, we obtain individual human-object interaction (HOIs) and human-human interaction (HHIs) from the HHOIs, and with these data, we train an text-to-HOI and text-to-HHI model using score-based diffusion model. Finally, we present a unified generative framework that integrates the two individual model, capable of synthesizing complete HHOIs in a single advanced sampling process. Our method extends HHOI generation to multi-human settings, enabling interactions involving more than two individuals. Experimental results show that our method generates realistic HHOIs conditioned on textual descriptions, outperforming previous approaches that focus only on single-human HOIs. Furthermore, we introduce multi-human motion generation involving objects as an application of our framework.
<div id='section'>Paperid: <span id='pid'>34, <a href='https://arxiv.org/pdf/2511.20157.pdf' target='_blank'>https://arxiv.org/pdf/2511.20157.pdf</a></span>   <span><a href='https://pokerman8.github.io/SKEL-CF/' target='_blank'>  GitHub</a></span> <span><a href='https://pokerman8.github.io/SKEL-CF/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Da Li, Jiping Jin, Xuanlong Yu, Wei Liu, Xiaodong Cun, Kai Chen, Rui Fan, Jiangang Kong, Xi Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.20157">SKEL-CF: Coarse-to-Fine Biomechanical Skeleton and Surface Mesh Recovery</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Parametric 3D human models such as SMPL have driven significant advances in human pose and shape estimation, yet their simplified kinematics limit biomechanical realism. The recently proposed SKEL model addresses this limitation by re-rigging SMPL with an anatomically accurate skeleton. However, estimating SKEL parameters directly remains challenging due to limited training data, perspective ambiguities, and the inherent complexity of human articulation. We introduce SKEL-CF, a coarse-to-fine framework for SKEL parameter estimation. SKEL-CF employs a transformer-based encoder-decoder architecture, where the encoder predicts coarse camera and SKEL parameters, and the decoder progressively refines them in successive layers. To ensure anatomically consistent supervision, we convert the existing SMPL-based dataset 4DHuman into a SKEL-aligned version, 4DHuman-SKEL, providing high-quality training data for SKEL estimation. In addition, to mitigate depth and scale ambiguities, we explicitly incorporate camera modeling into the SKEL-CF pipeline and demonstrate its importance across diverse viewpoints. Extensive experiments validate the effectiveness of the proposed design. On the challenging MOYO dataset, SKEL-CF achieves 85.0 MPJPE / 51.4 PA-MPJPE, significantly outperforming the previous SKEL-based state-of-the-art HSMR (104.5 / 79.6). These results establish SKEL-CF as a scalable and anatomically faithful framework for human motion analysis, bridging the gap between computer vision and biomechanics. Our implementation is available on the project page: https://pokerman8.github.io/SKEL-CF/.
<div id='section'>Paperid: <span id='pid'>35, <a href='https://arxiv.org/pdf/2511.15179.pdf' target='_blank'>https://arxiv.org/pdf/2511.15179.pdf</a></span>   <span><a href='https://github.com/placerkyo/MMCM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kyotaro Tokoro, Hiromu Taketsugu, Norimichi Ukita
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.15179">MMCM: Multimodality-aware Metric using Clustering-based Modes for Probabilistic Human Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper proposes a novel metric for Human Motion Prediction (HMP). Since a single past sequence can lead to multiple possible futures, a probabilistic HMP method predicts such multiple motions. While a single motion predicted by a deterministic method is evaluated only with the difference from its ground truth motion, multiple predicted motions should also be evaluated based on their distribution. For this evaluation, this paper focuses on the following two criteria. \textbf{(a) Coverage}: motions should be distributed among multiple motion modes to cover diverse possibilities. \textbf{(b) Validity}: motions should be kinematically valid as future motions observable from a given past motion. However, existing metrics simply appreciate widely distributed motions even if these motions are observed in a single mode and kinematically invalid. To resolve these disadvantages, this paper proposes a Multimodality-aware Metric using Clustering-based Modes (MMCM). For (a) coverage, MMCM divides a motion space into several clusters, each of which is regarded as a mode. These modes are used to explicitly evaluate whether predicted motions are distributed among multiple modes. For (b) validity, MMCM identifies valid modes by collecting possible future motions from a motion dataset. Our experiments validate that our clustering yields sensible mode definitions and that MMCM accurately scores multimodal predictions. Code: https://github.com/placerkyo/MMCM
<div id='section'>Paperid: <span id='pid'>36, <a href='https://arxiv.org/pdf/2511.13105.pdf' target='_blank'>https://arxiv.org/pdf/2511.13105.pdf</a></span>   <span><a href='https://github.com/VisualScienceLab-KHU/PlugTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Seungjae Kim, SeungJoon Lee, MyeongAh Cho
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.13105">PlugTrack: Multi-Perceptive Motion Analysis for Adaptive Fusion in Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) predominantly follows the tracking-by-detection paradigm, where Kalman filters serve as the standard motion predictor due to computational efficiency but inherently fail on non-linear motion patterns. Conversely, recent data-driven motion predictors capture complex non-linear dynamics but suffer from limited domain generalization and computational overhead. Through extensive analysis, we reveal that even in datasets dominated by non-linear motion, Kalman filter outperforms data-driven predictors in up to 34\% of cases, demonstrating that real-world tracking scenarios inherently involve both linear and non-linear patterns. To leverage this complementarity, we propose PlugTrack, a novel framework that adaptively fuses Kalman filter and data-driven motion predictors through multi-perceptive motion understanding. Our approach employs multi-perceptive motion analysis to generate adaptive blending factors. PlugTrack achieves significant performance gains on MOT17/MOT20 and state-of-the-art on DanceTrack without modifying existing motion predictors. To the best of our knowledge, PlugTrack is the first framework to bridge classical and modern motion prediction paradigms through adaptive fusion in MOT.
<div id='section'>Paperid: <span id='pid'>37, <a href='https://arxiv.org/pdf/2511.09484.pdf' target='_blank'>https://arxiv.org/pdf/2511.09484.pdf</a></span>   <span><a href='https://jc-bao.github.io/spider-project/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chaoyi Pan, Changhao Wang, Haozhi Qi, Zixi Liu, Homanga Bharadhwaj, Akash Sharma, Tingfan Wu, Guanya Shi, Jitendra Malik, Francois Hogan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.09484">SPIDER: Scalable Physics-Informed Dexterous Retargeting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning dexterous and agile policy for humanoid and dexterous hand control requires large-scale demonstrations, but collecting robot-specific data is prohibitively expensive. In contrast, abundant human motion data is readily available from motion capture, videos, and virtual reality, which could help address the data scarcity problem. However, due to the embodiment gap and missing dynamic information like force and torque, these demonstrations cannot be directly executed on robots. To bridge this gap, we propose Scalable Physics-Informed DExterous Retargeting (SPIDER), a physics-based retargeting framework to transform and augment kinematic-only human demonstrations to dynamically feasible robot trajectories at scale. Our key insight is that human demonstrations should provide global task structure and objective, while large-scale physics-based sampling with curriculum-style virtual contact guidance should refine trajectories to ensure dynamical feasibility and correct contact sequences. SPIDER scales across diverse 9 humanoid/dexterous hand embodiments and 6 datasets, improving success rates by 18% compared to standard sampling, while being 10X faster than reinforcement learning (RL) baselines, and enabling the generation of a 2.4M frames dynamic-feasible robot dataset for policy learning. As a universal physics-based retargeting method, SPIDER can work with diverse quality data and generate diverse and high-quality data to enable efficient policy learning with methods like RL.
<div id='section'>Paperid: <span id='pid'>38, <a href='https://arxiv.org/pdf/2511.09147.pdf' target='_blank'>https://arxiv.org/pdf/2511.09147.pdf</a></span>   <span><a href='https://github.com/Jiayue-Yuan/PressTrack-HMR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiayue Yuan, Fangting Xie, Guangwen Ouyang, Changhai Ma, Ziyu Wu, Heyu Ding, Quan Wan, Yi Ke, Yuchen Wu, Xiaohui Cai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.09147">PressTrack-HMR: Pressure-Based Top-Down Multi-Person Global Human Mesh Recovery</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-person global human mesh recovery (HMR) is crucial for understanding crowd dynamics and interactions. Traditional vision-based HMR methods sometimes face limitations in real-world scenarios due to mutual occlusions, insufficient lighting, and privacy concerns. Human-floor tactile interactions offer an occlusion-free and privacy-friendly alternative for capturing human motion. Existing research indicates that pressure signals acquired from tactile mats can effectively estimate human pose in single-person scenarios. However, when multiple individuals walk randomly on the mat simultaneously, how to distinguish intermingled pressure signals generated by different persons and subsequently acquire individual temporal pressure data remains a pending challenge for extending pressure-based HMR to the multi-person situation. In this paper, we present \textbf{PressTrack-HMR}, a top-down pipeline that recovers multi-person global human meshes solely from pressure signals. This pipeline leverages a tracking-by-detection strategy to first identify and segment each individual's pressure signal from the raw pressure data, and subsequently performs HMR for each extracted individual signal. Furthermore, we build a multi-person interaction pressure dataset \textbf{MIP}, which facilitates further research into pressure-based human motion analysis in multi-person scenarios. Experimental results demonstrate that our method excels in multi-person HMR using pressure data, with 89.2 $mm$ MPJPE and 112.6 $mm$ WA-MPJPE$_{100}$, and these showcase the potential of tactile mats for ubiquitous, privacy-preserving multi-person action recognition. Our dataset & code are available at https://github.com/Jiayue-Yuan/PressTrack-HMR.
<div id='section'>Paperid: <span id='pid'>39, <a href='https://arxiv.org/pdf/2511.08633.pdf' target='_blank'>https://arxiv.org/pdf/2511.08633.pdf</a></span>   <span><a href='https://time-to-move.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Assaf Singer, Noam Rotstein, Amir Mann, Ron Kimmel, Or Litany
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.08633">Time-to-Move: Training-Free Motion Controlled Video Generation via Dual-Clock Denoising</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion-based video generation can create realistic videos, yet existing image- and text-based conditioning fails to offer precise motion control. Prior methods for motion-conditioned synthesis typically require model-specific fine-tuning, which is computationally expensive and restrictive. We introduce Time-to-Move (TTM), a training-free, plug-and-play framework for motion- and appearance-controlled video generation with image-to-video (I2V) diffusion models. Our key insight is to use crude reference animations obtained through user-friendly manipulations such as cut-and-drag or depth-based reprojection. Motivated by SDEdit's use of coarse layout cues for image editing, we treat the crude animations as coarse motion cues and adapt the mechanism to the video domain. We preserve appearance with image conditioning and introduce dual-clock denoising, a region-dependent strategy that enforces strong alignment in motion-specified regions while allowing flexibility elsewhere, balancing fidelity to user intent with natural dynamics. This lightweight modification of the sampling process incurs no additional training or runtime cost and is compatible with any backbone. Extensive experiments on object and camera motion benchmarks show that TTM matches or exceeds existing training-based baselines in realism and motion control. Beyond this, TTM introduces a unique capability: precise appearance control through pixel-level conditioning, exceeding the limits of text-only prompting. Visit our project page for video examples and code: https://time-to-move.github.io/.
<div id='section'>Paperid: <span id='pid'>40, <a href='https://arxiv.org/pdf/2511.08377.pdf' target='_blank'>https://arxiv.org/pdf/2511.08377.pdf</a></span>   <span><a href='https://github.com/namwob44/Psychic' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Michael Bowman, Xiaoli Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.08377">Human Motion Intent Inferencing in Teleoperation Through a SINDy Paradigm</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Intent inferencing in teleoperation has been instrumental in aligning operator goals and coordinating actions with robotic partners. However, current intent inference methods often ignore subtle motion that can be strong indicators for a sudden change in intent. Specifically, we aim to tackle 1) if we can detect sudden jumps in operator trajectories, 2) how we appropriately use these sudden jump motions to infer an operator's goal state, and 3) how to incorporate these discontinuous and continuous dynamics to infer operator motion. Our framework, called Psychic, models these small indicative motions through a jump-drift-diffusion stochastic differential equation to cover discontinuous and continuous dynamics. Kramers-Moyal (KM) coefficients allow us to detect jumps with a trajectory which we pair with a statistical outlier detection algorithm to nominate goal transitions. Through identifying jumps, we can perform early detection of existing goals and discover undefined goals in unstructured scenarios. Our framework then applies a Sparse Identification of Nonlinear Dynamics (SINDy) model using KM coefficients with the goal transitions as a control input to infer an operator's motion behavior in unstructured scenarios. We demonstrate Psychic can produce probabilistic reachability sets and compare our strategy to a negative log-likelihood model fit. We perform a retrospective study on 600 operator trajectories in a hands-free teleoperation task to evaluate the efficacy of our opensource package, Psychic, in both offline and online learning.
<div id='section'>Paperid: <span id='pid'>41, <a href='https://arxiv.org/pdf/2511.07820.pdf' target='_blank'>https://arxiv.org/pdf/2511.07820.pdf</a></span>   <span><a href='https://nvlabs.github.io/SONIC/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhengyi Luo, Ye Yuan, Tingwu Wang, Chenran Li, Sirui Chen, Fernando Castañeda, Zi-Ang Cao, Jiefeng Li, David Minor, Qingwei Ben, Xingye Da, Runyu Ding, Cyrus Hogg, Lina Song, Edy Lim, Eugene Jeong, Tairan He, Haoru Xue, Wenli Xiao, Zi Wang, Simon Yuen, Jan Kautz, Yan Chang, Umar Iqbal, Linxi "Jim" Fan, Yuke Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.07820">SONIC: Supersizing Motion Tracking for Natural Humanoid Whole-Body Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite the rise of billion-parameter foundation models trained across thousands of GPUs, similar scaling gains have not been shown for humanoid control. Current neural controllers for humanoids remain modest in size, target a limited behavior set, and are trained on a handful of GPUs over several days. We show that scaling up model capacity, data, and compute yields a generalist humanoid controller capable of creating natural and robust whole-body movements. Specifically, we posit motion tracking as a natural and scalable task for humanoid control, leverageing dense supervision from diverse motion-capture data to acquire human motion priors without manual reward engineering. We build a foundation model for motion tracking by scaling along three axes: network size (from 1.2M to 42M parameters), dataset volume (over 100M frames, 700 hours of high-quality motion data), and compute (9k GPU hours). Beyond demonstrating the benefits of scale, we show the practical utility of our model through two mechanisms: (1) a real-time universal kinematic planner that bridges motion tracking to downstream task execution, enabling natural and interactive control, and (2) a unified token space that supports various motion input interfaces, such as VR teleoperation devices, human videos, and vision-language-action (VLA) models, all using the same policy. Scaling motion tracking exhibits favorable properties: performance improves steadily with increased compute and data diversity, and learned representations generalize to unseen motions, establishing motion tracking at scale as a practical foundation for humanoid control.
<div id='section'>Paperid: <span id='pid'>42, <a href='https://arxiv.org/pdf/2511.07819.pdf' target='_blank'>https://arxiv.org/pdf/2511.07819.pdf</a></span>   <span><a href='https://github.com/jingyugong/SSOMotion' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Gong Jingyu, Tong Kunkun, Chen Zhuoran, Yuan Chuanhan, Chen Mingang, Zhang Zhizhong, Tan Xin, Xie Yuan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.07819">Human Motion Synthesis in 3D Scenes via Unified Scene Semantic Occupancy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion synthesis in 3D scenes relies heavily on scene comprehension, while current methods focus mainly on scene structure but ignore the semantic understanding. In this paper, we propose a human motion synthesis framework that take an unified Scene Semantic Occupancy (SSO) for scene representation, termed SSOMotion. We design a bi-directional tri-plane decomposition to derive a compact version of the SSO, and scene semantics are mapped to an unified feature space via CLIP encoding and shared linear dimensionality reduction. Such strategy can derive the fine-grained scene semantic structures while significantly reduce redundant computations. We further take these scene hints and movement direction derived from instructions for motion control via frame-wise scene query. Extensive experiments and ablation studies conducted on cluttered scenes using ShapeNet furniture, as well as scanned scenes from PROX and Replica datasets, demonstrate its cutting-edge performance while validating its effectiveness and generalization ability. Code will be publicly available at https://github.com/jingyugong/SSOMotion.
<div id='section'>Paperid: <span id='pid'>43, <a href='https://arxiv.org/pdf/2511.04192.pdf' target='_blank'>https://arxiv.org/pdf/2511.04192.pdf</a></span>   <span><a href='https://github.com/CHMimilanlan/AStF' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hanmo Chen, Chenghao Xu, Jiexi Yan, Cheng Deng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.04192">AStF: Motion Style Transfer via Adaptive Statistics Fusor</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion style transfer allows characters to appear less rigidity and more realism with specific style. Traditional arbitrary image style transfer typically process mean and variance which is proved effective. Meanwhile, similar methods have been adapted for motion style transfer. However, due to the fundamental differences between images and motion, relying on mean and variance is insufficient to fully capture the complex dynamic patterns and spatiotemporal coherence properties of motion data. Building upon this, our key insight is to bring two more coefficient, skewness and kurtosis, into the analysis of motion style. Specifically, we propose a novel Adaptive Statistics Fusor (AStF) which consists of Style Disentanglement Module (SDM) and High-Order Multi-Statistics Attention (HOS-Attn). We trained our AStF in conjunction with a Motion Consistency Regularization (MCR) discriminator. Experimental results show that, by providing a more comprehensive model of the spatiotemporal statistical patterns inherent in dynamic styles, our proposed AStF shows proficiency superiority in motion style transfers over state-of-the-arts. Our code and model are available at https://github.com/CHMimilanlan/AStF.
<div id='section'>Paperid: <span id='pid'>44, <a href='https://arxiv.org/pdf/2511.03168.pdf' target='_blank'>https://arxiv.org/pdf/2511.03168.pdf</a></span>   <span><a href='https://github.com/etigerstudio/uncle-causal-discovery' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tingzhu Bi, Yicheng Pan, Xinrui Jiang, Huize Sun, Meng Ma, Ping Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.03168">UnCLe: Towards Scalable Dynamic Causal Discovery in Non-linear Temporal Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Uncovering cause-effect relationships from observational time series is fundamental to understanding complex systems. While many methods infer static causal graphs, real-world systems often exhibit dynamic causality-where relationships evolve over time. Accurately capturing these temporal dynamics requires time-resolved causal graphs. We propose UnCLe, a novel deep learning method for scalable dynamic causal discovery. UnCLe employs a pair of Uncoupler and Recoupler networks to disentangle input time series into semantic representations and learns inter-variable dependencies via auto-regressive Dependency Matrices. It estimates dynamic causal influences by analyzing datapoint-wise prediction errors induced by temporal perturbations. Extensive experiments demonstrate that UnCLe not only outperforms state-of-the-art baselines on static causal discovery benchmarks but, more importantly, exhibits a unique capability to accurately capture and represent evolving temporal causality in both synthetic and real-world dynamic systems (e.g., human motion). UnCLe offers a promising approach for revealing the underlying, time-varying mechanisms of complex phenomena.
<div id='section'>Paperid: <span id='pid'>45, <a href='https://arxiv.org/pdf/2511.01768.pdf' target='_blank'>https://arxiv.org/pdf/2511.01768.pdf</a></span>   <span><a href='https://github.com/happinesslz/UniLION' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhe Liu, Jinghua Hou, Xiaoqing Ye, Jingdong Wang, Hengshuang Zhao, Xiang Bai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.01768">UniLION: Towards Unified Autonomous Driving Model with Linear Group RNNs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Although transformers have demonstrated remarkable capabilities across various domains, their quadratic attention mechanisms introduce significant computational overhead when processing long-sequence data. In this paper, we present a unified autonomous driving model, UniLION, which efficiently handles large-scale LiDAR point clouds, high-resolution multi-view images, and even temporal sequences based on the linear group RNN operator (i.e., performs linear RNN for grouped features). Remarkably, UniLION serves as a single versatile architecture that can seamlessly support multiple specialized variants (i.e., LiDAR-only, temporal LiDAR, multi-modal, and multi-modal temporal fusion configurations) without requiring explicit temporal or multi-modal fusion modules. Moreover, UniLION consistently delivers competitive and even state-of-the-art performance across a wide range of core tasks, including 3D perception (e.g., 3D object detection, 3D object tracking, 3D occupancy prediction, BEV map segmentation), prediction (e.g., motion prediction), and planning (e.g., end-to-end planning). This unified paradigm naturally simplifies the design of multi-modal and multi-task autonomous driving systems while maintaining superior performance. Ultimately, we hope UniLION offers a fresh perspective on the development of 3D foundation models in autonomous driving. Code is available at https://github.com/happinesslz/UniLION
<div id='section'>Paperid: <span id='pid'>46, <a href='https://arxiv.org/pdf/2511.01200.pdf' target='_blank'>https://arxiv.org/pdf/2511.01200.pdf</a></span>   <span><a href='https://mosa-web.github.io/MoSa-web' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mengyuan Liu, Sheng Yan, Yong Wang, Yingjie Li, Gui-Bin Bian, Hong Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.01200">MoSa: Motion Generation with Scalable Autoregressive Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce MoSa, a novel hierarchical motion generation framework for text-driven 3D human motion generation that enhances the Vector Quantization-guided Generative Transformers (VQ-GT) paradigm through a coarse-to-fine scalable generation process. In MoSa, we propose a Multi-scale Token Preservation Strategy (MTPS) integrated into a hierarchical residual vector quantization variational autoencoder (RQ-VAE). MTPS employs interpolation at each hierarchical quantization to effectively retain coarse-to-fine multi-scale tokens. With this, the generative transformer supports Scalable Autoregressive (SAR) modeling, which predicts scale tokens, unlike traditional methods that predict only one token at each step. Consequently, MoSa requires only 10 inference steps, matching the number of RQ-VAE quantization layers. To address potential reconstruction degradation from frequent interpolation, we propose CAQ-VAE, a lightweight yet expressive convolution-attention hybrid VQ-VAE. CAQ-VAE enhances residual block design and incorporates attention mechanisms to better capture global dependencies. Extensive experiments show that MoSa achieves state-of-the-art generation quality and efficiency, outperforming prior methods in both fidelity and speed. On the Motion-X dataset, MoSa achieves an FID of 0.06 (versus MoMask's 0.20) while reducing inference time by 27 percent. Moreover, MoSa generalizes well to downstream tasks such as motion editing, requiring no additional fine-tuning. The code is available at https://mosa-web.github.io/MoSa-web
<div id='section'>Paperid: <span id='pid'>47, <a href='https://arxiv.org/pdf/2510.26786.pdf' target='_blank'>https://arxiv.org/pdf/2510.26786.pdf</a></span>   <span><a href='https://github.com/princeton-computational-imaging/HEIR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Cheng Zheng, William Koch, Baiang Li, Felix Heide
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.26786">HEIR: Learning Graph-Based Motion Hierarchies</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Hierarchical structures of motion exist across research fields, including computer vision, graphics, and robotics, where complex dynamics typically arise from coordinated interactions among simpler motion components. Existing methods to model such dynamics typically rely on manually-defined or heuristic hierarchies with fixed motion primitives, limiting their generalizability across different tasks. In this work, we propose a general hierarchical motion modeling method that learns structured, interpretable motion relationships directly from data. Our method represents observed motions using graph-based hierarchies, explicitly decomposing global absolute motions into parent-inherited patterns and local motion residuals. We formulate hierarchy inference as a differentiable graph learning problem, where vertices represent elemental motions and directed edges capture learned parent-child dependencies through graph neural networks. We evaluate our hierarchical reconstruction approach on three examples: 1D translational motion, 2D rotational motion, and dynamic 3D scene deformation via Gaussian splatting. Experimental results show that our method reconstructs the intrinsic motion hierarchy in 1D and 2D cases, and produces more realistic and interpretable deformations compared to the baseline on dynamic 3D Gaussian splatting scenes. By providing an adaptable, data-driven hierarchical modeling paradigm, our method offers a formulation applicable to a broad range of motion-centric tasks. Project Page: https://light.princeton.edu/HEIR/
<div id='section'>Paperid: <span id='pid'>48, <a href='https://arxiv.org/pdf/2510.26236.pdf' target='_blank'>https://arxiv.org/pdf/2510.26236.pdf</a></span>   <span><a href='https://davian-robotics.github.io/PHUMA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kyungmin Lee, Sibeen Kim, Minho Park, Hyunseung Kim, Dongyoon Hwang, Hojoon Lee, Jaegul Choo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.26236">PHUMA: Physically-Grounded Humanoid Locomotion Dataset</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion imitation is a promising approach for humanoid locomotion, enabling agents to acquire humanlike behaviors. Existing methods typically rely on high-quality motion capture datasets such as AMASS, but these are scarce and expensive, limiting scalability and diversity. Recent studies attempt to scale data collection by converting large-scale internet videos, exemplified by Humanoid-X. However, they often introduce physical artifacts such as floating, penetration, and foot skating, which hinder stable imitation. In response, we introduce PHUMA, a Physically-grounded HUMAnoid locomotion dataset that leverages human video at scale, while addressing physical artifacts through careful data curation and physics-constrained retargeting. PHUMA enforces joint limits, ensures ground contact, and eliminates foot skating, producing motions that are both large-scale and physically reliable. We evaluated PHUMA in two sets of conditions: (i) imitation of unseen motion from self-recorded test videos and (ii) path following with pelvis-only guidance. In both cases, PHUMA-trained policies outperform Humanoid-X and AMASS, achieving significant gains in imitating diverse motions. The code is available at https://davian-robotics.github.io/PHUMA.
<div id='section'>Paperid: <span id='pid'>49, <a href='https://arxiv.org/pdf/2510.23007.pdf' target='_blank'>https://arxiv.org/pdf/2510.23007.pdf</a></span>   <span><a href='https://como6.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Youcan Xu, Zhen Wang, Jiaxin Shi, Kexin Li, Feifei Shao, Jun Xiao, Yi Yang, Jun Yu, Long Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.23007">CoMo: Compositional Motion Customization for Text-to-Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While recent text-to-video models excel at generating diverse scenes, they struggle with precise motion control, particularly for complex, multi-subject motions. Although methods for single-motion customization have been developed to address this gap, they fail in compositional scenarios due to two primary challenges: motion-appearance entanglement and ineffective multi-motion blending. This paper introduces CoMo, a novel framework for $\textbf{compositional motion customization}$ in text-to-video generation, enabling the synthesis of multiple, distinct motions within a single video. CoMo addresses these issues through a two-phase approach. First, in the single-motion learning phase, a static-dynamic decoupled tuning paradigm disentangles motion from appearance to learn a motion-specific module. Second, in the multi-motion composition phase, a plug-and-play divide-and-merge strategy composes these learned motions without additional training by spatially isolating their influence during the denoising process. To facilitate research in this new domain, we also introduce a new benchmark and a novel evaluation metric designed to assess multi-motion fidelity and blending. Extensive experiments demonstrate that CoMo achieves state-of-the-art performance, significantly advancing the capabilities of controllable video generation. Our project page is at https://como6.github.io/.
<div id='section'>Paperid: <span id='pid'>50, <a href='https://arxiv.org/pdf/2510.21654.pdf' target='_blank'>https://arxiv.org/pdf/2510.21654.pdf</a></span>   <span><a href='https://github.com/eth-siplab/GroupInertialPoser' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ying Xue, Jiaxi Jiang, Rayan Armani, Dominik Hollidt, Yi-Chi Liao, Christian Holz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.21654">Group Inertial Poser: Multi-Person Pose and Global Translation from Sparse Inertial Sensors and Ultra-Wideband Ranging</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tracking human full-body motion using sparse wearable inertial measurement units (IMUs) overcomes the limitations of occlusion and instrumentation of the environment inherent in vision-based approaches. However, purely IMU-based tracking compromises translation estimates and accurate relative positioning between individuals, as inertial cues are inherently self-referential and provide no direct spatial reference for others. In this paper, we present a novel approach for robustly estimating body poses and global translation for multiple individuals by leveraging the distances between sparse wearable sensors - both on each individual and across multiple individuals. Our method Group Inertial Poser estimates these absolute distances between pairs of sensors from ultra-wideband ranging (UWB) and fuses them with inertial observations as input into structured state-space models to integrate temporal motion patterns for precise 3D pose estimation. Our novel two-step optimization further leverages the estimated distances for accurately tracking people's global trajectories through the world. We also introduce GIP-DB, the first IMU+UWB dataset for two-person tracking, which comprises 200 minutes of motion recordings from 14 participants. In our evaluation, Group Inertial Poser outperforms previous state-of-the-art methods in accuracy and robustness across synthetic and real-world data, showing the promise of IMU+UWB-based multi-human motion capture in the wild. Code, models, dataset: https://github.com/eth-siplab/GroupInertialPoser
<div id='section'>Paperid: <span id='pid'>51, <a href='https://arxiv.org/pdf/2510.19170.pdf' target='_blank'>https://arxiv.org/pdf/2510.19170.pdf</a></span>   <span><a href='https://github.com/keatonkraiger/Vision-to-Stability.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Keaton Kraiger, Jingjing Li, Skanda Bharadwaj, Jesse Scott, Robert T. Collins, Yanxi Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.19170">FootFormer: Estimating Stability from Visual Input</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose FootFormer, a cross-modality approach for jointly predicting human motion dynamics directly from visual input. On multiple datasets, FootFormer achieves statistically significantly better or equivalent estimates of foot pressure distributions, foot contact maps, and center of mass (CoM), as compared with existing methods that generate one or two of those measures. Furthermore, FootFormer achieves SOTA performance in estimating stability-predictive components (CoP, CoM, BoS) used in classic kinesiology metrics. Code and data are available at https://github.com/keatonkraiger/Vision-to-Stability.git.
<div id='section'>Paperid: <span id='pid'>52, <a href='https://arxiv.org/pdf/2510.18705.pdf' target='_blank'>https://arxiv.org/pdf/2510.18705.pdf</a></span>   <span><a href='https://github.com/PeiqinZhuang/EMIM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Peiqin Zhuang, Lei Bai, Yichao Wu, Ding Liang, Luping Zhou, Yali Wang, Wanli Ouyang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.18705">A Renaissance of Explicit Motion Information Mining from Transformers for Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, action recognition has been dominated by transformer-based methods, thanks to their spatiotemporal contextual aggregation capacities. However, despite the significant progress achieved on scene-related datasets, they do not perform well on motion-sensitive datasets due to the lack of elaborate motion modeling designs. Meanwhile, we observe that the widely-used cost volume in traditional action recognition is highly similar to the affinity matrix defined in self-attention, but equipped with powerful motion modeling capacities. In light of this, we propose to integrate those effective motion modeling properties into the existing transformer in a unified and neat way, with the proposal of the Explicit Motion Information Mining module (EMIM). In EMIM, we propose to construct the desirable affinity matrix in a cost volume style, where the set of key candidate tokens is sampled from the query-based neighboring area in the next frame in a sliding-window manner. Then, the constructed affinity matrix is used to aggregate contextual information for appearance modeling and is converted into motion features for motion modeling as well. We validate the motion modeling capacities of our method on four widely-used datasets, and our method performs better than existing state-of-the-art approaches, especially on motion-sensitive datasets, i.e., Something-Something V1 & V2. Our project is available at https://github.com/PeiqinZhuang/EMIM .
<div id='section'>Paperid: <span id='pid'>53, <a href='https://arxiv.org/pdf/2510.17101.pdf' target='_blank'>https://arxiv.org/pdf/2510.17101.pdf</a></span>   <span><a href='https://github.com/yinlu5942/SAIP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lu Yin, Ziying Shi, Yinghao Wu, Xinyu Yi, Feng Xu, Shihui Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.17101">Shape-aware Inertial Poser: Motion Tracking for Humans with Diverse Shapes Using Sparse Inertial Sensors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion capture with sparse inertial sensors has gained significant attention recently. However, existing methods almost exclusively rely on a template adult body shape to model the training data, which poses challenges when generalizing to individuals with largely different body shapes (such as a child). This is primarily due to the variation in IMU-measured acceleration caused by changes in body shape. To fill this gap, we propose Shape-aware Inertial Poser (SAIP), the first solution considering body shape differences in sparse inertial-based motion capture. Specifically, we decompose the sensor measurements related to shape and pose in order to effectively model their joint correlations. Firstly, we train a regression model to transfer the IMU-measured accelerations of a real body to match the template adult body model, compensating for the shape-related sensor measurements. Then, we can easily follow the state-of-the-art methods to estimate the full body motions of the template-shaped body. Finally, we utilize a second regression model to map the joint velocities back to the real body, combined with a shape-aware physical optimization strategy to calculate global motions on the subject. Furthermore, our method relies on body shape awareness, introducing the first inertial shape estimation scheme. This is accomplished by modeling the shape-conditioned IMU-pose correlation using an MLP-based network. To validate the effectiveness of SAIP, we also present the first IMU motion capture dataset containing individuals of different body sizes. This dataset features 10 children and 10 adults, with heights ranging from 110 cm to 190 cm, and a total of 400 minutes of paired IMU-Motion samples. Extensive experimental results demonstrate that SAIP can effectively handle motion capture tasks for diverse body shapes. The code and dataset are available at https://github.com/yinlu5942/SAIP.
<div id='section'>Paperid: <span id='pid'>54, <a href='https://arxiv.org/pdf/2510.14955.pdf' target='_blank'>https://arxiv.org/pdf/2510.14955.pdf</a></span>   <span><a href='https://vchitect.github.io/RealDPO-Project/' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/Vchitect/RealDPO' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Guo Cheng, Danni Yang, Ziqi Huang, Jianlou Si, Chenyang Si, Ziwei Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.14955">RealDPO: Real or Not Real, that is the Preference</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video generative models have recently achieved notable advancements in synthesis quality. However, generating complex motions remains a critical challenge, as existing models often struggle to produce natural, smooth, and contextually consistent movements. This gap between generated and real-world motions limits their practical applicability. To address this issue, we introduce RealDPO, a novel alignment paradigm that leverages real-world data as positive samples for preference learning, enabling more accurate motion synthesis. Unlike traditional supervised fine-tuning (SFT), which offers limited corrective feedback, RealDPO employs Direct Preference Optimization (DPO) with a tailored loss function to enhance motion realism. By contrasting real-world videos with erroneous model outputs, RealDPO enables iterative self-correction, progressively refining motion quality. To support post-training in complex motion synthesis, we propose RealAction-5K, a curated dataset of high-quality videos capturing human daily activities with rich and precise motion details. Extensive experiments demonstrate that RealDPO significantly improves video quality, text alignment, and motion realism compared to state-of-the-art models and existing preference optimization techniques.
<div id='section'>Paperid: <span id='pid'>55, <a href='https://arxiv.org/pdf/2510.13794.pdf' target='_blank'>https://arxiv.org/pdf/2510.13794.pdf</a></span>   <span><a href='https://github.com/xbpeng/MimicKit' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xue Bin Peng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.13794">MimicKit: A Reinforcement Learning Framework for Motion Imitation and Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>MimicKit is an open-source framework for training motion controllers using motion imitation and reinforcement learning. The codebase provides implementations of commonly-used motion-imitation techniques and RL algorithms. This framework is intended to support research and applications in computer graphics and robotics by providing a unified training framework, along with standardized environment, agent, and data structures. The codebase is designed to be modular and easily configurable, enabling convenient modification and extension to new characters and tasks. The open-source codebase is available at: https://github.com/xbpeng/MimicKit.
<div id='section'>Paperid: <span id='pid'>56, <a href='https://arxiv.org/pdf/2510.13244.pdf' target='_blank'>https://arxiv.org/pdf/2510.13244.pdf</a></span>   <span><a href='https://motionbeat2025.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuanchen Wang, Heng Wang, Weidong Cai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.13244">MotionBeat: Motion-Aligned Music Representation via Embodied Contrastive Learning and Bar-Equivariant Contact-Aware Encoding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Music is both an auditory and an embodied phenomenon, closely linked to human motion and naturally expressed through dance. However, most existing audio representations neglect this embodied dimension, limiting their ability to capture rhythmic and structural cues that drive movement. We propose MotionBeat, a framework for motion-aligned music representation learning. MotionBeat is trained with two newly proposed objectives: the Embodied Contrastive Loss (ECL), an enhanced InfoNCE formulation with tempo-aware and beat-jitter negatives to achieve fine-grained rhythmic discrimination, and the Structural Rhythm Alignment Loss (SRAL), which ensures rhythm consistency by aligning music accents with corresponding motion events. Architecturally, MotionBeat introduces bar-equivariant phase rotations to capture cyclic rhythmic patterns and contact-guided attention to emphasize motion events synchronized with musical accents. Experiments show that MotionBeat outperforms state-of-the-art audio encoders in music-to-dance generation and transfers effectively to beat tracking, music tagging, genre and instrument classification, emotion recognition, and audio-visual retrieval. Our project demo page: https://motionbeat2025.github.io/.
<div id='section'>Paperid: <span id='pid'>57, <a href='https://arxiv.org/pdf/2510.12573.pdf' target='_blank'>https://arxiv.org/pdf/2510.12573.pdf</a></span>   <span><a href='https://zquang2202.github.io/TCM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Quang Nguyen, Tri Le, Baoru Huang, Minh Nhat Vu, Ngan Le, Thieu Vo, Anh Nguyen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.12573">Learning Human Motion with Temporally Conditional Mamba</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning human motion based on a time-dependent input signal presents a challenging yet impactful task with various applications. The goal of this task is to generate or estimate human movement that consistently reflects the temporal patterns of conditioning inputs. Existing methods typically rely on cross-attention mechanisms to fuse the condition with motion. However, this approach primarily captures global interactions and struggles to maintain step-by-step temporal alignment. To address this limitation, we introduce Temporally Conditional Mamba, a new mamba-based model for human motion generation. Our approach integrates conditional information into the recurrent dynamics of the Mamba block, enabling better temporally aligned motion. To validate the effectiveness of our method, we evaluate it on a variety of human motion tasks. Extensive experiments demonstrate that our model significantly improves temporal alignment, motion realism, and condition consistency over state-of-the-art approaches. Our project page is available at https://zquang2202.github.io/TCM.
<div id='section'>Paperid: <span id='pid'>58, <a href='https://arxiv.org/pdf/2510.12419.pdf' target='_blank'>https://arxiv.org/pdf/2510.12419.pdf</a></span>   <span><a href='https://ssk-yoshimura.github.io/M3D-skin/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shunnosuke Yoshimura, Kento Kawaharazuka, Kei Okada
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.12419">M3D-skin: Multi-material 3D-printed Tactile Sensor with Hierarchical Infill Structures for Pressure Sensing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tactile sensors have a wide range of applications, from utilization in robotic grippers to human motion measurement. If tactile sensors could be fabricated and integrated more easily, their applicability would further expand. In this study, we propose a tactile sensor-M3D-skin-that can be easily fabricated with high versatility by leveraging the infill patterns of a multi-material fused deposition modeling (FDM) 3D printer as the sensing principle. This method employs conductive and non-conductive flexible filaments to create a hierarchical structure with a specific infill pattern. The flexible hierarchical structure deforms under pressure, leading to a change in electrical resistance, enabling the acquisition of tactile information. We measure the changes in characteristics of the proposed tactile sensor caused by modifications to the hierarchical structure. Additionally, we demonstrate the fabrication and use of a multi-tile sensor. Furthermore, as applications, we implement motion pattern measurement on the sole of a foot, integration with a robotic hand, and tactile-based robotic operations. Through these experiments, we validate the effectiveness of the proposed tactile sensor.
<div id='section'>Paperid: <span id='pid'>59, <a href='https://arxiv.org/pdf/2510.08131.pdf' target='_blank'>https://arxiv.org/pdf/2510.08131.pdf</a></span>   <span><a href='https://kesenzhao.github.io/AR-Drag.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kesen Zhao, Jiaxin Shi, Beier Zhu, Junbao Zhou, Xiaolong Shen, Yuan Zhou, Qianru Sun, Hanwang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.08131">Real-Time Motion-Controllable Autoregressive Video Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Real-time motion-controllable video generation remains challenging due to the inherent latency of bidirectional diffusion models and the lack of effective autoregressive (AR) approaches. Existing AR video diffusion models are limited to simple control signals or text-to-video generation, and often suffer from quality degradation and motion artifacts in few-step generation. To address these challenges, we propose AR-Drag, the first RL-enhanced few-step AR video diffusion model for real-time image-to-video generation with diverse motion control. We first fine-tune a base I2V model to support basic motion control, then further improve it via reinforcement learning with a trajectory-based reward model. Our design preserves the Markov property through a Self-Rollout mechanism and accelerates training by selectively introducing stochasticity in denoising steps. Extensive experiments demonstrate that AR-Drag achieves high visual fidelity and precise motion alignment, significantly reducing latency compared with state-of-the-art motion-controllable VDMs, while using only 1.3B parameters. Additional visualizations can be found on our project page: https://kesenzhao.github.io/AR-Drag.github.io/.
<div id='section'>Paperid: <span id='pid'>60, <a href='https://arxiv.org/pdf/2510.06219.pdf' target='_blank'>https://arxiv.org/pdf/2510.06219.pdf</a></span>   <span><a href='https://github.com/fanegg/Human3R' target='_blank'>  GitHub</a></span> <span><a href='https://fanegg.github.io/Human3R' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yue Chen, Xingyu Chen, Yuxuan Xue, Anpei Chen, Yuliang Xiu, Gerard Pons-Moll
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.06219">Human3R: Everyone Everywhere All at Once</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present Human3R, a unified, feed-forward framework for online 4D human-scene reconstruction, in the world frame, from casually captured monocular videos. Unlike previous approaches that rely on multi-stage pipelines, iterative contact-aware refinement between humans and scenes, and heavy dependencies, e.g., human detection, depth estimation, and SLAM pre-processing, Human3R jointly recovers global multi-person SMPL-X bodies ("everyone"), dense 3D scene ("everywhere"), and camera trajectories in a single forward pass ("all-at-once"). Our method builds upon the 4D online reconstruction model CUT3R, and uses parameter-efficient visual prompt tuning, to strive to preserve CUT3R's rich spatiotemporal priors, while enabling direct readout of multiple SMPL-X bodies. Human3R is a unified model that eliminates heavy dependencies and iterative refinement. After being trained on the relatively small-scale synthetic dataset BEDLAM for just one day on one GPU, it achieves superior performance with remarkable efficiency: it reconstructs multiple humans in a one-shot manner, along with 3D scenes, in one stage, at real-time speed (15 FPS) with a low memory footprint (8 GB). Extensive experiments demonstrate that Human3R delivers state-of-the-art or competitive performance across tasks, including global human motion estimation, local human mesh recovery, video depth estimation, and camera pose estimation, with a single unified model. We hope that Human3R will serve as a simple yet strong baseline, be easily extended for downstream applications.Code available in https://fanegg.github.io/Human3R
<div id='section'>Paperid: <span id='pid'>61, <a href='https://arxiv.org/pdf/2510.05097.pdf' target='_blank'>https://arxiv.org/pdf/2510.05097.pdf</a></span>   <span><a href='https://www.lix.polytechnique.fr/vista/projects/2025_pulpmotion_courant/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Robin Courant, Xi Wang, David Loiseaux, Marc Christie, Vicky Kalogeiton
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.05097">Pulp Motion: Framing-aware multimodal camera and human motion generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Treating human motion and camera trajectory generation separately overlooks a core principle of cinematography: the tight interplay between actor performance and camera work in the screen space. In this paper, we are the first to cast this task as a text-conditioned joint generation, aiming to maintain consistent on-screen framing while producing two heterogeneous, yet intrinsically linked, modalities: human motion and camera trajectories. We propose a simple, model-agnostic framework that enforces multimodal coherence via an auxiliary modality: the on-screen framing induced by projecting human joints onto the camera. This on-screen framing provides a natural and effective bridge between modalities, promoting consistency and leading to more precise joint distribution. We first design a joint autoencoder that learns a shared latent space, together with a lightweight linear transform from the human and camera latents to a framing latent. We then introduce auxiliary sampling, which exploits this linear transform to steer generation toward a coherent framing modality. To support this task, we also introduce the PulpMotion dataset, a human-motion and camera-trajectory dataset with rich captions, and high-quality human motions. Extensive experiments across DiT- and MAR-based architectures show the generality and effectiveness of our method in generating on-frame coherent human-camera motions, while also achieving gains on textual alignment for both modalities. Our qualitative results yield more cinematographically meaningful framings setting the new state of the art for this task. Code, models and data are available in our \href{https://www.lix.polytechnique.fr/vista/projects/2025_pulpmotion_courant/}{project page}.
<div id='section'>Paperid: <span id='pid'>62, <a href='https://arxiv.org/pdf/2510.03909.pdf' target='_blank'>https://arxiv.org/pdf/2510.03909.pdf</a></span>   <span><a href='https://hyelinnam.github.io/Cameo/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hyelin Nam, Hyojun Go, Byeongjun Park, Byung-Hoon Kim, Hyungjin Chung
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03909">Generating Human Motion Videos using a Cascaded Text-to-Video Framework</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human video generation is becoming an increasingly important task with broad applications in graphics, entertainment, and embodied AI. Despite the rapid progress of video diffusion models (VDMs), their use for general-purpose human video generation remains underexplored, with most works constrained to image-to-video setups or narrow domains like dance videos. In this work, we propose CAMEO, a cascaded framework for general human motion video generation. It seamlessly bridges Text-to-Motion (T2M) models and conditional VDMs, mitigating suboptimal factors that may arise in this process across both training and inference through carefully designed components. Specifically, we analyze and prepare both textual prompts and visual conditions to effectively train the VDM, ensuring robust alignment between motion descriptions, conditioning signals, and the generated videos. Furthermore, we introduce a camera-aware conditioning module that connects the two stages, automatically selecting viewpoints aligned with the input text to enhance coherence and reduce manual intervention. We demonstrate the effectiveness of our approach on both the MovieGen benchmark and a newly introduced benchmark tailored to the T2M-VDM combination, while highlighting its versatility across diverse use cases.
<div id='section'>Paperid: <span id='pid'>63, <a href='https://arxiv.org/pdf/2510.03031.pdf' target='_blank'>https://arxiv.org/pdf/2510.03031.pdf</a></span>   <span><a href='https://github.com/test-bai-cpu/LHMP-with-MoDs.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yufei Zhu, Andrey Rudenko, Tomasz P. Kucner, Achim J. Lilienthal, Martin Magnusson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03031">Long-Term Human Motion Prediction Using Spatio-Temporal Maps of Dynamics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Long-term human motion prediction (LHMP) is important for the safe and efficient operation of autonomous robots and vehicles in environments shared with humans. Accurate predictions are important for applications including motion planning, tracking, human-robot interaction, and safety monitoring. In this paper, we exploit Maps of Dynamics (MoDs), which encode spatial or spatio-temporal motion patterns as environment features, to achieve LHMP for horizons of up to 60 seconds. We propose an MoD-informed LHMP framework that supports various types of MoDs and includes a ranking method to output the most likely predicted trajectory, improving practical utility in robotics. Further, a time-conditioned MoD is introduced to capture motion patterns that vary across different times of day. We evaluate MoD-LHMP instantiated with three types of MoDs. Experiments on two real-world datasets show that MoD-informed method outperforms learning-based ones, with up to 50\% improvement in average displacement error, and the time-conditioned variant achieves the highest accuracy overall. Project code is available at https://github.com/test-bai-cpu/LHMP-with-MoDs.git
<div id='section'>Paperid: <span id='pid'>64, <a href='https://arxiv.org/pdf/2510.02722.pdf' target='_blank'>https://arxiv.org/pdf/2510.02722.pdf</a></span>   <span><a href='https://github.com/JunyuShi02/MoGIC' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Junyu Shi, Yong Sun, Zhiyuan Zhang, Lijiang Liu, Zhengjie Zhang, Yuxin He, Qiang Nie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.02722">MoGIC: Boosting Motion Generation via Intention Understanding and Visual Context</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing text-driven motion generation methods often treat synthesis as a bidirectional mapping between language and motion, but remain limited in capturing the causal logic of action execution and the human intentions that drive behavior. The absence of visual grounding further restricts precision and personalization, as language alone cannot specify fine-grained spatiotemporal details. We propose MoGIC, a unified framework that integrates intention modeling and visual priors into multimodal motion synthesis. By jointly optimizing multimodal-conditioned motion generation and intention prediction, MoGIC uncovers latent human goals, leverages visual priors to enhance generation, and exhibits versatile multimodal generative capability. We further introduce a mixture-of-attention mechanism with adaptive scope to enable effective local alignment between conditional tokens and motion subsequences. To support this paradigm, we curate Mo440H, a 440-hour benchmark from 21 high-quality motion datasets. Experiments show that after finetuning, MoGIC reduces FID by 38.6\% on HumanML3D and 34.6\% on Mo440H, surpasses LLM-based methods in motion captioning with a lightweight text head, and further enables intention prediction and vision-conditioned generation, advancing controllable motion synthesis and intention understanding. The code is available at https://github.com/JunyuShi02/MoGIC
<div id='section'>Paperid: <span id='pid'>65, <a href='https://arxiv.org/pdf/2510.02469.pdf' target='_blank'>https://arxiv.org/pdf/2510.02469.pdf</a></span>   <span><a href='https://sungyeonparkk.github.io/simsplat/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sung-Yeon Park, Adam Lee, Juanwu Lu, Can Cui, Luyang Jiang, Rohit Gupta, Kyungtae Han, Ahmadreza Moradipari, Ziran Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.02469">SIMSplat: Predictive Driving Scene Editing with Language-aligned 4D Gaussian Splatting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Driving scene manipulation with sensor data is emerging as a promising alternative to traditional virtual driving simulators. However, existing frameworks struggle to generate realistic scenarios efficiently due to limited editing capabilities. To address these challenges, we present SIMSplat, a predictive driving scene editor with language-aligned Gaussian splatting. As a language-controlled editor, SIMSplat enables intuitive manipulation using natural language prompts. By aligning language with Gaussian-reconstructed scenes, it further supports direct querying of road objects, allowing precise and flexible editing. Our method provides detailed object-level editing, including adding new objects and modifying the trajectories of both vehicles and pedestrians, while also incorporating predictive path refinement through multi-agent motion prediction to generate realistic interactions among all agents in the scene. Experiments on the Waymo dataset demonstrate SIMSplat's extensive editing capabilities and adaptability across a wide range of scenarios. Project page: https://sungyeonparkk.github.io/simsplat/
<div id='section'>Paperid: <span id='pid'>66, <a href='https://arxiv.org/pdf/2510.02252.pdf' target='_blank'>https://arxiv.org/pdf/2510.02252.pdf</a></span>   <span><a href='https://jaraujo98.github.io/retargeting_matters' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/YanjieZe/GMR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Joao Pedro Araujo, Yanjie Ze, Pei Xu, Jiajun Wu, C. Karen Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.02252">Retargeting Matters: General Motion Retargeting for Humanoid Motion Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humanoid motion tracking policies are central to building teleoperation pipelines and hierarchical controllers, yet they face a fundamental challenge: the embodiment gap between humans and humanoid robots. Current approaches address this gap by retargeting human motion data to humanoid embodiments and then training reinforcement learning (RL) policies to imitate these reference trajectories. However, artifacts introduced during retargeting, such as foot sliding, self-penetration, and physically infeasible motion are often left in the reference trajectories for the RL policy to correct. While prior work has demonstrated motion tracking abilities, they often require extensive reward engineering and domain randomization to succeed. In this paper, we systematically evaluate how retargeting quality affects policy performance when excessive reward tuning is suppressed. To address issues that we identify with existing retargeting methods, we propose a new retargeting method, General Motion Retargeting (GMR). We evaluate GMR alongside two open-source retargeters, PHC and ProtoMotions, as well as with a high-quality closed-source dataset from Unitree. Using BeyondMimic for policy training, we isolate retargeting effects without reward tuning. Our experiments on a diverse subset of the LAFAN1 dataset reveal that while most motions can be tracked, artifacts in retargeted data significantly reduce policy robustness, particularly for dynamic or long sequences. GMR consistently outperforms existing open-source methods in both tracking performance and faithfulness to the source motion, achieving perceptual fidelity and policy success rates close to the closed-source baseline. Website: https://jaraujo98.github.io/retargeting_matters. Code: https://github.com/YanjieZe/GMR.
<div id='section'>Paperid: <span id='pid'>67, <a href='https://arxiv.org/pdf/2509.24209.pdf' target='_blank'>https://arxiv.org/pdf/2509.24209.pdf</a></span>   <span><a href='https://zhenliuzju.github.io/huyingdong/Forge4D' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yingdong Hu, Yisheng He, Jinnan Chen, Weihao Yuan, Kejie Qiu, Zehong Lin, Siyu Zhu, Zilong Dong, Jun Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24209">Forge4D: Feed-Forward 4D Human Reconstruction and Interpolation from Uncalibrated Sparse-view Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Instant reconstruction of dynamic 3D humans from uncalibrated sparse-view videos is critical for numerous downstream applications. Existing methods, however, are either limited by the slow reconstruction speeds or incapable of generating novel-time representations. To address these challenges, we propose Forge4D, a feed-forward 4D human reconstruction and interpolation model that efficiently reconstructs temporally aligned representations from uncalibrated sparse-view videos, enabling both novel view and novel time synthesis. Our model simplifies the 4D reconstruction and interpolation problem as a joint task of streaming 3D Gaussian reconstruction and dense motion prediction. For the task of streaming 3D Gaussian reconstruction, we first reconstruct static 3D Gaussians from uncalibrated sparse-view images and then introduce learnable state tokens to enforce temporal consistency in a memory-friendly manner by interactively updating shared information across different timestamps. For novel time synthesis, we design a novel motion prediction module to predict dense motions for each 3D Gaussian between two adjacent frames, coupled with an occlusion-aware Gaussian fusion process to interpolate 3D Gaussians at arbitrary timestamps. To overcome the lack of the ground truth for dense motion supervision, we formulate dense motion prediction as a dense point matching task and introduce a self-supervised retargeting loss to optimize this module. An additional occlusion-aware optical flow loss is introduced to ensure motion consistency with plausible human movement, providing stronger regularization. Extensive experiments demonstrate the effectiveness of our model on both in-domain and out-of-domain datasets. Project page and code at: https://zhenliuzju.github.io/huyingdong/Forge4D.
<div id='section'>Paperid: <span id='pid'>68, <a href='https://arxiv.org/pdf/2509.19252.pdf' target='_blank'>https://arxiv.org/pdf/2509.19252.pdf</a></span>   <span><a href='https://github.com/TeCSAR-UNCC/Pose-Quantization' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Gabriel Maldonado, Narges Rashvand, Armin Danesh Pazho, Ghazal Alinezhad Noghre, Vinit Katariya, Hamed Tabkhi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.19252">Adversarially-Refined VQ-GAN with Dense Motion Tokenization for Spatio-Temporal Heatmaps</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Continuous human motion understanding remains a core challenge in computer vision due to its high dimensionality and inherent redundancy. Efficient compression and representation are crucial for analyzing complex motion dynamics. In this work, we introduce an adversarially-refined VQ-GAN framework with dense motion tokenization for compressing spatio-temporal heatmaps while preserving the fine-grained traces of human motion. Our approach combines dense motion tokenization with adversarial refinement, which eliminates reconstruction artifacts like motion smearing and temporal misalignment observed in non-adversarial baselines. Our experiments on the CMU Panoptic dataset provide conclusive evidence of our method's superiority, outperforming the dVAE baseline by 9.31% SSIM and reducing temporal instability by 37.1%. Furthermore, our dense tokenization strategy enables a novel analysis of motion complexity, revealing that 2D motion can be optimally represented with a compact 128-token vocabulary, while 3D motion's complexity demands a much larger 1024-token codebook for faithful reconstruction. These results establish practical deployment feasibility across diverse motion analysis applications. The code base for this work is available at https://github.com/TeCSAR-UNCC/Pose-Quantization.
<div id='section'>Paperid: <span id='pid'>69, <a href='https://arxiv.org/pdf/2509.17450.pdf' target='_blank'>https://arxiv.org/pdf/2509.17450.pdf</a></span>   <span><a href='http://rise-policy.github.io/DQ-RISE/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ying Feng, Hongjie Fang, Yinong He, Jingjing Chen, Chenxi Wang, Zihao He, Ruonan Liu, Cewu Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17450">Learning Dexterous Manipulation with Quantized Hand State</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dexterous robotic hands enable robots to perform complex manipulations that require fine-grained control and adaptability. Achieving such manipulation is challenging because the high degrees of freedom tightly couple hand and arm motions, making learning and control difficult. Successful dexterous manipulation relies not only on precise hand motions, but also on accurate spatial positioning of the arm and coordinated arm-hand dynamics. However, most existing visuomotor policies represent arm and hand actions in a single combined space, which often causes high-dimensional hand actions to dominate the coupled action space and compromise arm control. To address this, we propose DQ-RISE, which quantizes hand states to simplify hand motion prediction while preserving essential patterns, and applies a continuous relaxation that allows arm actions to diffuse jointly with these compact hand states. This design enables the policy to learn arm-hand coordination from data while preventing hand actions from overwhelming the action space. Experiments show that DQ-RISE achieves more balanced and efficient learning, paving the way toward structured and generalizable dexterous manipulation. Project website: http://rise-policy.github.io/DQ-RISE/
<div id='section'>Paperid: <span id='pid'>70, <a href='https://arxiv.org/pdf/2509.17323.pdf' target='_blank'>https://arxiv.org/pdf/2509.17323.pdf</a></span>   <span><a href='https://github.com/warriordby/DepTR-MOT' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/warriordby/DepTR-MOT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Buyin Deng, Lingxin Huang, Kai Luo, Fei Teng, Kailun Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17323">DepTR-MOT: Unveiling the Potential of Depth-Informed Trajectory Refinement for Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual Multi-Object Tracking (MOT) is a crucial component of robotic perception, yet existing Tracking-By-Detection (TBD) methods often rely on 2D cues, such as bounding boxes and motion modeling, which struggle under occlusions and close-proximity interactions. Trackers relying on these 2D cues are particularly unreliable in robotic environments, where dense targets and frequent occlusions are common. While depth information has the potential to alleviate these issues, most existing MOT datasets lack depth annotations, leading to its underexploited role in the domain. To unveil the potential of depth-informed trajectory refinement, we introduce DepTR-MOT, a DETR-based detector enhanced with instance-level depth information. Specifically, we propose two key innovations: (i) foundation model-based instance-level soft depth label supervision, which refines depth prediction, and (ii) the distillation of dense depth maps to maintain global depth consistency. These strategies enable DepTR-MOT to output instance-level depth during inference, without requiring foundation models and without additional computational cost. By incorporating depth cues, our method enhances the robustness of the TBD paradigm, effectively resolving occlusion and close-proximity challenges. Experiments on both the QuadTrack and DanceTrack datasets demonstrate the effectiveness of our approach, achieving HOTA scores of 27.59 and 44.47, respectively. In particular, results on QuadTrack, a robotic platform MOT dataset, highlight the advantages of our method in handling occlusion and close-proximity challenges in robotic tracking. The source code will be made publicly available at https://github.com/warriordby/DepTR-MOT.
<div id='section'>Paperid: <span id='pid'>71, <a href='https://arxiv.org/pdf/2509.15781.pdf' target='_blank'>https://arxiv.org/pdf/2509.15781.pdf</a></span>   <span><a href='https://github.com/2025-LSVOS-3rd-place/MOSEv2_3rd_place' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chang Soo Lim, Joonyoung Moon, Donghyeon Cho
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15781">Enriched Feature Representation and Motion Prediction Module for MOSEv2 Track of 7th LSVOS Challenge: 3rd Place Solution</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video object segmentation (VOS) is a challenging task with wide applications such as video editing and autonomous driving. While Cutie provides strong query-based segmentation and SAM2 offers enriched representations via a pretrained ViT encoder, each has limitations in feature capacity and temporal modeling. In this report, we propose a framework that integrates their complementary strengths by replacing the encoder of Cutie with the ViT encoder of SAM2 and introducing a motion prediction module for temporal stability. We further adopt an ensemble strategy combining Cutie, SAM2, and our variant, achieving 3rd place in the MOSEv2 track of the 7th LSVOS Challenge. We refer to our final model as SCOPE (SAM2-CUTIE Object Prediction Ensemble). This demonstrates the effectiveness of enriched feature representation and motion prediction for robust video object segmentation. The code is available at https://github.com/2025-LSVOS-3rd-place/MOSEv2_3rd_place.
<div id='section'>Paperid: <span id='pid'>72, <a href='https://arxiv.org/pdf/2509.14353.pdf' target='_blank'>https://arxiv.org/pdf/2509.14353.pdf</a></span>   <span><a href='https://genrobo.github.io/DreamControl/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dvij Kalaria, Sudarshan S Harithas, Pushkal Katara, Sangkyung Kwak, Sarthak Bhagat, Shankar Sastry, Srinath Sridhar, Sai Vemprala, Ashish Kapoor, Jonathan Chung-Kuan Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14353">DreamControl: Human-Inspired Whole-Body Humanoid Control for Scene Interaction via Guided Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce DreamControl, a novel methodology for learning autonomous whole-body humanoid skills. DreamControl leverages the strengths of diffusion models and Reinforcement Learning (RL): our core innovation is the use of a diffusion prior trained on human motion data, which subsequently guides an RL policy in simulation to complete specific tasks of interest (e.g., opening a drawer or picking up an object). We demonstrate that this human motion-informed prior allows RL to discover solutions unattainable by direct RL, and that diffusion models inherently promote natural looking motions, aiding in sim-to-real transfer. We validate DreamControl's effectiveness on a Unitree G1 robot across a diverse set of challenging tasks involving simultaneous lower and upper body control and object interaction. Project website at https://genrobo.github.io/DreamControl/
<div id='section'>Paperid: <span id='pid'>73, <a href='https://arxiv.org/pdf/2509.11090.pdf' target='_blank'>https://arxiv.org/pdf/2509.11090.pdf</a></span>   <span><a href='https://github.com/Joechencc/CAAPolicy' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chao Chen, Shunyu Yao, Yuanwu He, Tao Feng, Ruojing Song, Yuliang Guo, Xinyu Huang, Chenxu Wu, Ren Liu, Chen Feng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11090">End-to-End Visual Autonomous Parking via Control-Aided Attention</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Precise parking requires an end-to-end system where perception adaptively provides policy-relevant details-especially in critical areas where fine control decisions are essential. End-to-end learning offers a unified framework by directly mapping sensor inputs to control actions, but existing approaches lack effective synergy between perception and control. We find that transformer-based self-attention, when used alone, tends to produce unstable and temporally inconsistent spatial attention, which undermines the reliability of downstream policy decisions over time. Instead, we propose CAA-Policy, an end-to-end imitation learning system that allows control signal to guide the learning of visual attention via a novel Control-Aided Attention (CAA) mechanism. For the first time, we train such an attention module in a self-supervised manner, using backpropagated gradients from the control outputs instead of from the training loss. This strategy encourages the attention to focus on visual features that induce high variance in action outputs, rather than merely minimizing the training loss-a shift we demonstrate leads to a more robust and generalizable policy. To further enhance stability, CAA-Policy integrates short-horizon waypoint prediction as an auxiliary task, and introduces a separately trained motion prediction module to robustly track the target spot over time. Extensive experiments in the CARLA simulator show that \titlevariable~consistently surpasses both the end-to-end learning baseline and the modular BEV segmentation + hybrid A* pipeline, achieving superior accuracy, robustness, and interpretability. Code is released at https://github.com/Joechencc/CAAPolicy.
<div id='section'>Paperid: <span id='pid'>74, <a href='https://arxiv.org/pdf/2509.09555.pdf' target='_blank'>https://arxiv.org/pdf/2509.09555.pdf</a></span>   <span><a href='https://github.com/wzyabcas/InterAct,' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sirui Xu, Dongting Li, Yucheng Zhang, Xiyan Xu, Qi Long, Ziyin Wang, Yunzhi Lu, Shuchang Dong, Hezi Jiang, Akshat Gupta, Yu-Xiong Wang, Liang-Yan Gui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09555">InterAct: Advancing Large-Scale Versatile 3D Human-Object Interaction Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While large-scale human motion capture datasets have advanced human motion generation, modeling and generating dynamic 3D human-object interactions (HOIs) remain challenging due to dataset limitations. Existing datasets often lack extensive, high-quality motion and annotation and exhibit artifacts such as contact penetration, floating, and incorrect hand motions. To address these issues, we introduce InterAct, a large-scale 3D HOI benchmark featuring dataset and methodological advancements. First, we consolidate and standardize 21.81 hours of HOI data from diverse sources, enriching it with detailed textual annotations. Second, we propose a unified optimization framework to enhance data quality by reducing artifacts and correcting hand motions. Leveraging the principle of contact invariance, we maintain human-object relationships while introducing motion variations, expanding the dataset to 30.70 hours. Third, we define six benchmarking tasks and develop a unified HOI generative modeling perspective, achieving state-of-the-art performance. Extensive experiments validate the utility of our dataset as a foundational resource for advancing 3D human-object interaction generation. To support continued research in this area, the dataset is publicly available at https://github.com/wzyabcas/InterAct, and will be actively maintained.
<div id='section'>Paperid: <span id='pid'>75, <a href='https://arxiv.org/pdf/2509.09496.pdf' target='_blank'>https://arxiv.org/pdf/2509.09496.pdf</a></span>   <span><a href='https://hlinhn.github.io/momentum_bmvc' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ha Linh Nguyen, Tze Ho Elden Tse, Angela Yao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09496">Improving Human Motion Plausibility with Body Momentum</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Many studies decompose human motion into local motion in a frame attached to the root joint and global motion of the root joint in the world frame, treating them separately. However, these two components are not independent. Global movement arises from interactions with the environment, which are, in turn, driven by changes in the body configuration. Motion models often fail to precisely capture this physical coupling between local and global dynamics, while deriving global trajectories from joint torques and external forces is computationally expensive and complex. To address these challenges, we propose using whole-body linear and angular momentum as a constraint to link local motion with global movement. Since momentum reflects the aggregate effect of joint-level dynamics on the body's movement through space, it provides a physically grounded way to relate local joint behavior to global displacement. Building on this insight, we introduce a new loss term that enforces consistency between the generated momentum profiles and those observed in ground-truth data. Incorporating our loss reduces foot sliding and jitter, improves balance, and preserves the accuracy of the recovered motion. Code and data are available at the project page https://hlinhn.github.io/momentum_bmvc.
<div id='section'>Paperid: <span id='pid'>76, <a href='https://arxiv.org/pdf/2509.03883.pdf' target='_blank'>https://arxiv.org/pdf/2509.03883.pdf</a></span>   <span><a href='https://github.com/Winn1y/Awesome-Human-Motion-Video-Generation' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/Winn1y/Awesome-Human-Motion-Video-Generation' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haiwei Xue, Xiangyang Luo, Zhanghao Hu, Xin Zhang, Xunzhi Xiang, Yuqin Dai, Jianzhuang Liu, Zhensong Zhang, Minglei Li, Jian Yang, Fei Ma, Zhiyong Wu, Changpeng Yang, Zonghong Dai, Fei Richard Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03883">Human Motion Video Generation: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion video generation has garnered significant research interest due to its broad applications, enabling innovations such as photorealistic singing heads or dynamic avatars that seamlessly dance to music. However, existing surveys in this field focus on individual methods, lacking a comprehensive overview of the entire generative process. This paper addresses this gap by providing an in-depth survey of human motion video generation, encompassing over ten sub-tasks, and detailing the five key phases of the generation process: input, motion planning, motion video generation, refinement, and output. Notably, this is the first survey that discusses the potential of large language models in enhancing human motion video generation. Our survey reviews the latest developments and technological trends in human motion video generation across three primary modalities: vision, text, and audio. By covering over two hundred papers, we offer a thorough overview of the field and highlight milestone works that have driven significant technological breakthroughs. Our goal for this survey is to unveil the prospects of human motion video generation and serve as a valuable resource for advancing the comprehensive applications of digital humans. A complete list of the models examined in this survey is available in Our Repository https://github.com/Winn1y/Awesome-Human-Motion-Video-Generation.
<div id='section'>Paperid: <span id='pid'>77, <a href='https://arxiv.org/pdf/2509.00767.pdf' target='_blank'>https://arxiv.org/pdf/2509.00767.pdf</a></span>   <span><a href='https://mael-zys.github.io/InterPose/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yangsong Zhang, Abdul Ahad Butt, GÃ¼l Varol, Ivan Laptev
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.00767">InterPose: Learning to Generate Human-Object Interactions from Large-Scale Web Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion generation has shown great advances thanks to the recent diffusion models trained on large-scale motion capture data. Most of existing works, however, currently target animation of isolated people in empty scenes. Meanwhile, synthesizing realistic human-object interactions in complex 3D scenes remains a critical challenge in computer graphics and robotics. One obstacle towards generating versatile high-fidelity human-object interactions is the lack of large-scale datasets with diverse object manipulations. Indeed, existing motion capture data is typically restricted to single people and manipulations of limited sets of objects. To address this issue, we propose an automatic motion extraction pipeline and use it to collect interaction-rich human motions. Our new dataset InterPose contains 73.8K sequences of 3D human motions and corresponding text captions automatically obtained from 45.8K videos with human-object interactions. We perform extensive experiments and demonstrate InterPose to bring significant improvements to state-of-the-art methods for human motion generation. Moreover, using InterPose we develop an LLM-based agent enabling zero-shot animation of people interacting with diverse objects and scenes.
<div id='section'>Paperid: <span id='pid'>78, <a href='https://arxiv.org/pdf/2508.20920.pdf' target='_blank'>https://arxiv.org/pdf/2508.20920.pdf</a></span>   <span><a href='https://github.com/PARCO-LAB/COMETH' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Enrico Martini, Ho Jin Choi, Nadia Figueroa, Nicola Bombieri
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.20920">COMETH: Convex Optimization for Multiview Estimation and Tracking of Humans</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the era of Industry 5.0, monitoring human activity is essential for ensuring both ergonomic safety and overall well-being. While multi-camera centralized setups improve pose estimation accuracy, they often suffer from high computational costs and bandwidth requirements, limiting scalability and real-time applicability. Distributing processing across edge devices can reduce network bandwidth and computational load. On the other hand, the constrained resources of edge devices lead to accuracy degradation, and the distribution of computation leads to temporal and spatial inconsistencies. We address this challenge by proposing COMETH (Convex Optimization for Multiview Estimation and Tracking of Humans), a lightweight algorithm for real-time multi-view human pose fusion that relies on three concepts: it integrates kinematic and biomechanical constraints to increase the joint positioning accuracy; it employs convex optimization-based inverse kinematics for spatial fusion; and it implements a state observer to improve temporal consistency. We evaluate COMETH on both public and industrial datasets, where it outperforms state-of-the-art methods in localization, detection, and tracking accuracy. The proposed fusion pipeline enables accurate and scalable human motion tracking, making it well-suited for industrial and safety-critical applications. The code is publicly available at https://github.com/PARCO-LAB/COMETH.
<div id='section'>Paperid: <span id='pid'>79, <a href='https://arxiv.org/pdf/2508.20615.pdf' target='_blank'>https://arxiv.org/pdf/2508.20615.pdf</a></span>   <span><a href='https://github.com/GVCLab/EmoCAST' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiguo Jiang, Xiaodong Cun, Yong Zhang, Yudian Zheng, Fan Tang, Chi-Man Pun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.20615">EmoCAST: Emotional Talking Portrait via Emotive Text Description</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Emotional talking head synthesis aims to generate talking portrait videos with vivid expressions. Existing methods still exhibit limitations in control flexibility, motion naturalness, and expression quality. Moreover, currently available datasets are primarily collected in lab settings, further exacerbating these shortcomings. Consequently, these limitations substantially hinder practical applications in real-world scenarios. To address these challenges, we propose EmoCAST, a diffusion-based framework with two key modules for precise text-driven emotional synthesis. In appearance modeling, emotional prompts are integrated through a text-guided decoupled emotive module, enhancing the spatial knowledge to improve emotion comprehension. To improve the relationship between audio and emotion, we introduce an emotive audio attention module to capture the interplay between controlled emotion and driving audio, generating emotion-aware features to guide more precise facial motion synthesis. Additionally, we construct an emotional talking head dataset with comprehensive emotive text descriptions to optimize the framework's performance. Based on the proposed dataset, we propose an emotion-aware sampling training strategy and a progressive functional training strategy that further improve the model's ability to capture nuanced expressive features and achieve accurate lip-synchronization. Overall, EmoCAST achieves state-of-the-art performance in generating realistic, emotionally expressive, and audio-synchronized talking-head videos. Project Page: https://github.com/GVCLab/EmoCAST
<div id='section'>Paperid: <span id='pid'>80, <a href='https://arxiv.org/pdf/2508.20085.pdf' target='_blank'>https://arxiv.org/pdf/2508.20085.pdf</a></span>   <span><a href='https://gemcollector.github.io/HERMES/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhecheng Yuan, Tianming Wei, Langzhe Gu, Pu Hua, Tianhai Liang, Yuanpei Chen, Huazhe Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.20085">HERMES: Human-to-Robot Embodied Learning from Multi-Source Motion Data for Mobile Dexterous Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Leveraging human motion data to impart robots with versatile manipulation skills has emerged as a promising paradigm in robotic manipulation. Nevertheless, translating multi-source human hand motions into feasible robot behaviors remains challenging, particularly for robots equipped with multi-fingered dexterous hands characterized by complex, high-dimensional action spaces. Moreover, existing approaches often struggle to produce policies capable of adapting to diverse environmental conditions. In this paper, we introduce HERMES, a human-to-robot learning framework for mobile bimanual dexterous manipulation. First, HERMES formulates a unified reinforcement learning approach capable of seamlessly transforming heterogeneous human hand motions from multiple sources into physically plausible robotic behaviors. Subsequently, to mitigate the sim2real gap, we devise an end-to-end, depth image-based sim2real transfer method for improved generalization to real-world scenarios. Furthermore, to enable autonomous operation in varied and unstructured environments, we augment the navigation foundation model with a closed-loop Perspective-n-Point (PnP) localization mechanism, ensuring precise alignment of visual goals and effectively bridging autonomous navigation and dexterous manipulation. Extensive experimental results demonstrate that HERMES consistently exhibits generalizable behaviors across diverse, in-the-wild scenarios, successfully performing numerous complex mobile bimanual dexterous manipulation tasks. Project Page:https://gemcollector.github.io/HERMES/.
<div id='section'>Paperid: <span id='pid'>81, <a href='https://arxiv.org/pdf/2508.18691.pdf' target='_blank'>https://arxiv.org/pdf/2508.18691.pdf</a></span>   <span><a href='https://jirl-upenn.github.io/track_reward/' target='_blank'>  GitHub</a></span> <span><a href='https://hgaurav2k.github.io/trackr/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Himanshu Gaurav Singh, Pieter Abbeel, Jitendra Malik, Antonio Loquercio
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.18691">Deep Sensorimotor Control by Imitating Predictive Models of Human Motion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As the embodiment gap between a robot and a human narrows, new opportunities arise to leverage datasets of humans interacting with their surroundings for robot learning. We propose a novel technique for training sensorimotor policies with reinforcement learning by imitating predictive models of human motions. Our key insight is that the motion of keypoints on human-inspired robot end-effectors closely mirrors the motion of corresponding human body keypoints. This enables us to use a model trained to predict future motion on human data \emph{zero-shot} on robot data. We train sensorimotor policies to track the predictions of such a model, conditioned on a history of past robot states, while optimizing a relatively sparse task reward. This approach entirely bypasses gradient-based kinematic retargeting and adversarial losses, which limit existing methods from fully leveraging the scale and diversity of modern human-scene interaction datasets. Empirically, we find that our approach can work across robots and tasks, outperforming existing baselines by a large margin. In addition, we find that tracking a human motion model can substitute for carefully designed dense rewards and curricula in manipulation tasks. Code, data and qualitative results available at https://jirl-upenn.github.io/track_reward/.
<div id='section'>Paperid: <span id='pid'>82, <a href='https://arxiv.org/pdf/2508.17404.pdf' target='_blank'>https://arxiv.org/pdf/2508.17404.pdf</a></span>   <span><a href='https://hywang2002.github.io/MoCo' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoyu Wang, Hao Tang, Donglin Di, Zhilu Zhang, Wangmeng Zuo, Feng Gao, Siwei Ma, Shiliang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17404">MoCo: Motion-Consistent Human Video Generation via Structure-Appearance Decoupling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating human videos with consistent motion from text prompts remains a significant challenge, particularly for whole-body or long-range motion. Existing video generation models prioritize appearance fidelity, resulting in unrealistic or physically implausible human movements with poor structural coherence. Additionally, most existing human video datasets primarily focus on facial or upper-body motions, or consist of vertically oriented dance videos, limiting the scope of corresponding generation methods to simple movements. To overcome these challenges, we propose MoCo, which decouples the process of human video generation into two components: structure generation and appearance generation. Specifically, our method first employs an efficient 3D structure generator to produce a human motion sequence from a text prompt. The remaining video appearance is then synthesized under the guidance of the generated structural sequence. To improve fine-grained control over sparse human structures, we introduce Human-Aware Dynamic Control modules and integrate dense tracking constraints during training. Furthermore, recognizing the limitations of existing datasets, we construct a large-scale whole-body human video dataset featuring complex and diverse motions. Extensive experiments demonstrate that MoCo outperforms existing approaches in generating realistic and structurally coherent human videos.
<div id='section'>Paperid: <span id='pid'>83, <a href='https://arxiv.org/pdf/2508.13911.pdf' target='_blank'>https://arxiv.org/pdf/2508.13911.pdf</a></span>   <span><a href='https://hihixiaolv.github.io/PhysGM.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chunji Lv, Zequn Chen, Donglin Di, Weinan Zhang, Hao Li, Wei Chen, Changsheng Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.13911">PhysGM: Large Physical Gaussian Model for Feed-Forward 4D Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While physics-grounded 3D motion synthesis has seen significant progress, current methods face critical limitations. They typically rely on pre-reconstructed 3D Gaussian Splatting (3DGS) representations, while physics integration depends on either inflexible, manually defined physical attributes or unstable, optimization-heavy guidance from video models. To overcome these challenges, we introduce PhysGM, a feed-forward framework that jointly predicts a 3D Gaussian representation and its physical properties from a single image, enabling immediate, physical simulation and high-fidelity 4D rendering. We first establish a base model by jointly optimizing for Gaussian reconstruction and probabilistic physics prediction. The model is then refined with physically plausible reference videos to enhance both rendering fidelity and physics prediction accuracy. We adopt the Direct Preference Optimization (DPO) to align its simulations with reference videos, circumventing Score Distillation Sampling (SDS) optimization which needs back-propagating gradients through the complex differentiable simulation and rasterization. To facilitate the training, we introduce a new dataset PhysAssets of over 24,000 3D assets, annotated with physical properties and corresponding guiding videos. Experimental results demonstrate that our method effectively generates high-fidelity 4D simulations from a single image in one minute. This represents a significant speedup over prior works while delivering realistic rendering results. Our project page is at:https://hihixiaolv.github.io/PhysGM.github.io/
<div id='section'>Paperid: <span id='pid'>84, <a href='https://arxiv.org/pdf/2508.12184.pdf' target='_blank'>https://arxiv.org/pdf/2508.12184.pdf</a></span>   <span><a href='https://rhea-mal.github.io/humanoidsynergies.io' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Rhea Malhotra, William Chong, Catie Cuan, Oussama Khatib
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.12184">Humanoid Motion Scripting with Postural Synergies</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating sequences of human-like motions for humanoid robots presents challenges in collecting and analyzing reference human motions, synthesizing new motions based on these reference motions, and mapping the generated motion onto humanoid robots. To address these issues, we introduce SynSculptor, a humanoid motion analysis and editing framework that leverages postural synergies for training-free human-like motion scripting. To analyze human motion, we collect 3+ hours of motion capture data across 20 individuals where a real-time operational space controller mimics human motion on a simulated humanoid robot. The major postural synergies are extracted using principal component analysis (PCA) for velocity trajectories segmented by changes in robot momentum, constructing a style-conditioned synergy library for free-space motion generation. To evaluate generated motions using the synergy library, the foot-sliding ratio and proposed metrics for motion smoothness involving total momentum and kinetic energy deviations are computed for each generated motion, and compared with reference motions. Finally, we leverage the synergies with a motion-language transformer, where the humanoid, during execution of motion tasks with its end-effectors, adapts its posture based on the chosen synergy. Supplementary material, code, and videos are available at https://rhea-mal.github.io/humanoidsynergies.io.
<div id='section'>Paperid: <span id='pid'>85, <a href='https://arxiv.org/pdf/2508.10897.pdf' target='_blank'>https://arxiv.org/pdf/2508.10897.pdf</a></span>   <span><a href='https://github.com/BradleyWang0416/Human-in-Context' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mengyuan Liu, Xinshun Wang, Zhongbin Fang, Deheng Ye, Xia Li, Tao Tang, Songtao Wu, Xiangtai Li, Ming-Hsuan Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10897">Human-in-Context: Unified Cross-Domain 3D Human Motion Modeling via In-Context Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper aims to model 3D human motion across domains, where a single model is expected to handle multiple modalities, tasks, and datasets. Existing cross-domain models often rely on domain-specific components and multi-stage training, which limits their practicality and scalability. To overcome these challenges, we propose a new setting to train a unified cross-domain model through a single process, eliminating the need for domain-specific components and multi-stage training. We first introduce Pose-in-Context (PiC), which leverages in-context learning to create a pose-centric cross-domain model. While PiC generalizes across multiple pose-based tasks and datasets, it encounters difficulties with modality diversity, prompting strategy, and contextual dependency handling. We thus propose Human-in-Context (HiC), an extension of PiC that broadens generalization across modalities, tasks, and datasets. HiC combines pose and mesh representations within a unified framework, expands task coverage, and incorporates larger-scale datasets. Additionally, HiC introduces a max-min similarity prompt sampling strategy to enhance generalization across diverse domains and a network architecture with dual-branch context injection for improved handling of contextual dependencies. Extensive experimental results show that HiC performs better than PiC in terms of generalization, data scale, and performance across a wide range of domains. These results demonstrate the potential of HiC for building a unified cross-domain 3D human motion model with improved flexibility and scalability. The source codes and models are available at https://github.com/BradleyWang0416/Human-in-Context.
<div id='section'>Paperid: <span id='pid'>86, <a href='https://arxiv.org/pdf/2508.10881.pdf' target='_blank'>https://arxiv.org/pdf/2508.10881.pdf</a></span>   <span><a href='https://lg-li.github.io/project/tooncomposer' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lingen Li, Guangzhi Wang, Zhaoyang Zhang, Yaowei Li, Xiaoyu Li, Qi Dou, Jinwei Gu, Tianfan Xue, Ying Shan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10881">ToonComposer: Streamlining Cartoon Production with Generative Post-Keyframing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Traditional cartoon and anime production involves keyframing, inbetweening, and colorization stages, which require intensive manual effort. Despite recent advances in AI, existing methods often handle these stages separately, leading to error accumulation and artifacts. For instance, inbetweening approaches struggle with large motions, while colorization methods require dense per-frame sketches. To address this, we introduce ToonComposer, a generative model that unifies inbetweening and colorization into a single post-keyframing stage. ToonComposer employs a sparse sketch injection mechanism to provide precise control using keyframe sketches. Additionally, it uses a cartoon adaptation method with the spatial low-rank adapter to tailor a modern video foundation model to the cartoon domain while keeping its temporal prior intact. Requiring as few as a single sketch and a colored reference frame, ToonComposer excels with sparse inputs, while also supporting multiple sketches at any temporal location for more precise motion control. This dual capability reduces manual workload and improves flexibility, empowering artists in real-world scenarios. To evaluate our model, we further created PKBench, a benchmark featuring human-drawn sketches that simulate real-world use cases. Our evaluation demonstrates that ToonComposer outperforms existing methods in visual quality, motion consistency, and production efficiency, offering a superior and more flexible solution for AI-assisted cartoon production.
<div id='section'>Paperid: <span id='pid'>87, <a href='https://arxiv.org/pdf/2508.10567.pdf' target='_blank'>https://arxiv.org/pdf/2508.10567.pdf</a></span>   <span><a href='https://phi-wol.github.io/sparcad/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Philipp Wolters, Johannes Gilg, Torben Teepe, Gerhard Rigoll
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10567">SpaRC-AD: A Baseline for Radar-Camera Fusion in End-to-End Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>End-to-end autonomous driving systems promise stronger performance through unified optimization of perception, motion forecasting, and planning. However, vision-based approaches face fundamental limitations in adverse weather conditions, partial occlusions, and precise velocity estimation - critical challenges in safety-sensitive scenarios where accurate motion understanding and long-horizon trajectory prediction are essential for collision avoidance. To address these limitations, we propose SpaRC-AD, a query-based end-to-end camera-radar fusion framework for planning-oriented autonomous driving. Through sparse 3D feature alignment, and doppler-based velocity estimation, we achieve strong 3D scene representations for refinement of agent anchors, map polylines and motion modelling. Our method achieves strong improvements over the state-of-the-art vision-only baselines across multiple autonomous driving tasks, including 3D detection (+4.8% mAP), multi-object tracking (+8.3% AMOTA), online mapping (+1.8% mAP), motion prediction (-4.0% mADE), and trajectory planning (-0.1m L2 and -9% TPC). We achieve both spatial coherence and temporal consistency on multiple challenging benchmarks, including real-world open-loop nuScenes, long-horizon T-nuScenes, and closed-loop simulator Bench2Drive. We show the effectiveness of radar-based fusion in safety-critical scenarios where accurate motion understanding and long-horizon trajectory prediction are essential for collision avoidance. The source code of all experiments is available at https://phi-wol.github.io/sparcad/
<div id='section'>Paperid: <span id='pid'>88, <a href='https://arxiv.org/pdf/2508.10427.pdf' target='_blank'>https://arxiv.org/pdf/2508.10427.pdf</a></span>   <span><a href='https://turingmotors.github.io/stride-qa/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Keishi Ishihara, Kento Sasaki, Tsubasa Takahashi, Daiki Shiono, Yu Yamaguchi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10427">STRIDE-QA: Visual Question Answering Dataset for Spatiotemporal Reasoning in Urban Driving Scenes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language Models (VLMs) have been applied to autonomous driving to support decision-making in complex real-world scenarios. However, their training on static, web-sourced image-text pairs fundamentally limits the precise spatiotemporal reasoning required to understand and predict dynamic traffic scenes. We address this critical gap with STRIDE-QA, a large-scale visual question answering (VQA) dataset for physically grounded reasoning from an ego-centric perspective. Constructed from 100 hours of multi-sensor driving data in Tokyo, capturing diverse and challenging conditions, STRIDE-QA is the largest VQA dataset for spatiotemporal reasoning in urban driving, offering 16 million QA pairs over 285K frames. Grounded by dense, automatically generated annotations including 3D bounding boxes, segmentation masks, and multi-object tracks, the dataset uniquely supports both object-centric and ego-centric reasoning through three novel QA tasks that require spatial localization and temporal prediction. Our benchmarks demonstrate that existing VLMs struggle significantly, achieving near-zero scores on prediction consistency. In contrast, VLMs fine-tuned on STRIDE-QA exhibit dramatic performance gains, achieving 55% success in spatial localization and 28% consistency in future motion prediction, compared to near-zero scores from general-purpose VLMs. Therefore, STRIDE-QA establishes a comprehensive foundation for developing more reliable VLMs for safety-critical autonomous systems.
<div id='section'>Paperid: <span id='pid'>89, <a href='https://arxiv.org/pdf/2508.09575.pdf' target='_blank'>https://arxiv.org/pdf/2508.09575.pdf</a></span>   <span><a href='https://github.com/jwonkm/DRF' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiwon Kim, Pureum Kim, SeonHwa Kim, Soobin Park, Eunju Cha, Kyong Hwan Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09575">Dual Recursive Feedback on Generation and Appearance Latents for Pose-Robust Text-to-Image Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in controllable text-to-image (T2I) diffusion models, such as Ctrl-X and FreeControl, have demonstrated robust spatial and appearance control without requiring auxiliary module training. However, these models often struggle to accurately preserve spatial structures and fail to capture fine-grained conditions related to object poses and scene layouts. To address these challenges, we propose a training-free Dual Recursive Feedback (DRF) system that properly reflects control conditions in controllable T2I models. The proposed DRF consists of appearance feedback and generation feedback that recursively refines the intermediate latents to better reflect the given appearance information and the user's intent. This dual-update mechanism guides latent representations toward reliable manifolds, effectively integrating structural and appearance attributes. Our approach enables fine-grained generation even between class-invariant structure-appearance fusion, such as transferring human motion onto a tiger's form. Extensive experiments demonstrate the efficacy of our method in producing high-quality, semantically coherent, and structurally consistent image generations. Our source code is available at https://github.com/jwonkm/DRF.
<div id='section'>Paperid: <span id='pid'>90, <a href='https://arxiv.org/pdf/2508.09404.pdf' target='_blank'>https://arxiv.org/pdf/2508.09404.pdf</a></span>   <span><a href='https://github.com/GuangxunZhu/Waymo-3DSkelMo' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Guangxun Zhu, Shiyu Fan, Hang Dai, Edmond S. L. Ho
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09404">Waymo-3DSkelMo: A Multi-Agent 3D Skeletal Motion Dataset for Pedestrian Interaction Modeling in Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large-scale high-quality 3D motion datasets with multi-person interactions are crucial for data-driven models in autonomous driving to achieve fine-grained pedestrian interaction understanding in dynamic urban environments. However, existing datasets mostly rely on estimating 3D poses from monocular RGB video frames, which suffer from occlusion and lack of temporal continuity, thus resulting in unrealistic and low-quality human motion. In this paper, we introduce Waymo-3DSkelMo, the first large-scale dataset providing high-quality, temporally coherent 3D skeletal motions with explicit interaction semantics, derived from the Waymo Perception dataset. Our key insight is to utilize 3D human body shape and motion priors to enhance the quality of the 3D pose sequences extracted from the raw LiDRA point clouds. The dataset covers over 14,000 seconds across more than 800 real driving scenarios, including rich interactions among an average of 27 agents per scene (with up to 250 agents in the largest scene). Furthermore, we establish 3D pose forecasting benchmarks under varying pedestrian densities, and the results demonstrate its value as a foundational resource for future research on fine-grained human behavior understanding in complex urban environments. The dataset and code will be available at https://github.com/GuangxunZhu/Waymo-3DSkelMo
<div id='section'>Paperid: <span id='pid'>91, <a href='https://arxiv.org/pdf/2508.08588.pdf' target='_blank'>https://arxiv.org/pdf/2508.08588.pdf</a></span>   <span><a href='https://jingyunliang.github.io/RealisMotion' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingyun Liang, Jingkai Zhou, Shikai Li, Chenjie Cao, Lei Sun, Yichen Qian, Weihua Chen, Fan Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08588">RealisMotion: Decomposed Human Motion Control and Video Generation in the World Space</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating human videos with realistic and controllable motions is a challenging task. While existing methods can generate visually compelling videos, they lack separate control over four key video elements: foreground subject, background video, human trajectory and action patterns. In this paper, we propose a decomposed human motion control and video generation framework that explicitly decouples motion from appearance, subject from background, and action from trajectory, enabling flexible mix-and-match composition of these elements. Concretely, we first build a ground-aware 3D world coordinate system and perform motion editing directly in the 3D space. Trajectory control is implemented by unprojecting edited 2D trajectories into 3D with focal-length calibration and coordinate transformation, followed by speed alignment and orientation adjustment; actions are supplied by a motion bank or generated via text-to-motion methods. Then, based on modern text-to-video diffusion transformer models, we inject the subject as tokens for full attention, concatenate the background along the channel dimension, and add motion (trajectory and action) control signals by addition. Such a design opens up the possibility for us to generate realistic videos of anyone doing anything anywhere. Extensive experiments on benchmark datasets and real-world cases demonstrate that our method achieves state-of-the-art performance on both element-wise controllability and overall video quality.
<div id='section'>Paperid: <span id='pid'>92, <a href='https://arxiv.org/pdf/2508.08241.pdf' target='_blank'>https://arxiv.org/pdf/2508.08241.pdf</a></span>   <span><a href='https://beyondmimic.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiayuan Liao, Takara E. Truong, Xiaoyu Huang, Guy Tevet, Koushil Sreenath, C. Karen Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08241">BeyondMimic: From Motion Tracking to Versatile Humanoid Control via Guided Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning skills from human motions offers a promising path toward generalizable policies for versatile humanoid whole-body control, yet two key cornerstones are missing: (1) a high-quality motion tracking framework that faithfully transforms large-scale kinematic references into robust and extremely dynamic motions on real hardware, and (2) a distillation approach that can effectively learn these motion primitives and compose them to solve downstream tasks. We address these gaps with BeyondMimic, a real-world framework to learn from human motions for versatile and naturalistic humanoid control via guided diffusion. Our framework provides a motion tracking pipeline capable of challenging skills such as jumping spins, sprinting, and cartwheels with state-of-the-art motion quality. Moving beyond simply mimicking existing motions, we further introduce a unified diffusion policy that enables zero-shot task-specific control at test time using simple cost functions. Deployed on hardware, BeyondMimic performs diverse tasks at test time, including waypoint navigation, joystick teleoperation, and obstacle avoidance, bridging sim-to-real motion tracking and flexible synthesis of human motion primitives for whole-body control. https://beyondmimic.github.io/.
<div id='section'>Paperid: <span id='pid'>93, <a href='https://arxiv.org/pdf/2508.07863.pdf' target='_blank'>https://arxiv.org/pdf/2508.07863.pdf</a></span>   <span><a href='https://beingbeyond.github.io/Being-M0.5' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bin Cao, Sipeng Zheng, Ye Wang, Lujie Xia, Qianshan Wei, Qin Jin, Jing Liu, Zongqing Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.07863">Being-M0.5: A Real-Time Controllable Vision-Language-Motion Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion generation has emerged as a critical technology with transformative potential for real-world applications. However, existing vision-language-motion models (VLMMs) face significant limitations that hinder their practical deployment. We identify controllability as a main bottleneck, manifesting in five key aspects: inadequate response to diverse human commands, limited pose initialization capabilities, poor performance on long-term sequences, insufficient handling of unseen scenarios, and lack of fine-grained control over individual body parts. To overcome these limitations, we present Being-M0.5, the first real-time, controllable VLMM that achieves state-of-the-art performance across multiple motion generation tasks. Our approach is built upon HuMo100M, the largest and most comprehensive human motion dataset to date, comprising over 5 million self-collected motion sequences, 100 million multi-task instructional instances, and detailed part-level annotations that address a critical gap in existing datasets. We introduce a novel part-aware residual quantization technique for motion tokenization that enables precise, granular control over individual body parts during generation. Extensive experimental validation demonstrates Being-M0.5's superior performance across diverse motion benchmarks, while comprehensive efficiency analysis confirms its real-time capabilities. Our contributions include design insights and detailed computational analysis to guide future development of practical motion generators. We believe that HuMo100M and Being-M0.5 represent significant advances that will accelerate the adoption of motion generation technologies in real-world applications. The project page is available at https://beingbeyond.github.io/Being-M0.5.
<div id='section'>Paperid: <span id='pid'>94, <a href='https://arxiv.org/pdf/2508.06139.pdf' target='_blank'>https://arxiv.org/pdf/2508.06139.pdf</a></span>   <span><a href='https://shaohua-pan.github.io/diffcap-page' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shaohua Pan, Xinyu Yi, Yan Zhou, Weihua Jian, Yuan Zhang, Pengfei Wan, Feng Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.06139">DiffCap: Diffusion-based Real-time Human Motion Capture using Sparse IMUs and a Monocular Camera</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Combining sparse IMUs and a monocular camera is a new promising setting to perform real-time human motion capture. This paper proposes a diffusion-based solution to learn human motion priors and fuse the two modalities of signals together seamlessly in a unified framework. By delicately considering the characteristics of the two signals, the sequential visual information is considered as a whole and transformed into a condition embedding, while the inertial measurement is concatenated with the noisy body pose frame by frame to construct a sequential input for the diffusion model. Firstly, we observe that the visual information may be unavailable in some frames due to occlusions or subjects moving out of the camera view. Thus incorporating the sequential visual features as a whole to get a single feature embedding is robust to the occasional degenerations of visual information in those frames. On the other hand, the IMU measurements are robust to occlusions and always stable when signal transmission has no problem. So incorporating them frame-wisely could better explore the temporal information for the system. Experiments have demonstrated the effectiveness of the system design and its state-of-the-art performance in pose estimation compared with the previous works. Our codes are available for research at https://shaohua-pan.github.io/diffcap-page.
<div id='section'>Paperid: <span id='pid'>95, <a href='https://arxiv.org/pdf/2508.04681.pdf' target='_blank'>https://arxiv.org/pdf/2508.04681.pdf</a></span>   <span><a href='https://liangxuy.github.io/InterVLA/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Liang Xu, Chengqun Yang, Zili Lin, Fei Xu, Yifan Liu, Congsheng Xu, Yiyi Zhang, Jie Qin, Xingdong Sheng, Yunhui Liu, Xin Jin, Yichao Yan, Wenjun Zeng, Xiaokang Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.04681">Perceiving and Acting in First-Person: A Dataset and Benchmark for Egocentric Human-Object-Human Interactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning action models from real-world human-centric interaction datasets is important towards building general-purpose intelligent assistants with efficiency. However, most existing datasets only offer specialist interaction category and ignore that AI assistants perceive and act based on first-person acquisition. We urge that both the generalist interaction knowledge and egocentric modality are indispensable. In this paper, we embed the manual-assisted task into a vision-language-action framework, where the assistant provides services to the instructor following egocentric vision and commands. With our hybrid RGB-MoCap system, pairs of assistants and instructors engage with multiple objects and the scene following GPT-generated scripts. Under this setting, we accomplish InterVLA, the first large-scale human-object-human interaction dataset with 11.4 hours and 1.2M frames of multimodal data, spanning 2 egocentric and 5 exocentric videos, accurate human/object motions and verbal commands. Furthermore, we establish novel benchmarks on egocentric human motion estimation, interaction synthesis, and interaction prediction with comprehensive analysis. We believe that our InterVLA testbed and the benchmarks will foster future works on building AI agents in the physical world.
<div id='section'>Paperid: <span id='pid'>96, <a href='https://arxiv.org/pdf/2508.04228.pdf' target='_blank'>https://arxiv.org/pdf/2508.04228.pdf</a></span>   <span><a href='https://kr-panghu.github.io/LayerT2V/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kangrui Cen, Baixuan Zhao, Yi Xin, Siqi Luo, Guangtao Zhai, Xiaohong Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.04228">LayerT2V: Interactive Multi-Object Trajectory Layering for Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Controlling object motion trajectories in Text-to-Video (T2V) generation is a challenging and relatively under-explored area, particularly in scenarios involving multiple moving objects. Most community models and datasets in the T2V domain are designed for single-object motion, limiting the performance of current generative models in multi-object tasks. Additionally, existing motion control methods in T2V either lack support for multi-object motion scenes or experience severe performance degradation when object trajectories intersect, primarily due to the semantic conflicts in colliding regions. To address these limitations, we introduce LayerT2V, the first approach for generating video by compositing background and foreground objects layer by layer. This layered generation enables flexible integration of multiple independent elements within a video, positioning each element on a distinct "layer" and thus facilitating coherent multi-object synthesis while enhancing control over the generation process. Extensive experiments demonstrate the superiority of LayerT2V in generating complex multi-object scenarios, showcasing 1.4x and 4.5x improvements in mIoU and AP50 metrics over state-of-the-art (SOTA) methods. Project page and code are available at https://kr-panghu.github.io/LayerT2V/ .
<div id='section'>Paperid: <span id='pid'>97, <a href='https://arxiv.org/pdf/2508.02944.pdf' target='_blank'>https://arxiv.org/pdf/2508.02944.pdf</a></span>   <span><a href='https://byteaigc.github.io/X-Actor/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenxu Zhang, Zenan Li, Hongyi Xu, You Xie, Xiaochen Zhao, Tianpei Gu, Guoxian Song, Xin Chen, Chao Liang, Jianwen Jiang, Linjie Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02944">X-Actor: Emotional and Expressive Long-Range Portrait Acting from Audio</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present X-Actor, a novel audio-driven portrait animation framework that generates lifelike, emotionally expressive talking head videos from a single reference image and an input audio clip. Unlike prior methods that emphasize lip synchronization and short-range visual fidelity in constrained speaking scenarios, X-Actor enables actor-quality, long-form portrait performance capturing nuanced, dynamically evolving emotions that flow coherently with the rhythm and content of speech. Central to our approach is a two-stage decoupled generation pipeline: an audio-conditioned autoregressive diffusion model that predicts expressive yet identity-agnostic facial motion latent tokens within a long temporal context window, followed by a diffusion-based video synthesis module that translates these motions into high-fidelity video animations. By operating in a compact facial motion latent space decoupled from visual and identity cues, our autoregressive diffusion model effectively captures long-range correlations between audio and facial dynamics through a diffusion-forcing training paradigm, enabling infinite-length emotionally-rich motion prediction without error accumulation. Extensive experiments demonstrate that X-Actor produces compelling, cinematic-style performances that go beyond standard talking head animations and achieves state-of-the-art results in long-range, audio-driven emotional portrait acting.
<div id='section'>Paperid: <span id='pid'>98, <a href='https://arxiv.org/pdf/2508.02605.pdf' target='_blank'>https://arxiv.org/pdf/2508.02605.pdf</a></span>   <span><a href='https://github.com/AIGeeksGroup/ReMoMask' target='_blank'>  GitHub</a></span> <span><a href='https://aigeeksgroup.github.io/ReMoMask' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhengdao Li, Siheng Wang, Zeyu Zhang, Hao Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02605">ReMoMask: Retrieval-Augmented Masked Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-to-Motion (T2M) generation aims to synthesize realistic and semantically aligned human motion sequences from natural language descriptions. However, current approaches face dual challenges: Generative models (e.g., diffusion models) suffer from limited diversity, error accumulation, and physical implausibility, while Retrieval-Augmented Generation (RAG) methods exhibit diffusion inertia, partial-mode collapse, and asynchronous artifacts. To address these limitations, we propose ReMoMask, a unified framework integrating three key innovations: 1) A Bidirectional Momentum Text-Motion Model decouples negative sample scale from batch size via momentum queues, substantially improving cross-modal retrieval precision; 2) A Semantic Spatio-temporal Attention mechanism enforces biomechanical constraints during part-level fusion to eliminate asynchronous artifacts; 3) RAG-Classier-Free Guidance incorporates minor unconditional generation to enhance generalization. Built upon MoMask's RVQ-VAE, ReMoMask efficiently generates temporally coherent motions in minimal steps. Extensive experiments on standard benchmarks demonstrate the state-of-the-art performance of ReMoMask, achieving a 3.88% and 10.97% improvement in FID scores on HumanML3D and KIT-ML, respectively, compared to the previous SOTA method RAG-T2M. Code: https://github.com/AIGeeksGroup/ReMoMask. Website: https://aigeeksgroup.github.io/ReMoMask.
<div id='section'>Paperid: <span id='pid'>99, <a href='https://arxiv.org/pdf/2508.02362.pdf' target='_blank'>https://arxiv.org/pdf/2508.02362.pdf</a></span>   <span><a href='https://plyon1.github.io/Text2Lip/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xu Wang, Shengeng Tang, Fei Wang, Lechao Cheng, Dan Guo, Feng Xue, Richang Hong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02362">Text2Lip: Progressive Lip-Synced Talking Face Generation from Text via Viseme-Guided Rendering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating semantically coherent and visually accurate talking faces requires bridging the gap between linguistic meaning and facial articulation. Although audio-driven methods remain prevalent, their reliance on high-quality paired audio visual data and the inherent ambiguity in mapping acoustics to lip motion pose significant challenges in terms of scalability and robustness. To address these issues, we propose Text2Lip, a viseme-centric framework that constructs an interpretable phonetic-visual bridge by embedding textual input into structured viseme sequences. These mid-level units serve as a linguistically grounded prior for lip motion prediction. Furthermore, we design a progressive viseme-audio replacement strategy based on curriculum learning, enabling the model to gradually transition from real audio to pseudo-audio reconstructed from enhanced viseme features via cross-modal attention. This allows for robust generation in both audio-present and audio-free scenarios. Finally, a landmark-guided renderer synthesizes photorealistic facial videos with accurate lip synchronization. Extensive evaluations show that Text2Lip outperforms existing approaches in semantic fidelity, visual realism, and modality robustness, establishing a new paradigm for controllable and flexible talking face generation. Our project homepage is https://plyon1.github.io/Text2Lip/.
<div id='section'>Paperid: <span id='pid'>100, <a href='https://arxiv.org/pdf/2508.02140.pdf' target='_blank'>https://arxiv.org/pdf/2508.02140.pdf</a></span>   <span><a href='https://github.com/DriverlessMobility/AID4AD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Daniel Lengerer, Mathias Pechinger, Klaus Bogenberger, Carsten Markgraf
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02140">AID4AD: Aerial Image Data for Automated Driving Perception</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work investigates the integration of spatially aligned aerial imagery into perception tasks for automated vehicles (AVs). As a central contribution, we present AID4AD, a publicly available dataset that augments the nuScenes dataset with high-resolution aerial imagery precisely aligned to its local coordinate system. The alignment is performed using SLAM-based point cloud maps provided by nuScenes, establishing a direct link between aerial data and nuScenes local coordinate system. To ensure spatial fidelity, we propose an alignment workflow that corrects for localization and projection distortions. A manual quality control process further refines the dataset by identifying a set of high-quality alignments, which we publish as ground truth to support future research on automated registration. We demonstrate the practical value of AID4AD in two representative tasks: in online map construction, aerial imagery serves as a complementary input that improves the mapping process; in motion prediction, it functions as a structured environmental representation that replaces high-definition maps. Experiments show that aerial imagery leads to a 15-23% improvement in map construction accuracy and a 2% gain in trajectory prediction performance. These results highlight the potential of aerial imagery as a scalable and adaptable source of environmental context in automated vehicle systems, particularly in scenarios where high-definition maps are unavailable, outdated, or costly to maintain. AID4AD, along with evaluation code and pretrained models, is publicly released to foster further research in this direction: https://github.com/DriverlessMobility/AID4AD.
<div id='section'>Paperid: <span id='pid'>101, <a href='https://arxiv.org/pdf/2508.01984.pdf' target='_blank'>https://arxiv.org/pdf/2508.01984.pdf</a></span>   <span><a href='https://github.com/LUNAProject22/IMoRe' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chen Li, Chinthani Sugandhika, Yeo Keat Ee, Eric Peh, Hao Zhang, Hong Yang, Deepu Rajan, Basura Fernando
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01984">IMoRe: Implicit Program-Guided Reasoning for Human Motion Q&A</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing human motion Q\&A methods rely on explicit program execution, where the requirement for manually defined functional modules may limit the scalability and adaptability. To overcome this, we propose an implicit program-guided motion reasoning (IMoRe) framework that unifies reasoning across multiple query types without manually designed modules. Unlike existing implicit reasoning approaches that infer reasoning operations from question words, our model directly conditions on structured program functions, ensuring a more precise execution of reasoning steps. Additionally, we introduce a program-guided reading mechanism, which dynamically selects multi-level motion representations from a pretrained motion Vision Transformer (ViT), capturing both high-level semantics and fine-grained motion cues. The reasoning module iteratively refines memory representations, leveraging structured program functions to extract relevant information for different query types. Our model achieves state-of-the-art performance on Babel-QA and generalizes to a newly constructed motion Q\&A dataset based on HuMMan, demonstrating its adaptability across different motion reasoning datasets. Code and dataset are available at: https://github.com/LUNAProject22/IMoRe.
<div id='section'>Paperid: <span id='pid'>102, <a href='https://arxiv.org/pdf/2508.01126.pdf' target='_blank'>https://arxiv.org/pdf/2508.01126.pdf</a></span>   <span><a href='https://chaitanya100100.github.io/UniEgoMotion/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chaitanya Patel, Hiroki Nakamura, Yuta Kyuragi, Kazuki Kozuka, Juan Carlos Niebles, Ehsan Adeli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01126">UniEgoMotion: A Unified Model for Egocentric Motion Reconstruction, Forecasting, and Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Egocentric human motion generation and forecasting with scene-context is crucial for enhancing AR/VR experiences, improving human-robot interaction, advancing assistive technologies, and enabling adaptive healthcare solutions by accurately predicting and simulating movement from a first-person perspective. However, existing methods primarily focus on third-person motion synthesis with structured 3D scene contexts, limiting their effectiveness in real-world egocentric settings where limited field of view, frequent occlusions, and dynamic cameras hinder scene perception. To bridge this gap, we introduce Egocentric Motion Generation and Egocentric Motion Forecasting, two novel tasks that utilize first-person images for scene-aware motion synthesis without relying on explicit 3D scene. We propose UniEgoMotion, a unified conditional motion diffusion model with a novel head-centric motion representation tailored for egocentric devices. UniEgoMotion's simple yet effective design supports egocentric motion reconstruction, forecasting, and generation from first-person visual inputs in a unified framework. Unlike previous works that overlook scene semantics, our model effectively extracts image-based scene context to infer plausible 3D motion. To facilitate training, we introduce EE4D-Motion, a large-scale dataset derived from EgoExo4D, augmented with pseudo-ground-truth 3D motion annotations. UniEgoMotion achieves state-of-the-art performance in egocentric motion reconstruction and is the first to generate motion from a single egocentric image. Extensive evaluations demonstrate the effectiveness of our unified framework, setting a new benchmark for egocentric motion modeling and unlocking new possibilities for egocentric applications.
<div id='section'>Paperid: <span id='pid'>103, <a href='https://arxiv.org/pdf/2507.22567.pdf' target='_blank'>https://arxiv.org/pdf/2507.22567.pdf</a></span>   <span><a href='https://github.com/JoeyBGOfficial/Low-Cost-Accurate-Radar-Based-Human-Motion-Direction-Determination' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Weicheng Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.22567">Exploration of Low-Cost but Accurate Radar-Based Human Motion Direction Determination</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work is completed on a whim after discussions with my junior colleague. The motion direction angle affects the micro-Doppler spectrum width, thus determining the human motion direction can provide important prior information for downstream tasks such as gait recognition. However, Doppler-Time map (DTM)-based methods still have room for improvement in achieving feature augmentation and motion determination simultaneously. In response, a low-cost but accurate radar-based human motion direction determination (HMDD) method is explored in this paper. In detail, the radar-based human gait DTMs are first generated, and then the feature augmentation is achieved using feature linking model. Subsequently, the HMDD is implemented through a lightweight and fast Vision Transformer-Convolutional Neural Network hybrid model structure. The effectiveness of the proposed method is verified through open-source dataset. The open-source code of this work is released at: https://github.com/JoeyBGOfficial/Low-Cost-Accurate-Radar-Based-Human-Motion-Direction-Determination.
<div id='section'>Paperid: <span id='pid'>104, <a href='https://arxiv.org/pdf/2507.20562.pdf' target='_blank'>https://arxiv.org/pdf/2507.20562.pdf</a></span>   <span><a href='https://cau-irislab.github.io/ICCV25-MemoryTalker/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hyung Kyu Kim, Sangmin Lee, Hak Gu Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.20562">MemoryTalker: Personalized Speech-Driven 3D Facial Animation via Audio-Guided Stylization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Speech-driven 3D facial animation aims to synthesize realistic facial motion sequences from given audio, matching the speaker's speaking style. However, previous works often require priors such as class labels of a speaker or additional 3D facial meshes at inference, which makes them fail to reflect the speaking style and limits their practical use. To address these issues, we propose MemoryTalker which enables realistic and accurate 3D facial motion synthesis by reflecting speaking style only with audio input to maximize usability in applications. Our framework consists of two training stages: 1-stage is storing and retrieving general motion (i.e., Memorizing), and 2-stage is to perform the personalized facial motion synthesis (i.e., Animating) with the motion memory stylized by the audio-driven speaking style feature. In this second stage, our model learns about which facial motion types should be emphasized for a particular piece of audio. As a result, our MemoryTalker can generate a reliable personalized facial animation without additional prior information. With quantitative and qualitative evaluations, as well as user study, we show the effectiveness of our model and its performance enhancement for personalized facial animation over state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>105, <a href='https://arxiv.org/pdf/2507.18237.pdf' target='_blank'>https://arxiv.org/pdf/2507.18237.pdf</a></span>   <span><a href='https://github.com/ChengchangTian/DATA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chengchang Tian, Jianwei Ma, Yan Huang, Zhanye Chen, Honghao Wei, Hui Zhang, Wei Hong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.18237">DATA: Domain-And-Time Alignment for High-Quality Feature Fusion in Collaborative Perception</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Feature-level fusion shows promise in collaborative perception (CP) through balanced performance and communication bandwidth trade-off. However, its effectiveness critically relies on input feature quality. The acquisition of high-quality features faces domain gaps from hardware diversity and deployment conditions, alongside temporal misalignment from transmission delays. These challenges degrade feature quality with cumulative effects throughout the collaborative network. In this paper, we present the Domain-And-Time Alignment (DATA) network, designed to systematically align features while maximizing their semantic representations for fusion. Specifically, we propose a Consistency-preserving Domain Alignment Module (CDAM) that reduces domain gaps through proximal-region hierarchical downsampling and observability-constrained discriminator. We further propose a Progressive Temporal Alignment Module (PTAM) to handle transmission delays via multi-scale motion modeling and two-stage compensation. Building upon the aligned features, an Instance-focused Feature Aggregation Module (IFAM) is developed to enhance semantic representations. Extensive experiments demonstrate that DATA achieves state-of-the-art performance on three typical datasets, maintaining robustness with severe communication delays and pose errors. The code will be released at https://github.com/ChengchangTian/DATA.
<div id='section'>Paperid: <span id='pid'>106, <a href='https://arxiv.org/pdf/2507.15266.pdf' target='_blank'>https://arxiv.org/pdf/2507.15266.pdf</a></span>   <span><a href='https://github.com/henryhcliu/vlmudmc.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haichao Liu, Haoren Guo, Pei Liu, Benshan Ma, Yuxiang Zhang, Jun Ma, Tong Heng Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.15266">VLM-UDMC: VLM-Enhanced Unified Decision-Making and Motion Control for Urban Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scene understanding and risk-aware attentions are crucial for human drivers to make safe and effective driving decisions. To imitate this cognitive ability in urban autonomous driving while ensuring the transparency and interpretability, we propose a vision-language model (VLM)-enhanced unified decision-making and motion control framework, named VLM-UDMC. This framework incorporates scene reasoning and risk-aware insights into an upper-level slow system, which dynamically reconfigures the optimal motion planning for the downstream fast system. The reconfiguration is based on real-time environmental changes, which are encoded through context-aware potential functions. More specifically, the upper-level slow system employs a two-step reasoning policy with Retrieval-Augmented Generation (RAG), leveraging foundation models to process multimodal inputs and retrieve contextual knowledge, thereby generating risk-aware insights. Meanwhile, a lightweight multi-kernel decomposed LSTM provides real-time trajectory predictions for heterogeneous traffic participants by extracting smoother trend representations for short-horizon trajectory prediction. The effectiveness of the proposed VLM-UDMC framework is verified via both simulations and real-world experiments with a full-size autonomous vehicle. It is demonstrated that the presented VLM-UDMC effectively leverages scene understanding and attention decomposition for rational driving decisions, thus improving the overall urban driving performance. Our open-source project is available at https://github.com/henryhcliu/vlmudmc.git.
<div id='section'>Paperid: <span id='pid'>107, <a href='https://arxiv.org/pdf/2507.10792.pdf' target='_blank'>https://arxiv.org/pdf/2507.10792.pdf</a></span>   <span><a href='https://github.com/511205787/Phy_SSM-ICML2025' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuchen Wang, Hongjue Zhao, Haohong Lin, Enze Xu, Lifang He, Huajie Shao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.10792">A Generalizable Physics-Enhanced State Space Model for Long-Term Dynamics Forecasting in Complex Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work aims to address the problem of long-term dynamic forecasting in complex environments where data are noisy and irregularly sampled. While recent studies have introduced some methods to improve prediction performance, these approaches still face a significant challenge in handling long-term extrapolation tasks under such complex scenarios. To overcome this challenge, we propose Phy-SSM, a generalizable method that integrates partial physics knowledge into state space models (SSMs) for long-term dynamics forecasting in complex environments. Our motivation is that SSMs can effectively capture long-range dependencies in sequential data and model continuous dynamical systems, while the incorporation of physics knowledge improves generalization ability. The key challenge lies in how to seamlessly incorporate partially known physics into SSMs. To achieve this, we decompose partially known system dynamics into known and unknown state matrices, which are integrated into a Phy-SSM unit. To further enhance long-term prediction performance, we introduce a physics state regularization term to make the estimated latent states align with system dynamics. Besides, we theoretically analyze the uniqueness of the solutions for our method. Extensive experiments on three real-world applications, including vehicle motion prediction, drone state prediction, and COVID-19 epidemiology forecasting, demonstrate the superior performance of Phy-SSM over the baselines in both long-term interpolation and extrapolation tasks. The code is available at https://github.com/511205787/Phy_SSM-ICML2025.
<div id='section'>Paperid: <span id='pid'>108, <a href='https://arxiv.org/pdf/2507.09672.pdf' target='_blank'>https://arxiv.org/pdf/2507.09672.pdf</a></span>   <span><a href='https://github.com/CarmenQing/VST-Pose' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/CarmenQing/VST-Pose' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinyu Zhang, Zhonghao Ye, Jingwei Zhang, Xiang Tian, Zhisheng Liang, Shipeng Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.09672">VST-Pose: A Velocity-Integrated Spatiotem-poral Attention Network for Human WiFi Pose Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>WiFi-based human pose estimation has emerged as a promising non-visual alternative approaches due to its pene-trability and privacy advantages. This paper presents VST-Pose, a novel deep learning framework for accurate and continuous pose estimation using WiFi channel state information. The proposed method introduces ViSTA-Former, a spatiotemporal attention backbone with dual-stream architecture that adopts a dual-stream architecture to separately capture temporal dependencies and structural relationships among body joints. To enhance sensitivity to subtle human motions, a velocity modeling branch is integrated into the framework, which learns short-term keypoint dis-placement patterns and improves fine-grained motion representation. We construct a 2D pose dataset specifically designed for smart home care scenarios and demonstrate that our method achieves 92.2% accuracy on the PCK@50 metric, outperforming existing methods by 8.3% in PCK@50 on the self-collected dataset. Further evaluation on the public MMFi dataset confirms the model's robustness and effectiveness in 3D pose estimation tasks. The proposed system provides a reliable and privacy-aware solution for continuous human motion analysis in indoor environments. Our codes are available in https://github.com/CarmenQing/VST-Pose.
<div id='section'>Paperid: <span id='pid'>109, <a href='https://arxiv.org/pdf/2507.09446.pdf' target='_blank'>https://arxiv.org/pdf/2507.09446.pdf</a></span>   <span><a href='https://github.com/Yuanhong-Zheng/EMPMP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuanhong Zheng, Ruixuan Yu, Jian Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.09446">Efficient Multi-Person Motion Prediction by Lightweight Spatial and Temporal Interactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D multi-person motion prediction is a highly complex task, primarily due to the dependencies on both individual past movements and the interactions between agents. Moreover, effectively modeling these interactions often incurs substantial computational costs. In this work, we propose a computationally efficient model for multi-person motion prediction by simplifying spatial and temporal interactions. Our approach begins with the design of lightweight dual branches that learn local and global representations for individual and multiple persons separately. Additionally, we introduce a novel cross-level interaction block to integrate the spatial and temporal representations from both branches. To further enhance interaction modeling, we explicitly incorporate the spatial inter-person distance embedding. With above efficient temporal and spatial design, we achieve state-of-the-art performance for multiple metrics on standard datasets of CMU-Mocap, MuPoTS-3D, and 3DPW, while significantly reducing the computational cost. Code is available at https://github.com/Yuanhong-Zheng/EMPMP.
<div id='section'>Paperid: <span id='pid'>110, <a href='https://arxiv.org/pdf/2507.09122.pdf' target='_blank'>https://arxiv.org/pdf/2507.09122.pdf</a></span>   <span><a href='https://snap-research.github.io/SnapMoGen/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chuan Guo, Inwoo Hwang, Jian Wang, Bing Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.09122">SnapMoGen: Human Motion Generation from Expressive Texts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-to-motion generation has experienced remarkable progress in recent years. However, current approaches remain limited to synthesizing motion from short or general text prompts, primarily due to dataset constraints. This limitation undermines fine-grained controllability and generalization to unseen prompts. In this paper, we introduce SnapMoGen, a new text-motion dataset featuring high-quality motion capture data paired with accurate, expressive textual annotations. The dataset comprises 20K motion clips totaling 44 hours, accompanied by 122K detailed textual descriptions averaging 48 words per description (vs. 12 words of HumanML3D). Importantly, these motion clips preserve original temporal continuity as they were in long sequences, facilitating research in long-term motion generation and blending. We also improve upon previous generative masked modeling approaches. Our model, MoMask++, transforms motion into multi-scale token sequences that better exploit the token capacity, and learns to generate all tokens using a single generative masked transformer. MoMask++ achieves state-of-the-art performance on both HumanML3D and SnapMoGen benchmarks. Additionally, we demonstrate the ability to process casual user prompts by employing an LLM to reformat inputs to align with the expressivity and narration style of SnapMoGen. Project webpage: https://snap-research.github.io/SnapMoGen/
<div id='section'>Paperid: <span id='pid'>111, <a href='https://arxiv.org/pdf/2507.08307.pdf' target='_blank'>https://arxiv.org/pdf/2507.08307.pdf</a></span>   <span><a href='https://m2dao-talker.github.io/M2DAO-Talk.github.io' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kui Jiang, Shiyu Liu, Junjun Jiang, Hongxun Yao, Xiaopeng Fan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.08307">M2DAO-Talker: Harmonizing Multi-granular Motion Decoupling and Alternating Optimization for Talking-head Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Audio-driven talking head generation holds significant potential for film production. While existing 3D methods have advanced motion modeling and content synthesis, they often produce rendering artifacts, such as motion blur, temporal jitter, and local penetration, due to limitations in representing stable, fine-grained motion fields. Through systematic analysis, we reformulate talking head generation into a unified framework comprising three steps: video preprocessing, motion representation, and rendering reconstruction. This framework underpins our proposed M2DAO-Talker, which addresses current limitations via multi-granular motion decoupling and alternating optimization. Specifically, we devise a novel 2D portrait preprocessing pipeline to extract frame-wise deformation control conditions (motion region segmentation masks, and camera parameters) to facilitate motion representation. To ameliorate motion modeling, we elaborate a multi-granular motion decoupling strategy, which independently models non-rigid (oral and facial) and rigid (head) motions for improved reconstruction accuracy. Meanwhile, a motion consistency constraint is developed to ensure head-torso kinematic consistency, thereby mitigating penetration artifacts caused by motion aliasing. In addition, an alternating optimization strategy is designed to iteratively refine facial and oral motion parameters, enabling more realistic video generation. Experiments across multiple datasets show that M2DAO-Talker achieves state-of-the-art performance, with the 2.43 dB PSNR improvement in generation quality and 0.64 gain in user-evaluated video realness versus TalkingGaussian while with 150 FPS inference speed. Our project homepage is https://m2dao-talker.github.io/M2DAO-Talk.github.io.
<div id='section'>Paperid: <span id='pid'>112, <a href='https://arxiv.org/pdf/2507.08028.pdf' target='_blank'>https://arxiv.org/pdf/2507.08028.pdf</a></span>   <span><a href='https://github.com/dolphin-in-a-coma/sssumo' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Evgenii Rudakov, Jonathan Shock, Otto Lappi, Benjamin Ultan Cowley
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.08028">SSSUMO: Real-Time Semi-Supervised Submovement Decomposition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces a SSSUMO, semi-supervised deep learning approach for submovement decomposition that achieves state-of-the-art accuracy and speed. While submovement analysis offers valuable insights into motor control, existing methods struggle with reconstruction accuracy, computational cost, and validation, due to the difficulty of obtaining hand-labeled data. We address these challenges using a semi-supervised learning framework. This framework learns from synthetic data, initially generated from minimum-jerk principles and then iteratively refined through adaptation to unlabeled human movement data. Our fully convolutional architecture with differentiable reconstruction significantly surpasses existing methods on both synthetic and diverse human motion datasets, demonstrating robustness even in high-noise conditions. Crucially, the model operates in real-time (less than a millisecond per input second), a substantial improvement over optimization-based techniques. This enhanced performance facilitates new applications in human-computer interaction, rehabilitation medicine, and motor control studies. We demonstrate the model's effectiveness across diverse human-performed tasks such as steering, rotation, pointing, object moving, handwriting, and mouse-controlled gaming, showing notable improvements particularly on challenging datasets where traditional methods largely fail. Training and benchmarking source code, along with pre-trained model weights, are made publicly available at https://github.com/dolphin-in-a-coma/sssumo.
<div id='section'>Paperid: <span id='pid'>113, <a href='https://arxiv.org/pdf/2507.07515.pdf' target='_blank'>https://arxiv.org/pdf/2507.07515.pdf</a></span>   <span><a href='https://github.com/inkcat520/GGMotion.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuaijin Wan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07515">GGMotion: Group Graph Dynamics-Kinematics Networks for Human Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion is a continuous physical process in 3D space, governed by complex dynamic and kinematic constraints. Existing methods typically represent the human pose as an abstract graph structure, neglecting the intrinsic physical dependencies between joints, which increases learning difficulty and makes the model prone to generating unrealistic motions. In this paper, we propose GGMotion, a group graph dynamics-kinematics network that models human topology in groups to better leverage dynamics and kinematics priors. To preserve the geometric equivariance in 3D space, we propose a novel radial field for the graph network that captures more comprehensive spatio-temporal dependencies by aggregating joint features through spatial and temporal edges. Inter-group and intra-group interaction modules are employed to capture the dependencies of joints at different scales. Combined with equivariant multilayer perceptrons (MLP), joint position features are updated in each group through parallelized dynamics-kinematics propagation to improve physical plausibility. Meanwhile, we introduce an auxiliary loss to supervise motion priors during training. Extensive experiments on three standard benchmarks, including Human3.6M, CMU-Mocap, and 3DPW, demonstrate the effectiveness and superiority of our approach, achieving a significant performance margin in short-term motion prediction. The code is available at https://github.com/inkcat520/GGMotion.git.
<div id='section'>Paperid: <span id='pid'>114, <a href='https://arxiv.org/pdf/2507.07095.pdf' target='_blank'>https://arxiv.org/pdf/2507.07095.pdf</a></span>   <span><a href='https://vankouf.github.io/MotionMillion/' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/VankouF/MotionMillion-Codes' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ke Fan, Shunlin Lu, Minyue Dai, Runyi Yu, Lixing Xiao, Zhiyang Dou, Junting Dong, Lizhuang Ma, Jingbo Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07095">Go to Zero: Towards Zero-shot Motion Generation with Million-scale Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating diverse and natural human motion sequences based on textual descriptions constitutes a fundamental and challenging research area within the domains of computer vision, graphics, and robotics. Despite significant advancements in this field, current methodologies often face challenges regarding zero-shot generalization capabilities, largely attributable to the limited size of training datasets. Moreover, the lack of a comprehensive evaluation framework impedes the advancement of this task by failing to identify directions for improvement. In this work, we aim to push text-to-motion into a new era, that is, to achieve the generalization ability of zero-shot. To this end, firstly, we develop an efficient annotation pipeline and introduce MotionMillion-the largest human motion dataset to date, featuring over 2,000 hours and 2 million high-quality motion sequences. Additionally, we propose MotionMillion-Eval, the most comprehensive benchmark for evaluating zero-shot motion generation. Leveraging a scalable architecture, we scale our model to 7B parameters and validate its performance on MotionMillion-Eval. Our results demonstrate strong generalization to out-of-domain and complex compositional motions, marking a significant step toward zero-shot human motion generation. The code is available at https://github.com/VankouF/MotionMillion-Codes.
<div id='section'>Paperid: <span id='pid'>115, <a href='https://arxiv.org/pdf/2507.06233.pdf' target='_blank'>https://arxiv.org/pdf/2507.06233.pdf</a></span>   <span><a href='https://cvlab-kaist.github.io/AnthroTAP/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>InÃ¨s Hyeonsu Kim, Seokju Cho, Jahyeok Koo, Junghyun Park, Jiahui Huang, Joon-Young Lee, Seungryong Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06233">Learning to Track Any Points from Human Motion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion, with its inherent complexities, such as non-rigid deformations, articulated movements, clothing distortions, and frequent occlusions caused by limbs or other individuals, provides a rich and challenging source of supervision that is crucial for training robust and generalizable point trackers. Despite the suitability of human motion, acquiring extensive training data for point tracking remains difficult due to laborious manual annotation. Our proposed pipeline, AnthroTAP, addresses this by proposing an automated pipeline to generate pseudo-labeled training data, leveraging the Skinned Multi-Person Linear (SMPL) model. We first fit the SMPL model to detected humans in video frames, project the resulting 3D mesh vertices onto 2D image planes to generate pseudo-trajectories, handle occlusions using ray-casting, and filter out unreliable tracks based on optical flow consistency. A point tracking model trained on AnthroTAP annotated dataset achieves state-of-the-art performance on the TAP-Vid benchmark, surpassing other models trained on real videos while using 10,000 times less data and only 1 day in 4 GPUs, compared to 256 GPUs used in recent state-of-the-art.
<div id='section'>Paperid: <span id='pid'>116, <a href='https://arxiv.org/pdf/2507.05963.pdf' target='_blank'>https://arxiv.org/pdf/2507.05963.pdf</a></span>   <span><a href='https://ali-videoai.github.io/Tora2_page/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenghao Zhang, Junchao Liao, Xiangyu Meng, Long Qin, Weizhi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.05963">Tora2: Motion and Appearance Customized Diffusion Transformer for Multi-Entity Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in diffusion transformer models for motion-guided video generation, such as Tora, have shown significant progress. In this paper, we present Tora2, an enhanced version of Tora, which introduces several design improvements to expand its capabilities in both appearance and motion customization. Specifically, we introduce a decoupled personalization extractor that generates comprehensive personalization embeddings for multiple open-set entities, better preserving fine-grained visual details compared to previous methods. Building on this, we design a gated self-attention mechanism to integrate trajectory, textual description, and visual information for each entity. This innovation significantly reduces misalignment in multimodal conditioning during training. Moreover, we introduce a contrastive loss that jointly optimizes trajectory dynamics and entity consistency through explicit mapping between motion and personalization embeddings. Tora2 is, to our best knowledge, the first method to achieve simultaneous multi-entity customization of appearance and motion for video generation. Experimental results demonstrate that Tora2 achieves competitive performance with state-of-the-art customization methods while providing advanced motion control capabilities, which marks a critical advancement in multi-condition video generation. Project page: https://ali-videoai.github.io/Tora2_page/.
<div id='section'>Paperid: <span id='pid'>117, <a href='https://arxiv.org/pdf/2507.05254.pdf' target='_blank'>https://arxiv.org/pdf/2507.05254.pdf</a></span>   <span><a href='https://frommarginaltojointpred.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fabian Konstantinidis, Ariel Dallari Guerreiro, Raphael Trumpp, Moritz Sackmann, Ulrich Hofmann, Marco Caccamo, Christoph Stiller
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.05254">From Marginal to Joint Predictions: Evaluating Scene-Consistent Trajectory Prediction Approaches for Automated Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate motion prediction of surrounding traffic participants is crucial for the safe and efficient operation of automated vehicles in dynamic environments. Marginal prediction models commonly forecast each agent's future trajectories independently, often leading to sub-optimal planning decisions for an automated vehicle. In contrast, joint prediction models explicitly account for the interactions between agents, yielding socially and physically consistent predictions on a scene level. However, existing approaches differ not only in their problem formulation but also in the model architectures and implementation details used, making it difficult to compare them. In this work, we systematically investigate different approaches to joint motion prediction, including post-processing of the marginal predictions, explicitly training the model for joint predictions, and framing the problem as a generative task. We evaluate each approach in terms of prediction accuracy, multi-modality, and inference efficiency, offering a comprehensive analysis of the strengths and limitations of each approach. Several prediction examples are available at https://frommarginaltojointpred.github.io/.
<div id='section'>Paperid: <span id='pid'>118, <a href='https://arxiv.org/pdf/2507.04062.pdf' target='_blank'>https://arxiv.org/pdf/2507.04062.pdf</a></span>   <span><a href='https://hyqlat.github.io/STABACB.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianwei Tang, Hong Yang, Tengyue Chen, Jian-Fang Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.04062">Stochastic Human Motion Prediction with Memory of Action Transition and Action Characteristic</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Action-driven stochastic human motion prediction aims to generate future motion sequences of a pre-defined target action based on given past observed sequences performing non-target actions. This task primarily presents two challenges. Firstly, generating smooth transition motions is hard due to the varying transition speeds of different actions. Secondly, the action characteristic is difficult to be learned because of the similarity of some actions. These issues cause the predicted results to be unreasonable and inconsistent. As a result, we propose two memory banks, the Soft-transition Action Bank (STAB) and Action Characteristic Bank (ACB), to tackle the problems above. The STAB stores the action transition information. It is equipped with the novel soft searching approach, which encourages the model to focus on multiple possible action categories of observed motions. The ACB records action characteristic, which produces more prior information for predicting certain actions. To fuse the features retrieved from the two banks better, we further propose the Adaptive Attention Adjustment (AAA) strategy. Extensive experiments on four motion prediction datasets demonstrate that our approach consistently outperforms the previous state-of-the-art. The demo and code are available at https://hyqlat.github.io/STABACB.github.io/.
<div id='section'>Paperid: <span id='pid'>119, <a href='https://arxiv.org/pdf/2507.04060.pdf' target='_blank'>https://arxiv.org/pdf/2507.04060.pdf</a></span>   <span><a href='https://github.com/hyqlat/TCL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianwei Tang, Jiangxin Sun, Xiaotong Lin, Lifang Zhang, Wei-Shi Zheng, Jian-Fang Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.04060">Temporal Continual Learning with Prior Compensation for Human Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human Motion Prediction (HMP) aims to predict future poses at different moments according to past motion sequences. Previous approaches have treated the prediction of various moments equally, resulting in two main limitations: the learning of short-term predictions is hindered by the focus on long-term predictions, and the incorporation of prior information from past predictions into subsequent predictions is limited. In this paper, we introduce a novel multi-stage training framework called Temporal Continual Learning (TCL) to address the above challenges. To better preserve prior information, we introduce the Prior Compensation Factor (PCF). We incorporate it into the model training to compensate for the lost prior information. Furthermore, we derive a more reasonable optimization objective through theoretical derivation. It is important to note that our TCL framework can be easily integrated with different HMP backbone models and adapted to various datasets and applications. Extensive experiments on four HMP benchmark datasets demonstrate the effectiveness and flexibility of TCL. The code is available at https://github.com/hyqlat/TCL.
<div id='section'>Paperid: <span id='pid'>120, <a href='https://arxiv.org/pdf/2507.02363.pdf' target='_blank'>https://arxiv.org/pdf/2507.02363.pdf</a></span>   <span><a href='https://wujh2001.github.io/LocalDyGS/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiahao Wu, Rui Peng, Jianbo Jiao, Jiayu Yang, Luyang Tang, Kaiqiang Xiong, Jie Liang, Jinbo Yan, Runling Liu, Ronggang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02363">LocalDyGS: Multi-view Global Dynamic Scene Modeling via Adaptive Local Implicit Feature Decoupling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Due to the complex and highly dynamic motions in the real world, synthesizing dynamic videos from multi-view inputs for arbitrary viewpoints is challenging. Previous works based on neural radiance field or 3D Gaussian splatting are limited to modeling fine-scale motion, greatly restricting their application. In this paper, we introduce LocalDyGS, which consists of two parts to adapt our method to both large-scale and fine-scale motion scenes: 1) We decompose a complex dynamic scene into streamlined local spaces defined by seeds, enabling global modeling by capturing motion within each local space. 2) We decouple static and dynamic features for local space motion modeling. A static feature shared across time steps captures static information, while a dynamic residual field provides time-specific features. These are combined and decoded to generate Temporal Gaussians, modeling motion within each local space. As a result, we propose a novel dynamic scene reconstruction framework to model highly dynamic real-world scenes more realistically. Our method not only demonstrates competitive performance on various fine-scale datasets compared to state-of-the-art (SOTA) methods, but also represents the first attempt to model larger and more complex highly dynamic scenes. Project page: https://wujh2001.github.io/LocalDyGS/.
<div id='section'>Paperid: <span id='pid'>121, <a href='https://arxiv.org/pdf/2507.01857.pdf' target='_blank'>https://arxiv.org/pdf/2507.01857.pdf</a></span>   <span><a href='https://isee-laboratory.github.io/TypeTele' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuhao Lin, Yi-Lin Wei, Haoran Liao, Mu Lin, Chengyi Xing, Hao Li, Dandan Zhang, Mark Cutkosky, Wei-Shi Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.01857">TypeTele: Releasing Dexterity in Teleoperation by Dexterous Manipulation Types</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dexterous teleoperation plays a crucial role in robotic manipulation for real-world data collection and remote robot control. Previous dexterous teleoperation mostly relies on hand retargeting to closely mimic human hand postures. However, these approaches may fail to fully leverage the inherent dexterity of dexterous hands, which can execute unique actions through their structural advantages compared to human hands. To address this limitation, we propose TypeTele, a type-guided dexterous teleoperation system, which enables dexterous hands to perform actions that are not constrained by human motion patterns. This is achieved by introducing dexterous manipulation types into the teleoperation system, allowing operators to employ appropriate types to complete specific tasks. To support this system, we build an extensible dexterous manipulation type library to cover comprehensive dexterous postures used in manipulation tasks. During teleoperation, we employ a MLLM (Multi-modality Large Language Model)-assisted type retrieval module to identify the most suitable manipulation type based on the specific task and operator commands. Extensive experiments of real-world teleoperation and imitation learning demonstrate that the incorporation of manipulation types significantly takes full advantage of the dexterous robot's ability to perform diverse and complex tasks with higher success rates.
<div id='section'>Paperid: <span id='pid'>122, <a href='https://arxiv.org/pdf/2507.01340.pdf' target='_blank'>https://arxiv.org/pdf/2507.01340.pdf</a></span>   <span><a href='https://github.com/cuongle1206/Phys-GRD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Cuong Le, Huy-Phuong Le, Duc Le, Minh-Thien Duong, Van-Binh Nguyen, My-Ha Le
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.01340">Physics-informed Ground Reaction Dynamics from Human Motion Capture</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Body dynamics are crucial information for the analysis of human motions in important research fields, ranging from biomechanics, sports science to computer vision and graphics. Modern approaches collect the body dynamics, external reactive force specifically, via force plates, synchronizing with human motion capture data, and learn to estimate the dynamics from a black-box deep learning model. Being specialized devices, force plates can only be installed in laboratory setups, imposing a significant limitation on the learning of human dynamics. To this end, we propose a novel method for estimating human ground reaction dynamics directly from the more reliable motion capture data with physics laws and computational simulation as constrains. We introduce a highly accurate and robust method for computing ground reaction forces from motion capture data using Euler's integration scheme and PD algorithm. The physics-based reactive forces are used to inform the learning model about the physics-informed motion dynamics thus improving the estimation accuracy. The proposed approach was tested on the GroundLink dataset, outperforming the baseline model on: 1) the ground reaction force estimation accuracy compared to the force plates measurement; and 2) our simulated root trajectory precision. The implementation code is available at https://github.com/cuongle1206/Phys-GRD
<div id='section'>Paperid: <span id='pid'>123, <a href='https://arxiv.org/pdf/2507.01012.pdf' target='_blank'>https://arxiv.org/pdf/2507.01012.pdf</a></span>   <span><a href='https://kongzhecn.github.io/projects/dam-vsr/' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/kongzhecn/DAM-VSR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhe Kong, Le Li, Yong Zhang, Feng Gao, Shaoshu Yang, Tao Wang, Kaihao Zhang, Zhuoliang Kang, Xiaoming Wei, Guanying Chen, Wenhan Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.01012">DAM-VSR: Disentanglement of Appearance and Motion for Video Super-Resolution</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Real-world video super-resolution (VSR) presents significant challenges due to complex and unpredictable degradations. Although some recent methods utilize image diffusion models for VSR and have shown improved detail generation capabilities, they still struggle to produce temporally consistent frames. We attempt to use Stable Video Diffusion (SVD) combined with ControlNet to address this issue. However, due to the intrinsic image-animation characteristics of SVD, it is challenging to generate fine details using only low-quality videos. To tackle this problem, we propose DAM-VSR, an appearance and motion disentanglement framework for VSR. This framework disentangles VSR into appearance enhancement and motion control problems. Specifically, appearance enhancement is achieved through reference image super-resolution, while motion control is achieved through video ControlNet. This disentanglement fully leverages the generative prior of video diffusion models and the detail generation capabilities of image super-resolution models. Furthermore, equipped with the proposed motion-aligned bidirectional sampling strategy, DAM-VSR can conduct VSR on longer input videos. DAM-VSR achieves state-of-the-art performance on real-world data and AIGC data, demonstrating its powerful detail generation capabilities.
<div id='section'>Paperid: <span id='pid'>124, <a href='https://arxiv.org/pdf/2507.00792.pdf' target='_blank'>https://arxiv.org/pdf/2507.00792.pdf</a></span>   <span><a href='https://github.com/hvoss-techfak/JAX-IK' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hendric Voss, Stefan Kopp
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.00792">JAX-IK: Real-Time Inverse Kinematics for Generating Multi-Constrained Movements of Virtual Human Characters</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating accurate and realistic virtual human movements in real-time is of high importance for a variety of applications in computer graphics, interactive virtual environments, robotics, and biomechanics. This paper introduces a novel real-time inverse kinematics (IK) solver specifically designed for realistic human-like movement generation. Leveraging the automatic differentiation and just-in-time compilation of TensorFlow, the proposed solver efficiently handles complex articulated human skeletons with high degrees of freedom. By treating forward and inverse kinematics as differentiable operations, our method effectively addresses common challenges such as error accumulation and complicated joint limits in multi-constrained problems, which are critical for realistic human motion modeling. We demonstrate the solver's effectiveness on the SMPLX human skeleton model, evaluating its performance against widely used iterative-based IK algorithms, like Cyclic Coordinate Descent (CCD), FABRIK, and the nonlinear optimization algorithm IPOPT. Our experiments cover both simple end-effector tasks and sophisticated, multi-constrained problems with realistic joint limits. Results indicate that our IK solver achieves real-time performance, exhibiting rapid convergence, minimal computational overhead per iteration, and improved success rates compared to existing methods. The project code is available at https://github.com/hvoss-techfak/JAX-IK
<div id='section'>Paperid: <span id='pid'>125, <a href='https://arxiv.org/pdf/2507.00472.pdf' target='_blank'>https://arxiv.org/pdf/2507.00472.pdf</a></span>   <span><a href='https://jinyugy21.github.io/ARIG/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ying Guo, Xi Liu, Cheng Zhen, Pengfei Yan, Xiaoming Wei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.00472">ARIG: Autoregressive Interactive Head Generation for Real-time Conversations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Face-to-face communication, as a common human activity, motivates the research on interactive head generation. A virtual agent can generate motion responses with both listening and speaking capabilities based on the audio or motion signals of the other user and itself. However, previous clip-wise generation paradigm or explicit listener/speaker generator-switching methods have limitations in future signal acquisition, contextual behavioral understanding, and switching smoothness, making it challenging to be real-time and realistic. In this paper, we propose an autoregressive (AR) based frame-wise framework called ARIG to realize the real-time generation with better interaction realism. To achieve real-time generation, we model motion prediction as a non-vector-quantized AR process. Unlike discrete codebook-index prediction, we represent motion distribution using diffusion procedure, achieving more accurate predictions in continuous space. To improve interaction realism, we emphasize interactive behavior understanding (IBU) and detailed conversational state understanding (CSU). In IBU, based on dual-track dual-modal signals, we summarize short-range behaviors through bidirectional-integrated learning and perform contextual understanding over long ranges. In CSU, we use voice activity signals and context features of IBU to understand the various states (interruption, feedback, pause, etc.) that exist in actual conversations. These serve as conditions for the final progressive motion prediction. Extensive experiments have verified the effectiveness of our model.
<div id='section'>Paperid: <span id='pid'>126, <a href='https://arxiv.org/pdf/2507.00273.pdf' target='_blank'>https://arxiv.org/pdf/2507.00273.pdf</a></span>   <span><a href='https://github.com/alvister88/og_bruce' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yusuke Tanaka, Alvin Zhu, Quanyou Wang, Dennis Hong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.00273">Mechanical Intelligence-Aware Curriculum Reinforcement Learning for Humanoids with Parallel Actuation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reinforcement learning (RL) has enabled advances in humanoid robot locomotion, yet most learning frameworks do not account for mechanical intelligence embedded in parallel actuation mechanisms due to limitations in simulator support for closed kinematic chains. This omission can lead to inaccurate motion modeling and suboptimal policies, particularly for robots with high actuation complexity. This paper presents general formulations and simulation methods for three types of parallel mechanisms: a differential pulley, a five-bar linkage, and a four-bar linkage, and trains a parallel-mechanism aware policy through an end-to-end curriculum RL framework for BRUCE, a kid-sized humanoid robot. Unlike prior approaches that rely on simplified serial approximations, we simulate all closed-chain constraints natively using GPU-accelerated MuJoCo (MJX), preserving the hardware's mechanical nonlinear properties during training. We benchmark our RL approach against a model predictive controller (MPC), demonstrating better surface generalization and performance in real-world zero-shot deployment. This work highlights the computational approaches and performance benefits of fully simulating parallel mechanisms in end-to-end learning pipelines for legged humanoids. Project codes with parallel mechanisms: https://github.com/alvister88/og_bruce
<div id='section'>Paperid: <span id='pid'>127, <a href='https://arxiv.org/pdf/2506.23690.pdf' target='_blank'>https://arxiv.org/pdf/2506.23690.pdf</a></span>   <span><a href='https://lucaria-academy.github.io/SynMotion/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuai Tan, Biao Gong, Yujie Wei, Shiwei Zhang, Zhuoxin Liu, Dandan Zheng, Jingdong Chen, Yan Wang, Hao Ouyang, Kecheng Zheng, Yujun Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.23690">SynMotion: Semantic-Visual Adaptation for Motion Customized Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion-based video motion customization facilitates the acquisition of human motion representations from a few video samples, while achieving arbitrary subjects transfer through precise textual conditioning. Existing approaches often rely on semantic-level alignment, expecting the model to learn new motion concepts and combine them with other entities (e.g., ''cats'' or ''dogs'') to produce visually appealing results. However, video data involve complex spatio-temporal patterns, and focusing solely on semantics cause the model to overlook the visual complexity of motion. Conversely, tuning only the visual representation leads to semantic confusion in representing the intended action. To address these limitations, we propose SynMotion, a new motion-customized video generation model that jointly leverages semantic guidance and visual adaptation. At the semantic level, we introduce the dual-embedding semantic comprehension mechanism which disentangles subject and motion representations, allowing the model to learn customized motion features while preserving its generative capabilities for diverse subjects. At the visual level, we integrate parameter-efficient motion adapters into a pre-trained video generation model to enhance motion fidelity and temporal coherence. Furthermore, we introduce a new embedding-specific training strategy which \textbf{alternately optimizes} subject and motion embeddings, supported by the manually constructed Subject Prior Video (SPV) training dataset. This strategy promotes motion specificity while preserving generalization across diverse subjects. Lastly, we introduce MotionBench, a newly curated benchmark with diverse motion patterns. Experimental results across both T2V and I2V settings demonstrate that \method outperforms existing baselines. Project page: https://lucaria-academy.github.io/SynMotion/
<div id='section'>Paperid: <span id='pid'>128, <a href='https://arxiv.org/pdf/2506.23236.pdf' target='_blank'>https://arxiv.org/pdf/2506.23236.pdf</a></span>   <span><a href='https://markomih.github.io/VolumetricSMPL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Marko Mihajlovic, Siwei Zhang, Gen Li, Kaifeng Zhao, Lea MÃ¼ller, Siyu Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.23236">VolumetricSMPL: A Neural Volumetric Body Model for Efficient Interactions, Contacts, and Collisions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Parametric human body models play a crucial role in computer graphics and vision, enabling applications ranging from human motion analysis to understanding human-environment interactions. Traditionally, these models use surface meshes, which pose challenges in efficiently handling interactions with other geometric entities, such as objects and scenes, typically represented as meshes or point clouds. To address this limitation, recent research has explored volumetric neural implicit body models. However, existing works are either insufficiently robust for complex human articulations or impose high computational and memory costs, limiting their widespread use. To this end, we introduce VolumetricSMPL, a neural volumetric body model that leverages Neural Blend Weights (NBW) to generate compact, yet efficient MLP decoders. Unlike prior approaches that rely on large MLPs, NBW dynamically blends a small set of learned weight matrices using predicted shape- and pose-dependent coefficients, significantly improving computational efficiency while preserving expressiveness. VolumetricSMPL outperforms prior volumetric occupancy model COAP with 10x faster inference, 6x lower GPU memory usage, enhanced accuracy, and a Signed Distance Function (SDF) for efficient and differentiable contact modeling. We demonstrate VolumetricSMPL's strengths across four challenging tasks: (1) reconstructing human-object interactions from in-the-wild images, (2) recovering human meshes in 3D scenes from egocentric views, (3) scene-constrained motion synthesis, and (4) resolving self-intersections. Our results highlight its broad applicability and significant performance and efficiency gains.
<div id='section'>Paperid: <span id='pid'>129, <a href='https://arxiv.org/pdf/2506.23135.pdf' target='_blank'>https://arxiv.org/pdf/2506.23135.pdf</a></span>   <span><a href='https://github.com/tsinghua-fib-lab/RoboScape' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Shang, Xin Zhang, Yinzhou Tang, Lei Jin, Chen Gao, Wei Wu, Yong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.23135">RoboScape: Physics-informed Embodied World Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>World models have become indispensable tools for embodied intelligence, serving as powerful simulators capable of generating realistic robotic videos while addressing critical data scarcity challenges. However, current embodied world models exhibit limited physical awareness, particularly in modeling 3D geometry and motion dynamics, resulting in unrealistic video generation for contact-rich robotic scenarios. In this paper, we present RoboScape, a unified physics-informed world model that jointly learns RGB video generation and physics knowledge within an integrated framework. We introduce two key physics-informed joint training tasks: temporal depth prediction that enhances 3D geometric consistency in video rendering, and keypoint dynamics learning that implicitly encodes physical properties (e.g., object shape and material characteristics) while improving complex motion modeling. Extensive experiments demonstrate that RoboScape generates videos with superior visual fidelity and physical plausibility across diverse robotic scenarios. We further validate its practical utility through downstream applications including robotic policy training with generated data and policy evaluation. Our work provides new insights for building efficient physics-informed world models to advance embodied intelligence research. The code is available at: https://github.com/tsinghua-fib-lab/RoboScape.
<div id='section'>Paperid: <span id='pid'>130, <a href='https://arxiv.org/pdf/2506.21249.pdf' target='_blank'>https://arxiv.org/pdf/2506.21249.pdf</a></span>   <span><a href='https://github.com/mengxianghan123/TR2C' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xianghan Meng, Zhengyu Tong, Zhiyuan Huang, Chun-Guang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.21249">Temporal Rate Reduction Clustering for Human Motion Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human Motion Segmentation (HMS), which aims to partition videos into non-overlapping human motions, has attracted increasing research attention recently. Existing approaches for HMS are mainly dominated by subspace clustering methods, which are grounded on the assumption that high-dimensional temporal data align with a Union-of-Subspaces (UoS) distribution. However, the frames in video capturing complex human motions with cluttered backgrounds may not align well with the UoS distribution. In this paper, we propose a novel approach for HMS, named Temporal Rate Reduction Clustering ($\text{TR}^2\text{C}$), which jointly learns structured representations and affinity to segment the sequences of frames in video. Specifically, the structured representations learned by $\text{TR}^2\text{C}$ enjoy temporally consistency and are aligned well with a UoS structure, which is favorable for addressing the HMS task. We conduct extensive experiments on five benchmark HMS datasets and achieve state-of-the-art performances with different feature extractors. The code is available at: https://github.com/mengxianghan123/TR2C.
<div id='section'>Paperid: <span id='pid'>131, <a href='https://arxiv.org/pdf/2506.19851.pdf' target='_blank'>https://arxiv.org/pdf/2506.19851.pdf</a></span>   <span><a href='https://anima-x.github.io/' target='_blank'>  GitHub</a></span> <span><a href='https://anima-x.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zehuan Huang, Haoran Feng, Yangtian Sun, Yuanchen Guo, Yanpei Cao, Lu Sheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.19851">AnimaX: Animating the Inanimate in 3D with Joint Video-Pose Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present AnimaX, a feed-forward 3D animation framework that bridges the motion priors of video diffusion models with the controllable structure of skeleton-based animation. Traditional motion synthesis methods are either restricted to fixed skeletal topologies or require costly optimization in high-dimensional deformation spaces. In contrast, AnimaX effectively transfers video-based motion knowledge to the 3D domain, supporting diverse articulated meshes with arbitrary skeletons. Our method represents 3D motion as multi-view, multi-frame 2D pose maps, and enables joint video-pose diffusion conditioned on template renderings and a textual motion prompt. We introduce shared positional encodings and modality-aware embeddings to ensure spatial-temporal alignment between video and pose sequences, effectively transferring video priors to motion generation task. The resulting multi-view pose sequences are triangulated into 3D joint positions and converted into mesh animation via inverse kinematics. Trained on a newly curated dataset of 160,000 rigged sequences, AnimaX achieves state-of-the-art results on VBench in generalization, motion fidelity, and efficiency, offering a scalable solution for category-agnostic 3D animation. Project page: \href{https://anima-x.github.io/}{https://anima-x.github.io/}.
<div id='section'>Paperid: <span id='pid'>132, <a href='https://arxiv.org/pdf/2506.18343.pdf' target='_blank'>https://arxiv.org/pdf/2506.18343.pdf</a></span>   <span><a href='https://github.com/kawser-ahmed-byte/TritonZ' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kawser Ahmed, Mir Shahriar Fardin, Md Arif Faysal Nayem, Fahim Hafiz, Swakkhar Shatabda
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.18343">TritonZ: A Remotely Operated Underwater Rover with Manipulator Arm for Exploration and Rescue Operations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The increasing demand for underwater exploration and rescue operations enforces the development of advanced wireless or semi-wireless underwater vessels equipped with manipulator arms. This paper presents the implementation of a semi-wireless underwater vehicle, "TritonZ" equipped with a manipulator arm, tailored for effective underwater exploration and rescue operations. The vehicle's compact design enables deployment in different submarine surroundings, addressing the need for wireless systems capable of navigating challenging underwater terrains. The manipulator arm can interact with the environment, allowing the robot to perform sophisticated tasks during exploration and rescue missions in emergency situations. TritonZ is equipped with various sensors such as Pi-Camera, Humidity, and Temperature sensors to send real-time environmental data. Our underwater vehicle controlled using a customized remote controller can navigate efficiently in the water where Pi-Camera enables live streaming of the surroundings. Motion control and video capture are performed simultaneously using this camera. The manipulator arm is designed to perform various tasks, similar to grasping, manipulating, and collecting underwater objects. Experimental results shows the efficacy of the proposed remotely operated vehicle in performing a variety of underwater exploration and rescue tasks. Additionally, the results show that TritonZ can maintain an average of 13.5cm/s with a minimal delay of 2-3 seconds. Furthermore, the vehicle can sustain waves underwater by maintaining its position as well as average velocity. The full project details and source code can be accessed at this link: https://github.com/kawser-ahmed-byte/TritonZ
<div id='section'>Paperid: <span id='pid'>133, <a href='https://arxiv.org/pdf/2506.14198.pdf' target='_blank'>https://arxiv.org/pdf/2506.14198.pdf</a></span>   <span><a href='https://amplify-robotics.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jeremy A. Collins, LorÃ¡nd Cheng, Kunal Aneja, Albert Wilcox, Benjamin Joffe, Animesh Garg
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.14198">AMPLIFY: Actionless Motion Priors for Robot Learning from Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Action-labeled data for robotics is scarce and expensive, limiting the generalization of learned policies. In contrast, vast amounts of action-free video data are readily available, but translating these observations into effective policies remains a challenge. We introduce AMPLIFY, a novel framework that leverages large-scale video data by encoding visual dynamics into compact, discrete motion tokens derived from keypoint trajectories. Our modular approach separates visual motion prediction from action inference, decoupling the challenges of learning what motion defines a task from how robots can perform it. We train a forward dynamics model on abundant action-free videos and an inverse dynamics model on a limited set of action-labeled examples, allowing for independent scaling. Extensive evaluations demonstrate that the learned dynamics are both accurate, achieving up to 3.7x better MSE and over 2.5x better pixel prediction accuracy compared to prior approaches, and broadly useful. In downstream policy learning, our dynamics predictions enable a 1.2-2.2x improvement in low-data regimes, a 1.4x average improvement by learning from action-free human videos, and the first generalization to LIBERO tasks from zero in-distribution action data. Beyond robotic control, we find the dynamics learned by AMPLIFY to be a versatile latent world model, enhancing video prediction quality. Our results present a novel paradigm leveraging heterogeneous data sources to build efficient, generalizable world models. More information can be found at https://amplify-robotics.github.io/.
<div id='section'>Paperid: <span id='pid'>134, <a href='https://arxiv.org/pdf/2506.12848.pdf' target='_blank'>https://arxiv.org/pdf/2506.12848.pdf</a></span>   <span><a href='https://github.com/EGO-False-Sleep/Miga25_track1' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/EGO-False-Sleep/Miga25_track1' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Xu, Lechao Cheng, Yaxiong Wang, Shengeng Tang, Zhun Zhong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.12848">Towards Fine-Grained Emotion Understanding via Skeleton-Based Micro-Gesture Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present our solution to the MiGA Challenge at IJCAI 2025, which aims to recognize micro-gestures (MGs) from skeleton sequences for the purpose of hidden emotion understanding. MGs are characterized by their subtlety, short duration, and low motion amplitude, making them particularly challenging to model and classify. We adopt PoseC3D as the baseline framework and introduce three key enhancements: (1) a topology-aware skeleton representation specifically designed for the iMiGUE dataset to better capture fine-grained motion patterns; (2) an improved temporal processing strategy that facilitates smoother and more temporally consistent motion modeling; and (3) the incorporation of semantic label embeddings as auxiliary supervision to improve the model generalization. Our method achieves a Top-1 accuracy of 67.01\% on the iMiGUE test set. As a result of these contributions, our approach ranks third on the official MiGA Challenge leaderboard. The source code is available at \href{https://github.com/EGO-False-Sleep/Miga25_track1}{https://github.com/EGO-False-Sleep/Miga25\_track1}.
<div id='section'>Paperid: <span id='pid'>135, <a href='https://arxiv.org/pdf/2506.08541.pdf' target='_blank'>https://arxiv.org/pdf/2506.08541.pdf</a></span>   <span><a href='https://traj-flow.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qi Yan, Brian Zhang, Yutong Zhang, Daniel Yang, Joshua White, Di Chen, Jiachao Liu, Langechuan Liu, Binnan Zhuang, Shaoshuai Shi, Renjie Liao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.08541">TrajFlow: Multi-modal Motion Prediction via Flow Matching</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Efficient and accurate motion prediction is crucial for ensuring safety and informed decision-making in autonomous driving, particularly under dynamic real-world conditions that necessitate multi-modal forecasts. We introduce TrajFlow, a novel flow matching-based motion prediction framework that addresses the scalability and efficiency challenges of existing generative trajectory prediction methods. Unlike conventional generative approaches that employ i.i.d. sampling and require multiple inference passes to capture diverse outcomes, TrajFlow predicts multiple plausible future trajectories in a single pass, significantly reducing computational overhead while maintaining coherence across predictions. Moreover, we propose a ranking loss based on the Plackett-Luce distribution to improve uncertainty estimation of predicted trajectories. Additionally, we design a self-conditioning training technique that reuses the model's own predictions to construct noisy inputs during a second forward pass, thereby improving generalization and accelerating inference. Extensive experiments on the large-scale Waymo Open Motion Dataset (WOMD) demonstrate that TrajFlow achieves state-of-the-art performance across various key metrics, underscoring its effectiveness for safety-critical autonomous driving applications. The code and other details are available on the project website https://traj-flow.github.io/.
<div id='section'>Paperid: <span id='pid'>136, <a href='https://arxiv.org/pdf/2506.07456.pdf' target='_blank'>https://arxiv.org/pdf/2506.07456.pdf</a></span>   <span><a href='http://yw0208.github.io/physiinter' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Yao, Yunlian Sun, Chang Liu, Hongwen Zhang, Jinhui Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.07456">PhysiInter: Integrating Physical Mapping for High-Fidelity Human Interaction Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Driven by advancements in motion capture and generative artificial intelligence, leveraging large-scale MoCap datasets to train generative models for synthesizing diverse, realistic human motions has become a promising research direction. However, existing motion-capture techniques and generative models often neglect physical constraints, leading to artifacts such as interpenetration, sliding, and floating. These issues are exacerbated in multi-person motion generation, where complex interactions are involved. To address these limitations, we introduce physical mapping, integrated throughout the human interaction generation pipeline. Specifically, motion imitation within a physics-based simulation environment is used to project target motions into a physically valid space. The resulting motions are adjusted to adhere to real-world physics constraints while retaining their original semantic meaning. This mapping not only improves MoCap data quality but also directly informs post-processing of generated motions. Given the unique interactivity of multi-person scenarios, we propose a tailored motion representation framework. Motion Consistency (MC) and Marker-based Interaction (MI) loss functions are introduced to improve model performance. Experiments show our method achieves impressive results in generated human motion quality, with a 3%-89% improvement in physical fidelity. Project page http://yw0208.github.io/physiinter
<div id='section'>Paperid: <span id='pid'>137, <a href='https://arxiv.org/pdf/2506.07216.pdf' target='_blank'>https://arxiv.org/pdf/2506.07216.pdf</a></span>   <span><a href='https://github.com/NadaAbodeshish/Random-Cropping-augmentation-HGR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Nada Aboudeshish, Dmitry Ignatov, Radu Timofte
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.07216">AugmentGest: Can Random Data Cropping Augmentation Boost Gesture Recognition Performance?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Data augmentation is a crucial technique in deep learning, particularly for tasks with limited dataset diversity, such as skeleton-based datasets. This paper proposes a comprehensive data augmentation framework that integrates geometric transformations, random cropping, rotation, zooming and intensity-based transformations, brightness and contrast adjustments to simulate real-world variations. Random cropping ensures the preservation of spatio-temporal integrity while addressing challenges such as viewpoint bias and occlusions. The augmentation pipeline generates three augmented versions for each sample in addition to the data set sample, thus quadrupling the data set size and enriching the diversity of gesture representations. The proposed augmentation strategy is evaluated on three models: multi-stream e2eET, FPPR point cloud-based hand gesture recognition (HGR), and DD-Network. Experiments are conducted on benchmark datasets including DHG14/28, SHREC'17, and JHMDB. The e2eET model, recognized as the state-of-the-art for hand gesture recognition on DHG14/28 and SHREC'17. The FPPR-PCD model, the second-best performing model on SHREC'17, excels in point cloud-based gesture recognition. DD-Net, a lightweight and efficient architecture for skeleton-based action recognition, is evaluated on SHREC'17 and the Human Motion Data Base (JHMDB). The results underline the effectiveness and versatility of the proposed augmentation strategy, significantly improving model generalization and robustness across diverse datasets and architectures. This framework not only establishes state-of-the-art results on all three evaluated models but also offers a scalable solution to advance HGR and action recognition applications in real-world scenarios. The framework is available at https://github.com/NadaAbodeshish/Random-Cropping-augmentation-HGR
<div id='section'>Paperid: <span id='pid'>138, <a href='https://arxiv.org/pdf/2506.03408.pdf' target='_blank'>https://arxiv.org/pdf/2506.03408.pdf</a></span>   <span><a href='https://github.com/colorfulfuture/Awesome-Trajectory-Motion-Prediction-Papers' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi Xu, Ruining Yang, Yitian Zhang, Yizhou Wang, Jianglin Lu, Mingyuan Zhang, Lili Su, Yun Fu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.03408">Trajectory Prediction Meets Large Language Models: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in large language models (LLMs) have sparked growing interest in integrating language-driven techniques into trajectory prediction. By leveraging their semantic and reasoning capabilities, LLMs are reshaping how autonomous systems perceive, model, and predict trajectories. This survey provides a comprehensive overview of this emerging field, categorizing recent work into five directions: (1) Trajectory prediction via language modeling paradigms, (2) Direct trajectory prediction with pretrained language models, (3) Language-guided scene understanding for trajectory prediction, (4) Language-driven data generation for trajectory prediction, (5) Language-based reasoning and interpretability for trajectory prediction. For each, we analyze representative methods, highlight core design choices, and identify open challenges. This survey bridges natural language processing and trajectory prediction, offering a unified perspective on how language can enrich trajectory prediction.
<div id='section'>Paperid: <span id='pid'>139, <a href='https://arxiv.org/pdf/2506.02845.pdf' target='_blank'>https://arxiv.org/pdf/2506.02845.pdf</a></span>   <span><a href='https://github.com/LEI-QI-233/HAR-in-Space' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/LEI-QI-233/HAR-in-Space' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Di Wen, Lei Qi, Kunyu Peng, Kailun Yang, Fei Teng, Ao Luo, Jia Fu, Yufan Chen, Ruiping Liu, Yitian Shi, M. Saquib Sarfraz, Rainer Stiefelhagen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.02845">Go Beyond Earth: Understanding Human Actions and Scenes in Microgravity Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite substantial progress in video understanding, most existing datasets are limited to Earth's gravitational conditions. However, microgravity alters human motion, interactions, and visual semantics, revealing a critical gap for real-world vision systems. This presents a challenge for domain-robust video understanding in safety-critical space applications. To address this, we introduce MicroG-4M, the first benchmark for spatio-temporal and semantic understanding of human activities in microgravity. Constructed from real-world space missions and cinematic simulations, the dataset includes 4,759 clips covering 50 actions, 1,238 context-rich captions, and over 7,000 question-answer pairs on astronaut activities and scene understanding. MicroG-4M supports three core tasks: fine-grained multi-label action recognition, temporal video captioning, and visual question answering, enabling a comprehensive evaluation of both spatial localization and semantic reasoning in microgravity contexts. We establish baselines using state-of-the-art models. All data, annotations, and code are available at https://github.com/LEI-QI-233/HAR-in-Space.
<div id='section'>Paperid: <span id='pid'>140, <a href='https://arxiv.org/pdf/2506.01608.pdf' target='_blank'>https://arxiv.org/pdf/2506.01608.pdf</a></span>   <span><a href='https://github.com/amathislab/EPFL-Smart-Kitchen' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Andy Bonnetto, Haozhe Qi, Franklin Leong, Matea Tashkovska, Mahdi Rad, Solaiman Shokur, Friedhelm Hummel, Silvestro Micera, Marc Pollefeys, Alexander Mathis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.01608">EPFL-Smart-Kitchen-30: Densely annotated cooking dataset with 3D kinematics to challenge video and language models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding behavior requires datasets that capture humans while carrying out complex tasks. The kitchen is an excellent environment for assessing human motor and cognitive function, as many complex actions are naturally exhibited in kitchens from chopping to cleaning. Here, we introduce the EPFL-Smart-Kitchen-30 dataset, collected in a noninvasive motion capture platform inside a kitchen environment. Nine static RGB-D cameras, inertial measurement units (IMUs) and one head-mounted HoloLens~2 headset were used to capture 3D hand, body, and eye movements. The EPFL-Smart-Kitchen-30 dataset is a multi-view action dataset with synchronized exocentric, egocentric, depth, IMUs, eye gaze, body and hand kinematics spanning 29.7 hours of 16 subjects cooking four different recipes. Action sequences were densely annotated with 33.78 action segments per minute. Leveraging this multi-modal dataset, we propose four benchmarks to advance behavior understanding and modeling through 1) a vision-language benchmark, 2) a semantic text-to-motion generation benchmark, 3) a multi-modal action recognition benchmark, 4) a pose-based action segmentation benchmark. We expect the EPFL-Smart-Kitchen-30 dataset to pave the way for better methods as well as insights to understand the nature of ecologically-valid human behavior. Code and data are available at https://github.com/amathislab/EPFL-Smart-Kitchen
<div id='section'>Paperid: <span id='pid'>141, <a href='https://arxiv.org/pdf/2506.00173.pdf' target='_blank'>https://arxiv.org/pdf/2506.00173.pdf</a></span>   <span><a href='https://motionpersona25.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingyi Shi, Wei Liu, Jidong Mei, Wangpok Tse, Rui Chen, Xuelin Chen, Taku Komura
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.00173">MotionPersona: Characteristics-aware Locomotion Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present MotionPersona, a novel real-time character controller that allows users to characterize a character by specifying attributes such as physical traits, mental states, and demographics, and projects these properties into the generated motions for animating the character. In contrast to existing deep learning-based controllers, which typically produce homogeneous animations tailored to a single, predefined character, MotionPersona accounts for the impact of various traits on human motion as observed in the real world. To achieve this, we develop a block autoregressive motion diffusion model conditioned on SMPLX parameters, textual prompts, and user-defined locomotion control signals. We also curate a comprehensive dataset featuring a wide range of locomotion types and actor traits to enable the training of this characteristic-aware controller. Unlike prior work, MotionPersona is the first method capable of generating motion that faithfully reflects user-specified characteristics (e.g., an elderly person's shuffling gait) while responding in real time to dynamic control inputs. Additionally, we introduce a few-shot characterization technique as a complementary conditioning mechanism, enabling customization via short motion clips when language prompts fall short. Through extensive experiments, we demonstrate that MotionPersona outperforms existing methods in characteristics-aware locomotion control, achieving superior motion quality and diversity. Results, code, and demo can be found at: https://motionpersona25.github.io/.
<div id='section'>Paperid: <span id='pid'>142, <a href='https://arxiv.org/pdf/2505.22944.pdf' target='_blank'>https://arxiv.org/pdf/2505.22944.pdf</a></span>   <span><a href='https://anytraj.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Angtian Wang, Haibin Huang, Jacob Zhiyuan Fang, Yiding Yang, Chongyang Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.22944">ATI: Any Trajectory Instruction for Controllable Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a unified framework for motion control in video generation that seamlessly integrates camera movement, object-level translation, and fine-grained local motion using trajectory-based inputs. In contrast to prior methods that address these motion types through separate modules or task-specific designs, our approach offers a cohesive solution by projecting user-defined trajectories into the latent space of pre-trained image-to-video generation models via a lightweight motion injector. Users can specify keypoints and their motion paths to control localized deformations, entire object motion, virtual camera dynamics, or combinations of these. The injected trajectory signals guide the generative process to produce temporally consistent and semantically aligned motion sequences. Our framework demonstrates superior performance across multiple video motion control tasks, including stylized motion effects (e.g., motion brushes), dynamic viewpoint changes, and precise local motion manipulation. Experiments show that our method provides significantly better controllability and visual quality compared to prior approaches and commercial solutions, while remaining broadly compatible with various state-of-the-art video generation backbones. Project page: https://anytraj.github.io/.
<div id='section'>Paperid: <span id='pid'>143, <a href='https://arxiv.org/pdf/2505.20287.pdf' target='_blank'>https://arxiv.org/pdf/2505.20287.pdf</a></span>   <span><a href='https://zhw-zhang.github.io/MotionPro-page/' target='_blank'>  GitHub</a></span> <span><a href='https://zhw-zhang.github.io/MotionPro-page/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhongwei Zhang, Fuchen Long, Zhaofan Qiu, Yingwei Pan, Wu Liu, Ting Yao, Tao Mei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.20287">MotionPro: A Precise Motion Controller for Image-to-Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Animating images with interactive motion control has garnered popularity for image-to-video (I2V) generation. Modern approaches typically rely on large Gaussian kernels to extend motion trajectories as condition without explicitly defining movement region, leading to coarse motion control and failing to disentangle object and camera moving. To alleviate these, we present MotionPro, a precise motion controller that novelly leverages region-wise trajectory and motion mask to regulate fine-grained motion synthesis and identify target motion category (i.e., object or camera moving), respectively. Technically, MotionPro first estimates the flow maps on each training video via a tracking model, and then samples the region-wise trajectories to simulate inference scenario. Instead of extending flow through large Gaussian kernels, our region-wise trajectory approach enables more precise control by directly utilizing trajectories within local regions, thereby effectively characterizing fine-grained movements. A motion mask is simultaneously derived from the predicted flow maps to capture the holistic motion dynamics of the movement regions. To pursue natural motion control, MotionPro further strengthens video denoising by incorporating both region-wise trajectories and motion mask through feature modulation. More remarkably, we meticulously construct a benchmark, i.e., MC-Bench, with 1.1K user-annotated image-trajectory pairs, for the evaluation of both fine-grained and object-level I2V motion control. Extensive experiments conducted on WebVid-10M and MC-Bench demonstrate the effectiveness of MotionPro. Please refer to our project page for more results: https://zhw-zhang.github.io/MotionPro-page/.
<div id='section'>Paperid: <span id='pid'>144, <a href='https://arxiv.org/pdf/2505.20255.pdf' target='_blank'>https://arxiv.org/pdf/2505.20255.pdf</a></span>   <span><a href='https://github.com/MyNiuuu/AniCrafter' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/MyNiuuu/AniCrafter' target='_blank'>  GitHub</a></span> <span><a href='https://myniuuu.github.io/AniCrafter' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Muyao Niu, Mingdeng Cao, Yifan Zhan, Qingtian Zhu, Mingze Ma, Jiancheng Zhao, Yanhong Zeng, Zhihang Zhong, Xiao Sun, Yinqiang Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.20255">AniCrafter: Customizing Realistic Human-Centric Animation via Avatar-Background Conditioning in Video Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in video diffusion models have significantly improved character animation techniques. However, current approaches rely on basic structural conditions such as DWPose or SMPL-X to animate character images, limiting their effectiveness in open-domain scenarios with dynamic backgrounds or challenging human poses. In this paper, we introduce \textbf{AniCrafter}, a diffusion-based human-centric animation model that can seamlessly integrate and animate a given character into open-domain dynamic backgrounds while following given human motion sequences. Built on cutting-edge Image-to-Video (I2V) diffusion architectures, our model incorporates an innovative ''avatar-background'' conditioning mechanism that reframes open-domain human-centric animation as a restoration task, enabling more stable and versatile animation outputs. Experimental results demonstrate the superior performance of our method. Codes are available at https://github.com/MyNiuuu/AniCrafter.
<div id='section'>Paperid: <span id='pid'>145, <a href='https://arxiv.org/pdf/2505.19742.pdf' target='_blank'>https://arxiv.org/pdf/2505.19742.pdf</a></span>   <span><a href='https://github.com/gobunu/HAODiff' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/gobunu/HAODiff' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jue Gong, Tingyu Yang, Jingkai Wang, Zheng Chen, Xing Liu, Hong Gu, Yulun Zhang, Xiaokang Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.19742">HAODiff: Human-Aware One-Step Diffusion via Dual-Prompt Guidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human-centered images often suffer from severe generic degradation during transmission and are prone to human motion blur (HMB), making restoration challenging. Existing research lacks sufficient focus on these issues, as both problems often coexist in practice. To address this, we design a degradation pipeline that simulates the coexistence of HMB and generic noise, generating synthetic degraded data to train our proposed HAODiff, a human-aware one-step diffusion. Specifically, we propose a triple-branch dual-prompt guidance (DPG), which leverages high-quality images, residual noise (LQ minus HQ), and HMB segmentation masks as training targets. It produces a positive-negative prompt pair for classifier-free guidance (CFG) in a single diffusion step. The resulting adaptive dual prompts let HAODiff exploit CFG more effectively, boosting robustness against diverse degradations. For fair evaluation, we introduce MPII-Test, a benchmark rich in combined noise and HMB cases. Extensive experiments show that our HAODiff surpasses existing state-of-the-art (SOTA) methods in terms of both quantitative metrics and visual quality on synthetic and real-world datasets, including our introduced MPII-Test. Code is available at: https://github.com/gobunu/HAODiff.
<div id='section'>Paperid: <span id='pid'>146, <a href='https://arxiv.org/pdf/2505.18229.pdf' target='_blank'>https://arxiv.org/pdf/2505.18229.pdf</a></span>   <span><a href='https://github.com/lostwolves/BEDI' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingning Guo, Mengwei Wu, Jiarun He, Shaoxian Li, Haifeng Li, Chao Tao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.18229">BEDI: A Comprehensive Benchmark for Evaluating Embodied Agents on UAVs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapid advancement of low-altitude remote sensing and Vision-Language Models (VLMs), Embodied Agents based on Unmanned Aerial Vehicles (UAVs) have shown significant potential in autonomous tasks. However, current evaluation methods for UAV-Embodied Agents (UAV-EAs) remain constrained by the lack of standardized benchmarks, diverse testing scenarios and open system interfaces. To address these challenges, we propose BEDI (Benchmark for Embodied Drone Intelligence), a systematic and standardized benchmark designed for evaluating UAV-EAs. Specifically, we introduce a novel Dynamic Chain-of-Embodied-Task paradigm based on the perception-decision-action loop, which decomposes complex UAV tasks into standardized, measurable subtasks. Building on this paradigm, we design a unified evaluation framework encompassing five core sub-skills: semantic perception, spatial perception, motion control, tool utilization, and task planning. Furthermore, we construct a hybrid testing platform that integrates static real-world environments with dynamic virtual scenarios, enabling comprehensive performance assessment of UAV-EAs across varied contexts. The platform also offers open and standardized interfaces, allowing researchers to customize tasks and extend scenarios, thereby enhancing flexibility and scalability in the evaluation process. Finally, through empirical evaluations of several state-of-the-art (SOTA) VLMs, we reveal their limitations in embodied UAV tasks, underscoring the critical role of the BEDI benchmark in advancing embodied intelligence research and model optimization. By filling the gap in systematic and standardized evaluation within this field, BEDI facilitates objective model comparison and lays a robust foundation for future development in this field. Our benchmark will be released at https://github.com/lostwolves/BEDI .
<div id='section'>Paperid: <span id='pid'>147, <a href='https://arxiv.org/pdf/2505.17860.pdf' target='_blank'>https://arxiv.org/pdf/2505.17860.pdf</a></span>   <span><a href='http://wenningxu.github.io/multicharacter/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenning Xu, Shiyu Fan, Paul Henderson, Edmond S. L. Ho
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.17860">Multi-Person Interaction Generation from Two-Person Motion Priors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating realistic human motion with high-level controls is a crucial task for social understanding, robotics, and animation. With high-quality MOCAP data becoming more available recently, a wide range of data-driven approaches have been presented. However, modelling multi-person interactions still remains a less explored area. In this paper, we present Graph-driven Interaction Sampling, a method that can generate realistic and diverse multi-person interactions by leveraging existing two-person motion diffusion models as motion priors. Instead of training a new model specific to multi-person interaction synthesis, our key insight is to spatially and temporally separate complex multi-person interactions into a graph structure of two-person interactions, which we name the Pairwise Interaction Graph. We thus decompose the generation task into simultaneous single-person motion generation conditioned on one other's motion. In addition, to reduce artifacts such as interpenetrations of body parts in generated multi-person interactions, we introduce two graph-dependent guidance terms into the diffusion sampling scheme. Unlike previous work, our method can produce various high-quality multi-person interactions without having repetitive individual motions. Extensive experiments demonstrate that our approach consistently outperforms existing methods in reducing artifacts when generating a wide range of two-person and multi-person interactions.
<div id='section'>Paperid: <span id='pid'>148, <a href='https://arxiv.org/pdf/2505.14866.pdf' target='_blank'>https://arxiv.org/pdf/2505.14866.pdf</a></span>   <span><a href='https://nisarganc.github.io/UPTor-page/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Nisarga Nilavadi, Andrey Rudenko, Timm Linder
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.14866">UPTor: Unified 3D Human Pose Dynamics and Trajectory Prediction for Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a unified approach to forecast the dynamics of human keypoints along with the motion trajectory based on a short sequence of input poses. While many studies address either full-body pose prediction or motion trajectory prediction, only a few attempt to merge them. We propose a motion transformation technique to simultaneously predict full-body pose and trajectory key-points in a global coordinate frame. We utilize an off-the-shelf 3D human pose estimation module, a graph attention network to encode the skeleton structure, and a compact, non-autoregressive transformer suitable for real-time motion prediction for human-robot interaction and human-aware navigation. We introduce a human navigation dataset ``DARKO'' with specific focus on navigational activities that are relevant for human-aware mobile robot navigation. We perform extensive evaluation on Human3.6M, CMU-Mocap, and our DARKO dataset. In comparison to prior work, we show that our approach is compact, real-time, and accurate in predicting human navigation motion across all datasets. Result animations, our dataset, and code will be available at https://nisarganc.github.io/UPTor-page/
<div id='section'>Paperid: <span id='pid'>149, <a href='https://arxiv.org/pdf/2505.13921.pdf' target='_blank'>https://arxiv.org/pdf/2505.13921.pdf</a></span>   <span><a href='https://github.com/hwj20/APEX_EXP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wanjing Huang, Weixiang Yan, Zhen Zhang, Ambuj Singh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.13921">APEX: Empowering LLMs with Physics-Based Task Planning for Real-time Insight</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs) demonstrate strong reasoning and task planning capabilities but remain fundamentally limited in physical interaction modeling. Existing approaches integrate perception via Vision-Language Models (VLMs) or adaptive decision-making through Reinforcement Learning (RL), but they fail to capture dynamic object interactions or require task-specific training, limiting their real-world applicability. We introduce APEX (Anticipatory Physics-Enhanced Execution), a framework that equips LLMs with physics-driven foresight for real-time task planning. APEX constructs structured graphs to identify and model the most relevant dynamic interactions in the environment, providing LLMs with explicit physical state updates. Simultaneously, APEX provides low-latency forward simulations of physically feasible actions, allowing LLMs to select optimal strategies based on predictive outcomes rather than static observations. We evaluate APEX on three benchmarks designed to assess perception, prediction, and decision-making: (1) Physics Reasoning Benchmark, testing causal inference and object motion prediction; (2) Tetris, evaluating whether physics-informed prediction enhances decision-making performance in long-horizon planning tasks; (3) Dynamic Obstacle Avoidance, assessing the immediate integration of perception and action feasibility analysis. APEX significantly outperforms standard LLMs and VLM-based models, demonstrating the necessity of explicit physics reasoning for bridging the gap between language-based intelligence and real-world task execution. The source code and experiment setup are publicly available at https://github.com/hwj20/APEX_EXP .
<div id='section'>Paperid: <span id='pid'>150, <a href='https://arxiv.org/pdf/2505.11013.pdf' target='_blank'>https://arxiv.org/pdf/2505.11013.pdf</a></span>   <span><a href='https://github.com/zzysteve/MoMADiff' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zongye Zhang, Bohan Kong, Qingjie Liu, Yunhong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.11013">Towards Robust and Controllable Text-to-Motion via Masked Autoregressive Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating 3D human motion from text descriptions remains challenging due to the diverse and complex nature of human motion. While existing methods excel within the training distribution, they often struggle with out-of-distribution motions, limiting their applicability in real-world scenarios. Existing VQVAE-based methods often fail to represent novel motions faithfully using discrete tokens, which hampers their ability to generalize beyond seen data. Meanwhile, diffusion-based methods operating on continuous representations often lack fine-grained control over individual frames. To address these challenges, we propose a robust motion generation framework MoMADiff, which combines masked modeling with diffusion processes to generate motion using frame-level continuous representations. Our model supports flexible user-provided keyframe specification, enabling precise control over both spatial and temporal aspects of motion synthesis. MoMADiff demonstrates strong generalization capability on novel text-to-motion datasets with sparse keyframes as motion prompts. Extensive experiments on two held-out datasets and two standard benchmarks show that our method consistently outperforms state-of-the-art models in motion quality, instruction fidelity, and keyframe adherence. The code is available at: https://github.com/zzysteve/MoMADiff
<div id='section'>Paperid: <span id='pid'>151, <a href='https://arxiv.org/pdf/2505.09731.pdf' target='_blank'>https://arxiv.org/pdf/2505.09731.pdf</a></span>   <span><a href='https://scalingforce.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>William Xie, Max Conway, Yutong Zhang, Nikolaus Correll
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.09731">Unfettered Forceful Skill Acquisition with Physical Reasoning and Coordinate Frame Labeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision language models (VLMs) exhibit vast knowledge of the physical world, including intuition of physical and spatial properties, affordances, and motion. With fine-tuning, VLMs can also natively produce robot trajectories. We demonstrate that eliciting wrenches, not trajectories, allows VLMs to explicitly reason about forces and leads to zero-shot generalization in a series of manipulation tasks without pretraining. We achieve this by overlaying a consistent visual representation of relevant coordinate frames on robot-attached camera images to augment our query. First, we show how this addition enables a versatile motion control framework evaluated across four tasks (opening and closing a lid, pushing a cup or chair) spanning prismatic and rotational motion, an order of force and position magnitude, different camera perspectives, annotation schemes, and two robot platforms over 220 experiments, resulting in 51% success across the four tasks. Then, we demonstrate that the proposed framework enables VLMs to continually reason about interaction feedback to recover from task failure or incompletion, with and without human supervision. Finally, we observe that prompting schemes with visual annotation and embodied reasoning can bypass VLM safeguards. We characterize prompt component contribution to harmful behavior elicitation and discuss its implications for developing embodied reasoning. Our code, videos, and data are available at: https://scalingforce.github.io/.
<div id='section'>Paperid: <span id='pid'>152, <a href='https://arxiv.org/pdf/2505.09074.pdf' target='_blank'>https://arxiv.org/pdf/2505.09074.pdf</a></span>   <span><a href='https://trends-in-motion-prediction-2025.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Letian Wang, Marc-Antoine Lavoie, Sandro Papais, Barza Nisar, Yuxiao Chen, Wenhao Ding, Boris Ivanovic, Hao Shao, Abulikemu Abuduweili, Evan Cook, Yang Zhou, Peter Karkus, Jiachen Li, Changliu Liu, Marco Pavone, Steven Waslander
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.09074">Trends in Motion Prediction Toward Deployable and Generalizable Autonomy: A Revisit and Perspectives</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion prediction, the anticipation of future agent states or scene evolution, is rooted in human cognition, bridging perception and decision-making. It enables intelligent systems, such as robots and self-driving cars, to act safely in dynamic, human-involved environments, and informs broader time-series reasoning challenges. With advances in methods, representations, and datasets, the field has seen rapid progress, reflected in quickly evolving benchmark results. Yet, when state-of-the-art methods are deployed in the real world, they often struggle to generalize to open-world conditions and fall short of deployment standards. This reveals a gap between research benchmarks, which are often idealized or ill-posed, and real-world complexity.
  To address this gap, this survey revisits the generalization and deployability of motion prediction models, with an emphasis on the applications of robotics, autonomous driving, and human motion. We first offer a comprehensive taxonomy of motion prediction methods, covering representations, modeling strategies, application domains, and evaluation protocols. We then study two key challenges: (1) how to push motion prediction models to be deployable to realistic deployment standards, where motion prediction does not act in a vacuum, but functions as one module of closed-loop autonomy stacks - it takes input from the localization and perception, and informs downstream planning and control. 2) how to generalize motion prediction models from limited seen scenarios/datasets to the open-world settings. Throughout the paper, we highlight critical open challenges to guide future work, aiming to recalibrate the community's efforts, fostering progress that is not only measurable but also meaningful for real-world applications. The project webpage corresponding to this paper can be found here https://trends-in-motion-prediction-2025.github.io/.
<div id='section'>Paperid: <span id='pid'>153, <a href='https://arxiv.org/pdf/2505.07539.pdf' target='_blank'>https://arxiv.org/pdf/2505.07539.pdf</a></span>   <span><a href='https://xdimlab.github.io/GIFStream' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Li, Sicheng Li, Xiang Gao, Abudouaihati Batuer, Lu Yu, Yiyi Liao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.07539">GIFStream: 4D Gaussian-based Immersive Video with Feature Stream</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Immersive video offers a 6-Dof-free viewing experience, potentially playing a key role in future video technology. Recently, 4D Gaussian Splatting has gained attention as an effective approach for immersive video due to its high rendering efficiency and quality, though maintaining quality with manageable storage remains challenging. To address this, we introduce GIFStream, a novel 4D Gaussian representation using a canonical space and a deformation field enhanced with time-dependent feature streams. These feature streams enable complex motion modeling and allow efficient compression by leveraging temporal correspondence and motion-aware pruning. Additionally, we incorporate both temporal and spatial compression networks for end-to-end compression. Experimental results show that GIFStream delivers high-quality immersive video at 30 Mbps, with real-time rendering and fast decoding on an RTX 4090. Project page: https://xdimlab.github.io/GIFStream
<div id='section'>Paperid: <span id='pid'>154, <a href='https://arxiv.org/pdf/2505.07096.pdf' target='_blank'>https://arxiv.org/pdf/2505.07096.pdf</a></span>   <span><a href='https://portal-cornell.github.io/X-Sim/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Prithwish Dan, Kushal Kedia, Angela Chao, Edward Weiyi Duan, Maximus Adrian Pace, Wei-Chiu Ma, Sanjiban Choudhury
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.07096">X-Sim: Cross-Embodiment Learning via Real-to-Sim-to-Real</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human videos offer a scalable way to train robot manipulation policies, but lack the action labels needed by standard imitation learning algorithms. Existing cross-embodiment approaches try to map human motion to robot actions, but often fail when the embodiments differ significantly. We propose X-Sim, a real-to-sim-to-real framework that uses object motion as a dense and transferable signal for learning robot policies. X-Sim starts by reconstructing a photorealistic simulation from an RGBD human video and tracking object trajectories to define object-centric rewards. These rewards are used to train a reinforcement learning (RL) policy in simulation. The learned policy is then distilled into an image-conditioned diffusion policy using synthetic rollouts rendered with varied viewpoints and lighting. To transfer to the real world, X-Sim introduces an online domain adaptation technique that aligns real and simulated observations during deployment. Importantly, X-Sim does not require any robot teleoperation data. We evaluate it across 5 manipulation tasks in 2 environments and show that it: (1) improves task progress by 30% on average over hand-tracking and sim-to-real baselines, (2) matches behavior cloning with 10x less data collection time, and (3) generalizes to new camera viewpoints and test-time changes. Code and videos are available at https://portal-cornell.github.io/X-Sim/.
<div id='section'>Paperid: <span id='pid'>155, <a href='https://arxiv.org/pdf/2505.05638.pdf' target='_blank'>https://arxiv.org/pdf/2505.05638.pdf</a></span>   <span><a href='https://github.com/continental/pred2plan' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohamed-Khalil Bouzidi, Christian Schlauch, Nicole Scheuerer, Yue Yao, Nadja Klein, Daniel GÃ¶hring, JÃ¶rg Reichardt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.05638">Closing the Loop: Motion Prediction Models beyond Open-Loop Benchmarks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fueled by motion prediction competitions and benchmarks, recent years have seen the emergence of increasingly large learning based prediction models, many with millions of parameters, focused on improving open-loop prediction accuracy by mere centimeters. However, these benchmarks fail to assess whether such improvements translate to better performance when integrated into an autonomous driving stack. In this work, we systematically evaluate the interplay between state-of-the-art motion predictors and motion planners. Our results show that higher open-loop accuracy does not always correlate with better closed-loop driving behavior and that other factors, such as temporal consistency of predictions and planner compatibility, also play a critical role. Furthermore, we investigate downsized variants of these models, and, surprisingly, find that in some cases models with up to 86% fewer parameters yield comparable or even superior closed-loop driving performance. Our code is available at https://github.com/continental/pred2plan.
<div id='section'>Paperid: <span id='pid'>156, <a href='https://arxiv.org/pdf/2505.05010.pdf' target='_blank'>https://arxiv.org/pdf/2505.05010.pdf</a></span>   <span><a href='https://xinyu-yi.github.io/GlobalPose/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinyu Yi, Shaohua Pan, Feng Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.05010">Improving Global Motion Estimation in Sparse IMU-based Motion Capture with Physics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>By learning human motion priors, motion capture can be achieved by 6 inertial measurement units (IMUs) in recent years with the development of deep learning techniques, even though the sensor inputs are sparse and noisy. However, human global motions are still challenging to be reconstructed by IMUs. This paper aims to solve this problem by involving physics. It proposes a physical optimization scheme based on multiple contacts to enable physically plausible translation estimation in the full 3D space where the z-directional motion is usually challenging for previous works. It also considers gravity in local pose estimation which well constrains human global orientations and refines local pose estimation in a joint estimation manner. Experiments demonstrate that our method achieves more accurate motion capture for both local poses and global motions. Furthermore, by deeply integrating physics, we can also estimate 3D contact, contact forces, joint torques, and interacting proxy surfaces.
<div id='section'>Paperid: <span id='pid'>157, <a href='https://arxiv.org/pdf/2505.04974.pdf' target='_blank'>https://arxiv.org/pdf/2505.04974.pdf</a></span>   <span><a href='https://wengwanjiang.github.io/ReAlign-page/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wanjiang Weng, Xiaofeng Tan, Hongsong Wang, Pan Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.04974">ReAlign: Bilingual Text-to-Motion Generation via Step-Aware Reward-Guided Alignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Bilingual text-to-motion generation, which synthesizes 3D human motions from bilingual text inputs, holds immense potential for cross-linguistic applications in gaming, film, and robotics. However, this task faces critical challenges: the absence of bilingual motion-language datasets and the misalignment between text and motion distributions in diffusion models, leading to semantically inconsistent or low-quality motions. To address these challenges, we propose BiHumanML3D, a novel bilingual human motion dataset, which establishes a crucial benchmark for bilingual text-to-motion generation models. Furthermore, we propose a Bilingual Motion Diffusion model (BiMD), which leverages cross-lingual aligned representations to capture semantics, thereby achieving a unified bilingual model. Building upon this, we propose Reward-guided sampling Alignment (ReAlign) method, comprising a step-aware reward model to assess alignment quality during sampling and a reward-guided strategy that directs the diffusion process toward an optimally aligned distribution. This reward model integrates step-aware tokens and combines a text-aligned module for semantic consistency and a motion-aligned module for realism, refining noisy motions at each timestep to balance probability density and alignment. Experiments demonstrate that our approach significantly improves text-motion alignment and motion quality compared to existing state-of-the-art methods. Project page: https://wengwanjiang.github.io/ReAlign-page/.
<div id='section'>Paperid: <span id='pid'>158, <a href='https://arxiv.org/pdf/2505.01182.pdf' target='_blank'>https://arxiv.org/pdf/2505.01182.pdf</a></span>   <span><a href='https://tstmotion.github.io/' target='_blank'>  GitHub</a></span> <span><a href='https://tstmotion.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyan Guo, Haoxuan Qu, Hossein Rahmani, Dewen Soh, Ping Hu, Qiuhong Ke, Jun Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.01182">TSTMotion: Training-free Scene-aware Text-to-motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-to-motion generation has recently garnered significant research interest, primarily focusing on generating human motion sequences in blank backgrounds. However, human motions commonly occur within diverse 3D scenes, which has prompted exploration into scene-aware text-to-motion generation methods. Yet, existing scene-aware methods often rely on large-scale ground-truth motion sequences in diverse 3D scenes, which poses practical challenges due to the expensive cost. To mitigate this challenge, we are the first to propose a \textbf{T}raining-free \textbf{S}cene-aware \textbf{T}ext-to-\textbf{Motion} framework, dubbed as \textbf{TSTMotion}, that efficiently empowers pre-trained blank-background motion generators with the scene-aware capability. Specifically, conditioned on the given 3D scene and text description, we adopt foundation models together to reason, predict and validate a scene-aware motion guidance. Then, the motion guidance is incorporated into the blank-background motion generators with two modifications, resulting in scene-aware text-driven motion sequences. Extensive experiments demonstrate the efficacy and generalizability of our proposed framework. We release our code in \href{https://tstmotion.github.io/}{Project Page}.
<div id='section'>Paperid: <span id='pid'>159, <a href='https://arxiv.org/pdf/2504.21497.pdf' target='_blank'>https://arxiv.org/pdf/2504.21497.pdf</a></span>   <span><a href='https://github.com/weimengting/MagicPortrait' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mengting Wei, Yante Li, Tuomas Varanka, Yan Jiang, Guoying Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.21497">MagicPortrait: Temporally Consistent Face Reenactment with 3D Geometric Guidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this study, we propose a method for video face reenactment that integrates a 3D face parametric model into a latent diffusion framework, aiming to improve shape consistency and motion control in existing video-based face generation approaches. Our approach employs the FLAME (Faces Learned with an Articulated Model and Expressions) model as the 3D face parametric representation, providing a unified framework for modeling face expressions and head pose. This not only enables precise extraction of motion features from driving videos, but also contributes to the faithful preservation of face shape and geometry. Specifically, we enhance the latent diffusion model with rich 3D expression and detailed pose information by incorporating depth maps, normal maps, and rendering maps derived from FLAME sequences. These maps serve as motion guidance and are encoded into the denoising UNet through a specifically designed Geometric Guidance Encoder (GGE). A multi-layer feature fusion module with integrated self-attention mechanisms is used to combine facial appearance and motion latent features within the spatial domain. By utilizing the 3D face parametric model as motion guidance, our method enables parametric alignment of face identity between the reference image and the motion captured from the driving video. Experimental results on benchmark datasets show that our method excels at generating high-quality face animations with precise expression and head pose variation modeling. In addition, it demonstrates strong generalization performance on out-of-domain images. Code is publicly available at https://github.com/weimengting/MagicPortrait.
<div id='section'>Paperid: <span id='pid'>160, <a href='https://arxiv.org/pdf/2504.19189.pdf' target='_blank'>https://arxiv.org/pdf/2504.19189.pdf</a></span>   <span><a href='https://zhongleilz.github.io/Sketch2Anim/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lei Zhong, Chuan Guo, Yiming Xie, Jiawei Wang, Changjian Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.19189">Sketch2Anim: Towards Transferring Sketch Storyboards into 3D Animation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Storyboarding is widely used for creating 3D animations. Animators use the 2D sketches in storyboards as references to craft the desired 3D animations through a trial-and-error process. The traditional approach requires exceptional expertise and is both labor-intensive and time-consuming. Consequently, there is a high demand for automated methods that can directly translate 2D storyboard sketches into 3D animations. This task is under-explored to date and inspired by the significant advancements of motion diffusion models, we propose to address it from the perspective of conditional motion synthesis. We thus present Sketch2Anim, composed of two key modules for sketch constraint understanding and motion generation. Specifically, due to the large domain gap between the 2D sketch and 3D motion, instead of directly conditioning on 2D inputs, we design a 3D conditional motion generator that simultaneously leverages 3D keyposes, joint trajectories, and action words, to achieve precise and fine-grained motion control. Then, we invent a neural mapper dedicated to aligning user-provided 2D sketches with their corresponding 3D keyposes and trajectories in a shared embedding space, enabling, for the first time, direct 2D control of motion generation. Our approach successfully transfers storyboards into high-quality 3D motions and inherently supports direct 3D animation editing, thanks to the flexibility of our multi-conditional motion generator. Comprehensive experiments and evaluations, and a user perceptual study demonstrate the effectiveness of our approach.
<div id='section'>Paperid: <span id='pid'>161, <a href='https://arxiv.org/pdf/2504.19056.pdf' target='_blank'>https://arxiv.org/pdf/2504.19056.pdf</a></span>   <span><a href='https://github.com/llm-lab-org/Generative-AI-for-Character-Animation-Survey' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/llm-lab-org/Generative-AI-for-Character-Animation-Survey' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohammad Mahdi Abootorabi, Omid Ghahroodi, Pardis Sadat Zahraei, Hossein Behzadasl, Alireza Mirrokni, Mobina Salimipanah, Arash Rasouli, Bahar Behzadipour, Sara Azarnoush, Benyamin Maleki, Erfan Sadraiye, Kiarash Kiani Feriz, Mahdi Teymouri Nahad, Ali Moghadasi, Abolfazl Eshagh Abianeh, Nizi Nazar, Hamid R. Rabiee, Mahdieh Soleymani Baghshah, Meisam Ahmadi, Ehsaneddin Asgari
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.19056">Generative AI for Character Animation: A Comprehensive Survey of Techniques, Applications, and Future Directions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generative AI is reshaping art, gaming, and most notably animation. Recent breakthroughs in foundation and diffusion models have reduced the time and cost of producing animated content. Characters are central animation components, involving motion, emotions, gestures, and facial expressions. The pace and breadth of advances in recent months make it difficult to maintain a coherent view of the field, motivating the need for an integrative review. Unlike earlier overviews that treat avatars, gestures, or facial animation in isolation, this survey offers a single, comprehensive perspective on all the main generative AI applications for character animation. We begin by examining the state-of-the-art in facial animation, expression rendering, image synthesis, avatar creation, gesture modeling, motion synthesis, object generation, and texture synthesis. We highlight leading research, practical deployments, commonly used datasets, and emerging trends for each area. To support newcomers, we also provide a comprehensive background section that introduces foundational models and evaluation metrics, equipping readers with the knowledge needed to enter the field. We discuss open challenges and map future research directions, providing a roadmap to advance AI-driven character-animation technologies. This survey is intended as a resource for researchers and developers entering the field of generative AI animation or adjacent fields. Resources are available at: https://github.com/llm-lab-org/Generative-AI-for-Character-Animation-Survey.
<div id='section'>Paperid: <span id='pid'>162, <a href='https://arxiv.org/pdf/2504.15122.pdf' target='_blank'>https://arxiv.org/pdf/2504.15122.pdf</a></span>   <span><a href='https://kaist-viclab.github.io/mobgs-site/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Minh-Quan Viet Bui, Jongmin Park, Juan Luis Gonzalez Bello, Jaeho Moon, Jihyong Oh, Munchurl Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.15122">MoBGS: Motion Deblurring Dynamic 3D Gaussian Splatting for Blurry Monocular Video</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present MoBGS, a novel deblurring dynamic 3D Gaussian Splatting (3DGS) framework capable of reconstructing sharp and high-quality novel spatio-temporal views from blurry monocular videos in an end-to-end manner. Existing dynamic novel view synthesis (NVS) methods are highly sensitive to motion blur in casually captured videos, resulting in significant degradation of rendering quality. While recent approaches address motion-blurred inputs for NVS, they primarily focus on static scene reconstruction and lack dedicated motion modeling for dynamic objects. To overcome these limitations, our MoBGS introduces a novel Blur-adaptive Latent Camera Estimation (BLCE) method for effective latent camera trajectory estimation, improving global camera motion deblurring. In addition, we propose a physically-inspired Latent Camera-induced Exposure Estimation (LCEE) method to ensure consistent deblurring of both global camera and local object motion. Our MoBGS framework ensures the temporal consistency of unseen latent timestamps and robust motion decomposition of static and dynamic regions. Extensive experiments on the Stereo Blur dataset and real-world blurry videos show that our MoBGS significantly outperforms the very recent advanced methods (DyBluRF and Deblur4DGS), achieving state-of-the-art performance for dynamic NVS under motion blur.
<div id='section'>Paperid: <span id='pid'>163, <a href='https://arxiv.org/pdf/2504.14899.pdf' target='_blank'>https://arxiv.org/pdf/2504.14899.pdf</a></span>   <span><a href='https://github.com/ewrfcas/Uni3C' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenjie Cao, Jingkai Zhou, Shikai Li, Jingyun Liang, Chaohui Yu, Fan Wang, Xiangyang Xue, Yanwei Fu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.14899">Uni3C: Unifying Precisely 3D-Enhanced Camera and Human Motion Controls for Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Camera and human motion controls have been extensively studied for video generation, but existing approaches typically address them separately, suffering from limited data with high-quality annotations for both aspects. To overcome this, we present Uni3C, a unified 3D-enhanced framework for precise control of both camera and human motion in video generation. Uni3C includes two key contributions. First, we propose a plug-and-play control module trained with a frozen video generative backbone, PCDController, which utilizes unprojected point clouds from monocular depth to achieve accurate camera control. By leveraging the strong 3D priors of point clouds and the powerful capacities of video foundational models, PCDController shows impressive generalization, performing well regardless of whether the inference backbone is frozen or fine-tuned. This flexibility enables different modules of Uni3C to be trained in specific domains, i.e., either camera control or human motion control, reducing the dependency on jointly annotated data. Second, we propose a jointly aligned 3D world guidance for the inference phase that seamlessly integrates both scenic point clouds and SMPL-X characters to unify the control signals for camera and human motion, respectively. Extensive experiments confirm that PCDController enjoys strong robustness in driving camera motion for fine-tuned backbones of video generation. Uni3C substantially outperforms competitors in both camera controllability and human motion quality. Additionally, we collect tailored validation sets featuring challenging camera movements and human actions to validate the effectiveness of our method.
<div id='section'>Paperid: <span id='pid'>164, <a href='https://arxiv.org/pdf/2504.14602.pdf' target='_blank'>https://arxiv.org/pdf/2504.14602.pdf</a></span>   <span><a href='https://k2muse.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiwei Li, Bi Zhang, Xiaowei Tan, Wanxin Chen, Zhaoyuan Liu, Juanjuan Zhang, Weiguang Huo, Jian Huang, Lianqing Liu, Xingang Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.14602">K2MUSE: A human lower limb multimodal dataset under diverse conditions for facilitating rehabilitation robotics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The natural interaction and control performance of lower limb rehabilitation robots are closely linked to biomechanical information from various human locomotion activities. Multidimensional human motion data significantly deepen the understanding of the complex mechanisms governing neuromuscular alterations, thereby facilitating the development and application of rehabilitation robots in multifaceted real-world environments. However, currently available lower limb datasets are inadequate for supplying the essential multimodal data and large-scale gait samples necessary for effective data-driven approaches, and they neglect the significant effects of acquisition interference in real applications.To fill this gap, we present the K2MUSE dataset, which includes a comprehensive collection of multimodal data, comprising kinematic, kinetic, amplitude-mode ultrasound (AUS), and surface electromyography (sEMG) measurements. The proposed dataset includes lower limb multimodal data from 30 able-bodied participants walking under different inclines (0$^\circ$, $\pm$5$^\circ$, and $\pm$10$^\circ$), various speeds (0.5 m/s, 1.0 m/s, and 1.5 m/s), and different nonideal acquisition conditions (muscle fatigue, electrode shifts, and inter-day differences). The kinematic and ground reaction force data were collected via a Vicon motion capture system and an instrumented treadmill with embedded force plates, whereas the sEMG and AUS data were synchronously recorded for thirteen muscles on the bilateral lower limbs. This dataset offers a new resource for designing control frameworks for rehabilitation robots and conducting biomechanical analyses of lower limb locomotion. The dataset is available at https://k2muse.github.io/.
<div id='section'>Paperid: <span id='pid'>165, <a href='https://arxiv.org/pdf/2504.14305.pdf' target='_blank'>https://arxiv.org/pdf/2504.14305.pdf</a></span>   <span><a href='https://github.com/TeleHuman/ALMI-Open,' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiyuan Shi, Xinzhe Liu, Dewei Wang, Ouyang Lu, SÃ¶ren Schwertfeger, Fuchun Sun, Chenjia Bai, Xuelong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.14305">Adversarial Locomotion and Motion Imitation for Humanoid Policy Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humans exhibit diverse and expressive whole-body movements. However, attaining human-like whole-body coordination in humanoid robots remains challenging, as conventional approaches that mimic whole-body motions often neglect the distinct roles of upper and lower body. This oversight leads to computationally intensive policy learning and frequently causes robot instability and falls during real-world execution. To address these issues, we propose Adversarial Locomotion and Motion Imitation (ALMI), a novel framework that enables adversarial policy learning between upper and lower body. Specifically, the lower body aims to provide robust locomotion capabilities to follow velocity commands while the upper body tracks various motions. Conversely, the upper-body policy ensures effective motion tracking when the robot executes velocity-based movements. Through iterative updates, these policies achieve coordinated whole-body control, which can be extended to loco-manipulation tasks with teleoperation systems. Extensive experiments demonstrate that our method achieves robust locomotion and precise motion tracking in both simulation and on the full-size Unitree H1 robot. Additionally, we release a large-scale whole-body motion control dataset featuring high-quality episodic trajectories from MuJoCo simulations deployable on real robots. The project page is https://almi-humanoid.github.io.
<div id='section'>Paperid: <span id='pid'>166, <a href='https://arxiv.org/pdf/2504.12826.pdf' target='_blank'>https://arxiv.org/pdf/2504.12826.pdf</a></span>   <span><a href='https://github.com/pengxuanyang/UncAD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Pengxuan Yang, Yupeng Zheng, Qichao Zhang, Kefei Zhu, Zebin Xing, Qiao Lin, Yun-Fu Liu, Zhiguo Su, Dongbin Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.12826">UncAD: Towards Safe End-to-end Autonomous Driving via Online Map Uncertainty</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>End-to-end autonomous driving aims to produce planning trajectories from raw sensors directly. Currently, most approaches integrate perception, prediction, and planning modules into a fully differentiable network, promising great scalability. However, these methods typically rely on deterministic modeling of online maps in the perception module for guiding or constraining vehicle planning, which may incorporate erroneous perception information and further compromise planning safety. To address this issue, we delve into the importance of online map uncertainty for enhancing autonomous driving safety and propose a novel paradigm named UncAD. Specifically, UncAD first estimates the uncertainty of the online map in the perception module. It then leverages the uncertainty to guide motion prediction and planning modules to produce multi-modal trajectories. Finally, to achieve safer autonomous driving, UncAD proposes an uncertainty-collision-aware planning selection strategy according to the online map uncertainty to evaluate and select the best trajectory. In this study, we incorporate UncAD into various state-of-the-art (SOTA) end-to-end methods. Experiments on the nuScenes dataset show that integrating UncAD, with only a 1.9% increase in parameters, can reduce collision rates by up to 26% and drivable area conflict rate by up to 42%. Codes, pre-trained models, and demo videos can be accessed at https://github.com/pengxuanyang/UncAD.
<div id='section'>Paperid: <span id='pid'>167, <a href='https://arxiv.org/pdf/2504.09885.pdf' target='_blank'>https://arxiv.org/pdf/2504.09885.pdf</a></span>   <span><a href='https://monkek123king.github.io/S2C_page/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zihao Liu, Mingwen Ou, Zunnan Xu, Jiaqi Huang, Haonan Han, Ronghui Li, Xiu Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.09885">Separate to Collaborate: Dual-Stream Diffusion Model for Coordinated Piano Hand Motion Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automating the synthesis of coordinated bimanual piano performances poses significant challenges, particularly in capturing the intricate choreography between the hands while preserving their distinct kinematic signatures. In this paper, we propose a dual-stream neural framework designed to generate synchronized hand gestures for piano playing from audio input, addressing the critical challenge of modeling both hand independence and coordination. Our framework introduces two key innovations: (i) a decoupled diffusion-based generation framework that independently models each hand's motion via dual-noise initialization, sampling distinct latent noise for each while leveraging a shared positional condition, and (ii) a Hand-Coordinated Asymmetric Attention (HCAA) mechanism suppresses symmetric (common-mode) noise to highlight asymmetric hand-specific features, while adaptively enhancing inter-hand coordination during denoising. Comprehensive evaluations demonstrate that our framework outperforms existing state-of-the-art methods across multiple metrics. Our project is available at https://monkek123king.github.io/S2C_page/.
<div id='section'>Paperid: <span id='pid'>168, <a href='https://arxiv.org/pdf/2504.09862.pdf' target='_blank'>https://arxiv.org/pdf/2504.09862.pdf</a></span>   <span><a href='https://inowlzy.github.io/RadarLLM/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zengyuan Lai, Jiarui Yang, Songpengcheng Xia, Lizhou Lin, Lan Sun, Renwen Wang, Jianran Liu, Qi Wu, Ling Pei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.09862">RadarLLM: Empowering Large Language Models to Understand Human Motion from Millimeter-wave Point Cloud Sequence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Millimeter-wave radar provides a privacy-preserving solution for human motion analysis, yet its sparse point clouds pose significant challenges for semantic understanding. We present Radar-LLM, the first framework that leverages large language models (LLMs) for human motion understanding using millimeter-wave radar as the sensing modality. Our approach introduces two key innovations: (1) a motion-guided radar tokenizer based on our Aggregate VQ-VAE architecture that incorporates deformable body templates and masked trajectory modeling to encode spatiotemporal point clouds into compact semantic tokens, and (2) a radar-aware language model that establishes cross-modal alignment between radar and text in a shared embedding space. To address data scarcity, we introduce a physics-aware synthesis pipeline that generates realistic radar-text pairs from motion-text datasets. Extensive experiments demonstrate that Radar-LLM achieves state-of-the-art performance across both synthetic and real-world benchmarks, enabling accurate translation of millimeter-wave signals to natural language descriptions. This breakthrough facilitates comprehensive motion understanding in privacy-sensitive applications like healthcare and smart homes. We will release the full implementation to support further research on https://inowlzy.github.io/RadarLLM/.
<div id='section'>Paperid: <span id='pid'>169, <a href='https://arxiv.org/pdf/2504.06504.pdf' target='_blank'>https://arxiv.org/pdf/2504.06504.pdf</a></span>   <span><a href='https://github.com/XiaohangYang829/STaR' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/XiaohangYang829/STaR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaohang Yang, Qing Wang, Jiahao Yang, Gregory Slabaugh, Shanxin Yuan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.06504">STaR: Seamless Spatial-Temporal Aware Motion Retargeting with Penetration and Consistency Constraints</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion retargeting seeks to faithfully replicate the spatio-temporal motion characteristics of a source character onto a target character with a different body shape. Apart from motion semantics preservation, ensuring geometric plausibility and maintaining temporal consistency are also crucial for effective motion retargeting. However, many existing methods prioritize either geometric plausibility or temporal consistency. Neglecting geometric plausibility results in interpenetration while neglecting temporal consistency leads to motion jitter. In this paper, we propose a novel sequence-to-sequence model for seamless Spatial-Temporal aware motion Retargeting (STaR), with penetration and consistency constraints. STaR consists of two modules: (1) a spatial module that incorporates dense shape representation and a novel limb penetration constraint to ensure geometric plausibility while preserving motion semantics, and (2) a temporal module that utilizes a temporal transformer and a novel temporal consistency constraint to predict the entire motion sequence at once while enforcing multi-level trajectory smoothness. The seamless combination of the two modules helps us achieve a good balance between the semantic, geometric, and temporal targets. Extensive experiments on the Mixamo and ScanRet datasets demonstrate that our method produces plausible and coherent motions while significantly reducing interpenetration rates compared with other approaches. Code page: https://github.com/XiaohangYang829/STaR.
<div id='section'>Paperid: <span id='pid'>170, <a href='https://arxiv.org/pdf/2504.05046.pdf' target='_blank'>https://arxiv.org/pdf/2504.05046.pdf</a></span>   <span><a href='https://nju-cite-mocaphumanoid.github.io/MotionPRO/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shenghao Ren, Yi Lu, Jiayi Huang, Jiayi Zhao, He Zhang, Tao Yu, Qiu Shen, Xun Cao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.05046">MotionPRO: Exploring the Role of Pressure in Human MoCap and Beyond</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing human Motion Capture (MoCap) methods mostly focus on the visual similarity while neglecting the physical plausibility. As a result, downstream tasks such as driving virtual human in 3D scene or humanoid robots in real world suffer from issues such as timing drift and jitter, spatial problems like sliding and penetration, and poor global trajectory accuracy. In this paper, we revisit human MoCap from the perspective of interaction between human body and physical world by exploring the role of pressure. Firstly, we construct a large-scale human Motion capture dataset with Pressure, RGB and Optical sensors (named MotionPRO), which comprises 70 volunteers performing 400 types of motion, encompassing a total of 12.4M pose frames. Secondly, we examine both the necessity and effectiveness of the pressure signal through two challenging tasks: (1) pose and trajectory estimation based solely on pressure: We propose a network that incorporates a small kernel decoder and a long-short-term attention module, and proof that pressure could provide accurate global trajectory and plausible lower body pose. (2) pose and trajectory estimation by fusing pressure and RGB: We impose constraints on orthographic similarity along the camera axis and whole-body contact along the vertical axis to enhance the cross-attention strategy to fuse pressure and RGB feature maps. Experiments demonstrate that fusing pressure with RGB features not only significantly improves performance in terms of objective metrics, but also plausibly drives virtual humans (SMPL) in 3D scene. Furthermore, we demonstrate that incorporating physical perception enables humanoid robots to perform more precise and stable actions, which is highly beneficial for the development of embodied artificial intelligence. Project page is available at: https://nju-cite-mocaphumanoid.github.io/MotionPRO/
<div id='section'>Paperid: <span id='pid'>171, <a href='https://arxiv.org/pdf/2504.04956.pdf' target='_blank'>https://arxiv.org/pdf/2504.04956.pdf</a></span>   <span><a href='https://jyunlee.github.io/projects/rewind/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jihyun Lee, Weipeng Xu, Alexander Richard, Shih-En Wei, Shunsuke Saito, Shaojie Bai, Te-Li Wang, Minhyuk Sung, Tae-Kyun Kim, Jason Saragih
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.04956">REWIND: Real-Time Egocentric Whole-Body Motion Diffusion with Exemplar-Based Identity Conditioning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present REWIND (Real-Time Egocentric Whole-Body Motion Diffusion), a one-step diffusion model for real-time, high-fidelity human motion estimation from egocentric image inputs. While an existing method for egocentric whole-body (i.e., body and hands) motion estimation is non-real-time and acausal due to diffusion-based iterative motion refinement to capture correlations between body and hand poses, REWIND operates in a fully causal and real-time manner. To enable real-time inference, we introduce (1) cascaded body-hand denoising diffusion, which effectively models the correlation between egocentric body and hand motions in a fast, feed-forward manner, and (2) diffusion distillation, which enables high-quality motion estimation with a single denoising step. Our denoising diffusion model is based on a modified Transformer architecture, designed to causally model output motions while enhancing generalizability to unseen motion lengths. Additionally, REWIND optionally supports identity-conditioned motion estimation when identity prior is available. To this end, we propose a novel identity conditioning method based on a small set of pose exemplars of the target identity, which further enhances motion estimation quality. Through extensive experiments, we demonstrate that REWIND significantly outperforms the existing baselines both with and without exemplar-based identity conditioning.
<div id='section'>Paperid: <span id='pid'>172, <a href='https://arxiv.org/pdf/2504.04842.pdf' target='_blank'>https://arxiv.org/pdf/2504.04842.pdf</a></span>   <span><a href='https://fantasy-amap.github.io/fantasy-talking/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mengchao Wang, Qiang Wang, Fan Jiang, Yaqi Fan, Yunpeng Zhang, Yonggang Qi, Kun Zhao, Mu Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.04842">FantasyTalking: Realistic Talking Portrait Generation via Coherent Motion Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Creating a realistic animatable avatar from a single static portrait remains challenging. Existing approaches often struggle to capture subtle facial expressions, the associated global body movements, and the dynamic background. To address these limitations, we propose a novel framework that leverages a pretrained video diffusion transformer model to generate high-fidelity, coherent talking portraits with controllable motion dynamics. At the core of our work is a dual-stage audio-visual alignment strategy. In the first stage, we employ a clip-level training scheme to establish coherent global motion by aligning audio-driven dynamics across the entire scene, including the reference portrait, contextual objects, and background. In the second stage, we refine lip movements at the frame level using a lip-tracing mask, ensuring precise synchronization with audio signals. To preserve identity without compromising motion flexibility, we replace the commonly used reference network with a facial-focused cross-attention module that effectively maintains facial consistency throughout the video. Furthermore, we integrate a motion intensity modulation module that explicitly controls expression and body motion intensity, enabling controllable manipulation of portrait movements beyond mere lip motion. Extensive experimental results show that our proposed approach achieves higher quality with better realism, coherence, motion intensity, and identity preservation. Ours project page: https://fantasy-amap.github.io/fantasy-talking/.
<div id='section'>Paperid: <span id='pid'>173, <a href='https://arxiv.org/pdf/2504.02451.pdf' target='_blank'>https://arxiv.org/pdf/2504.02451.pdf</a></span>   <span><a href='https://github.com/Andyplus1/ConMo' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiayi Gao, Zijin Yin, Changcheng Hua, Yuxin Peng, Kongming Liang, Zhanyu Ma, Jun Guo, Yang Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.02451">ConMo: Controllable Motion Disentanglement and Recomposition for Zero-Shot Motion Transfer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The development of Text-to-Video (T2V) generation has made motion transfer possible, enabling the control of video motion based on existing footage. However, current methods have two limitations: 1) struggle to handle multi-subjects videos, failing to transfer specific subject motion; 2) struggle to preserve the diversity and accuracy of motion as transferring to subjects with varying shapes. To overcome these, we introduce \textbf{ConMo}, a zero-shot framework that disentangle and recompose the motions of subjects and camera movements. ConMo isolates individual subject and background motion cues from complex trajectories in source videos using only subject masks, and reassembles them for target video generation. This approach enables more accurate motion control across diverse subjects and improves performance in multi-subject scenarios. Additionally, we propose soft guidance in the recomposition stage which controls the retention of original motion to adjust shape constraints, aiding subject shape adaptation and semantic transformation. Unlike previous methods, ConMo unlocks a wide range of applications, including subject size and position editing, subject removal, semantic modifications, and camera motion simulation. Extensive experiments demonstrate that ConMo significantly outperforms state-of-the-art methods in motion fidelity and semantic consistency. The code is available at https://github.com/Andyplus1/ConMo.
<div id='section'>Paperid: <span id='pid'>174, <a href='https://arxiv.org/pdf/2504.02004.pdf' target='_blank'>https://arxiv.org/pdf/2504.02004.pdf</a></span>   <span><a href='https://yaomingshuai.github.io/Beyond-Static-Scenes.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingshuai Yao, Mengting Chen, Qinye Zhou, Yabo Zhang, Ming Liu, Xiaoming Li, Shaohui Liu, Chen Ju, Shuai Xiao, Qingwen Liu, Jinsong Lan, Wangmeng Zuo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.02004">Beyond Static Scenes: Camera-controllable Background Generation for Human Motion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we investigate the generation of new video backgrounds given a human foreground video, a camera pose, and a reference scene image. This task presents three key challenges. First, the generated background should precisely follow the camera movements corresponding to the human foreground. Second, as the camera shifts in different directions, newly revealed content should appear seamless and natural. Third, objects within the video frame should maintain consistent textures as the camera moves to ensure visual coherence. To address these challenges, we propose DynaScene, a new framework that uses camera poses extracted from the original video as an explicit control to drive background motion. Specifically, we design a multi-task learning paradigm that incorporates auxiliary tasks, namely background outpainting and scene variation, to enhance the realism of the generated backgrounds. Given the scarcity of suitable data, we constructed a large-scale, high-quality dataset tailored for this task, comprising video foregrounds, reference scene images, and corresponding camera poses. This dataset contains 200K video clips, ten times larger than existing real-world human video datasets, providing a significantly richer and more diverse training resource. Project page: https://yaomingshuai.github.io/Beyond-Static-Scenes.github.io/
<div id='section'>Paperid: <span id='pid'>175, <a href='https://arxiv.org/pdf/2504.01724.pdf' target='_blank'>https://arxiv.org/pdf/2504.01724.pdf</a></span>   <span><a href='https://grisoon.github.io/DreamActor-M1/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxuan Luo, Zhengkun Rong, Lizhen Wang, Longhao Zhang, Tianshu Hu, Yongming Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.01724">DreamActor-M1: Holistic, Expressive and Robust Human Image Animation with Hybrid Guidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While recent image-based human animation methods achieve realistic body and facial motion synthesis, critical gaps remain in fine-grained holistic controllability, multi-scale adaptability, and long-term temporal coherence, which leads to their lower expressiveness and robustness. We propose a diffusion transformer (DiT) based framework, DreamActor-M1, with hybrid guidance to overcome these limitations. For motion guidance, our hybrid control signals that integrate implicit facial representations, 3D head spheres, and 3D body skeletons achieve robust control of facial expressions and body movements, while producing expressive and identity-preserving animations. For scale adaptation, to handle various body poses and image scales ranging from portraits to full-body views, we employ a progressive training strategy using data with varying resolutions and scales. For appearance guidance, we integrate motion patterns from sequential frames with complementary visual references, ensuring long-term temporal coherence for unseen regions during complex movements. Experiments demonstrate that our method outperforms the state-of-the-art works, delivering expressive results for portraits, upper-body, and full-body generation with robust long-term consistency. Project Page: https://grisoon.github.io/DreamActor-M1/.
<div id='section'>Paperid: <span id='pid'>176, <a href='https://arxiv.org/pdf/2504.00432.pdf' target='_blank'>https://arxiv.org/pdf/2504.00432.pdf</a></span>   <span><a href='https://chongjg.github.io/DecoFuse/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chong Li, Jingyang Huo, Weikang Gong, Yanwei Fu, Xiangyang Xue, Jianfeng Feng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.00432">DecoFuse: Decomposing and Fusing the "What", "Where", and "How" for Brain-Inspired fMRI-to-Video Decoding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Decoding visual experiences from brain activity is a significant challenge. Existing fMRI-to-video methods often focus on semantic content while overlooking spatial and motion information. However, these aspects are all essential and are processed through distinct pathways in the brain. Motivated by this, we propose DecoFuse, a novel brain-inspired framework for decoding videos from fMRI signals. It first decomposes the video into three components - semantic, spatial, and motion - then decodes each component separately before fusing them to reconstruct the video. This approach not only simplifies the complex task of video decoding by decomposing it into manageable sub-tasks, but also establishes a clearer connection between learned representations and their biological counterpart, as supported by ablation studies. Further, our experiments show significant improvements over previous state-of-the-art methods, achieving 82.4% accuracy for semantic classification, 70.6% accuracy in spatial consistency, a 0.212 cosine similarity for motion prediction, and 21.9% 50-way accuracy for video generation. Additionally, neural encoding analyses for semantic and spatial information align with the two-streams hypothesis, further validating the distinct roles of the ventral and dorsal pathways. Overall, DecoFuse provides a strong and biologically plausible framework for fMRI-to-video decoding. Project page: https://chongjg.github.io/DecoFuse/.
<div id='section'>Paperid: <span id='pid'>177, <a href='https://arxiv.org/pdf/2503.22605.pdf' target='_blank'>https://arxiv.org/pdf/2503.22605.pdf</a></span>   <span><a href='https://sstzal.github.io/Audio-Plane/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuai Shen, Wanhua Li, Yunpeng Zhang, Yap-Peng Tan, Jiwen Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.22605">Audio-Plane: Audio Factorization Plane Gaussian Splatting for Real-Time Talking Head Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Talking head synthesis has emerged as a prominent research topic in computer graphics and multimedia, yet most existing methods often struggle to strike a balance between generation quality and computational efficiency, particularly under real-time constraints. In this paper, we propose a novel framework that integrates Gaussian Splatting with a structured Audio Factorization Plane (Audio-Plane) to enable high-quality, audio-synchronized, and real-time talking head generation. For modeling a dynamic talking head, a 4D volume representation, which consists of three axes in 3D space and one temporal axis aligned with audio progression, is typically required. However, directly storing and processing a dense 4D grid is impractical due to the high memory and computation cost, and lack of scalability for longer durations. We address this challenge by decomposing the 4D volume representation into a set of audio-independent spatial planes and audio-dependent planes, forming a compact and interpretable representation for talking head modeling that we refer to as the Audio-Plane. This factorized design allows for efficient and fine-grained audio-aware spatial encoding, and significantly enhances the model's ability to capture complex lip dynamics driven by speech signals. To further improve region-specific motion modeling, we introduce an audio-guided saliency splatting mechanism based on region-aware modulation, which adaptively emphasizes highly dynamic regions such as the mouth area. This allows the model to focus its learning capacity on where it matters most for accurate speech-driven animation. Extensive experiments on both the self-driven and the cross-driven settings demonstrate that our method achieves state-of-the-art visual quality, precise audio-lip synchronization, and real-time performance, outperforming prior approaches across both 2D- and 3D-based paradigms.
<div id='section'>Paperid: <span id='pid'>178, <a href='https://arxiv.org/pdf/2503.22204.pdf' target='_blank'>https://arxiv.org/pdf/2503.22204.pdf</a></span>   <span><a href='https://vulab-ai.github.io/Segment-then-Splat/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiren Lu, Yunlai Zhou, Yiran Qiao, Chaoda Song, Tuo Liang, Jing Ma, Yu Yin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.22204">Segment then Splat: A Unified Approach for 3D Open-Vocabulary Segmentation based on Gaussian Splatting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Open-vocabulary querying in 3D space is crucial for enabling more intelligent perception in applications such as robotics, autonomous systems, and augmented reality. However, most existing methods rely on 2D pixel-level parsing, leading to multi-view inconsistencies and poor 3D object retrieval. Moreover, they are limited to static scenes and struggle with dynamic scenes due to the complexities of motion modeling. In this paper, we propose Segment then Splat, a 3D-aware open vocabulary segmentation approach for both static and dynamic scenes based on Gaussian Splatting. Segment then Splat reverses the long established approach of "segmentation after reconstruction" by dividing Gaussians into distinct object sets before reconstruction. Once the reconstruction is complete, the scene is naturally segmented into individual objects, achieving true 3D segmentation. This approach not only eliminates Gaussian-object misalignment issues in dynamic scenes but also accelerates the optimization process, as it eliminates the need for learning a separate language field. After optimization, a CLIP embedding is assigned to each object to enable open-vocabulary querying. Extensive experiments on various datasets demonstrate the effectiveness of our proposed method in both static and dynamic scenarios.
<div id='section'>Paperid: <span id='pid'>179, <a href='https://arxiv.org/pdf/2503.21847.pdf' target='_blank'>https://arxiv.org/pdf/2503.21847.pdf</a></span>   <span><a href='https://yong-xie-xy.github.io/ReCoM/' target='_blank'>  GitHub</a></span> <span><a href='https://yong-xie-xy.github.io/ReCoM/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yong Xie, Yunlian Sun, Hongwen Zhang, Yebin Liu, Jinhui Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.21847">ReCoM: Realistic Co-Speech Motion Generation with Recurrent Embedded Transformer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present ReCoM, an efficient framework for generating high-fidelity and generalizable human body motions synchronized with speech. The core innovation lies in the Recurrent Embedded Transformer (RET), which integrates Dynamic Embedding Regularization (DER) into a Vision Transformer (ViT) core architecture to explicitly model co-speech motion dynamics. This architecture enables joint spatial-temporal dependency modeling, thereby enhancing gesture naturalness and fidelity through coherent motion synthesis. To enhance model robustness, we incorporate the proposed DER strategy, which equips the model with dual capabilities of noise resistance and cross-domain generalization, thereby improving the naturalness and fluency of zero-shot motion generation for unseen speech inputs. To mitigate inherent limitations of autoregressive inference, including error accumulation and limited self-correction, we propose an iterative reconstruction inference (IRI) strategy. IRI refines motion sequences via cyclic pose reconstruction, driven by two key components: (1) classifier-free guidance improves distribution alignment between generated and real gestures without auxiliary supervision, and (2) a temporal smoothing process eliminates abrupt inter-frame transitions while ensuring kinematic continuity. Extensive experiments on benchmark datasets validate ReCoM's effectiveness, achieving state-of-the-art performance across metrics. Notably, it reduces the FrÃ©chet Gesture Distance (FGD) from 18.70 to 2.48, demonstrating an 86.7% improvement in motion realism. Our project page is https://yong-xie-xy.github.io/ReCoM/.
<div id='section'>Paperid: <span id='pid'>180, <a href='https://arxiv.org/pdf/2503.21779.pdf' target='_blank'>https://arxiv.org/pdf/2503.21779.pdf</a></span>   <span><a href='https://x2-gaussian.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Weihao Yu, Yuanhao Cai, Ruyi Zha, Zhiwen Fan, Chenxin Li, Yixuan Yuan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.21779">X$^{2}$-Gaussian: 4D Radiative Gaussian Splatting for Continuous-time Tomographic Reconstruction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Four-dimensional computed tomography (4D CT) reconstruction is crucial for capturing dynamic anatomical changes but faces inherent limitations from conventional phase-binning workflows. Current methods discretize temporal resolution into fixed phases with respiratory gating devices, introducing motion misalignment and restricting clinical practicality. In this paper, We propose X$^2$-Gaussian, a novel framework that enables continuous-time 4D-CT reconstruction by integrating dynamic radiative Gaussian splatting with self-supervised respiratory motion learning. Our approach models anatomical dynamics through a spatiotemporal encoder-decoder architecture that predicts time-varying Gaussian deformations, eliminating phase discretization. To remove dependency on external gating devices, we introduce a physiology-driven periodic consistency loss that learns patient-specific breathing cycles directly from projections via differentiable optimization. Extensive experiments demonstrate state-of-the-art performance, achieving a 9.93 dB PSNR gain over traditional methods and 2.25 dB improvement against prior Gaussian splatting techniques. By unifying continuous motion modeling with hardware-free period learning, X$^2$-Gaussian advances high-fidelity 4D CT reconstruction for dynamic clinical imaging. Project website at: https://x2-gaussian.github.io/.
<div id='section'>Paperid: <span id='pid'>181, <a href='https://arxiv.org/pdf/2503.21268.pdf' target='_blank'>https://arxiv.org/pdf/2503.21268.pdf</a></span>   <span><a href='this link' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ming Yan, Xincheng Lin, Yuhua Luo, Shuqi Fan, Yudi Dai, Qixin Zhong, Lincai Zhong, Yuexin Ma, Lan Xu, Chenglu Wen, Siqi Shen, Cheng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.21268">ClimbingCap: Multi-Modal Dataset and Method for Rock Climbing in World Coordinate</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human Motion Recovery (HMR) research mainly focuses on ground-based motions such as running. The study on capturing climbing motion, an off-ground motion, is sparse. This is partly due to the limited availability of climbing motion datasets, especially large-scale and challenging 3D labeled datasets. To address the insufficiency of climbing motion datasets, we collect AscendMotion, a large-scale well-annotated, and challenging climbing motion dataset. It consists of 412k RGB, LiDAR frames, and IMU measurements, including the challenging climbing motions of 22 skilled climbing coaches across 12 different rock walls. Capturing the climbing motions is challenging as it requires precise recovery of not only the complex pose but also the global position of climbers. Although multiple global HMR methods have been proposed, they cannot faithfully capture climbing motions. To address the limitations of HMR methods for climbing, we propose ClimbingCap, a motion recovery method that reconstructs continuous 3D human climbing motion in a global coordinate system. One key insight is to use the RGB and LiDAR modalities to separately reconstruct motions in camera coordinates and global coordinates and to optimize them jointly. We demonstrate the quality of the AscendMotion dataset and present promising results from ClimbingCap. The AscendMotion dataset and source code release publicly at \href{this link}{http://www.lidarhumanmotion.net/climbingcap/}
<div id='section'>Paperid: <span id='pid'>182, <a href='https://arxiv.org/pdf/2503.20218.pdf' target='_blank'>https://arxiv.org/pdf/2503.20218.pdf</a></span>   <span><a href='https://h-liu1997.github.io/Video-Motion-Graphs/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haiyang Liu, Zhan Xu, Fa-Ting Hong, Hsin-Ping Huang, Yi Zhou, Yang Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.20218">Video Motion Graphs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present Video Motion Graphs, a system designed to generate realistic human motion videos. Using a reference video and conditional signals such as music or motion tags, the system synthesizes new videos by first retrieving video clips with gestures matching the conditions and then generating interpolation frames to seamlessly connect clip boundaries. The core of our approach is HMInterp, a robust Video Frame Interpolation (VFI) model that enables seamless interpolation of discontinuous frames, even for complex motion scenarios like dancing. HMInterp i) employs a dual-branch interpolation approach, combining a Motion Diffusion Model for human skeleton motion interpolation with a diffusion-based video frame interpolation model for final frame generation. ii) adopts condition progressive training to effectively leverage identity strong and weak conditions, such as images and pose. These designs ensure both high video texture quality and accurate motion trajectory. Results show that our Video Motion Graphs outperforms existing generative- and retrieval-based methods for multi-modal conditioned human motion video generation. Project page can be found at https://h-liu1997.github.io/Video-Motion-Graphs/
<div id='section'>Paperid: <span id='pid'>183, <a href='https://arxiv.org/pdf/2503.19914.pdf' target='_blank'>https://arxiv.org/pdf/2503.19914.pdf</a></span>   <span><a href='https://tlb-miss.github.io/oor/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sangwon Baik, Hyeonwoo Kim, Hanbyul Joo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.19914">Learning 3D Object Spatial Relationships from Pre-trained 2D Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a method for learning 3D spatial relationships between object pairs, referred to as object-object spatial relationships (OOR), by leveraging synthetically generated 3D samples from pre-trained 2D diffusion models. We hypothesize that images synthesized by 2D diffusion models inherently capture realistic OOR cues, enabling efficient collection of a 3D dataset to learn OOR for various unbounded object categories. Our approach synthesizes diverse images that capture plausible OOR cues, which we then uplift into 3D samples. Leveraging our diverse collection of 3D samples for the object pairs, we train a score-based OOR diffusion model to learn the distribution of their relative spatial relationships. Additionally, we extend our pairwise OOR to multi-object OOR by enforcing consistency across pairwise relations and preventing object collisions. Extensive experiments demonstrate the robustness of our method across various object-object spatial relationships, along with its applicability to 3D scene arrangement tasks and human motion synthesis using our OOR diffusion model.
<div id='section'>Paperid: <span id='pid'>184, <a href='https://arxiv.org/pdf/2503.19557.pdf' target='_blank'>https://arxiv.org/pdf/2503.19557.pdf</a></span>   <span><a href='https://haimsaw.github.io/LoRA-MDM/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haim Sawdayee, Chuan Guo, Guy Tevet, Bing Zhou, Jian Wang, Amit H. Bermano
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.19557">Dance Like a Chicken: Low-Rank Stylization for Human Motion Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-to-motion generative models span a wide range of 3D human actions but struggle with nuanced stylistic attributes such as a "Chicken" style. Due to the scarcity of style-specific data, existing approaches pull the generative prior towards a reference style, which often results in out-of-distribution low quality generations. In this work, we introduce LoRA-MDM, a lightweight framework for motion stylization that generalizes to complex actions while maintaining editability. Our key insight is that adapting the generative prior to include the style, while preserving its overall distribution, is more effective than modifying each individual motion during generation. Building on this idea, LoRA-MDM learns to adapt the prior to include the reference style using only a few samples. The style can then be used in the context of different textual prompts for generation. The low-rank adaptation shifts the motion manifold in a semantically meaningful way, enabling realistic style infusion even for actions not present in the reference samples. Moreover, preserving the distribution structure enables advanced operations such as style blending and motion editing. We compare LoRA-MDM to state-of-the-art stylized motion generation methods and demonstrate a favorable balance between text fidelity and style consistency.
<div id='section'>Paperid: <span id='pid'>185, <a href='https://arxiv.org/pdf/2503.18950.pdf' target='_blank'>https://arxiv.org/pdf/2503.18950.pdf</a></span>   <span><a href='https://taeksuu.github.io/tavid/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Taeksoo Kim, Hanbyul Joo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.18950">Target-Aware Video Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a target-aware video diffusion model that generates videos from an input image in which an actor interacts with a specified target while performing a desired action. The target is defined by a segmentation mask and the desired action is described via a text prompt. Unlike existing controllable image-to-video diffusion models that often rely on dense structural or motion cues to guide the actor's movements toward the target, our target-aware model requires only a simple mask to indicate the target, leveraging the generalization capabilities of pretrained models to produce plausible actions. This makes our method particularly effective for human-object interaction (HOI) scenarios, where providing precise action guidance is challenging, and further enables the use of video diffusion models for high-level action planning in applications such as robotics. We build our target-aware model by extending a baseline model to incorporate the target mask as an additional input. To enforce target awareness, we introduce a special token that encodes the target's spatial information within the text prompt. We then fine-tune the model with our curated dataset using a novel cross-attention loss that aligns the cross-attention maps associated with this token with the input target mask. To further improve performance, we selectively apply this loss to the most semantically relevant transformer blocks and attention regions. Experimental results show that our target-aware model outperforms existing solutions in generating videos where actors interact accurately with the specified targets. We further demonstrate its efficacy in two downstream applications: video content creation and zero-shot 3D HOI motion synthesis.
<div id='section'>Paperid: <span id='pid'>186, <a href='https://arxiv.org/pdf/2503.18674.pdf' target='_blank'>https://arxiv.org/pdf/2503.18674.pdf</a></span>   <span><a href='https://www.pinlab.org/hmu' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Edoardo De Matteis, Matteo Migliarini, Alessio Sampieri, Indro Spinelli, Fabio Galasso
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.18674">Human Motion Unlearning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce the task of human motion unlearning to prevent the synthesis of toxic animations while preserving the general text-to-motion generative performance. Unlearning toxic motions is challenging as those can be generated from explicit text prompts and from implicit toxic combinations of safe motions (e.g., ``kicking" is ``loading and swinging a leg"). We propose the first motion unlearning benchmark by filtering toxic motions from the large and recent text-to-motion datasets of HumanML3D and Motion-X. We propose baselines, by adapting state-of-the-art image unlearning techniques to process spatio-temporal signals. Finally, we propose a novel motion unlearning model based on Latent Code Replacement, which we dub LCR. LCR is training-free and suitable to the discrete latent spaces of state-of-the-art text-to-motion diffusion models. LCR is simple and consistently outperforms baselines qualitatively and quantitatively. Project page: \href{https://www.pinlab.org/hmu}{https://www.pinlab.org/hmu}.
<div id='section'>Paperid: <span id='pid'>187, <a href='https://arxiv.org/pdf/2503.18211.pdf' target='_blank'>https://arxiv.org/pdf/2503.18211.pdf</a></span>   <span><a href='https://github.com/lzhyu/SimMotionEdit' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhengyuan Li, Kai Cheng, Anindita Ghosh, Uttaran Bhattacharya, Liangyan Gui, Aniket Bera
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.18211">SimMotionEdit: Text-Based Human Motion Editing with Motion Similarity Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-based 3D human motion editing is a critical yet challenging task in computer vision and graphics. While training-free approaches have been explored, the recent release of the MotionFix dataset, which includes source-text-motion triplets, has opened new avenues for training, yielding promising results. However, existing methods struggle with precise control, often leading to misalignment between motion semantics and language instructions. In this paper, we introduce a related task, motion similarity prediction, and propose a multi-task training paradigm, where we train the model jointly on motion editing and motion similarity prediction to foster the learning of semantically meaningful representations. To complement this task, we design an advanced Diffusion-Transformer-based architecture that separately handles motion similarity prediction and motion editing. Extensive experiments demonstrate the state-of-the-art performance of our approach in both editing alignment and fidelity.
<div id='section'>Paperid: <span id='pid'>188, <a href='https://arxiv.org/pdf/2503.17544.pdf' target='_blank'>https://arxiv.org/pdf/2503.17544.pdf</a></span>   <span><a href='https://yz-cnsdqz.github.io/eigenmotion/PRIMAL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yan Zhang, Yao Feng, AlpÃ¡r Cseke, Nitin Saini, Nathan Bajandas, Nicolas Heron, Michael J. Black
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.17544">PRIMAL: Physically Reactive and Interactive Motor Model for Avatar Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We formulate the motor system of an interactive avatar as a generative motion model that can drive the body to move through 3D space in a perpetual, realistic, controllable, and responsive manner. Although human motion generation has been extensively studied, many existing methods lack the responsiveness and realism of real human movements. Inspired by recent advances in foundation models, we propose PRIMAL, which is learned with a two-stage paradigm. In the pretraining stage, the model learns body movements from a large number of sub-second motion segments, providing a generative foundation from which more complex motions are built. This training is fully unsupervised without annotations. Given a single-frame initial state during inference, the pretrained model not only generates unbounded, realistic, and controllable motion, but also enables the avatar to be responsive to induced impulses in real time. In the adaptation phase, we employ a novel ControlNet-like adaptor to fine-tune the base model efficiently, adapting it to new tasks such as few-shot personalized action generation and spatial target reaching. Evaluations show that our proposed method outperforms state-of-the-art baselines. We leverage the model to create a real-time character animation system in Unreal Engine that feels highly responsive and natural. Code, models, and more results are available at: https://yz-cnsdqz.github.io/eigenmotion/PRIMAL
<div id='section'>Paperid: <span id='pid'>189, <a href='https://arxiv.org/pdf/2503.16421.pdf' target='_blank'>https://arxiv.org/pdf/2503.16421.pdf</a></span>   <span><a href='https://quanhaol.github.io/magicmotion-site' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Quanhao Li, Zhen Xing, Rui Wang, Hui Zhang, Qi Dai, Zuxuan Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.16421">MagicMotion: Controllable Video Generation with Dense-to-Sparse Trajectory Guidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in video generation have led to remarkable improvements in visual quality and temporal coherence. Upon this, trajectory-controllable video generation has emerged to enable precise object motion control through explicitly defined spatial paths. However, existing methods struggle with complex object movements and multi-object motion control, resulting in imprecise trajectory adherence, poor object consistency, and compromised visual quality. Furthermore, these methods only support trajectory control in a single format, limiting their applicability in diverse scenarios. Additionally, there is no publicly available dataset or benchmark specifically tailored for trajectory-controllable video generation, hindering robust training and systematic evaluation. To address these challenges, we introduce MagicMotion, a novel image-to-video generation framework that enables trajectory control through three levels of conditions from dense to sparse: masks, bounding boxes, and sparse boxes. Given an input image and trajectories, MagicMotion seamlessly animates objects along defined trajectories while maintaining object consistency and visual quality. Furthermore, we present MagicData, a large-scale trajectory-controlled video dataset, along with an automated pipeline for annotation and filtering. We also introduce MagicBench, a comprehensive benchmark that assesses both video quality and trajectory control accuracy across different numbers of objects. Extensive experiments demonstrate that MagicMotion outperforms previous methods across various metrics. Our project page are publicly available at https://quanhaol.github.io/magicmotion-site.
<div id='section'>Paperid: <span id='pid'>190, <a href='https://arxiv.org/pdf/2503.14637.pdf' target='_blank'>https://arxiv.org/pdf/2503.14637.pdf</a></span>   <span><a href='https://github.com/amathislab/Kinesis' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Merkourios Simos, Alberto Silvio Chiappa, Alexander Mathis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.14637">Reinforcement learning-based motion imitation for physiologically plausible musculoskeletal motor control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>How do humans move? The quest to understand human motion has broad applications in numerous fields, ranging from computer animation and motion synthesis to neuroscience, human prosthetics and rehabilitation. Although advances in reinforcement learning (RL) have produced impressive results in capturing human motion using simplified humanoids, controlling physiologically accurate models of the body remains an open challenge. In this work, we present a model-free motion imitation framework (KINESIS) to advance the understanding of muscle-based motor control. Using a musculoskeletal model of the lower body with 80 muscle actuators and 20 DoF, we demonstrate that KINESIS achieves strong imitation performance on 1.9 hours of motion capture data, is controllable by natural language through pre-trained text-to-motion generative models, and can be fine-tuned to carry out high-level tasks such as target goal reaching. Importantly, KINESIS generates muscle activity patterns that correlate well with human EMG activity. The physiological plausibility makes KINESIS a promising model for tackling challenging problems in human motor control theory, which we highlight by investigating Bernstein's redundancy problem in the context of locomotion. Code, videos and benchmarks will be available at https://github.com/amathislab/Kinesis.
<div id='section'>Paperid: <span id='pid'>191, <a href='https://arxiv.org/pdf/2503.13836.pdf' target='_blank'>https://arxiv.org/pdf/2503.13836.pdf</a></span>   <span><a href='https://seokhyeonhong.github.io/projects/salad/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Seokhyeon Hong, Chaelin Kim, Serin Yoon, Junghyun Nam, Sihun Cha, Junyong Noh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.13836">SALAD: Skeleton-aware Latent Diffusion for Text-driven Motion Generation and Editing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-driven motion generation has advanced significantly with the rise of denoising diffusion models. However, previous methods often oversimplify representations for the skeletal joints, temporal frames, and textual words, limiting their ability to fully capture the information within each modality and their interactions. Moreover, when using pre-trained models for downstream tasks, such as editing, they typically require additional efforts, including manual interventions, optimization, or fine-tuning. In this paper, we introduce a skeleton-aware latent diffusion (SALAD), a model that explicitly captures the intricate inter-relationships between joints, frames, and words. Furthermore, by leveraging cross-attention maps produced during the generation process, we enable attention-based zero-shot text-driven motion editing using a pre-trained SALAD model, requiring no additional user input beyond text prompts. Our approach significantly outperforms previous methods in terms of text-motion alignment without compromising generation quality, and demonstrates practical versatility by providing diverse editing capabilities beyond generation. Code is available at project page.
<div id='section'>Paperid: <span id='pid'>192, <a href='https://arxiv.org/pdf/2503.13300.pdf' target='_blank'>https://arxiv.org/pdf/2503.13300.pdf</a></span>   <span><a href='https://github.com/qinghuannn/PMG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ling-An Zeng, Gaojie Wu, Ancong Wu, Jian-Fang Hu, Wei-Shi Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.13300">Progressive Human Motion Generation Based on Text and Few Motion Frames</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Although existing text-to-motion (T2M) methods can produce realistic human motion from text description, it is still difficult to align the generated motion with the desired postures since using text alone is insufficient for precisely describing diverse postures. To achieve more controllable generation, an intuitive way is to allow the user to input a few motion frames describing precise desired postures. Thus, we explore a new Text-Frame-to-Motion (TF2M) generation task that aims to generate motions from text and very few given frames. Intuitively, the closer a frame is to a given frame, the lower the uncertainty of this frame is when conditioned on this given frame. Hence, we propose a novel Progressive Motion Generation (PMG) method to progressively generate a motion from the frames with low uncertainty to those with high uncertainty in multiple stages. During each stage, new frames are generated by a Text-Frame Guided Generator conditioned on frame-aware semantics of the text, given frames, and frames generated in previous stages. Additionally, to alleviate the train-test gap caused by multi-stage accumulation of incorrectly generated frames during testing, we propose a Pseudo-frame Replacement Strategy for training. Experimental results show that our PMG outperforms existing T2M generation methods by a large margin with even one given frame, validating the effectiveness of our PMG. Code is available at https://github.com/qinghuannn/PMG.
<div id='section'>Paperid: <span id='pid'>193, <a href='https://arxiv.org/pdf/2503.13229.pdf' target='_blank'>https://arxiv.org/pdf/2503.13229.pdf</a></span>   <span><a href='https://cyk990422.github.io/HoloGest.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yongkang Cheng, Shaoli Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.13229">HoloGest: Decoupled Diffusion and Motion Priors for Generating Holisticly Expressive Co-speech Gestures</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Animating virtual characters with holistic co-speech gestures is a challenging but critical task. Previous systems have primarily focused on the weak correlation between audio and gestures, leading to physically unnatural outcomes that degrade the user experience. To address this problem, we introduce HoleGest, a novel neural network framework based on decoupled diffusion and motion priors for the automatic generation of high-quality, expressive co-speech gestures. Our system leverages large-scale human motion datasets to learn a robust prior with low audio dependency and high motion reliance, enabling stable global motion and detailed finger movements. To improve the generation efficiency of diffusion-based models, we integrate implicit joint constraints with explicit geometric and conditional constraints, capturing complex motion distributions between large strides. This integration significantly enhances generation speed while maintaining high-quality motion. Furthermore, we design a shared embedding space for gesture-transcription text alignment, enabling the generation of semantically correct gesture actions. Extensive experiments and user feedback demonstrate the effectiveness and potential applications of our model, with our method achieving a level of realism close to the ground truth, providing an immersive user experience. Our code, model, and demo are are available at https://cyk990422.github.io/HoloGest.github.io/.
<div id='section'>Paperid: <span id='pid'>194, <a href='https://arxiv.org/pdf/2503.13047.pdf' target='_blank'>https://arxiv.org/pdf/2503.13047.pdf</a></span>   <span><a href='https://github.com/songruiqi/InsightDrive' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruiqi Song, Xianda Guo, Hangbin Wu, Qinggong Wei, Long Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.13047">InsightDrive: Insight Scene Representation for End-to-End Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Directly generating planning results from raw sensors has become increasingly prevalent due to its adaptability and robustness in complex scenarios. Scene representation, as a key module in the pipeline, has traditionally relied on conventional perception, which focus on the global scene. However, in driving scenarios, human drivers typically focus only on regions that directly impact driving, which often coincide with those required for end-to-end autonomous driving. In this paper, a novel end-to-end autonomous driving method called InsightDrive is proposed, which organizes perception by language-guided scene representation. We introduce an instance-centric scene tokenizer that transforms the surrounding environment into map- and object-aware instance tokens. Scene attention language descriptions, which highlight key regions and obstacles affecting the ego vehicle's movement, are generated by a vision-language model that leverages the cognitive reasoning capabilities of foundation models. We then align scene descriptions with visual features using the vision-language model, guiding visual attention through these descriptions to give effectively scene representation. Furthermore, we employ self-attention and cross-attention mechanisms to model the ego-agents and ego-map relationships to comprehensively build the topological relationships of the scene. Finally, based on scene understanding, we jointly perform motion prediction and planning. Extensive experiments on the widely used nuScenes benchmark demonstrate that the proposed InsightDrive achieves state-of-the-art performance in end-to-end autonomous driving. The code is available at https://github.com/songruiqi/InsightDrive
<div id='section'>Paperid: <span id='pid'>195, <a href='https://arxiv.org/pdf/2503.12955.pdf' target='_blank'>https://arxiv.org/pdf/2503.12955.pdf</a></span>   <span><a href='https://github.com/ZJHTerry18/HumanInScene' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiahe Zhao, Ruibing Hou, Zejie Tian, Hong Chang, Shiguang Shan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.12955">HIS-GPT: Towards 3D Human-In-Scene Multimodal Understanding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a new task to benchmark human-in-scene understanding for embodied agents: Human-In-Scene Question Answering (HIS-QA). Given a human motion within a 3D scene, HIS-QA requires the agent to comprehend human states and behaviors, reason about its surrounding environment, and answer human-related questions within the scene. To support this new task, we present HIS-Bench, a multimodal benchmark that systematically evaluates HIS understanding across a broad spectrum, from basic perception to commonsense reasoning and planning. Our evaluation of various vision-language models on HIS-Bench reveals significant limitations in their ability to handle HIS-QA tasks. To this end, we propose HIS-GPT, the first foundation model for HIS understanding. HIS-GPT integrates 3D scene context and human motion dynamics into large language models while incorporating specialized mechanisms to capture human-scene interactions. Extensive experiments demonstrate that HIS-GPT sets a new state-of-the-art on HIS-QA tasks. We hope this work inspires future research on human behavior analysis in 3D scenes, advancing embodied AI and world models. The codes and data: https://github.com/ZJHTerry18/HumanInScene.
<div id='section'>Paperid: <span id='pid'>196, <a href='https://arxiv.org/pdf/2503.11038.pdf' target='_blank'>https://arxiv.org/pdf/2503.11038.pdf</a></span>   <span><a href='https://mjwei3d.github.io/ACMo/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingjie Wei, Xuemei Xie, Guangming Shi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.11038">ACMo: Attribute Controllable Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Attributes such as style, fine-grained text, and trajectory are specific conditions for describing motion. However, existing methods often lack precise user control over motion attributes and suffer from limited generalizability to unseen motions. This work introduces an Attribute Controllable Motion generation architecture, to address these challenges via decouple any conditions and control them separately. Firstly, we explored the Attribute Diffusion Model to imporve text-to-motion performance via decouple text and motion learning, as the controllable model relies heavily on the pre-trained model. Then, we introduce Motion Adpater to quickly finetune previously unseen motion patterns. Its motion prompts inputs achieve multimodal text-to-motion generation that captures user-specified styles. Finally, we propose a LLM Planner to bridge the gap between unseen attributes and dataset-specific texts via local knowledage for user-friendly interaction. Our approach introduces the capability for motion prompts for stylize generation, enabling fine-grained and user-friendly attribute control while providing performance comparable to state-of-the-art methods. Project page: https://mjwei3d.github.io/ACMo/
<div id='section'>Paperid: <span id='pid'>197, <a href='https://arxiv.org/pdf/2503.09154.pdf' target='_blank'>https://arxiv.org/pdf/2503.09154.pdf</a></span>   <span><a href='https://github.com/PKU-YuanGroup/SwapAnyone' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chengshu Zhao, Yunyang Ge, Xinhua Cheng, Bin Zhu, Yatian Pang, Bin Lin, Fan Yang, Feng Gao, Li Yuan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.09154">SwapAnyone: Consistent and Realistic Video Synthesis for Swapping Any Person into Any Video</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video body-swapping aims to replace the body in an existing video with a new body from arbitrary sources, which has garnered more attention in recent years. Existing methods treat video body-swapping as a composite of multiple tasks instead of an independent task and typically rely on various models to achieve video body-swapping sequentially. However, these methods fail to achieve end-to-end optimization for the video body-swapping which causes issues such as variations in luminance among frames, disorganized occlusion relationships, and the noticeable separation between bodies and background. In this work, we define video body-swapping as an independent task and propose three critical consistencies: identity consistency, motion consistency, and environment consistency. We introduce an end-to-end model named SwapAnyone, treating video body-swapping as a video inpainting task with reference fidelity and motion control. To improve the ability to maintain environmental harmony, particularly luminance harmony in the resulting video, we introduce a novel EnvHarmony strategy for training our model progressively. Additionally, we provide a dataset named HumanAction-32K covering various videos about human actions. Extensive experiments demonstrate that our method achieves State-Of-The-Art (SOTA) performance among open-source methods while approaching or surpassing closed-source models across multiple dimensions. All code, model weights, and the HumanAction-32K dataset will be open-sourced at https://github.com/PKU-YuanGroup/SwapAnyone.
<div id='section'>Paperid: <span id='pid'>198, <a href='https://arxiv.org/pdf/2503.08664.pdf' target='_blank'>https://arxiv.org/pdf/2503.08664.pdf</a></span>   <span><a href='https://github.com/johannwyh/MEAT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuhan Wang, Fangzhou Hong, Shuai Yang, Liming Jiang, Wayne Wu, Chen Change Loy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.08664">MEAT: Multiview Diffusion Model for Human Generation on Megapixels with Mesh Attention</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multiview diffusion models have shown considerable success in image-to-3D generation for general objects. However, when applied to human data, existing methods have yet to deliver promising results, largely due to the challenges of scaling multiview attention to higher resolutions. In this paper, we explore human multiview diffusion models at the megapixel level and introduce a solution called mesh attention to enable training at 1024x1024 resolution. Using a clothed human mesh as a central coarse geometric representation, the proposed mesh attention leverages rasterization and projection to establish direct cross-view coordinate correspondences. This approach significantly reduces the complexity of multiview attention while maintaining cross-view consistency. Building on this foundation, we devise a mesh attention block and combine it with keypoint conditioning to create our human-specific multiview diffusion model, MEAT. In addition, we present valuable insights into applying multiview human motion videos for diffusion training, addressing the longstanding issue of data scarcity. Extensive experiments show that MEAT effectively generates dense, consistent multiview human images at the megapixel level, outperforming existing multiview diffusion methods.
<div id='section'>Paperid: <span id='pid'>199, <a href='https://arxiv.org/pdf/2503.08270.pdf' target='_blank'>https://arxiv.org/pdf/2503.08270.pdf</a></span>   <span><a href='https://jackyu6.github.io/HERO' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chengjun Yu, Wei Zhai, Yuhang Yang, Yang Cao, Zheng-Jun Zha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.08270">HERO: Human Reaction Generation from Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human reaction generation represents a significant research domain for interactive AI, as humans constantly interact with their surroundings. Previous works focus mainly on synthesizing the reactive motion given a human motion sequence. This paradigm limits interaction categories to human-human interactions and ignores emotions that may influence reaction generation. In this work, we propose to generate 3D human reactions from RGB videos, which involves a wider range of interaction categories and naturally provides information about expressions that may reflect the subject's emotions. To cope with this task, we present HERO, a simple yet powerful framework for Human rEaction geneRation from videOs. HERO considers both global and frame-level local representations of the video to extract the interaction intention, and then uses the extracted interaction intention to guide the synthesis of the reaction. Besides, local visual representations are continuously injected into the model to maximize the exploitation of the dynamic properties inherent in videos. Furthermore, the ViMo dataset containing paired Video-Motion data is collected to support the task. In addition to human-human interactions, these video-motion pairs also cover animal-human interactions and scene-human interactions. Extensive experiments demonstrate the superiority of our methodology. The code and dataset will be publicly available at https://jackyu6.github.io/HERO.
<div id='section'>Paperid: <span id='pid'>200, <a href='https://arxiv.org/pdf/2503.07597.pdf' target='_blank'>https://arxiv.org/pdf/2503.07597.pdf</a></span>   <span><a href='https://zhangyuhong01.github.io/HumanMM/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuhong Zhang, Guanlin Wu, Ling-Hao Chen, Zhuokai Zhao, Jing Lin, Xiaoke Jiang, Jiamin Wu, Zhuoheng Li, Hao Frank Yang, Haoqian Wang, Lei Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.07597">HumanMM: Global Human Motion Recovery from Multi-shot Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we present a novel framework designed to reconstruct long-sequence 3D human motion in the world coordinates from in-the-wild videos with multiple shot transitions. Such long-sequence in-the-wild motions are highly valuable to applications such as motion generation and motion understanding, but are of great challenge to be recovered due to abrupt shot transitions, partial occlusions, and dynamic backgrounds presented in such videos. Existing methods primarily focus on single-shot videos, where continuity is maintained within a single camera view, or simplify multi-shot alignment in camera space only. In this work, we tackle the challenges by integrating an enhanced camera pose estimation with Human Motion Recovery (HMR) by incorporating a shot transition detector and a robust alignment module for accurate pose and orientation continuity across shots. By leveraging a custom motion integrator, we effectively mitigate the problem of foot sliding and ensure temporal consistency in human pose. Extensive evaluations on our created multi-shot dataset from public 3D human datasets demonstrate the robustness of our method in reconstructing realistic human motion in world coordinates.
<div id='section'>Paperid: <span id='pid'>201, <a href='https://arxiv.org/pdf/2503.04257.pdf' target='_blank'>https://arxiv.org/pdf/2503.04257.pdf</a></span>   <span><a href='https://t2m4lvo.github.io' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wonkwang Lee, Jongwon Jeong, Taehong Moon, Hyeon-Jong Kim, Jaehyeon Kim, Gunhee Kim, Byeong-Uk Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.04257">How to Move Your Dragon: Text-to-Motion Synthesis for Large-Vocabulary Objects</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion synthesis for diverse object categories holds great potential for 3D content creation but remains underexplored due to two key challenges: (1) the lack of comprehensive motion datasets that include a wide range of high-quality motions and annotations, and (2) the absence of methods capable of handling heterogeneous skeletal templates from diverse objects. To address these challenges, we contribute the following: First, we augment the Truebones Zoo dataset, a high-quality animal motion dataset covering over 70 species, by annotating it with detailed text descriptions, making it suitable for text-based motion synthesis. Second, we introduce rig augmentation techniques that generate diverse motion data while preserving consistent dynamics, enabling models to adapt to various skeletal configurations. Finally, we redesign existing motion diffusion models to dynamically adapt to arbitrary skeletal templates, enabling motion synthesis for a diverse range of objects with varying structures. Experiments show that our method learns to generate high-fidelity motions from textual descriptions for diverse and even unseen objects, setting a strong foundation for motion synthesis across diverse object categories and skeletal templates. Qualitative results are available at: $\href{https://t2m4lvo.github.io}{https://t2m4lvo.github.io}$.
<div id='section'>Paperid: <span id='pid'>202, <a href='https://arxiv.org/pdf/2503.03222.pdf' target='_blank'>https://arxiv.org/pdf/2503.03222.pdf</a></span>   <span><a href='https://wangzhumei.github.io/mocap-2-to-3/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhumei Wang, Zechen Hu, Ruoxi Guo, Huaijin Pi, Ziyong Feng, Sida Peng, Xiaowei Zhou, Mingtao Pei, Siyuan Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.03222">Mocap-2-to-3: Multi-view Lifting for Monocular Motion Recovery with 2D Pretraining</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recovering absolute human motion from monocular inputs is challenging due to two main issues. First, existing methods depend on 3D training data collected from limited environments, constraining out-of-distribution generalization. The second issue is the difficulty of estimating metric-scale poses from monocular input. To address these challenges, we introduce Mocap-2-to-3, a novel framework that performs multi-view lifting from monocular input by leveraging 2D data pre-training, enabling the reconstruction of metrically accurate 3D motions with absolute positions. To leverage abundant 2D data, we decompose complex 3D motion into multi-view syntheses. We first pretrain a single-view diffusion model on extensive 2D datasets, then fine-tune a multi-view model using public 3D data to enable view-consistent motion generation from monocular input, allowing the model to acquire action priors and diversity through 2D data. Furthermore, to recover absolute poses, we propose a novel human motion representation that decouples the learning of local pose and global movements, while encoding geometric priors of the ground to accelerate convergence. This enables progressive recovery of motion in absolute space during inference. Experimental results on in-the-wild benchmarks demonstrate that our method surpasses state-of-the-art approaches in both camera-space motion realism and world-grounded human positioning, while exhibiting superior generalization capability. Our code will be made publicly available.
<div id='section'>Paperid: <span id='pid'>203, <a href='https://arxiv.org/pdf/2503.01616.pdf' target='_blank'>https://arxiv.org/pdf/2503.01616.pdf</a></span>   <span><a href='https://henryhcliu.github.io/robodexvlm' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haichao Liu, Sikai Guo, Pengfei Mai, Jiahang Cao, Haoang Li, Jun Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.01616">RoboDexVLM: Visual Language Model-Enabled Task Planning and Motion Control for Dexterous Robot Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces RoboDexVLM, an innovative framework for robot task planning and grasp detection tailored for a collaborative manipulator equipped with a dexterous hand. Previous methods focus on simplified and limited manipulation tasks, which often neglect the complexities associated with grasping a diverse array of objects in a long-horizon manner. In contrast, our proposed framework utilizes a dexterous hand capable of grasping objects of varying shapes and sizes while executing tasks based on natural language commands. The proposed approach has the following core components: First, a robust task planner with a task-level recovery mechanism that leverages vision-language models (VLMs) is designed, which enables the system to interpret and execute open-vocabulary commands for long sequence tasks. Second, a language-guided dexterous grasp perception algorithm is presented based on robot kinematics and formal methods, tailored for zero-shot dexterous manipulation with diverse objects and commands. Comprehensive experimental results validate the effectiveness, adaptability, and robustness of RoboDexVLM in handling long-horizon scenarios and performing dexterous grasping. These results highlight the framework's ability to operate in complex environments, showcasing its potential for open-vocabulary dexterous manipulation. Our open-source project page can be found at https://henryhcliu.github.io/robodexvlm.
<div id='section'>Paperid: <span id='pid'>204, <a href='https://arxiv.org/pdf/2503.01291.pdf' target='_blank'>https://arxiv.org/pdf/2503.01291.pdf</a></span>   <span><a href='https://4dvlab.github.io/project_page/semgeomo/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Peishan Cong, Ziyi Wang, Yuexin Ma, Xiangyu Yue
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.01291">SemGeoMo: Dynamic Contextual Human Motion Generation with Semantic and Geometric Guidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating reasonable and high-quality human interactive motions in a given dynamic environment is crucial for understanding, modeling, transferring, and applying human behaviors to both virtual and physical robots. In this paper, we introduce an effective method, SemGeoMo, for dynamic contextual human motion generation, which fully leverages the text-affordance-joint multi-level semantic and geometric guidance in the generation process, improving the semantic rationality and geometric correctness of generative motions. Our method achieves state-of-the-art performance on three datasets and demonstrates superior generalization capability for diverse interaction scenarios. The project page and code can be found at https://4dvlab.github.io/project_page/semgeomo/.
<div id='section'>Paperid: <span id='pid'>205, <a href='https://arxiv.org/pdf/2502.20390.pdf' target='_blank'>https://arxiv.org/pdf/2502.20390.pdf</a></span>   <span><a href='https://sirui-xu.github.io/InterMimic/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sirui Xu, Hung Yu Ling, Yu-Xiong Wang, Liang-Yan Gui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.20390">InterMimic: Towards Universal Whole-Body Control for Physics-Based Human-Object Interactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Achieving realistic simulations of humans interacting with a wide range of objects has long been a fundamental goal. Extending physics-based motion imitation to complex human-object interactions (HOIs) is challenging due to intricate human-object coupling, variability in object geometries, and artifacts in motion capture data, such as inaccurate contacts and limited hand detail. We introduce InterMimic, a framework that enables a single policy to robustly learn from hours of imperfect MoCap data covering diverse full-body interactions with dynamic and varied objects. Our key insight is to employ a curriculum strategy -- perfect first, then scale up. We first train subject-specific teacher policies to mimic, retarget, and refine motion capture data. Next, we distill these teachers into a student policy, with the teachers acting as online experts providing direct supervision, as well as high-quality references. Notably, we incorporate RL fine-tuning on the student policy to surpass mere demonstration replication and achieve higher-quality solutions. Our experiments demonstrate that InterMimic produces realistic and diverse interactions across multiple HOI datasets. The learned policy generalizes in a zero-shot manner and seamlessly integrates with kinematic generators, elevating the framework from mere imitation to generative modeling of complex human-object interactions.
<div id='section'>Paperid: <span id='pid'>206, <a href='https://arxiv.org/pdf/2502.19868.pdf' target='_blank'>https://arxiv.org/pdf/2502.19868.pdf</a></span>   <span><a href='https://github.com/WesLee88524/C-Drag-Official-Repo' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuhao Li, Mirana Claire Angel, Salman Khan, Yu Zhu, Jinqiu Sun, Yanning Zhang, Fahad Shahbaz Khan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.19868">C-Drag: Chain-of-Thought Driven Motion Controller for Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Trajectory-based motion control has emerged as an intuitive and efficient approach for controllable video generation. However, the existing trajectory-based approaches are usually limited to only generating the motion trajectory of the controlled object and ignoring the dynamic interactions between the controlled object and its surroundings. To address this limitation, we propose a Chain-of-Thought-based motion controller for controllable video generation, named C-Drag. Instead of directly generating the motion of some objects, our C-Drag first performs object perception and then reasons the dynamic interactions between different objects according to the given motion control of the objects. Specifically, our method includes an object perception module and a Chain-of-Thought-based motion reasoning module. The object perception module employs visual language models to capture the position and category information of various objects within the image. The Chain-of-Thought-based motion reasoning module takes this information as input and conducts a stage-wise reasoning process to generate motion trajectories for each of the affected objects, which are subsequently fed to the diffusion model for video synthesis. Furthermore, we introduce a new video object interaction (VOI) dataset to evaluate the generation quality of motion controlled video generation methods. Our VOI dataset contains three typical types of interactions and provides the motion trajectories of objects that can be used for accurate performance evaluation. Experimental results show that C-Drag achieves promising performance across multiple metrics, excelling in object motion control. Our benchmark, codes, and models will be available at https://github.com/WesLee88524/C-Drag-Official-Repo.
<div id='section'>Paperid: <span id='pid'>207, <a href='https://arxiv.org/pdf/2502.17327.pdf' target='_blank'>https://arxiv.org/pdf/2502.17327.pdf</a></span>   <span><a href='https://anytop2025.github.io/Anytop-page,' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/Anytop2025/Anytop' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Inbar Gat, Sigal Raab, Guy Tevet, Yuval Reshef, Amit H. Bermano, Daniel Cohen-Or
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.17327">AnyTop: Character Animation Diffusion with Any Topology</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating motion for arbitrary skeletons is a longstanding challenge in computer graphics, remaining largely unexplored due to the scarcity of diverse datasets and the irregular nature of the data. In this work, we introduce AnyTop, a diffusion model that generates motions for diverse characters with distinct motion dynamics, using only their skeletal structure as input. Our work features a transformer-based denoising network, tailored for arbitrary skeleton learning, integrating topology information into the traditional attention mechanism. Additionally, by incorporating textual joint descriptions into the latent feature representation, AnyTop learns semantic correspondences between joints across diverse skeletons. Our evaluation demonstrates that AnyTop generalizes well, even with as few as three training examples per topology, and can produce motions for unseen skeletons as well. Furthermore, our model's latent space is highly informative, enabling downstream tasks such as joint correspondence, temporal segmentation and motion editing. Our webpage, https://anytop2025.github.io/Anytop-page, includes links to videos and code.
<div id='section'>Paperid: <span id='pid'>208, <a href='https://arxiv.org/pdf/2502.16175.pdf' target='_blank'>https://arxiv.org/pdf/2502.16175.pdf</a></span>   <span><a href='https://koyui.github.io/mojito/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziwei Shan, Yaoyu He, Chengfeng Zhao, Jiashen Du, Jingyan Zhang, Qixuan Zhang, Jingyi Yu, Lan Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.16175">Mojito: LLM-Aided Motion Instructor with Jitter-Reduced Inertial Tokens</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human bodily movements convey critical insights into action intentions and cognitive processes, yet existing multimodal systems primarily focused on understanding human motion via language, vision, and audio, which struggle to capture the dynamic forces and torques inherent in 3D motion. Inertial measurement units (IMUs) present a promising alternative, offering lightweight, wearable, and privacy-conscious motion sensing. However, processing of streaming IMU data faces challenges such as wireless transmission instability, sensor noise, and drift, limiting their utility for long-term real-time motion capture (MoCap), and more importantly, online motion analysis. To address these challenges, we introduce Mojito, an intelligent motion agent that integrates inertial sensing with large language models (LLMs) for interactive motion capture and behavioral analysis.
<div id='section'>Paperid: <span id='pid'>209, <a href='https://arxiv.org/pdf/2502.12975.pdf' target='_blank'>https://arxiv.org/pdf/2502.12975.pdf</a></span>   <span><a href='https://npucvr.github.io/EvInsMOS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhexiong Wan, Bin Fan, Le Hui, Yuchao Dai, Gim Hee Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.12975">Instance-Level Moving Object Segmentation from a Single Image with Events</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Moving object segmentation plays a crucial role in understanding dynamic scenes involving multiple moving objects, while the difficulties lie in taking into account both spatial texture structures and temporal motion cues. Existing methods based on video frames encounter difficulties in distinguishing whether pixel displacements of an object are caused by camera motion or object motion due to the complexities of accurate image-based motion modeling. Recent advances exploit the motion sensitivity of novel event cameras to counter conventional images' inadequate motion modeling capabilities, but instead lead to challenges in segmenting pixel-level object masks due to the lack of dense texture structures in events. To address these two limitations imposed by unimodal settings, we propose the first instance-level moving object segmentation framework that integrates complementary texture and motion cues. Our model incorporates implicit cross-modal masked attention augmentation, explicit contrastive feature learning, and flow-guided motion enhancement to exploit dense texture information from a single image and rich motion information from events, respectively. By leveraging the augmented texture and motion features, we separate mask segmentation from motion classification to handle varying numbers of independently moving objects. Through extensive evaluations on multiple datasets, as well as ablation experiments with different input settings and real-time efficiency analysis of the proposed framework, we believe that our first attempt to incorporate image and event data for practical deployment can provide new insights for future work in event-based motion related works. The source code with model training and pre-trained weights is released at https://npucvr.github.io/EvInsMOS
<div id='section'>Paperid: <span id='pid'>210, <a href='https://arxiv.org/pdf/2502.11644.pdf' target='_blank'>https://arxiv.org/pdf/2502.11644.pdf</a></span>   <span><a href='https://github.com/IDASLab/InTec_Framework' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Habib Larian, Faramarz Safi-Esfahani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.11644">InTec: integrated things-edge computing: a framework for distributing machine learning pipelines in edge AI systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapid expansion of the Internet of Things (IoT), sensors, smartphones, and wearables have become integral to daily life, powering smart applications in home automation, healthcare, and intelligent transportation. However, these advancements face significant challenges due to latency and bandwidth constraints imposed by traditional cloud based machine learning (ML) frameworks. The need for innovative solutions is evident as cloud computing struggles with increased latency and network congestion. Previous attempts to offload parts of the ML pipeline to edge and cloud layers have yet to fully resolve these issues, often worsening system response times and network congestion due to the computational limitations of edge devices. In response to these challenges, this study introduces the InTec (Integrated Things Edge Computing) framework, a groundbreaking innovation in IoT architecture. Unlike existing methods, InTec fully leverages the potential of a three tier architecture by strategically distributing ML tasks across the Things, Edge, and Cloud layers. This comprehensive approach enables real time data processing at the point of data generation, significantly reducing latency, optimizing network traffic, and enhancing system reliability. InTec effectiveness is validated through empirical evaluation using the MHEALTH dataset for human motion detection in smart homes, demonstrating notable improvements in key metrics: an 81.56 percent reduction in response time, a 10.92 percent decrease in network traffic, a 9.82 percent improvement in throughput, a 21.86 percent reduction in edge energy consumption, and a 25.83 percent reduction in cloud energy consumption. These advancements establish InTec as a new benchmark for scalable, responsive, and energy efficient IoT applications, demonstrating its potential to revolutionize how the ML pipeline is integrated into Edge AI (EI) systems.
<div id='section'>Paperid: <span id='pid'>211, <a href='https://arxiv.org/pdf/2502.08244.pdf' target='_blank'>https://arxiv.org/pdf/2502.08244.pdf</a></span>   <span><a href='https://github.com/JinWonjoon/FloVD' target='_blank'>  GitHub</a></span> <span><a href='https://jinwonjoon.github.io/flovd_site/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wonjoon Jin, Qi Dai, Chong Luo, Seung-Hwan Baek, Sunghyun Cho
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.08244">FloVD: Optical Flow Meets Video Diffusion Model for Enhanced Camera-Controlled Video Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present FloVD, a novel video diffusion model for camera-controllable video generation. FloVD leverages optical flow to represent the motions of the camera and moving objects. This approach offers two key benefits. Since optical flow can be directly estimated from videos, our approach allows for the use of arbitrary training videos without ground-truth camera parameters. Moreover, as background optical flow encodes 3D correlation across different viewpoints, our method enables detailed camera control by leveraging the background motion. To synthesize natural object motion while supporting detailed camera control, our framework adopts a two-stage video synthesis pipeline consisting of optical flow generation and flow-conditioned video synthesis. Extensive experiments demonstrate the superiority of our method over previous approaches in terms of accurate camera control and natural object motion synthesis.
<div id='section'>Paperid: <span id='pid'>212, <a href='https://arxiv.org/pdf/2502.07531.pdf' target='_blank'>https://arxiv.org/pdf/2502.07531.pdf</a></span>   <span><a href='https://sixiaozheng.github.io/VidCRAFT3/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sixiao Zheng, Zimian Peng, Yanpeng Zhou, Yi Zhu, Hang Xu, Xiangru Huang, Yanwei Fu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.07531">VidCRAFT3: Camera, Object, and Lighting Control for Image-to-Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Controllable image-to-video (I2V) generation transforms a reference image into a coherent video guided by user-specified control signals. In content creation workflows, precise and simultaneous control over camera motion, object motion, and lighting direction enhances both accuracy and flexibility. However, existing approaches typically treat these control signals separately, largely due to the scarcity of datasets with high-quality joint annotations and mismatched control spaces across modalities. We present VidCRAFT3, a unified and flexible I2V framework that supports both independent and joint control over camera motion, object motion, and lighting direction by integrating three core components. Image2Cloud reconstructs a 3D point cloud from the reference image to enable precise camera motion control. ObjMotionNet encodes sparse object trajectories into multi-scale optical flow features to guide object motion. The Spatial Triple-Attention Transformer integrates lighting direction embeddings via parallel cross-attention. To address the scarcity of jointly annotated data, we curate the VideoLightingDirection (VLD) dataset of synthetic static-scene video clips with per-frame lighting-direction labels, and adopt a three-stage training strategy that enables robust learning without fully joint annotations. Extensive experiments show that VidCRAFT3 outperforms existing methods in control precision and visual coherence. Code and data will be released. Project page: https://sixiaozheng.github.io/VidCRAFT3/.
<div id='section'>Paperid: <span id='pid'>213, <a href='https://arxiv.org/pdf/2502.06221.pdf' target='_blank'>https://arxiv.org/pdf/2502.06221.pdf</a></span>   <span><a href='https://github.com/tedhuang96/icp' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhe Huang, Tianchen Ji, Heling Zhang, Fatemeh Cheraghi Pouria, Katherine Driggs-Campbell, Roy Dong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.06221">Interaction-aware Conformal Prediction for Crowd Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>During crowd navigation, robot motion plan needs to consider human motion uncertainty, and the human motion uncertainty is dependent on the robot motion plan. We introduce Interaction-aware Conformal Prediction (ICP) to alternate uncertainty-aware robot motion planning and decision-dependent human motion uncertainty quantification. ICP is composed of a trajectory predictor to predict human trajectories, a model predictive controller to plan robot motion with confidence interval radii added for probabilistic safety, a human simulator to collect human trajectory calibration dataset conditioned on the planned robot motion, and a conformal prediction module to quantify trajectory prediction error on the decision-dependent calibration dataset. Crowd navigation simulation experiments show that ICP strikes a good balance of performance among navigation efficiency, social awareness, and uncertainty quantification compared to previous works. ICP generalizes well to navigation tasks under various crowd densities. The fast runtime and efficient memory usage make ICP practical for real-world applications. Code is available at https://github.com/tedhuang96/icp.
<div id='section'>Paperid: <span id='pid'>214, <a href='https://arxiv.org/pdf/2502.05857.pdf' target='_blank'>https://arxiv.org/pdf/2502.05857.pdf</a></span>   <span><a href='https://github.com/zju3dv/EgoAgent' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lu Chen, Yizhou Wang, Shixiang Tang, Qianhong Ma, Tong He, Wanli Ouyang, Xiaowei Zhou, Hujun Bao, Sida Peng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.05857">EgoAgent: A Joint Predictive Agent Model in Egocentric Worlds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning an agent model that behaves like humans-capable of jointly perceiving the environment, predicting the future, and taking actions from a first-person perspective-is a fundamental challenge in computer vision. Existing methods typically train separate models for these abilities, which fail to capture their intrinsic relationships and prevent them from learning from each other. Inspired by how humans learn through the perception-action loop, we propose EgoAgent, a unified agent model that simultaneously learns to represent, predict, and act within a single transformer. EgoAgent explicitly models the causal and temporal dependencies among these abilities by formulating the task as an interleaved sequence of states and actions. It further introduces a joint embedding-action-prediction architecture with temporally asymmetric predictor and observer branches, enabling synergistic optimization across all three capabilities. Comprehensive evaluations of EgoAgent on representative tasks such as image classification, egocentric future state prediction, and 3D human motion prediction demonstrate the superiority of our method. The code and trained models will be publicly available at https://github.com/zju3dv/EgoAgent.
<div id='section'>Paperid: <span id='pid'>215, <a href='https://arxiv.org/pdf/2502.05792.pdf' target='_blank'>https://arxiv.org/pdf/2502.05792.pdf</a></span>   <span><a href='https://github.com/centiLinda/AToM-human-prediction.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuwen Liao, Muqing Cao, Xinhang Xu, Lihua Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.05792">AToM: Adaptive Theory-of-Mind-Based Human Motion Prediction in Long-Term Human-Robot Interactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humans learn from observations and experiences to adjust their behaviours towards better performance. Interacting with such dynamic humans is challenging, as the robot needs to predict the humans accurately for safe and efficient operations. Long-term interactions with dynamic humans have not been extensively studied by prior works. We propose an adaptive human prediction model based on the Theory-of-Mind (ToM), a fundamental social-cognitive ability that enables humans to infer others' behaviours and intentions. We formulate the human internal belief about others using a game-theoretic model, which predicts the future motions of all agents in a navigation scenario. To estimate an evolving belief, we use an Unscented Kalman Filter to update the behavioural parameters in the human internal model. Our formulation provides unique interpretability to dynamic human behaviours by inferring how the human predicts the robot. We demonstrate through long-term experiments in both simulations and real-world settings that our prediction effectively promotes safety and efficiency in downstream robot planning. Code will be available at https://github.com/centiLinda/AToM-human-prediction.git.
<div id='section'>Paperid: <span id='pid'>216, <a href='https://arxiv.org/pdf/2502.04847.pdf' target='_blank'>https://arxiv.org/pdf/2502.04847.pdf</a></span>   <span><a href='https://agnjason.github.io/HumanDiT-page/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qijun Gan, Yi Ren, Chen Zhang, Zhenhui Ye, Pan Xie, Xiang Yin, Zehuan Yuan, Bingyue Peng, Jianke Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.04847">HumanDiT: Pose-Guided Diffusion Transformer for Long-form Human Motion Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion video generation has advanced significantly, while existing methods still struggle with accurately rendering detailed body parts like hands and faces, especially in long sequences and intricate motions. Current approaches also rely on fixed resolution and struggle to maintain visual consistency. To address these limitations, we propose HumanDiT, a pose-guided Diffusion Transformer (DiT)-based framework trained on a large and wild dataset containing 14,000 hours of high-quality video to produce high-fidelity videos with fine-grained body rendering. Specifically, (i) HumanDiT, built on DiT, supports numerous video resolutions and variable sequence lengths, facilitating learning for long-sequence video generation; (ii) we introduce a prefix-latent reference strategy to maintain personalized characteristics across extended sequences. Furthermore, during inference, HumanDiT leverages Keypoint-DiT to generate subsequent pose sequences, facilitating video continuation from static images or existing videos. It also utilizes a Pose Adapter to enable pose transfer with given sequences. Extensive experiments demonstrate its superior performance in generating long-form, pose-accurate videos across diverse scenarios.
<div id='section'>Paperid: <span id='pid'>217, <a href='https://arxiv.org/pdf/2502.02817.pdf' target='_blank'>https://arxiv.org/pdf/2502.02817.pdf</a></span>   <span><a href='https://haoyin116.github.io/Survey_of_AQA/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Yin, Paritosh Parmar, Daoliang Xu, Yang Zhang, Tianyou Zheng, Weiwei Fu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.02817">A Decade of Action Quality Assessment: Largest Systematic Survey of Trends, Challenges, and Future Directions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Action Quality Assessment (AQA) -- the ability to quantify the quality of human motion, actions, or skill levels and provide feedback -- has far-reaching implications in areas such as low-cost physiotherapy, sports training, and workforce development. As such, it has become a critical field in computer vision & video understanding over the past decade. Significant progress has been made in AQA methodologies, datasets, & applications, yet a pressing need remains for a comprehensive synthesis of this rapidly evolving field. In this paper, we present a thorough survey of the AQA landscape, systematically reviewing over 200 research papers using the preferred reporting items for systematic reviews & meta-analyses (PRISMA) framework. We begin by covering foundational concepts & definitions, then move to general frameworks & performance metrics, & finally discuss the latest advances in methodologies & datasets. This survey provides a detailed analysis of research trends, performance comparisons, challenges, & future directions. Through this work, we aim to offer a valuable resource for both newcomers & experienced researchers, promoting further exploration & progress in AQA. Data are available at https://haoyin116.github.io/Survey_of_AQA/
<div id='section'>Paperid: <span id='pid'>218, <a href='https://arxiv.org/pdf/2502.02492.pdf' target='_blank'>https://arxiv.org/pdf/2502.02492.pdf</a></span>   <span><a href='https://hila-chefer.github.io/videojam-paper.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hila Chefer, Uriel Singer, Amit Zohar, Yuval Kirstain, Adam Polyak, Yaniv Taigman, Lior Wolf, Shelly Sheynin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.02492">VideoJAM: Joint Appearance-Motion Representations for Enhanced Motion Generation in Video Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite tremendous recent progress, generative video models still struggle to capture real-world motion, dynamics, and physics. We show that this limitation arises from the conventional pixel reconstruction objective, which biases models toward appearance fidelity at the expense of motion coherence. To address this, we introduce VideoJAM, a novel framework that instills an effective motion prior to video generators, by encouraging the model to learn a joint appearance-motion representation. VideoJAM is composed of two complementary units. During training, we extend the objective to predict both the generated pixels and their corresponding motion from a single learned representation. During inference, we introduce Inner-Guidance, a mechanism that steers the generation toward coherent motion by leveraging the model's own evolving motion prediction as a dynamic guidance signal. Notably, our framework can be applied to any video model with minimal adaptations, requiring no modifications to the training data or scaling of the model. VideoJAM achieves state-of-the-art performance in motion coherence, surpassing highly competitive proprietary models while also enhancing the perceived visual quality of the generations. These findings emphasize that appearance and motion can be complementary and, when effectively integrated, enhance both the visual quality and the coherence of video generation. Project website: https://hila-chefer.github.io/videojam-paper.github.io/
<div id='section'>Paperid: <span id='pid'>219, <a href='https://arxiv.org/pdf/2502.02358.pdf' target='_blank'>https://arxiv.org/pdf/2502.02358.pdf</a></span>   <span><a href='https://diouo.github.io/motionlab.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyan Guo, Zeyu Hu, De Wen Soh, Na Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.02358">MotionLab: Unified Human Motion Generation and Editing via the Motion-Condition-Motion Paradigm</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion generation and editing are key components of computer vision. However, current approaches in this field tend to offer isolated solutions tailored to specific tasks, which can be inefficient and impractical for real-world applications. While some efforts have aimed to unify motion-related tasks, these methods simply use different modalities as conditions to guide motion generation. Consequently, they lack editing capabilities, fine-grained control, and fail to facilitate knowledge sharing across tasks. To address these limitations and provide a versatile, unified framework capable of handling both human motion generation and editing, we introduce a novel paradigm: \textbf{Motion-Condition-Motion}, which enables the unified formulation of diverse tasks with three concepts: source motion, condition, and target motion. Based on this paradigm, we propose a unified framework, \textbf{MotionLab}, which incorporates rectified flows to learn the mapping from source motion to target motion, guided by the specified conditions. In MotionLab, we introduce the 1) MotionFlow Transformer to enhance conditional generation and editing without task-specific modules; 2) Aligned Rotational Position Encoding to guarantee the time synchronization between source motion and target motion; 3) Task Specified Instruction Modulation; and 4) Motion Curriculum Learning for effective multi-task learning and knowledge sharing across tasks. Notably, our MotionLab demonstrates promising generalization capabilities and inference efficiency across multiple benchmarks for human motion. Our code and additional video results are available at: https://diouo.github.io/motionlab.github.io/.
<div id='section'>Paperid: <span id='pid'>220, <a href='https://arxiv.org/pdf/2502.00395.pdf' target='_blank'>https://arxiv.org/pdf/2502.00395.pdf</a></span>   <span><a href='https://github.com/TUMFTM/FlexCloud' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Maximilian Leitenstern, Marko Alten, Christian Bolea-Schaser, Dominik Kulmer, Marcel Weinmann, Markus Lienkamp
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.00395">FlexCloud: Direct, Modular Georeferencing and Drift-Correction of Point Cloud Maps</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current software stacks for real-world applications of autonomous driving leverage map information to ensure reliable localization, path planning, and motion prediction. An important field of research is the generation of point cloud maps, referring to the topic of simultaneous localization and mapping (SLAM). As most recent developments do not include global position data, the resulting point cloud maps suffer from internal distortion and missing georeferencing, preventing their use for map-based localization approaches. Therefore, we propose FlexCloud for an automatic georeferencing of point cloud maps created from SLAM. Our approach is designed to work modularly with different SLAM methods, utilizing only the generated local point cloud map and its odometry. Using the corresponding GNSS positions enables direct georeferencing without additional control points. By leveraging a 3D rubber-sheet transformation, we can correct distortions within the map caused by long-term drift while maintaining its structure. Our approach enables the creation of consistent, globally referenced point cloud maps from data collected by a mobile mapping system (MMS). The source code of our work is available at https://github.com/TUMFTM/FlexCloud.
<div id='section'>Paperid: <span id='pid'>221, <a href='https://arxiv.org/pdf/2501.15648.pdf' target='_blank'>https://arxiv.org/pdf/2501.15648.pdf</a></span>   <span><a href='https://github.com/matyasbohacek/pose-transfer-human-motion' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Vaclav Knapp, Matyas Bohacek
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.15648">Can Pose Transfer Models Generate Realistic Human Motion?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent pose-transfer methods aim to generate temporally consistent and fully controllable videos of human action where the motion from a reference video is reenacted by a new identity. We evaluate three state-of-the-art pose-transfer methods -- AnimateAnyone, MagicAnimate, and ExAvatar -- by generating videos with actions and identities outside the training distribution and conducting a participant study about the quality of these videos. In a controlled environment of 20 distinct human actions, we find that participants, presented with the pose-transferred videos, correctly identify the desired action only 42.92% of the time. Moreover, the participants find the actions in the generated videos consistent with the reference (source) videos only 36.46% of the time. These results vary by method: participants find the splatting-based ExAvatar more consistent and photorealistic than the diffusion-based AnimateAnyone and MagicAnimate.
<div id='section'>Paperid: <span id='pid'>222, <a href='https://arxiv.org/pdf/2501.11260.pdf' target='_blank'>https://arxiv.org/pdf/2501.11260.pdf</a></span>   <span><a href='https://github.com/FengZicai/WMAD-Benchmarks' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/FengZicai/AwesomeWMAD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tuo Feng, Wenguan Wang, Yi Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.11260">A Survey of World Models for Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent breakthroughs in autonomous driving have been propelled by advances in robust world modeling, fundamentally transforming how vehicles interpret dynamic scenes and execute safe decision-making. World models have emerged as a linchpin technology, offering high-fidelity representations of the driving environment that integrate multi-sensor data, semantic cues, and temporal dynamics. This paper systematically reviews recent advances in world models for autonomous driving, proposing a three-tiered taxonomy: (i) Generation of Future Physical World, covering Image-, BEV-, OG-, and PC-based generation methods that enhance scene evolution modeling through diffusion models and 4D occupancy forecasting; (ii) Behavior Planning for Intelligent Agents, combining rule-driven and learning-based paradigms with cost map optimization and reinforcement learning for trajectory generation in complex traffic conditions; (ii) Interaction between Prediction and Planning, achieving multi-agent collaborative decision-making through latent space diffusion and memory-augmented architectures. The study further analyzes training paradigms, including self-supervised learning, multimodal pretraining, and generative data augmentation, while evaluating world models' performance in scene understanding and motion prediction tasks. Future research must address key challenges in self-supervised representation learning, multimodal fusion, and advanced simulation to advance the practical deployment of world models in complex urban environments. Overall, the comprehensive analysis provides a technical roadmap for harnessing the transformative potential of world models in advancing safe and reliable autonomous driving solutions.
<div id='section'>Paperid: <span id='pid'>223, <a href='https://arxiv.org/pdf/2501.10356.pdf' target='_blank'>https://arxiv.org/pdf/2501.10356.pdf</a></span>   <span><a href='https://clairelc.github.io/dexforce.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Claire Chen, Zhongchun Yu, Hojung Choi, Mark Cutkosky, Jeannette Bohg
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.10356">DexForce: Extracting Force-informed Actions from Kinesthetic Demonstrations for Dexterous Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Imitation learning requires high-quality demonstrations consisting of sequences of state-action pairs. For contact-rich dexterous manipulation tasks that require dexterity, the actions in these state-action pairs must produce the right forces. Current widely-used methods for collecting dexterous manipulation demonstrations are difficult to use for demonstrating contact-rich tasks due to unintuitive human-to-robot motion retargeting and the lack of direct haptic feedback. Motivated by these concerns, we propose DexForce. DexForce leverages contact forces, measured during kinesthetic demonstrations, to compute force-informed actions for policy learning. We collect demonstrations for six tasks and show that policies trained on our force-informed actions achieve an average success rate of 76% across all tasks. In contrast, policies trained directly on actions that do not account for contact forces have near-zero success rates. We also conduct a study ablating the inclusion of force data in policy observations. We find that while using force data never hurts policy performance, it helps most for tasks that require advanced levels of precision and coordination, like opening an AirPods case and unscrewing a nut.
<div id='section'>Paperid: <span id='pid'>224, <a href='https://arxiv.org/pdf/2501.10021.pdf' target='_blank'>https://arxiv.org/pdf/2501.10021.pdf</a></span>   <span><a href='https://github.com/bytedance/X-Dyna' target='_blank'>  GitHub</a></span> <span><a href='https://x-dyna.github.io/xdyna.github.io/' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/bytedance/X-Dyna' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Di Chang, Hongyi Xu, You Xie, Yipeng Gao, Zhengfei Kuang, Shengqu Cai, Chenxu Zhang, Guoxian Song, Chao Wang, Yichun Shi, Zeyuan Chen, Shijie Zhou, Linjie Luo, Gordon Wetzstein, Mohammad Soleymani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.10021">X-Dyna: Expressive Dynamic Human Image Animation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce X-Dyna, a novel zero-shot, diffusion-based pipeline for animating a single human image using facial expressions and body movements derived from a driving video, that generates realistic, context-aware dynamics for both the subject and the surrounding environment. Building on prior approaches centered on human pose control, X-Dyna addresses key shortcomings causing the loss of dynamic details, enhancing the lifelike qualities of human video animations. At the core of our approach is the Dynamics-Adapter, a lightweight module that effectively integrates reference appearance context into the spatial attentions of the diffusion backbone while preserving the capacity of motion modules in synthesizing fluid and intricate dynamic details. Beyond body pose control, we connect a local control module with our model to capture identity-disentangled facial expressions, facilitating accurate expression transfer for enhanced realism in animated scenes. Together, these components form a unified framework capable of learning physical human motion and natural scene dynamics from a diverse blend of human and scene videos. Comprehensive qualitative and quantitative evaluations demonstrate that X-Dyna outperforms state-of-the-art methods, creating highly lifelike and expressive animations. The code is available at https://github.com/bytedance/X-Dyna.
<div id='section'>Paperid: <span id='pid'>225, <a href='https://arxiv.org/pdf/2501.08333.pdf' target='_blank'>https://arxiv.org/pdf/2501.08333.pdf</a></span>   <span><a href='https://snuvclab.github.io/david/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hyeonwoo Kim, Sangwon Baik, Hanbyul Joo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.08333">DAViD: Modeling Dynamic Affordance of 3D Objects Using Pre-trained Video Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modeling how humans interact with objects is crucial for AI to effectively assist or mimic human behaviors. Existing studies for learning such ability primarily focus on static human-object interaction (HOI) patterns, such as contact and spatial relationships, while dynamic HOI patterns, capturing the movement of humans and objects over time, remain relatively underexplored. In this paper, we present a novel framework for learning Dynamic Affordance across various target object categories. To address the scarcity of 4D HOI datasets, our method learns the 3D dynamic affordance from synthetically generated 4D HOI samples. Specifically, we propose a pipeline that first generates 2D HOI videos from a given 3D target object using a pre-trained video diffusion model, then lifts them into 3D to generate 4D HOI samples. Leveraging these synthesized 4D HOI samples, we train DAViD, our generative 4D human-object interaction model, which is composed of two key components: (1) a human motion diffusion model (MDM) with Low-Rank Adaptation (LoRA) module to fine-tune a pre-trained MDM to learn the HOI motion concepts from limited HOI motion samples, (2) a motion diffusion model for 4D object poses conditioned by produced human interaction motions. Interestingly, DAViD can integrate newly learned HOI motion concepts with pre-trained human motions to create novel HOI motions, even for multiple HOI motion concepts, demonstrating the advantage of our pipeline with LoRA in integrating dynamic HOI concepts. Through extensive experiments, we demonstrate that DAViD outperforms baselines in synthesizing HOI motion.
<div id='section'>Paperid: <span id='pid'>226, <a href='https://arxiv.org/pdf/2501.08331.pdf' target='_blank'>https://arxiv.org/pdf/2501.08331.pdf</a></span>   <span><a href='https://github.com/Eyeline-Labs/Go-with-the-Flow' target='_blank'>  GitHub</a></span> <span><a href='https://eyeline-labs.github.io/Go-with-the-Flow' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ryan Burgert, Yuancheng Xu, Wenqi Xian, Oliver Pilarski, Pascal Clausen, Mingming He, Li Ma, Yitong Deng, Lingxiao Li, Mohsen Mousavi, Michael Ryoo, Paul Debevec, Ning Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.08331">Go-with-the-Flow: Motion-Controllable Video Diffusion Models Using Real-Time Warped Noise</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generative modeling aims to transform random noise into structured outputs. In this work, we enhance video diffusion models by allowing motion control via structured latent noise sampling. This is achieved by just a change in data: we pre-process training videos to yield structured noise. Consequently, our method is agnostic to diffusion model design, requiring no changes to model architectures or training pipelines. Specifically, we propose a novel noise warping algorithm, fast enough to run in real time, that replaces random temporal Gaussianity with correlated warped noise derived from optical flow fields, while preserving the spatial Gaussianity. The efficiency of our algorithm enables us to fine-tune modern video diffusion base models using warped noise with minimal overhead, and provide a one-stop solution for a wide range of user-friendly motion control: local object motion control, global camera movement control, and motion transfer. The harmonization between temporal coherence and spatial Gaussianity in our warped noise leads to effective motion control while maintaining per-frame pixel quality. Extensive experiments and user studies demonstrate the advantages of our method, making it a robust and scalable approach for controlling motion in video diffusion models. Video results are available on our webpage: https://eyeline-labs.github.io/Go-with-the-Flow. Source code and model checkpoints are available on GitHub: https://github.com/Eyeline-Labs/Go-with-the-Flow.
<div id='section'>Paperid: <span id='pid'>227, <a href='https://arxiv.org/pdf/2501.07563.pdf' target='_blank'>https://arxiv.org/pdf/2501.07563.pdf</a></span>   <span><a href='https://zhangxinyu-xyz.github.io/SimulateMotion.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinyu Zhang, Zicheng Duan, Dong Gong, Lingqiao Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.07563">Training-Free Motion-Guided Video Generation with Enhanced Temporal Consistency Using Motion Consistency Loss</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we address the challenge of generating temporally consistent videos with motion guidance. While many existing methods depend on additional control modules or inference-time fine-tuning, recent studies suggest that effective motion guidance is achievable without altering the model architecture or requiring extra training. Such approaches offer promising compatibility with various video generation foundation models. However, existing training-free methods often struggle to maintain consistent temporal coherence across frames or to follow guided motion accurately. In this work, we propose a simple yet effective solution that combines an initial-noise-based approach with a novel motion consistency loss, the latter being our key innovation. Specifically, we capture the inter-frame feature correlation patterns of intermediate features from a video diffusion model to represent the motion pattern of the reference video. We then design a motion consistency loss to maintain similar feature correlation patterns in the generated video, using the gradient of this loss in the latent space to guide the generation process for precise motion control. This approach improves temporal consistency across various motion control tasks while preserving the benefits of a training-free setup. Extensive experiments show that our method sets a new standard for efficient, temporally coherent video generation.
<div id='section'>Paperid: <span id='pid'>228, <a href='https://arxiv.org/pdf/2501.06035.pdf' target='_blank'>https://arxiv.org/pdf/2501.06035.pdf</a></span>   <span><a href='https://ceveloper.github.io/publications/skeletondiffusion' target='_blank'>  GitHub</a></span> <span><a href='https://ceveloper.github.io/publications/skeletondiffusion/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Cecilia Curreli, Dominik Muhle, Abhishek Saroha, Zhenzhang Ye, Riccardo Marin, Daniel Cremers
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.06035">Nonisotropic Gaussian Diffusion for Realistic 3D Human Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Probabilistic human motion prediction aims to forecast multiple possible future movements from past observations. While current approaches report high diversity and realism, they often generate motions with undetected limb stretching and jitter. To address this, we introduce SkeletonDiffusion, a latent diffusion model that embeds an explicit inductive bias on the human body within its architecture and training. Our model is trained with a novel nonisotropic Gaussian diffusion formulation that aligns with the natural kinematic structure of the human skeleton. Results show that our approach outperforms conventional isotropic alternatives, consistently generating realistic predictions while avoiding artifacts such as limb distortion. Additionally, we identify a limitation in commonly used diversity metrics, which may inadvertently favor models that produce inconsistent limb lengths within the same sequence. SkeletonDiffusion sets a new benchmark on real-world datasets, outperforming various baselines across multiple evaluation metrics. Visit our project page at https://ceveloper.github.io/publications/skeletondiffusion/ .
<div id='section'>Paperid: <span id='pid'>229, <a href='https://arxiv.org/pdf/2501.05020.pdf' target='_blank'>https://arxiv.org/pdf/2501.05020.pdf</a></span>   <span><a href='https://chen-yingjie.github.io/projects/Perception-as-Control' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yingjie Chen, Yifang Men, Yuan Yao, Miaomiao Cui, Liefeng Bo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.05020">Perception-as-Control: Fine-grained Controllable Image Animation with 3D-aware Motion Representation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion-controllable image animation is a fundamental task with a wide range of potential applications. Recent works have made progress in controlling camera or object motion via various motion representations, while they still struggle to support collaborative camera and object motion control with adaptive control granularity. To this end, we introduce 3D-aware motion representation and propose an image animation framework, called Perception-as-Control, to achieve fine-grained collaborative motion control. Specifically, we construct 3D-aware motion representation from a reference image, manipulate it based on interpreted user instructions, and perceive it from different viewpoints. In this way, camera and object motions are transformed into intuitive and consistent visual changes. Then, our framework leverages the perception results as motion control signals, enabling it to support various motion-related video synthesis tasks in a unified and flexible way. Experiments demonstrate the superiority of the proposed approach. For more details and qualitative results, please refer to our anonymous project webpage: https://chen-yingjie.github.io/projects/Perception-as-Control.
<div id='section'>Paperid: <span id='pid'>230, <a href='https://arxiv.org/pdf/2501.04169.pdf' target='_blank'>https://arxiv.org/pdf/2501.04169.pdf</a></span>   <span><a href='https://rureadyo.github.io/MocapRobot/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sungjae Park, Seungho Lee, Mingi Choi, Jiye Lee, Jeonghwan Kim, Jisoo Kim, Hanbyul Joo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.04169">Learning to Transfer Human Hand Skills for Robot Manipulations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a method for teaching dexterous manipulation tasks to robots from human hand motion demonstrations. Unlike existing approaches that solely rely on kinematics information without taking into account the plausibility of robot and object interaction, our method directly infers plausible robot manipulation actions from human motion demonstrations. To address the embodiment gap between the human hand and the robot system, our approach learns a joint motion manifold that maps human hand movements, robot hand actions, and object movements in 3D, enabling us to infer one motion component from others. Our key idea is the generation of pseudo-supervision triplets, which pair human, object, and robot motion trajectories synthetically. Through real-world experiments with robot hand manipulation, we demonstrate that our data-driven retargeting method significantly outperforms conventional retargeting techniques, effectively bridging the embodiment gap between human and robotic hands. Website at https://rureadyo.github.io/MocapRobot/.
<div id='section'>Paperid: <span id='pid'>231, <a href='https://arxiv.org/pdf/2501.02690.pdf' target='_blank'>https://arxiv.org/pdf/2501.02690.pdf</a></span>   <span><a href='https://wkbian.github.io/Projects/GS-DiT/' target='_blank'>  GitHub</a></span> <span><a href='https://wkbian.github.io/Projects/GS-DiT/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Weikang Bian, Zhaoyang Huang, Xiaoyu Shi, Yijin Li, Fu-Yun Wang, Hongsheng Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.02690">GS-DiT: Advancing Video Generation with Pseudo 4D Gaussian Fields through Efficient Dense 3D Point Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>4D video control is essential in video generation as it enables the use of sophisticated lens techniques, such as multi-camera shooting and dolly zoom, which are currently unsupported by existing methods. Training a video Diffusion Transformer (DiT) directly to control 4D content requires expensive multi-view videos. Inspired by Monocular Dynamic novel View Synthesis (MDVS) that optimizes a 4D representation and renders videos according to different 4D elements, such as camera pose and object motion editing, we bring pseudo 4D Gaussian fields to video generation. Specifically, we propose a novel framework that constructs a pseudo 4D Gaussian field with dense 3D point tracking and renders the Gaussian field for all video frames. Then we finetune a pretrained DiT to generate videos following the guidance of the rendered video, dubbed as GS-DiT. To boost the training of the GS-DiT, we also propose an efficient Dense 3D Point Tracking (D3D-PT) method for the pseudo 4D Gaussian field construction. Our D3D-PT outperforms SpatialTracker, the state-of-the-art sparse 3D point tracking method, in accuracy and accelerates the inference speed by two orders of magnitude. During the inference stage, GS-DiT can generate videos with the same dynamic content while adhering to different camera parameters, addressing a significant limitation of current video generation models. GS-DiT demonstrates strong generalization capabilities and extends the 4D controllability of Gaussian splatting to video generation beyond just camera poses. It supports advanced cinematic effects through the manipulation of the Gaussian field and camera intrinsics, making it a powerful tool for creative video production. Demos are available at https://wkbian.github.io/Projects/GS-DiT/.
<div id='section'>Paperid: <span id='pid'>232, <a href='https://arxiv.org/pdf/2501.02530.pdf' target='_blank'>https://arxiv.org/pdf/2501.02530.pdf</a></span>   <span><a href='https://github.com/henryhcliu/udmc_carla.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haichao Liu, Kai Chen, Yulin Li, Zhenmin Huang, Ming Liu, Jun Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.02530">UDMC: Unified Decision-Making and Control Framework for Urban Autonomous Driving with Motion Prediction of Traffic Participants</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current autonomous driving systems often struggle to balance decision-making and motion control while ensuring safety and traffic rule compliance, especially in complex urban environments. Existing methods may fall short due to separate handling of these functionalities, leading to inefficiencies and safety compromises. To address these challenges, we introduce UDMC, an interpretable and unified Level 4 autonomous driving framework. UDMC integrates decision-making and motion control into a single optimal control problem (OCP), considering the dynamic interactions with surrounding vehicles, pedestrians, road lanes, and traffic signals. By employing innovative potential functions to model traffic participants and regulations, and incorporating a specialized motion prediction module, our framework enhances on-road safety and rule adherence. The integrated design allows for real-time execution of flexible maneuvers suited to diverse driving scenarios. High-fidelity simulations conducted in CARLA exemplify the framework's computational efficiency, robustness, and safety, resulting in superior driving performance when compared against various baseline models. Our open-source project is available at https://github.com/henryhcliu/udmc_carla.git.
<div id='section'>Paperid: <span id='pid'>233, <a href='https://arxiv.org/pdf/2501.02158.pdf' target='_blank'>https://arxiv.org/pdf/2501.02158.pdf</a></span>   <span><a href='https://genforce.github.io/JOSH/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhizheng Liu, Joe Lin, Wayne Wu, Bolei Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.02158">Joint Optimization for 4D Human-Scene Reconstruction in the Wild</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reconstructing human motion and its surrounding environment is crucial for understanding human-scene interaction and predicting human movements in the scene. While much progress has been made in capturing human-scene interaction in constrained environments, those prior methods can hardly reconstruct the natural and diverse human motion and scene context from web videos. In this work, we propose JOSH, a novel optimization-based method for 4D human-scene reconstruction in the wild from monocular videos. JOSH uses techniques in both dense scene reconstruction and human mesh recovery as initialization, and then it leverages the human-scene contact constraints to jointly optimize the scene, the camera poses, and the human motion. Experiment results show JOSH achieves better results on both global human motion estimation and dense scene reconstruction by joint optimization of scene geometry and human motion. We further design a more efficient model, JOSH3R, and directly train it with pseudo-labels from web videos. JOSH3R outperforms other optimization-free methods by only training with labels predicted from JOSH, further demonstrating its accuracy and generalization ability.
<div id='section'>Paperid: <span id='pid'>234, <a href='https://arxiv.org/pdf/2501.01425.pdf' target='_blank'>https://arxiv.org/pdf/2501.01425.pdf</a></span>   <span><a href='https://henghuiding.github.io/SynFMC/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xincheng Shuai, Henghui Ding, Zhenyuan Qin, Hao Luo, Xingjun Ma, Dacheng Tao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.01425">Free-Form Motion Control: Controlling the 6D Poses of Camera and Objects in Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Controlling the movements of dynamic objects and the camera within generated videos is a meaningful yet challenging task. Due to the lack of datasets with comprehensive 6D pose annotations, existing text-to-video methods can not simultaneously control the motions of both camera and objects in 3D-aware manner, resulting in limited controllability over generated contents. To address this issue and facilitate the research in this field, we introduce a Synthetic Dataset for Free-Form Motion Control (SynFMC). The proposed SynFMC dataset includes diverse object and environment categories and covers various motion patterns according to specific rules, simulating common and complex real-world scenarios. The complete 6D pose information facilitates models learning to disentangle the motion effects from objects and the camera in a video.~To provide precise 3D-aware motion control, we further propose a method trained on SynFMC, Free-Form Motion Control (FMC). FMC can control the 6D poses of objects and camera independently or simultaneously, producing high-fidelity videos. Moreover, it is compatible with various personalized text-to-image (T2I) models for different content styles. Extensive experiments demonstrate that the proposed FMC outperforms previous methods across multiple scenarios.
<div id='section'>Paperid: <span id='pid'>235, <a href='https://arxiv.org/pdf/2412.18600.pdf' target='_blank'>https://arxiv.org/pdf/2412.18600.pdf</a></span>   <span><a href='https://awfuact.github.io/zerohsi/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongjie Li, Hong-Xing Yu, Jiaman Li, Jiajun Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.18600">ZeroHSI: Zero-Shot 4D Human-Scene Interaction by Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human-scene interaction (HSI) generation is crucial for applications in embodied AI, virtual reality, and robotics. Yet, existing methods cannot synthesize interactions in unseen environments such as in-the-wild scenes or reconstructed scenes, as they rely on paired 3D scenes and captured human motion data for training, which are unavailable for unseen environments. We present ZeroHSI, a novel approach that enables zero-shot 4D human-scene interaction synthesis, eliminating the need for training on any MoCap data. Our key insight is to distill human-scene interactions from state-of-the-art video generation models, which have been trained on vast amounts of natural human movements and interactions, and use differentiable rendering to reconstruct human-scene interactions. ZeroHSI can synthesize realistic human motions in both static scenes and environments with dynamic objects, without requiring any ground-truth motion data. We evaluate ZeroHSI on a curated dataset of different types of various indoor and outdoor scenes with different interaction prompts, demonstrating its ability to generate diverse and contextually appropriate human-scene interactions.
<div id='section'>Paperid: <span id='pid'>236, <a href='https://arxiv.org/pdf/2412.17210.pdf' target='_blank'>https://arxiv.org/pdf/2412.17210.pdf</a></span>   <span><a href='https://github.com/guijiejie/DCMD-main' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongsong Wang, Andi Xu, Pinle Ding, Jie Gui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.17210">Dual Conditioned Motion Diffusion for Pose-Based Video Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video Anomaly Detection (VAD) is essential for computer vision research. Existing VAD methods utilize either reconstruction-based or prediction-based frameworks. The former excels at detecting irregular patterns or structures, whereas the latter is capable of spotting abnormal deviations or trends. We address pose-based video anomaly detection and introduce a novel framework called Dual Conditioned Motion Diffusion (DCMD), which enjoys the advantages of both approaches. The DCMD integrates conditioned motion and conditioned embedding to comprehensively utilize the pose characteristics and latent semantics of observed movements, respectively. In the reverse diffusion process, a motion transformer is proposed to capture potential correlations from multi-layered characteristics within the spectrum space of human motion. To enhance the discriminability between normal and abnormal instances, we design a novel United Association Discrepancy (UAD) regularization that primarily relies on a Gaussian kernel-based time association and a self-attention-based global association. Finally, a mask completion strategy is introduced during the inference stage of the reverse diffusion process to enhance the utilization of conditioned motion for the prediction branch of anomaly detection. Extensive experiments on four datasets demonstrate that our method dramatically outperforms state-of-the-art methods and exhibits superior generalization performance.
<div id='section'>Paperid: <span id='pid'>237, <a href='https://arxiv.org/pdf/2412.14706.pdf' target='_blank'>https://arxiv.org/pdf/2412.14706.pdf</a></span>   <span><a href='https://jiro-zhang.github.io/EnergyMoGen/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianrong Zhang, Hehe Fan, Yi Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.14706">EnergyMoGen: Compositional Human Motion Generation with Energy-Based Diffusion Model in Latent Space</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion models, particularly latent diffusion models, have demonstrated remarkable success in text-driven human motion generation. However, it remains challenging for latent diffusion models to effectively compose multiple semantic concepts into a single, coherent motion sequence. To address this issue, we propose EnergyMoGen, which includes two spectrums of Energy-Based Models: (1) We interpret the diffusion model as a latent-aware energy-based model that generates motions by composing a set of diffusion models in latent space; (2) We introduce a semantic-aware energy model based on cross-attention, which enables semantic composition and adaptive gradient descent for text embeddings. To overcome the challenges of semantic inconsistency and motion distortion across these two spectrums, we introduce Synergistic Energy Fusion. This design allows the motion latent diffusion model to synthesize high-quality, complex motions by combining multiple energy terms corresponding to textual descriptions. Experiments show that our approach outperforms existing state-of-the-art models on various motion generation tasks, including text-to-motion generation, compositional motion generation, and multi-concept motion generation. Additionally, we demonstrate that our method can be used to extend motion datasets and improve the text-to-motion task.
<div id='section'>Paperid: <span id='pid'>238, <a href='https://arxiv.org/pdf/2412.14018.pdf' target='_blank'>https://arxiv.org/pdf/2412.14018.pdf</a></span>   <span><a href='https://surgsora.github.io/surgsora.github.io' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tong Chen, Shuya Yang, Junyi Wang, Long Bai, Hongliang Ren, Luping Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.14018">SurgSora: Object-Aware Diffusion Model for Controllable Surgical Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Surgical video generation can enhance medical education and research, but existing methods lack fine-grained motion control and realism. We introduce SurgSora, a framework that generates high-fidelity, motion-controllable surgical videos from a single input frame and user-specified motion cues. Unlike prior approaches that treat objects indiscriminately or rely on ground-truth segmentation masks, SurgSora leverages self-predicted object features and depth information to refine RGB appearance and optical flow for precise video synthesis. It consists of three key modules: (1) the Dual Semantic Injector, which extracts object-specific RGB-D features and segmentation cues to enhance spatial representations; (2) the Decoupled Flow Mapper, which fuses multi-scale optical flow with semantic features for realistic motion dynamics; and (3) the Trajectory Controller, which estimates sparse optical flow and enables user-guided object movement. By conditioning these enriched features within the Stable Video Diffusion, SurgSora achieves state-of-the-art visual authenticity and controllability in advancing surgical video synthesis, as demonstrated by extensive quantitative and qualitative comparisons. Our human evaluation in collaboration with expert surgeons further demonstrates the high realism of SurgSora-generated videos, highlighting the potential of our method for surgical training and education. Our project is available at https://surgsora.github.io/surgsora.github.io.
<div id='section'>Paperid: <span id='pid'>239, <a href='https://arxiv.org/pdf/2412.13729.pdf' target='_blank'>https://arxiv.org/pdf/2412.13729.pdf</a></span>   <span><a href='https://github.com/tmralmeida/thor-magni-actions' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tiago Rodrigues de Almeida, Tim Schreiter, Andrey Rudenko, Luigi Palmieiri, Johannes A. Stork, Achim J. Lilienthal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.13729">THÃR-MAGNI Act: Actions for Human Motion Modeling in Robot-Shared Industrial Spaces</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate human activity and trajectory prediction are crucial for ensuring safe and reliable human-robot interactions in dynamic environments, such as industrial settings, with mobile robots. Datasets with fine-grained action labels for moving people in industrial environments with mobile robots are scarce, as most existing datasets focus on social navigation in public spaces. This paper introduces the THÃR-MAGNI Act dataset, a substantial extension of the THÃR-MAGNI dataset, which captures participant movements alongside robots in diverse semantic and spatial contexts. THÃR-MAGNI Act provides 8.3 hours of manually labeled participant actions derived from egocentric videos recorded via eye-tracking glasses. These actions, aligned with the provided THÃR-MAGNI motion cues, follow a long-tailed distribution with diversified acceleration, velocity, and navigation distance profiles. We demonstrate the utility of THÃR-MAGNI Act for two tasks: action-conditioned trajectory prediction and joint action and trajectory prediction. We propose two efficient transformer-based models that outperform the baselines to address these tasks. These results underscore the potential of THÃR-MAGNI Act to develop predictive models for enhanced human-robot interaction in complex environments.
<div id='section'>Paperid: <span id='pid'>240, <a href='https://arxiv.org/pdf/2412.13185.pdf' target='_blank'>https://arxiv.org/pdf/2412.13185.pdf</a></span>   <span><a href='https://hhsinping.github.io/Move-in-2D/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hsin-Ping Huang, Yang Zhou, Jui-Hsien Wang, Difan Liu, Feng Liu, Ming-Hsuan Yang, Zhan Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.13185">Move-in-2D: 2D-Conditioned Human Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating realistic human videos remains a challenging task, with the most effective methods currently relying on a human motion sequence as a control signal. Existing approaches often use existing motion extracted from other videos, which restricts applications to specific motion types and global scene matching. We propose Move-in-2D, a novel approach to generate human motion sequences conditioned on a scene image, allowing for diverse motion that adapts to different scenes. Our approach utilizes a diffusion model that accepts both a scene image and text prompt as inputs, producing a motion sequence tailored to the scene. To train this model, we collect a large-scale video dataset featuring single-human activities, annotating each video with the corresponding human motion as the target output. Experiments demonstrate that our method effectively predicts human motion that aligns with the scene image after projection. Furthermore, we show that the generated motion sequence improves human motion quality in video synthesis tasks.
<div id='section'>Paperid: <span id='pid'>241, <a href='https://arxiv.org/pdf/2412.13111.pdf' target='_blank'>https://arxiv.org/pdf/2412.13111.pdf</a></span>   <span><a href='https://zju3dv.github.io/Motion-2-to-3/' target='_blank'>  GitHub</a></span> <span><a href='https://zju3dv.github.io/Motion-2-to-3/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Huaijin Pi, Ruoxi Guo, Zehong Shen, Qing Shuai, Zechen Hu, Zhumei Wang, Yajiao Dong, Ruizhen Hu, Taku Komura, Sida Peng, Xiaowei Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.13111">Motion-2-to-3: Leveraging 2D Motion Data to Boost 3D Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-driven human motion synthesis is capturing significant attention for its ability to effortlessly generate intricate movements from abstract text cues, showcasing its potential for revolutionizing motion design not only in film narratives but also in virtual reality experiences and computer game development. Existing methods often rely on 3D motion capture data, which require special setups resulting in higher costs for data acquisition, ultimately limiting the diversity and scope of human motion. In contrast, 2D human videos offer a vast and accessible source of motion data, covering a wider range of styles and activities. In this paper, we explore leveraging 2D human motion extracted from videos as an alternative data source to improve text-driven 3D motion generation. Our approach introduces a novel framework that disentangles local joint motion from global movements, enabling efficient learning of local motion priors from 2D data. We first train a single-view 2D local motion generator on a large dataset of text-motion pairs. To enhance this model to synthesize 3D motion, we fine-tune the generator with 3D data, transforming it into a multi-view generator that predicts view-consistent local joint motion and root dynamics. Experiments on the HumanML3D dataset and novel text prompts demonstrate that our method efficiently utilizes 2D data, supporting realistic 3D human motion generation and broadening the range of motion types it supports. Our code will be made publicly available at https://zju3dv.github.io/Motion-2-to-3/.
<div id='section'>Paperid: <span id='pid'>242, <a href='https://arxiv.org/pdf/2412.11495.pdf' target='_blank'>https://arxiv.org/pdf/2412.11495.pdf</a></span>   <span><a href='https://github.com/ShiqiYu/OpenGait' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongyang Jin, Chao Fan, Weihua Chen, Shiqi Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.11495">Exploring More from Multiple Gait Modalities for Human Identification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The gait, as a kind of soft biometric characteristic, can reflect the distinct walking patterns of individuals at a distance, exhibiting a promising technique for unrestrained human identification. With largely excluding gait-unrelated cues hidden in RGB videos, the silhouette and skeleton, though visually compact, have acted as two of the most prevailing gait modalities for a long time. Recently, several attempts have been made to introduce more informative data forms like human parsing and optical flow images to capture gait characteristics, along with multi-branch architectures. However, due to the inconsistency within model designs and experiment settings, we argue that a comprehensive and fair comparative study among these popular gait modalities, involving the representational capacity and fusion strategy exploration, is still lacking. From the perspectives of fine vs. coarse-grained shape and whole vs. pixel-wise motion modeling, this work presents an in-depth investigation of three popular gait representations, i.e., silhouette, human parsing, and optical flow, with various fusion evaluations, and experimentally exposes their similarities and differences. Based on the obtained insights, we further develop a C$^2$Fusion strategy, consequently building our new framework MultiGait++. C$^2$Fusion preserves commonalities while highlighting differences to enrich the learning of gait features. To verify our findings and conclusions, extensive experiments on Gait3D, GREW, CCPG, and SUSTech1K are conducted. The code is available at https://github.com/ShiqiYu/OpenGait.
<div id='section'>Paperid: <span id='pid'>243, <a href='https://arxiv.org/pdf/2412.11193.pdf' target='_blank'>https://arxiv.org/pdf/2412.11193.pdf</a></span>   <span><a href='https://github.com/qinghuannn/light-t2m' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ling-An Zeng, Guohong Huang, Gaojie Wu, Wei-Shi Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.11193">Light-T2M: A Lightweight and Fast Model for Text-to-motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite the significant role text-to-motion (T2M) generation plays across various applications, current methods involve a large number of parameters and suffer from slow inference speeds, leading to high usage costs. To address this, we aim to design a lightweight model to reduce usage costs. First, unlike existing works that focus solely on global information modeling, we recognize the importance of local information modeling in the T2M task by reconsidering the intrinsic properties of human motion, leading us to propose a lightweight Local Information Modeling Module. Second, we introduce Mamba to the T2M task, reducing the number of parameters and GPU memory demands, and we have designed a novel Pseudo-bidirectional Scan to replicate the effects of a bidirectional scan without increasing parameter count. Moreover, we propose a novel Adaptive Textual Information Injector that more effectively integrates textual information into the motion during generation. By integrating the aforementioned designs, we propose a lightweight and fast model named Light-T2M. Compared to the state-of-the-art method, MoMask, our Light-T2M model features just 10\% of the parameters (4.48M vs 44.85M) and achieves a 16\% faster inference time (0.152s vs 0.180s), while surpassing MoMask with an FID of \textbf{0.040} (vs. 0.045) on HumanML3D dataset and 0.161 (vs. 0.228) on KIT-ML dataset. The code is available at https://github.com/qinghuannn/light-t2m.
<div id='section'>Paperid: <span id='pid'>244, <a href='https://arxiv.org/pdf/2412.09623.pdf' target='_blank'>https://arxiv.org/pdf/2412.09623.pdf</a></span>   <span><a href='https://lwq20020127.github.io/OmniDrag' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Weiqi Li, Shijie Zhao, Chong Mou, Xuhan Sheng, Zhenyu Zhang, Qian Wang, Junlin Li, Li Zhang, Jian Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.09623">OmniDrag: Enabling Motion Control for Omnidirectional Image-to-Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As virtual reality gains popularity, the demand for controllable creation of immersive and dynamic omnidirectional videos (ODVs) is increasing. While previous text-to-ODV generation methods achieve impressive results, they struggle with content inaccuracies and inconsistencies due to reliance solely on textual inputs. Although recent motion control techniques provide fine-grained control for video generation, directly applying these methods to ODVs often results in spatial distortion and unsatisfactory performance, especially with complex spherical motions. To tackle these challenges, we propose OmniDrag, the first approach enabling both scene- and object-level motion control for accurate, high-quality omnidirectional image-to-video generation. Building on pretrained video diffusion models, we introduce an omnidirectional control module, which is jointly fine-tuned with temporal attention layers to effectively handle complex spherical motion. In addition, we develop a novel spherical motion estimator that accurately extracts motion-control signals and allows users to perform drag-style ODV generation by simply drawing handle and target points. We also present a new dataset, named Move360, addressing the scarcity of ODV data with large scene and object motions. Experiments demonstrate the significant superiority of OmniDrag in achieving holistic scene-level and fine-grained object-level control for ODV generation. The project page is available at https://lwq20020127.github.io/OmniDrag.
<div id='section'>Paperid: <span id='pid'>245, <a href='https://arxiv.org/pdf/2412.08643.pdf' target='_blank'>https://arxiv.org/pdf/2412.08643.pdf</a></span>   <span><a href='https://github.com/wzzheng/GPD' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/wzzheng/GPD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zixun Xie, Sicheng Zuo, Wenzhao Zheng, Yunpeng Zhang, Dalong Du, Jie Zhou, Jiwen Lu, Shanghang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.08643">GPD-1: Generative Pre-training for Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modeling the evolutions of driving scenarios is important for the evaluation and decision-making of autonomous driving systems. Most existing methods focus on one aspect of scene evolution such as map generation, motion prediction, and trajectory planning. In this paper, we propose a unified Generative Pre-training for Driving (GPD-1) model to accomplish all these tasks altogether without additional fine-tuning. We represent each scene with ego, agent, and map tokens and formulate autonomous driving as a unified token generation problem. We adopt the autoregressive transformer architecture and use a scene-level attention mask to enable intra-scene bi-directional interactions. For the ego and agent tokens, we propose a hierarchical positional tokenizer to effectively encode both 2D positions and headings. For the map tokens, we train a map vector-quantized autoencoder to efficiently compress ego-centric semantic maps into discrete tokens. We pre-train our GPD-1 on the large-scale nuPlan dataset and conduct extensive experiments to evaluate its effectiveness. With different prompts, our GPD-1 successfully generalizes to various tasks without finetuning, including scene generation, traffic simulation, closed-loop simulation, map prediction, and motion planning. Code: https://github.com/wzzheng/GPD.
<div id='section'>Paperid: <span id='pid'>246, <a href='https://arxiv.org/pdf/2412.07721.pdf' target='_blank'>https://arxiv.org/pdf/2412.07721.pdf</a></span>   <span><a href='https://wzhouxiff.github.io/projects/ObjCtrl-2.5D/' target='_blank'>  GitHub</a></span> <span><a href='https://wzhouxiff.github.io/projects/ObjCtrl-2.5D/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhouxia Wang, Yushi Lan, Shangchen Zhou, Chen Change Loy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.07721">ObjCtrl-2.5D: Training-free Object Control with Camera Poses</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study aims to achieve more precise and versatile object control in image-to-video (I2V) generation. Current methods typically represent the spatial movement of target objects with 2D trajectories, which often fail to capture user intention and frequently produce unnatural results. To enhance control, we present ObjCtrl-2.5D, a training-free object control approach that uses a 3D trajectory, extended from a 2D trajectory with depth information, as a control signal. By modeling object movement as camera movement, ObjCtrl-2.5D represents the 3D trajectory as a sequence of camera poses, enabling object motion control using an existing camera motion control I2V generation model (CMC-I2V) without training. To adapt the CMC-I2V model originally designed for global motion control to handle local object motion, we introduce a module to isolate the target object from the background, enabling independent local control. In addition, we devise an effective way to achieve more accurate object control by sharing low-frequency warped latent within the object's region across frames. Extensive experiments demonstrate that ObjCtrl-2.5D significantly improves object control accuracy compared to training-free methods and offers more diverse control capabilities than training-based approaches using 2D trajectories, enabling complex effects like object rotation. Code and results are available at https://wzhouxiff.github.io/projects/ObjCtrl-2.5D/.
<div id='section'>Paperid: <span id='pid'>247, <a href='https://arxiv.org/pdf/2412.07320.pdf' target='_blank'>https://arxiv.org/pdf/2412.07320.pdf</a></span>   <span><a href='https://gabrie-l.github.io/coma-page/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shanlin Sun, Gabriel De Araujo, Jiaqi Xu, Shenghan Zhou, Hanwen Zhang, Ziheng Huang, Chenyu You, Xiaohui Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.07320">CoMA: Compositional Human Motion Generation with Multi-modal Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D human motion generation has seen substantial advancement in recent years. While state-of-the-art approaches have improved performance significantly, they still struggle with complex and detailed motions unseen in training data, largely due to the scarcity of motion datasets and the prohibitive cost of generating new training examples. To address these challenges, we introduce CoMA, an agent-based solution for complex human motion generation, editing, and comprehension. CoMA leverages multiple collaborative agents powered by large language and vision models, alongside a mask transformer-based motion generator featuring body part-specific encoders and codebooks for fine-grained control. Our framework enables generation of both short and long motion sequences with detailed instructions, text-guided motion editing, and self-correction for improved quality. Evaluations on the HumanML3D dataset demonstrate competitive performance against state-of-the-art methods. Additionally, we create a set of context-rich, compositional, and long text prompts, where user studies show our method significantly outperforms existing approaches.
<div id='section'>Paperid: <span id='pid'>248, <a href='https://arxiv.org/pdf/2412.06146.pdf' target='_blank'>https://arxiv.org/pdf/2412.06146.pdf</a></span>   <span><a href='https://foruck.github.io/HDyS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinpeng Liu, Junxuan Liang, Chenshuo Zhang, Zixuan Cai, Cewu Lu, Yong-Lu Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.06146">Homogeneous Dynamics Space for Heterogeneous Humans</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Analyses of human motion kinematics have achieved tremendous advances. However, the production mechanism, known as human dynamics, is still undercovered. In this paper, we aim to push data-driven human dynamics understanding forward. We identify a major obstacle to this as the heterogeneity of existing human motion understanding efforts. Specifically, heterogeneity exists in not only the diverse kinematics representations and hierarchical dynamics representations but also in the data from different domains, namely biomechanics and reinforcement learning. With an in-depth analysis of the existing heterogeneity, we propose to emphasize the beneath homogeneity: all of them represent the homogeneous fact of human motion, though from different perspectives. Given this, we propose Homogeneous Dynamics Space (HDyS) as a fundamental space for human dynamics by aggregating heterogeneous data and training a homogeneous latent space with inspiration from the inverse-forward dynamics procedure. Leveraging the heterogeneous representations and datasets, HDyS achieves decent mapping between human kinematics and dynamics. We demonstrate the feasibility of HDyS with extensive experiments and applications. The project page is https://foruck.github.io/HDyS.
<div id='section'>Paperid: <span id='pid'>249, <a href='https://arxiv.org/pdf/2412.05095.pdf' target='_blank'>https://arxiv.org/pdf/2412.05095.pdf</a></span>   <span><a href='https://xiaofeng-tan.github.io/projects/SoPo/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaofeng Tan, Hongsong Wang, Xin Geng, Pan Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.05095">SoPo: Text-to-Motion Generation Using Semi-Online Preference Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-to-motion generation is essential for advancing the creative industry but often presents challenges in producing consistent, realistic motions. To address this, we focus on fine-tuning text-to-motion models to consistently favor high-quality, human-preferred motions, a critical yet largely unexplored problem. In this work, we theoretically investigate the DPO under both online and offline settings, and reveal their respective limitation: overfitting in offline DPO, and biased sampling in online DPO. Building on our theoretical insights, we introduce Semi-online Preference Optimization (SoPo), a DPO-based method for training text-to-motion models using "semi-online" data pair, consisting of unpreferred motion from online distribution and preferred motion in offline datasets. This method leverages both online and offline DPO, allowing each to compensate for the other's limitations. Extensive experiments demonstrate that SoPo outperforms other preference alignment methods, with an MM-Dist of 3.25% (vs e.g. 0.76% of MoDiPO) on the MLD model, 2.91% (vs e.g. 0.66% of MoDiPO) on MDM model, respectively. Additionally, the MLD model fine-tuned by our SoPo surpasses the SoTA model in terms of R-precision and MM Dist. Visualization results also show the efficacy of our SoPo in preference alignment. Project page: https://xiaofeng-tan.github.io/projects/SoPo/ .
<div id='section'>Paperid: <span id='pid'>250, <a href='https://arxiv.org/pdf/2412.02261.pdf' target='_blank'>https://arxiv.org/pdf/2412.02261.pdf</a></span>   <span><a href='https://jingyugong.github.io/DiffusionImplicitPolicy/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingyu Gong, Chong Zhang, Fengqi Liu, Ke Fan, Qianyu Zhou, Xin Tan, Zhizhong Zhang, Yuan Xie, Lizhuang Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.02261">Diffusion Implicit Policy for Unpaired Scene-aware Motion Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion generation is a long-standing problem, and scene-aware motion synthesis has been widely researched recently due to its numerous applications. Prevailing methods rely heavily on paired motion-scene data whose quantity is limited. Meanwhile, it is difficult to generalize to diverse scenes when trained only on a few specific ones. Thus, we propose a unified framework, termed Diffusion Implicit Policy (DIP), for scene-aware motion synthesis, where paired motion-scene data are no longer necessary. In this framework, we disentangle human-scene interaction from motion synthesis during training and then introduce an interaction-based implicit policy into motion diffusion during inference. Synthesized motion can be derived through iterative diffusion denoising and implicit policy optimization, thus motion naturalness and interaction plausibility can be maintained simultaneously. The proposed implicit policy optimizes the intermediate noised motion in a GAN Inversion manner to maintain motion continuity and control keyframe poses though the ControlNet branch and motion inpainting. For long-term motion synthesis, we introduce motion blending for stable transitions between multiple sub-tasks, where motions are fused in rotation power space and translation linear space. The proposed method is evaluated on synthesized scenes with ShapeNet furniture, and real scenes from PROX and Replica. Results show that our framework presents better motion naturalness and interaction plausibility than cutting-edge methods. This also indicates the feasibility of utilizing the DIP for motion synthesis in more general tasks and versatile scenes. https://jingyugong.github.io/DiffusionImplicitPolicy/
<div id='section'>Paperid: <span id='pid'>251, <a href='https://arxiv.org/pdf/2412.01343.pdf' target='_blank'>https://arxiv.org/pdf/2412.01343.pdf</a></span>   <span><a href='https://github.com/XiaominLi1997/MoTrans' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaomin Li, Xu Jia, Qinghe Wang, Haiwen Diao, Mengmeng Ge, Pengxiang Li, You He, Huchuan Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.01343">MoTrans: Customized Motion Transfer with Text-driven Video Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing pretrained text-to-video (T2V) models have demonstrated impressive abilities in generating realistic videos with basic motion or camera movement. However, these models exhibit significant limitations when generating intricate, human-centric motions. Current efforts primarily focus on fine-tuning models on a small set of videos containing a specific motion. They often fail to effectively decouple motion and the appearance in the limited reference videos, thereby weakening the modeling capability of motion patterns. To this end, we propose MoTrans, a customized motion transfer method enabling video generation of similar motion in new context. Specifically, we introduce a multimodal large language model (MLLM)-based recaptioner to expand the initial prompt to focus more on appearance and an appearance injection module to adapt appearance prior from video frames to the motion modeling process. These complementary multimodal representations from recaptioned prompt and video frames promote the modeling of appearance and facilitate the decoupling of appearance and motion. In addition, we devise a motion-specific embedding for further enhancing the modeling of the specific motion. Experimental results demonstrate that our method effectively learns specific motion pattern from singular or multiple reference videos, performing favorably against existing methods in customized video generation.
<div id='section'>Paperid: <span id='pid'>252, <a href='https://arxiv.org/pdf/2412.01179.pdf' target='_blank'>https://arxiv.org/pdf/2412.01179.pdf</a></span>   <span><a href='https://github.com/TangTao-PKU/DGTR' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/TangTao-PKU/DGTR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tao Tang, Hong Liu, Yingxuan You, Ti Wang, Wenhao Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.01179">Dual-Branch Graph Transformer Network for 3D Human Mesh Reconstruction from Video</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human Mesh Reconstruction (HMR) from monocular video plays an important role in human-robot interaction and collaboration. However, existing video-based human mesh reconstruction methods face a trade-off between accurate reconstruction and smooth motion. These methods design networks based on either RNNs or attention mechanisms to extract local temporal correlations or global temporal dependencies, but the lack of complementary long-term information and local details limits their performance. To address this problem, we propose a \textbf{D}ual-branch \textbf{G}raph \textbf{T}ransformer network for 3D human mesh \textbf{R}econstruction from video, named DGTR. DGTR employs a dual-branch network including a Global Motion Attention (GMA) branch and a Local Details Refine (LDR) branch to parallelly extract long-term dependencies and local crucial information, helping model global human motion and local human details (e.g., local motion, tiny movement). Specifically, GMA utilizes a global transformer to model long-term human motion. LDR combines modulated graph convolutional networks and the transformer framework to aggregate local information in adjacent frames and extract crucial information of human details. Experiments demonstrate that our DGTR outperforms state-of-the-art video-based methods in reconstruction accuracy and maintains competitive motion smoothness. Moreover, DGTR utilizes fewer parameters and FLOPs, which validate the effectiveness and efficiency of the proposed DGTR. Code is publicly available at \href{https://github.com/TangTao-PKU/DGTR}{\textcolor{myBlue}{https://github.com/TangTao-PKU/DGTR}}.
<div id='section'>Paperid: <span id='pid'>253, <a href='https://arxiv.org/pdf/2412.00851.pdf' target='_blank'>https://arxiv.org/pdf/2412.00851.pdf</a></span>   <span><a href='https://colin-de.github.io/DynSUP/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Weihang Li, Weirong Chen, Shenhan Qian, Jiajie Chen, Daniel Cremers, Haoang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.00851">DynSUP: Dynamic Gaussian Splatting from An Unposed Image Pair</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in 3D Gaussian Splatting have shown promising results. Existing methods typically assume static scenes and/or multiple images with prior poses. Dynamics, sparse views, and unknown poses significantly increase the problem complexity due to insufficient geometric constraints. To overcome this challenge, we propose a method that can use only two images without prior poses to fit Gaussians in dynamic environments. To achieve this, we introduce two technical contributions. First, we propose an object-level two-view bundle adjustment. This strategy decomposes dynamic scenes into piece-wise rigid components, and jointly estimates the camera pose and motions of dynamic objects. Second, we design an SE(3) field-driven Gaussian training method. It enables fine-grained motion modeling through learnable per-Gaussian transformations. Our method leads to high-fidelity novel view synthesis of dynamic scenes while accurately preserving temporal consistency and object motion. Experiments on both synthetic and real-world datasets demonstrate that our method significantly outperforms state-of-the-art approaches designed for the cases of static environments, multiple images, and/or known poses. Our project page is available at https://colin-de.github.io/DynSUP/.
<div id='section'>Paperid: <span id='pid'>254, <a href='https://arxiv.org/pdf/2412.00420.pdf' target='_blank'>https://arxiv.org/pdf/2412.00420.pdf</a></span>   <span><a href='https://github.com/vita-epfl/TAROT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lan Feng, Fan Nie, Yuejiang Liu, Alexandre Alahi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.00420">TAROT: Targeted Data Selection via Optimal Transport</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose TAROT, a targeted data selection framework grounded in optimal transport theory. Previous targeted data selection methods primarily rely on influence-based greedy heuristics to enhance domain-specific performance. While effective on limited, unimodal data (i.e., data following a single pattern), these methods struggle as target data complexity increases. Specifically, in multimodal distributions, these heuristics fail to account for multiple inherent patterns, leading to suboptimal data selection. This work identifies two primary factors contributing to this limitation: (i) the disproportionate impact of dominant feature components in high-dimensional influence estimation, and (ii) the restrictive linear additive assumptions inherent in greedy selection strategies. To address these challenges, TAROT incorporates whitened feature distance to mitigate dominant feature bias, providing a more reliable measure of data influence. Building on this, TAROT uses whitened feature distance to quantify and minimize the optimal transport distance between the selected data and target domains. Notably, this minimization also facilitates the estimation of optimal selection ratios. We evaluate TAROT across multiple tasks, including semantic segmentation, motion prediction, and instruction tuning. Results consistently show that TAROT outperforms state-of-the-art methods, highlighting its versatility across various deep learning tasks. Code is available at https://github.com/vita-epfl/TAROT.
<div id='section'>Paperid: <span id='pid'>255, <a href='https://arxiv.org/pdf/2412.00115.pdf' target='_blank'>https://arxiv.org/pdf/2412.00115.pdf</a></span>   <span><a href='https://fudan-generative-vision.github.io/OpenHumanVid' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hui Li, Mingwang Xu, Yun Zhan, Shan Mu, Jiaye Li, Kaihui Cheng, Yuxuan Chen, Tan Chen, Mao Ye, Jingdong Wang, Siyu Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.00115">OpenHumanVid: A Large-Scale High-Quality Dataset for Enhancing Human-Centric Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in visual generation technologies have markedly increased the scale and availability of video datasets, which are crucial for training effective video generation models. However, a significant lack of high-quality, human-centric video datasets presents a challenge to progress in this field. To bridge this gap, we introduce OpenHumanVid, a large-scale and high-quality human-centric video dataset characterized by precise and detailed captions that encompass both human appearance and motion states, along with supplementary human motion conditions, including skeleton sequences and speech audio. To validate the efficacy of this dataset and the associated training strategies, we propose an extension of existing classical diffusion transformer architectures and conduct further pretraining of our models on the proposed dataset. Our findings yield two critical insights: First, the incorporation of a large-scale, high-quality dataset substantially enhances evaluation metrics for generated human videos while preserving performance in general video generation tasks. Second, the effective alignment of text with human appearance, human motion, and facial motion is essential for producing high-quality video outputs. Based on these insights and corresponding methodologies, the straightforward extended network trained on the proposed dataset demonstrates an obvious improvement in the generation of human-centric videos. Project page https://fudan-generative-vision.github.io/OpenHumanVid
<div id='section'>Paperid: <span id='pid'>256, <a href='https://arxiv.org/pdf/2411.19527.pdf' target='_blank'>https://arxiv.org/pdf/2411.19527.pdf</a></span>   <span><a href='https://whwjdqls.github.io/discord-motion/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jungbin Cho, Junwan Kim, Jisoo Kim, Minseo Kim, Mingu Kang, Sungeun Hong, Tae-Hyun Oh, Youngjae Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.19527">DisCoRD: Discrete Tokens to Continuous Motion via Rectified Flow Decoding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion is inherently continuous and dynamic, posing significant challenges for generative models. While discrete generation methods are widely used, they suffer from limited expressiveness and frame-wise noise artifacts. In contrast, continuous approaches produce smoother, more natural motion but often struggle to adhere to conditioning signals due to high-dimensional complexity and limited training data. To resolve this 'discord' between discrete and continuous representations we introduce DisCoRD: Discrete Tokens to Continuous Motion via Rectified Flow Decoding, a novel method that leverages rectified flow to decode discrete motion tokens in the continuous, raw motion space. Our core idea is to frame token decoding as a conditional generation task, ensuring that DisCoRD captures fine-grained dynamics and achieves smoother, more natural motions. Compatible with any discrete-based framework, our method enhances naturalness without compromising faithfulness to the conditioning signals on diverse settings. Extensive evaluations demonstrate that DisCoRD achieves state-of-the-art performance, with FID of 0.032 on HumanML3D and 0.169 on KIT-ML. These results establish DisCoRD as a robust solution for bridging the divide between discrete efficiency and continuous realism. Project website: https://whwjdqls.github.io/discord-motion/
<div id='section'>Paperid: <span id='pid'>257, <a href='https://arxiv.org/pdf/2411.17765.pdf' target='_blank'>https://arxiv.org/pdf/2411.17765.pdf</a></span>   <span><a href='https://wanquanf.github.io/I2VControl' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wanquan Feng, Tianhao Qi, Jiawei Liu, Mingzhen Sun, Pengqi Tu, Tianxiang Ma, Fei Dai, Songtao Zhao, Siyu Zhou, Qian He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.17765">I2VControl: Disentangled and Unified Video Motion Synthesis Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion controllability is crucial in video synthesis. However, most previous methods are limited to single control types, and combining them often results in logical conflicts. In this paper, we propose a disentangled and unified framework, namely I2VControl, to overcome the logical conflicts. We rethink camera control, object dragging, and motion brush, reformulating all tasks into a consistent representation based on point trajectories, each managed by a dedicated formulation. Accordingly, we propose a spatial partitioning strategy, where each unit is assigned to a concomitant control category, enabling diverse control types to be dynamically orchestrated within a single synthesis pipeline without conflicts. Furthermore, we design an adapter structure that functions as a plug-in for pre-trained models and is agnostic to specific model architectures. We conduct extensive experiments, achieving excellent performance on various control tasks, and our method further facilitates user-driven creative combinations, enhancing innovation and creativity. Project page: https://wanquanf.github.io/I2VControl .
<div id='section'>Paperid: <span id='pid'>258, <a href='https://arxiv.org/pdf/2411.17383.pdf' target='_blank'>https://arxiv.org/pdf/2411.17383.pdf</a></span>   <span><a href='https://github.com/cangcz/AnchorCrafter' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyi Xu, Ziyao Huang, Juan Cao, Yong Zhang, Xiaodong Cun, Qing Shuai, Yuchen Wang, Linchao Bao, Jintao Li, Fan Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.17383">AnchorCrafter: Animate Cyber-Anchors Selling Your Products via Human-Object Interacting Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The generation of anchor-style product promotion videos presents promising opportunities in e-commerce, advertising, and consumer engagement. Despite advancements in pose-guided human video generation, creating product promotion videos remains challenging. In addressing this challenge, we identify the integration of human-object interactions (HOI) into pose-guided human video generation as a core issue. To this end, we introduce AnchorCrafter, a novel diffusion-based system designed to generate 2D videos featuring a target human and a customized object, achieving high visual fidelity and controllable interactions. Specifically, we propose two key innovations: the HOI-appearance perception, which enhances object appearance recognition from arbitrary multi-view perspectives and disentangles object and human appearance, and the HOI-motion injection, which enables complex human-object interactions by overcoming challenges in object trajectory conditioning and inter-occlusion management. Extensive experiments show that our system improves object appearance preservation by 7.5\% and doubles the object localization accuracy compared to existing state-of-the-art approaches. It also outperforms existing approaches in maintaining human motion consistency and high-quality video generation. Project page including data, code, and Huggingface demo: https://github.com/cangcz/AnchorCrafter.
<div id='section'>Paperid: <span id='pid'>259, <a href='https://arxiv.org/pdf/2411.16964.pdf' target='_blank'>https://arxiv.org/pdf/2411.16964.pdf</a></span>   <span><a href='https://frank-zy-dou.github.io/projects/MotionWavelet/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuming Feng, Zhiyang Dou, Ling-Hao Chen, Yuan Liu, Tianyu Li, Jingbo Wang, Zeyu Cao, Wenping Wang, Taku Komura, Lingjie Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.16964">MotionWavelet: Human Motion Prediction via Wavelet Manifold Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modeling temporal characteristics and the non-stationary dynamics of body movement plays a significant role in predicting human future motions. However, it is challenging to capture these features due to the subtle transitions involved in the complex human motions. This paper introduces MotionWavelet, a human motion prediction framework that utilizes Wavelet Transformation and studies human motion patterns in the spatial-frequency domain. In MotionWavelet, a Wavelet Diffusion Model (WDM) learns a Wavelet Manifold by applying Wavelet Transformation on the motion data therefore encoding the intricate spatial and temporal motion patterns. Once the Wavelet Manifold is built, WDM trains a diffusion model to generate human motions from Wavelet latent vectors. In addition to the WDM, MotionWavelet also presents a Wavelet Space Shaping Guidance mechanism to refine the denoising process to improve conformity with the manifold structure. WDM also develops Temporal Attention-Based Guidance to enhance prediction accuracy. Extensive experiments validate the effectiveness of MotionWavelet, demonstrating improved prediction accuracy and enhanced generalization across various benchmarks. Our code and models will be released upon acceptance.
<div id='section'>Paperid: <span id='pid'>260, <a href='https://arxiv.org/pdf/2411.16805.pdf' target='_blank'>https://arxiv.org/pdf/2411.16805.pdf</a></span>   <span><a href='https://github.com/ILGLJ/LLaMo' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lei Li, Sen Jia, Jianhao Wang, Zhongyu Jiang, Feng Zhou, Ju Dai, Tianfang Zhang, Zongkai Wu, Jenq-Neng Hwang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.16805">Human Motion Instruction Tuning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents LLaMo (Large Language and Human Motion Assistant), a multimodal framework for human motion instruction tuning. In contrast to conventional instruction-tuning approaches that convert non-linguistic inputs, such as video or motion sequences, into language tokens, LLaMo retains motion in its native form for instruction tuning. This method preserves motion-specific details that are often diminished in tokenization, thereby improving the model's ability to interpret complex human behaviors. By processing both video and motion data alongside textual inputs, LLaMo enables a flexible, human-centric analysis. Experimental evaluations across high-complexity domains, including human behaviors and professional activities, indicate that LLaMo effectively captures domain-specific knowledge, enhancing comprehension and prediction in motion-intensive scenarios. We hope LLaMo offers a foundation for future multimodal AI systems with broad applications, from sports analytics to behavioral prediction. Our code and models are available on the project website: https://github.com/ILGLJ/LLaMo.
<div id='section'>Paperid: <span id='pid'>261, <a href='https://arxiv.org/pdf/2411.16758.pdf' target='_blank'>https://arxiv.org/pdf/2411.16758.pdf</a></span>   <span><a href='https://github.com/MyNiuuu/BAGA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Muyao Niu, Yifan Zhan, Qingtian Zhu, Zhuoxiao Li, Wei Wang, Zhihang Zhong, Xiao Sun, Yinqiang Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.16758">Bundle Adjusted Gaussian Avatars Deblurring</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The development of 3D human avatars from multi-view videos represents a significant yet challenging task in the field. Recent advancements, including 3D Gaussian Splattings (3DGS), have markedly progressed this domain. Nonetheless, existing techniques necessitate the use of high-quality sharp images, which are often impractical to obtain in real-world settings due to variations in human motion speed and intensity. In this study, we attempt to explore deriving sharp intrinsic 3D human Gaussian avatars from blurry video footage in an end-to-end manner. Our approach encompasses a 3D-aware, physics-oriented model of blur formation attributable to human movement, coupled with a 3D human motion model to clarify ambiguities found in motion-induced blurry images. This methodology facilitates the concurrent learning of avatar model parameters and the refinement of sub-frame motion parameters from a coarse initialization. We have established benchmarks for this task through a synthetic dataset derived from existing multi-view captures, alongside a real-captured dataset acquired through a 360-degree synchronous hybrid-exposure camera system. Comprehensive evaluations demonstrate that our model surpasses existing baselines.
<div id='section'>Paperid: <span id='pid'>262, <a href='https://arxiv.org/pdf/2411.16657.pdf' target='_blank'>https://arxiv.org/pdf/2411.16657.pdf</a></span>   <span><a href='https://zunwang1.github.io/DreamRunner' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zun Wang, Jialu Li, Han Lin, Jaehong Yoon, Mohit Bansal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.16657">DreamRunner: Fine-Grained Compositional Story-to-Video Generation with Retrieval-Augmented Motion Adaptation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Storytelling video generation (SVG) aims to produce coherent and visually rich multi-scene videos that follow a structured narrative. Existing methods primarily employ LLM for high-level planning to decompose a story into scene-level descriptions, which are then independently generated and stitched together. However, these approaches struggle with generating high-quality videos aligned with the complex single-scene description, as visualizing such complex description involves coherent composition of multiple characters and events, complex motion synthesis and muti-character customization. To address these challenges, we propose DreamRunner, a novel story-to-video generation method: First, we structure the input script using a large language model (LLM) to facilitate both coarse-grained scene planning as well as fine-grained object-level layout and motion planning. Next, DreamRunner presents retrieval-augmented test-time adaptation to capture target motion priors for objects in each scene, supporting diverse motion customization based on retrieved videos, thus facilitating the generation of new videos with complex, scripted motions. Lastly, we propose a novel spatial-temporal region-based 3D attention and prior injection module SR3AI for fine-grained object-motion binding and frame-by-frame semantic control. We compare DreamRunner with various SVG baselines, demonstrating state-of-the-art performance in character consistency, text alignment, and smooth transitions. Additionally, DreamRunner exhibits strong fine-grained condition-following ability in compositional text-to-video generation, significantly outperforming baselines on T2V-ComBench. Finally, we validate DreamRunner's robust ability to generate multi-object interactions with qualitative examples.
<div id='section'>Paperid: <span id='pid'>263, <a href='https://arxiv.org/pdf/2411.15582.pdf' target='_blank'>https://arxiv.org/pdf/2411.15582.pdf</a></span>   <span><a href='https://qingpowuwu.github.io/emd' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaobao Wei, Qingpo Wuwu, Zhongyu Zhao, Zhuangzhe Wu, Nan Huang, Ming Lu, Ningning MA, Shanghang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.15582">EMD: Explicit Motion Modeling for High-Quality Street Gaussian Splatting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Photorealistic reconstruction of street scenes is essential for developing real-world simulators in autonomous driving. While recent methods based on 3D/4D Gaussian Splatting (GS) have demonstrated promising results, they still encounter challenges in complex street scenes due to the unpredictable motion of dynamic objects. Current methods typically decompose street scenes into static and dynamic objects, learning the Gaussians in either a supervised manner (e.g., w/ 3D bounding-box) or a self-supervised manner (e.g., w/o 3D bounding-box). However, these approaches do not effectively model the motions of dynamic objects (e.g., the motion speed of pedestrians is clearly different from that of vehicles), resulting in suboptimal scene decomposition. To address this, we propose Explicit Motion Decomposition (EMD), which models the motions of dynamic objects by introducing learnable motion embeddings to the Gaussians, enhancing the decomposition in street scenes. The proposed plug-and-play EMD module compensates for the lack of motion modeling in self-supervised street Gaussian splatting methods. We also introduce tailored training strategies to extend EMD to supervised approaches. Comprehensive experiments demonstrate the effectiveness of our method, achieving state-of-the-art novel view synthesis performance in self-supervised settings. The code is available at: https://qingpowuwu.github.io/emd.
<div id='section'>Paperid: <span id='pid'>264, <a href='https://arxiv.org/pdf/2411.15472.pdf' target='_blank'>https://arxiv.org/pdf/2411.15472.pdf</a></span>   <span><a href='https://andypinxinliu.github.io/KinMo' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Pengfei Zhang, Pinxin Liu, Pablo Garrido, Hyeongwoo Kim, Bindita Chaudhuri
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.15472">KinMo: Kinematic-aware Human Motion Understanding and Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current human motion synthesis frameworks rely on global action descriptions, creating a modality gap that limits both motion understanding and generation capabilities. A single coarse description, such as run, fails to capture details such as variations in speed, limb positioning, and kinematic dynamics, leading to ambiguities between text and motion modalities. To address this challenge, we introduce KinMo, a unified framework built on a hierarchical describable motion representation that extends beyond global actions by incorporating kinematic group movements and their interactions. We design an automated annotation pipeline to generate high-quality, fine-grained descriptions for this decomposition, resulting in the KinMo dataset and offering a scalable and cost-efficient solution for dataset enrichment. To leverage these structured descriptions, we propose Hierarchical Text-Motion Alignment that progressively integrates additional motion details, thereby improving semantic motion understanding. Furthermore, we introduce a coarse-to-fine motion generation procedure to leverage enhanced spatial understanding to improve motion synthesis. Experimental results show that KinMo significantly improves motion understanding, demonstrated by enhanced text-motion retrieval performance and enabling more fine-grained motion generation and editing capabilities. Project Page: https://andypinxinliu.github.io/KinMo
<div id='section'>Paperid: <span id='pid'>265, <a href='https://arxiv.org/pdf/2411.13079.pdf' target='_blank'>https://arxiv.org/pdf/2411.13079.pdf</a></span>   <span><a href='https://github.com/thu-uav/NeuralIMC' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Feng Gao, Chao Yu, Yu Wang, Yi Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.13079">Neural Internal Model Control: Learning a Robust Control Policy via Predictive Error Feedback</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate motion control in the face of disturbances within complex environments remains a major challenge in robotics. Classical model-based approaches often struggle with nonlinearities and unstructured disturbances, while RL-based methods can be fragile when encountering unseen scenarios. In this paper, we propose a novel framework, Neural Internal Model Control, which integrates model-based control with RL-based control to enhance robustness. Our framework streamlines the predictive model by applying Newton-Euler equations for rigid-body dynamics, eliminating the need to capture complex high-dimensional nonlinearities. This internal model combines model-free RL algorithms with predictive error feedback. Such a design enables a closed-loop control structure to enhance the robustness and generalizability of the control system. We demonstrate the effectiveness of our framework on both quadrotors and quadrupedal robots, achieving superior performance compared to state-of-the-art methods. Furthermore, real-world deployment on a quadrotor with rope-suspended payloads highlights the framework's robustness in sim-to-real transfer. Our code is released at https://github.com/thu-uav/NeuralIMC.
<div id='section'>Paperid: <span id='pid'>266, <a href='https://arxiv.org/pdf/2411.08328.pdf' target='_blank'>https://arxiv.org/pdf/2411.08328.pdf</a></span>   <span><a href='https://mvideo-v1.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiang Zhou, Shaofeng Zhang, Nianzu Yang, Ye Qian, Hao Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.08328">Motion Control for Enhanced Complex Action Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing text-to-video (T2V) models often struggle with generating videos with sufficiently pronounced or complex actions. A key limitation lies in the text prompt's inability to precisely convey intricate motion details. To address this, we propose a novel framework, MVideo, designed to produce long-duration videos with precise, fluid actions. MVideo overcomes the limitations of text prompts by incorporating mask sequences as an additional motion condition input, providing a clearer, more accurate representation of intended actions. Leveraging foundational vision models such as GroundingDINO and SAM2, MVideo automatically generates mask sequences, enhancing both efficiency and robustness. Our results demonstrate that, after training, MVideo effectively aligns text prompts with motion conditions to produce videos that simultaneously meet both criteria. This dual control mechanism allows for more dynamic video generation by enabling alterations to either the text prompt or motion condition independently, or both in tandem. Furthermore, MVideo supports motion condition editing and composition, facilitating the generation of videos with more complex actions. MVideo thus advances T2V motion generation, setting a strong benchmark for improved action depiction in current video diffusion models. Our project page is available at https://mvideo-v1.github.io/.
<div id='section'>Paperid: <span id='pid'>267, <a href='https://arxiv.org/pdf/2411.06481.pdf' target='_blank'>https://arxiv.org/pdf/2411.06481.pdf</a></span>   <span><a href='https://steve-zeyu-zhang.github.io/KMM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zeyu Zhang, Hang Gao, Akide Liu, Qi Chen, Feng Chen, Yiran Wang, Danning Li, Rui Zhao, Zhenming Li, Zhongwen Zhou, Hao Tang, Bohan Zhuang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.06481">KMM: Key Frame Mask Mamba for Extended Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion generation is a cut-edge area of research in generative computer vision, with promising applications in video creation, game development, and robotic manipulation. The recent Mamba architecture shows promising results in efficiently modeling long and complex sequences, yet two significant challenges remain: Firstly, directly applying Mamba to extended motion generation is ineffective, as the limited capacity of the implicit memory leads to memory decay. Secondly, Mamba struggles with multimodal fusion compared to Transformers, and lack alignment with textual queries, often confusing directions (left or right) or omitting parts of longer text queries. To address these challenges, our paper presents three key contributions: Firstly, we introduce KMM, a novel architecture featuring Key frame Masking Modeling, designed to enhance Mamba's focus on key actions in motion segments. This approach addresses the memory decay problem and represents a pioneering method in customizing strategic frame-level masking in SSMs. Additionally, we designed a contrastive learning paradigm for addressing the multimodal fusion problem in Mamba and improving the motion-text alignment. Finally, we conducted extensive experiments on the go-to dataset, BABEL, achieving state-of-the-art performance with a reduction of more than 57% in FID and 70% parameters compared to previous state-of-the-art methods. See project website: https://steve-zeyu-zhang.github.io/KMM
<div id='section'>Paperid: <span id='pid'>268, <a href='https://arxiv.org/pdf/2411.04079.pdf' target='_blank'>https://arxiv.org/pdf/2411.04079.pdf</a></span>   <span><a href='https://vankouf.github.io/DSONet/' target='_blank'>  GitHub</a></span> <span><a href='https://vankouf.github.io/DSONet/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ke Fan, Jiangning Zhang, Ran Yi, Jingyu Gong, Yabiao Wang, Yating Wang, Xin Tan, Chengjie Wang, Lizhuang Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.04079">Textual Decomposition Then Sub-motion-space Scattering for Open-Vocabulary Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-to-motion generation is a crucial task in computer vision, which generates the target 3D motion by the given text. The existing annotated datasets are limited in scale, resulting in most existing methods overfitting to the small datasets and unable to generalize to the motions of the open domain. Some methods attempt to solve the open-vocabulary motion generation problem by aligning to the CLIP space or using the Pretrain-then-Finetuning paradigm. However, the current annotated dataset's limited scale only allows them to achieve mapping from sub-text-space to sub-motion-space, instead of mapping between full-text-space and full-motion-space (full mapping), which is the key to attaining open-vocabulary motion generation. To this end, this paper proposes to leverage the atomic motion (simple body part motions over a short time period) as an intermediate representation, and leverage two orderly coupled steps, i.e., Textual Decomposition and Sub-motion-space Scattering, to address the full mapping problem. For Textual Decomposition, we design a fine-grained description conversion algorithm, and combine it with the generalization ability of a large language model to convert any given motion text into atomic texts. Sub-motion-space Scattering learns the compositional process from atomic motions to the target motions, to make the learned sub-motion-space scattered to form the full-motion-space. For a given motion of the open domain, it transforms the extrapolation into interpolation and thereby significantly improves generalization. Our network, $DSO$-Net, combines textual $d$ecomposition and sub-motion-space $s$cattering to solve the $o$pen-vocabulary motion generation. Extensive experiments demonstrate that our DSO-Net achieves significant improvements over the state-of-the-art methods on open-vocabulary motion generation. Code is available at https://vankouf.github.io/DSONet/.
<div id='section'>Paperid: <span id='pid'>269, <a href='https://arxiv.org/pdf/2411.04005.pdf' target='_blank'>https://arxiv.org/pdf/2411.04005.pdf</a></span>   <span><a href='https://cypypccpy.github.io/obj-dex.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuanpei Chen, Chen Wang, Yaodong Yang, C. Karen Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.04005">Object-Centric Dexterous Manipulation from Human Motion Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Manipulating objects to achieve desired goal states is a basic but important skill for dexterous manipulation. Human hand motions demonstrate proficient manipulation capability, providing valuable data for training robots with multi-finger hands. Despite this potential, substantial challenges arise due to the embodiment gap between human and robot hands. In this work, we introduce a hierarchical policy learning framework that uses human hand motion data for training object-centric dexterous robot manipulation. At the core of our method is a high-level trajectory generative model, learned with a large-scale human hand motion capture dataset, to synthesize human-like wrist motions conditioned on the desired object goal states. Guided by the generated wrist motions, deep reinforcement learning is further used to train a low-level finger controller that is grounded in the robot's embodiment to physically interact with the object to achieve the goal. Through extensive evaluation across 10 household objects, our approach not only demonstrates superior performance but also showcases generalization capability to novel object geometries and goal states. Furthermore, we transfer the learned policies from simulation to a real-world bimanual dexterous robot system, further demonstrating its applicability in real-world scenarios. Project website: https://cypypccpy.github.io/obj-dex.github.io/.
<div id='section'>Paperid: <span id='pid'>270, <a href='https://arxiv.org/pdf/2411.02673.pdf' target='_blank'>https://arxiv.org/pdf/2411.02673.pdf</a></span>   <span><a href='https://github.com/vita-epfl/multi-transmotion' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yang Gao, Po-Chien Luan, Alexandre Alahi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.02673">Multi-Transmotion: Pre-trained Model for Human Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The ability of intelligent systems to predict human behaviors is crucial, particularly in fields such as autonomous vehicle navigation and social robotics. However, the complexity of human motion have prevented the development of a standardized dataset for human motion prediction, thereby hindering the establishment of pre-trained models. In this paper, we address these limitations by integrating multiple datasets, encompassing both trajectory and 3D pose keypoints, to propose a pre-trained model for human motion prediction. We merge seven distinct datasets across varying modalities and standardize their formats. To facilitate multimodal pre-training, we introduce Multi-Transmotion, an innovative transformer-based model designed for cross-modality pre-training. Additionally, we present a novel masking strategy to capture rich representations. Our methodology demonstrates competitive performance across various datasets on several downstream tasks, including trajectory prediction in the NBA and JTA datasets, as well as pose prediction in the AMASS and 3DPW datasets. The code is publicly available: https://github.com/vita-epfl/multi-transmotion
<div id='section'>Paperid: <span id='pid'>271, <a href='https://arxiv.org/pdf/2411.01814.pdf' target='_blank'>https://arxiv.org/pdf/2411.01814.pdf</a></span>   <span><a href='https://github.com/thanhnguyencanh/SGan-TEB.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Thanh Nguyen Canh, Xiem HoangVan, Nak Young Chong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.01814">Enhancing Social Robot Navigation with Integrated Motion Prediction and Trajectory Planning in Dynamic Human Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Navigating safely in dynamic human environments is crucial for mobile service robots, and social navigation is a key aspect of this process. In this paper, we proposed an integrative approach that combines motion prediction and trajectory planning to enable safe and socially-aware robot navigation. The main idea of the proposed method is to leverage the advantages of Socially Acceptable trajectory prediction and Timed Elastic Band (TEB) by incorporating human interactive information including position, orientation, and motion into the objective function of the TEB algorithms. In addition, we designed social constraints to ensure the safety of robot navigation. The proposed system is evaluated through physical simulation using both quantitative and qualitative metrics, demonstrating its superior performance in avoiding human and dynamic obstacles, thereby ensuring safe navigation. The implementations are open source at: \url{https://github.com/thanhnguyencanh/SGan-TEB.git}
<div id='section'>Paperid: <span id='pid'>272, <a href='https://arxiv.org/pdf/2411.00128.pdf' target='_blank'>https://arxiv.org/pdf/2411.00128.pdf</a></span>   <span><a href='https://simplexsigil.github.io/mint' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>David Schneider, Simon ReiÃ, Marco Kugler, Alexander Jaus, Kunyu Peng, Susanne Sutschet, M. Saquib Sarfraz, Sven Matthiesen, Rainer Stiefelhagen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.00128">Muscles in Time: Learning to Understand Human Motion by Simulating Muscle Activations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Exploring the intricate dynamics between muscular and skeletal structures is pivotal for understanding human motion. This domain presents substantial challenges, primarily attributed to the intensive resources required for acquiring ground truth muscle activation data, resulting in a scarcity of datasets. In this work, we address this issue by establishing Muscles in Time (MinT), a large-scale synthetic muscle activation dataset. For the creation of MinT, we enriched existing motion capture datasets by incorporating muscle activation simulations derived from biomechanical human body models using the OpenSim platform, a common approach in biomechanics and human motion research. Starting from simple pose sequences, our pipeline enables us to extract detailed information about the timing of muscle activations within the human musculoskeletal system. Muscles in Time contains over nine hours of simulation data covering 227 subjects and 402 simulated muscle strands. We demonstrate the utility of this dataset by presenting results on neural network-based muscle activation estimation from human pose sequences with two different sequence-to-sequence architectures. Data and code are provided under https://simplexsigil.github.io/mint.
<div id='section'>Paperid: <span id='pid'>273, <a href='https://arxiv.org/pdf/2410.24219.pdf' target='_blank'>https://arxiv.org/pdf/2410.24219.pdf</a></span>   <span><a href='https://PR-Ryan.github.io/DEMO-project/' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/PR-Ryan/DEMO' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Penghui Ruan, Pichao Wang, Divya Saxena, Jiannong Cao, Yuhui Shi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.24219">Enhancing Motion in Text-to-Video Generation with Decomposed Encoding and Conditioning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite advancements in Text-to-Video (T2V) generation, producing videos with realistic motion remains challenging. Current models often yield static or minimally dynamic outputs, failing to capture complex motions described by text. This issue stems from the internal biases in text encoding, which overlooks motions, and inadequate conditioning mechanisms in T2V generation models. To address this, we propose a novel framework called DEcomposed MOtion (DEMO), which enhances motion synthesis in T2V generation by decomposing both text encoding and conditioning into content and motion components. Our method includes a content encoder for static elements and a motion encoder for temporal dynamics, alongside separate content and motion conditioning mechanisms. Crucially, we introduce text-motion and video-motion supervision to improve the model's understanding and generation of motion. Evaluations on benchmarks such as MSR-VTT, UCF-101, WebVid-10M, EvalCrafter, and VBench demonstrate DEMO's superior ability to produce videos with enhanced motion dynamics while maintaining high visual quality. Our approach significantly advances T2V generation by integrating comprehensive motion understanding directly from textual descriptions. Project page: https://PR-Ryan.github.io/DEMO-project/
<div id='section'>Paperid: <span id='pid'>274, <a href='https://arxiv.org/pdf/2410.20986.pdf' target='_blank'>https://arxiv.org/pdf/2410.20986.pdf</a></span>   <span><a href='https://github.com/abcyzj/MeshRet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zijie Ye, Jia-Wei Liu, Jia Jia, Shikun Sun, Mike Zheng Shou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.20986">Skinned Motion Retargeting with Dense Geometric Interaction Perception</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Capturing and maintaining geometric interactions among different body parts is crucial for successful motion retargeting in skinned characters. Existing approaches often overlook body geometries or add a geometry correction stage after skeletal motion retargeting. This results in conflicts between skeleton interaction and geometry correction, leading to issues such as jittery, interpenetration, and contact mismatches. To address these challenges, we introduce a new retargeting framework, MeshRet, which directly models the dense geometric interactions in motion retargeting. Initially, we establish dense mesh correspondences between characters using semantically consistent sensors (SCS), effective across diverse mesh topologies. Subsequently, we develop a novel spatio-temporal representation called the dense mesh interaction (DMI) field. This field, a collection of interacting SCS feature vectors, skillfully captures both contact and non-contact interactions between body geometries. By aligning the DMI field during retargeting, MeshRet not only preserves motion semantics but also prevents self-interpenetration and ensures contact preservation. Extensive experiments on the public Mixamo dataset and our newly-collected ScanRet dataset demonstrate that MeshRet achieves state-of-the-art performance. Code available at https://github.com/abcyzj/MeshRet.
<div id='section'>Paperid: <span id='pid'>275, <a href='https://arxiv.org/pdf/2410.17610.pdf' target='_blank'>https://arxiv.org/pdf/2410.17610.pdf</a></span>   <span><a href='https://foruck.github.io/ImDy/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinpeng Liu, Junxuan Liang, Zili Lin, Haowen Hou, Yong-Lu Li, Cewu Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.17610">ImDy: Human Inverse Dynamics from Imitated Observations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Inverse dynamics (ID), which aims at reproducing the driven torques from human kinematic observations, has been a critical tool for gait analysis. However, it is hindered from wider application to general motion due to its limited scalability. Conventional optimization-based ID requires expensive laboratory setups, restricting its availability. To alleviate this problem, we propose to exploit the recently progressive human motion imitation algorithms to learn human inverse dynamics in a data-driven manner. The key insight is that the human ID knowledge is implicitly possessed by motion imitators, though not directly applicable. In light of this, we devise an efficient data collection pipeline with state-of-the-art motion imitation algorithms and physics simulators, resulting in a large-scale human inverse dynamics benchmark as Imitated Dynamics (ImDy). ImDy contains over 150 hours of motion with joint torque and full-body ground reaction force data. With ImDy, we train a data-driven human inverse dynamics solver ImDyS(olver) in a fully supervised manner, which conducts ID and ground reaction force estimation simultaneously. Experiments on ImDy and real-world data demonstrate the impressive competency of ImDyS in human inverse dynamics and ground reaction force estimation. Moreover, the potential of ImDy(-S) as a fundamental motion analysis tool is exhibited with downstream applications. The project page is https://foruck.github.io/ImDy/.
<div id='section'>Paperid: <span id='pid'>276, <a href='https://arxiv.org/pdf/2410.16864.pdf' target='_blank'>https://arxiv.org/pdf/2410.16864.pdf</a></span>   <span><a href='https://github.com/dmytrozabolotnii/autoware_mini' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dmytro Zabolotnii, Yar Muhammad, Naveed Muhammad
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.16864">Pedestrian motion prediction evaluation for urban autonomous driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pedestrian motion prediction is a key part of the modular-based autonomous driving pipeline, ensuring safe, accurate, and timely awareness of human agents' possible future trajectories. The autonomous vehicle can use this information to prevent any possible accidents and create a comfortable and pleasant driving experience for the passengers and pedestrians. A wealth of research was done on the topic from the authors of robotics, computer vision, intelligent transportation systems, and other fields. However, a relatively unexplored angle is the integration of the state-of-art solutions into existing autonomous driving stacks and evaluating them in real-life conditions rather than sanitized datasets. We analyze selected publications with provided open-source solutions and provide a perspective obtained by integrating them into existing Autonomous Driving framework - Autoware Mini and performing experiments in natural urban conditions in Tartu, Estonia to determine valuability of traditional motion prediction metrics. This perspective should be valuable to any potential autonomous driving or robotics engineer looking for the real-world performance of the existing state-of-art pedestrian motion prediction problem. The code with instructions on accessing the dataset is available at https://github.com/dmytrozabolotnii/autoware_mini.
<div id='section'>Paperid: <span id='pid'>277, <a href='https://arxiv.org/pdf/2410.15819.pdf' target='_blank'>https://arxiv.org/pdf/2410.15819.pdf</a></span>   <span><a href='https://github.com/Cing2/LiMTR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Camiel Oerlemans, Bram Grooten, Michiel Braat, Alaa Alassi, Emilia Silvas, Decebal Constantin Mocanu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.15819">LiMTR: Time Series Motion Prediction for Diverse Road Users through Multimodal Feature Integration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Predicting the behavior of road users accurately is crucial to enable the safe operation of autonomous vehicles in urban or densely populated areas. Therefore, there has been a growing interest in time series motion prediction research, leading to significant advancements in state-of-the-art techniques in recent years. However, the potential of using LiDAR data to capture more detailed local features, such as a person's gaze or posture, remains largely unexplored. To address this, we develop a novel multimodal approach for motion prediction based on the PointNet foundation model architecture, incorporating local LiDAR features. Evaluation on the Waymo Open Dataset shows a performance improvement of 6.20% and 1.58% in minADE and mAP respectively, when integrated and compared with the previous state-of-the-art MTR. We open-source the code of our LiMTR model.
<div id='section'>Paperid: <span id='pid'>278, <a href='https://arxiv.org/pdf/2410.15582.pdf' target='_blank'>https://arxiv.org/pdf/2410.15582.pdf</a></span>   <span><a href='https://github.com/TangTao-PKU/ARTS' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/TangTao-PKU/ARTS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tao Tang, Hong Liu, Yingxuan You, Ti Wang, Wenhao Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.15582">ARTS: Semi-Analytical Regressor using Disentangled Skeletal Representations for Human Mesh Recovery from Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Although existing video-based 3D human mesh recovery methods have made significant progress, simultaneously estimating human pose and shape from low-resolution image features limits their performance. These image features lack sufficient spatial information about the human body and contain various noises (e.g., background, lighting, and clothing), which often results in inaccurate pose and inconsistent motion. Inspired by the rapid advance in human pose estimation, we discover that compared to image features, skeletons inherently contain accurate human pose and motion. Therefore, we propose a novel semiAnalytical Regressor using disenTangled Skeletal representations for human mesh recovery from videos, called ARTS. Specifically, a skeleton estimation and disentanglement module is proposed to estimate the 3D skeletons from a video and decouple them into disentangled skeletal representations (i.e., joint position, bone length, and human motion). Then, to fully utilize these representations, we introduce a semi-analytical regressor to estimate the parameters of the human mesh model. The regressor consists of three modules: Temporal Inverse Kinematics (TIK), Bone-guided Shape Fitting (BSF), and Motion-Centric Refinement (MCR). TIK utilizes joint position to estimate initial pose parameters and BSF leverages bone length to regress bone-aligned shape parameters. Finally, MCR combines human motion representation with image features to refine the initial human model parameters. Extensive experiments demonstrate that our ARTS surpasses existing state-of-the-art video-based methods in both per-frame accuracy and temporal consistency on popular benchmarks: 3DPW, MPI-INF-3DHP, and Human3.6M. Code is available at https://github.com/TangTao-PKU/ARTS.
<div id='section'>Paperid: <span id='pid'>279, <a href='https://arxiv.org/pdf/2410.13817.pdf' target='_blank'>https://arxiv.org/pdf/2410.13817.pdf</a></span>   <span><a href='https://leggedrobotics.github.io/guided-rl-locoma/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jean-Pierre Sleiman, Mayank Mittal, Marco Hutter
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.13817">Guided Reinforcement Learning for Robust Multi-Contact Loco-Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reinforcement learning (RL) often necessitates a meticulous Markov Decision Process (MDP) design tailored to each task. This work aims to address this challenge by proposing a systematic approach to behavior synthesis and control for multi-contact loco-manipulation tasks, such as navigating spring-loaded doors and manipulating heavy dishwashers. We define a task-independent MDP to train RL policies using only a single demonstration per task generated from a model-based trajectory optimizer. Our approach incorporates an adaptive phase dynamics formulation to robustly track the demonstrations while accommodating dynamic uncertainties and external disturbances. We compare our method against prior motion imitation RL works and show that the learned policies achieve higher success rates across all considered tasks. These policies learn recovery maneuvers that are not present in the demonstration, such as re-grasping objects during execution or dealing with slippages. Finally, we successfully transfer the policies to a real robot, demonstrating the practical viability of our approach.
<div id='section'>Paperid: <span id='pid'>280, <a href='https://arxiv.org/pdf/2410.13790.pdf' target='_blank'>https://arxiv.org/pdf/2410.13790.pdf</a></span>   <span><a href='https://github.com/liangxuy/MotionBank' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Liang Xu, Shaoyang Hua, Zili Lin, Yifan Liu, Feipeng Ma, Yichao Yan, Xin Jin, Xiaokang Yang, Wenjun Zeng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.13790">MotionBank: A Large-scale Video Motion Benchmark with Disentangled Rule-based Annotations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we tackle the problem of how to build and benchmark a large motion model (LMM). The ultimate goal of LMM is to serve as a foundation model for versatile motion-related tasks, e.g., human motion generation, with interpretability and generalizability. Though advanced, recent LMM-related works are still limited by small-scale motion data and costly text descriptions. Besides, previous motion benchmarks primarily focus on pure body movements, neglecting the ubiquitous motions in context, i.e., humans interacting with humans, objects, and scenes. To address these limitations, we consolidate large-scale video action datasets as knowledge banks to build MotionBank, which comprises 13 video action datasets, 1.24M motion sequences, and 132.9M frames of natural and diverse human motions. Different from laboratory-captured motions, in-the-wild human-centric videos contain abundant motions in context. To facilitate better motion text alignment, we also meticulously devise a motion caption generation algorithm to automatically produce rule-based, unbiased, and disentangled text descriptions via the kinematic characteristics for each motion. Extensive experiments show that our MotionBank is beneficial for general motion-related tasks of human motion generation, motion in-context generation, and motion understanding. Video motions together with the rule-based text annotations could serve as an efficient alternative for larger LMMs. Our dataset, codes, and benchmark will be publicly available at https://github.com/liangxuy/MotionBank.
<div id='section'>Paperid: <span id='pid'>281, <a href='https://arxiv.org/pdf/2410.12773.pdf' target='_blank'>https://arxiv.org/pdf/2410.12773.pdf</a></span>   <span><a href='https://ut-austin-rpl.github.io/Harmon/' target='_blank'>  GitHub</a></span> <span><a href='https://ut-austin-rpl.github.io/Harmon/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenyu Jiang, Yuqi Xie, Jinhan Li, Ye Yuan, Yifeng Zhu, Yuke Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.12773">Harmon: Whole-Body Motion Generation of Humanoid Robots from Language Descriptions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humanoid robots, with their human-like embodiment, have the potential to integrate seamlessly into human environments. Critical to their coexistence and cooperation with humans is the ability to understand natural language communications and exhibit human-like behaviors. This work focuses on generating diverse whole-body motions for humanoid robots from language descriptions. We leverage human motion priors from extensive human motion datasets to initialize humanoid motions and employ the commonsense reasoning capabilities of Vision Language Models (VLMs) to edit and refine these motions. Our approach demonstrates the capability to produce natural, expressive, and text-aligned humanoid motions, validated through both simulated and real-world experiments. More videos can be found at https://ut-austin-rpl.github.io/Harmon/.
<div id='section'>Paperid: <span id='pid'>282, <a href='https://arxiv.org/pdf/2410.10790.pdf' target='_blank'>https://arxiv.org/pdf/2410.10790.pdf</a></span>   <span><a href='https://windvchen.github.io/Sitcom-Crafter' target='_blank'>  GitHub</a></span> <span><a href='https://windvchen.github.io/Sitcom-Crafter' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianqi Chen, Panwen Hu, Xiaojun Chang, Zhenwei Shi, Michael Kampffmeyer, Xiaodan Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.10790">Sitcom-Crafter: A Plot-Driven Human Motion Generation System in 3D Scenes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in human motion synthesis have focused on specific types of motions, such as human-scene interaction, locomotion or human-human interaction, however, there is a lack of a unified system capable of generating a diverse combination of motion types. In response, we introduce Sitcom-Crafter, a comprehensive and extendable system for human motion generation in 3D space, which can be guided by extensive plot contexts to enhance workflow efficiency for anime and game designers. The system is comprised of eight modules, three of which are dedicated to motion generation, while the remaining five are augmentation modules that ensure consistent fusion of motion sequences and system functionality. Central to the generation modules is our novel 3D scene-aware human-human interaction module, which addresses collision issues by synthesizing implicit 3D Signed Distance Function (SDF) points around motion spaces, thereby minimizing human-scene collisions without additional data collection costs. Complementing this, our locomotion and human-scene interaction modules leverage existing methods to enrich the system's motion generation capabilities. Augmentation modules encompass plot comprehension for command generation, motion synchronization for seamless integration of different motion types, hand pose retrieval to enhance motion realism, motion collision revision to prevent human collisions, and 3D retargeting to ensure visual fidelity. Experimental evaluations validate the system's ability to generate high-quality, diverse, and physically realistic motions, underscoring its potential for advancing creative workflows. Project page: https://windvchen.github.io/Sitcom-Crafter.
<div id='section'>Paperid: <span id='pid'>283, <a href='https://arxiv.org/pdf/2410.10780.pdf' target='_blank'>https://arxiv.org/pdf/2410.10780.pdf</a></span>   <span><a href='https://exitudio.github.io/ControlMM-page' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ekkasit Pinyoanuntapong, Muhammad Usama Saleem, Korrawe Karunratanakul, Pu Wang, Hongfei Xue, Chen Chen, Chuan Guo, Junli Cao, Jian Ren, Sergey Tulyakov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.10780">MaskControl: Spatio-Temporal Control for Masked Motion Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in motion diffusion models have enabled spatially controllable text-to-motion generation. However, these models struggle to achieve high-precision control while maintaining high-quality motion generation. To address these challenges, we propose MaskControl, the first approach to introduce controllability to the generative masked motion model. Our approach introduces two key innovations. First, \textit{Logits Regularizer} implicitly perturbs logits at training time to align the distribution of motion tokens with the controlled joint positions, while regularizing the categorical token prediction to ensure high-fidelity generation. Second, \textit{Logit Optimization} explicitly optimizes the predicted logits during inference time, directly reshaping the token distribution that forces the generated motion to accurately align with the controlled joint positions. Moreover, we introduce \textit{Differentiable Expectation Sampling (DES)} to combat the non-differential distribution sampling process encountered by logits regularizer and optimization. Extensive experiments demonstrate that MaskControl outperforms state-of-the-art methods, achieving superior motion quality (FID decreases by ~77\%) and higher control precision (average error 0.91 vs. 1.08). Additionally, MaskControl enables diverse applications, including any-joint-any-frame control, body-part timeline control, and zero-shot objective control. Video visualization can be found at https://www.ekkasit.com/ControlMM-page/
<div id='section'>Paperid: <span id='pid'>284, <a href='https://arxiv.org/pdf/2410.08669.pdf' target='_blank'>https://arxiv.org/pdf/2410.08669.pdf</a></span>   <span><a href='https://github.com/youngzhou1999/SmartPretrain' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yang Zhou, Hao Shao, Letian Wang, Steven L. Waslander, Hongsheng Li, Yu Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.08669">SmartPretrain: Model-Agnostic and Dataset-Agnostic Representation Learning for Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Predicting the future motion of surrounding agents is essential for autonomous vehicles (AVs) to operate safely in dynamic, human-robot-mixed environments. However, the scarcity of large-scale driving datasets has hindered the development of robust and generalizable motion prediction models, limiting their ability to capture complex interactions and road geometries. Inspired by recent advances in natural language processing (NLP) and computer vision (CV), self-supervised learning (SSL) has gained significant attention in the motion prediction community for learning rich and transferable scene representations. Nonetheless, existing pre-training methods for motion prediction have largely focused on specific model architectures and single dataset, limiting their scalability and generalizability. To address these challenges, we propose SmartPretrain, a general and scalable SSL framework for motion prediction that is both model-agnostic and dataset-agnostic. Our approach integrates contrastive and reconstructive SSL, leveraging the strengths of both generative and discriminative paradigms to effectively represent spatiotemporal evolution and interactions without imposing architectural constraints. Additionally, SmartPretrain employs a dataset-agnostic scenario sampling strategy that integrates multiple datasets, enhancing data volume, diversity, and robustness. Extensive experiments on multiple datasets demonstrate that SmartPretrain consistently improves the performance of state-of-the-art prediction models across datasets, data splits and main metrics. For instance, SmartPretrain significantly reduces the MissRate of Forecast-MAE by 10.6%. These results highlight SmartPretrain's effectiveness as a unified, scalable solution for motion prediction, breaking free from the limitations of the small-data regime. Codes are available at https://github.com/youngzhou1999/SmartPretrain
<div id='section'>Paperid: <span id='pid'>285, <a href='https://arxiv.org/pdf/2410.07795.pdf' target='_blank'>https://arxiv.org/pdf/2410.07795.pdf</a></span>   <span><a href='https://github.com/cuongle1206/OSDCap' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Cuong Le, Viktor Johansson, Manon Kok, Bastian Wandt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.07795">Optimal-state Dynamics Estimation for Physics-based Human Motion Capture from Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion capture from monocular videos has made significant progress in recent years. However, modern approaches often produce temporal artifacts, e.g. in form of jittery motion and struggle to achieve smooth and physically plausible motions. Explicitly integrating physics, in form of internal forces and exterior torques, helps alleviating these artifacts. Current state-of-the-art approaches make use of an automatic PD controller to predict torques and reaction forces in order to re-simulate the input kinematics, i.e. the joint angles of a predefined skeleton. However, due to imperfect physical models, these methods often require simplifying assumptions and extensive preprocessing of the input kinematics to achieve good performance. To this end, we propose a novel method to selectively incorporate the physics models with the kinematics observations in an online setting, inspired by a neural Kalman-filtering approach. We develop a control loop as a meta-PD controller to predict internal joint torques and external reaction forces, followed by a physics-based motion simulation. A recurrent neural network is introduced to realize a Kalman filter that attentively balances the kinematics input and simulated motion, resulting in an optimal-state dynamics prediction. We show that this filtering step is crucial to provide an online supervision that helps balancing the shortcoming of the respective input motions, thus being important for not only capturing accurate global motion trajectories but also producing physically plausible human poses. The proposed approach excels in the physics-based human pose estimation task and demonstrates the physical plausibility of the predictive dynamics, compared to state of the art. The code is available on https://github.com/cuongle1206/OSDCap
<div id='section'>Paperid: <span id='pid'>286, <a href='https://arxiv.org/pdf/2410.06327.pdf' target='_blank'>https://arxiv.org/pdf/2410.06327.pdf</a></span>   <span><a href='https://genea-workshop.github.io/leaderboard/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Rajmund Nagy, Hendric Voss, Youngwoo Yoon, Taras Kucherenko, Teodor Nikolov, Thanh Hoang-Minh, Rachel McDonnell, Stefan Kopp, Michael Neff, Gustav Eje Henter
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.06327">Towards a GENEA Leaderboard -- an Extended, Living Benchmark for Evaluating and Advancing Conversational Motion Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current evaluation practices in speech-driven gesture generation lack standardisation and focus on aspects that are easy to measure over aspects that actually matter. This leads to a situation where it is impossible to know what is the state of the art, or to know which method works better for which purpose when comparing two publications. In this position paper, we review and give details on issues with existing gesture-generation evaluation, and present a novel proposal for remedying them. Specifically, we announce an upcoming living leaderboard to benchmark progress in conversational motion synthesis. Unlike earlier gesture-generation challenges, the leaderboard will be updated with large-scale user studies of new gesture-generation systems multiple times per year, and systems on the leaderboard can be submitted to any publication venue that their authors prefer. By evolving the leaderboard evaluation data and tasks over time, the effort can keep driving progress towards the most important end goals identified by the community. We actively seek community involvement across the entire evaluation pipeline: from data and tasks for the evaluation, via tooling, to the systems evaluated. In other words, our proposal will not only make it easier for researchers to perform good evaluations, but their collective input and contributions will also help drive the future of gesture-generation research.
<div id='section'>Paperid: <span id='pid'>287, <a href='https://arxiv.org/pdf/2410.05931.pdf' target='_blank'>https://arxiv.org/pdf/2410.05931.pdf</a></span>   <span><a href='https://sahara-yuta.github.io/projects/shoulder-complex-simulation' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuta Sahara, Akihiro Miki, Yoshimoto Ribayashi, Shunnosuke Yoshimura, Kento Kawaharazuka, Kei Okada, Masayuki Inaba
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.05931">Construction of Musculoskeletal Simulation for Shoulder Complex with Ligaments and Its Validation via Model Predictive Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The complex ways in which humans utilize their bodies in sports and martial arts are remarkable, and human motion analysis is one of the most effective tools for robot body design and control. On the other hand, motion analysis is not easy, and it is difficult to measure complex body motions in detail due to the influence of numerous muscles and soft tissues, mainly ligaments. In response, various musculoskeletal simulators have been developed and applied to motion analysis and robotics. However, none of them reproduce the ligaments but only the muscles, nor do they focus on the shoulder complex, including the clavicle and scapula, which is one of the most complex parts of the body. Therefore, in this study, a detailed simulation model of the shoulder complex including ligaments is constructed. The model will mimic not only the skeletal structure and muscle arrangement but also the ligament arrangement and maximum muscle strength. Through model predictive control based on the constructed simulation, we confirmed that the ligaments contribute to joint stabilization in the first movement and that the proper distribution of maximum muscle force contributes to the equalization of the load on each muscle, demonstrating the effectiveness of this simulation.
<div id='section'>Paperid: <span id='pid'>288, <a href='https://arxiv.org/pdf/2410.05260.pdf' target='_blank'>https://arxiv.org/pdf/2410.05260.pdf</a></span>   <span><a href='https://zkf1997.github.io/DART/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaifeng Zhao, Gen Li, Siyu Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.05260">DartControl: A Diffusion-Based Autoregressive Motion Model for Real-Time Text-Driven Motion Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-conditioned human motion generation, which allows for user interaction through natural language, has become increasingly popular. Existing methods typically generate short, isolated motions based on a single input sentence. However, human motions are continuous and can extend over long periods, carrying rich semantics. Creating long, complex motions that precisely respond to streams of text descriptions, particularly in an online and real-time setting, remains a significant challenge. Furthermore, incorporating spatial constraints into text-conditioned motion generation presents additional challenges, as it requires aligning the motion semantics specified by text descriptions with geometric information, such as goal locations and 3D scene geometry. To address these limitations, we propose DartControl, in short DART, a Diffusion-based Autoregressive motion primitive model for Real-time Text-driven motion control. Our model effectively learns a compact motion primitive space jointly conditioned on motion history and text inputs using latent diffusion models. By autoregressively generating motion primitives based on the preceding history and current text input, DART enables real-time, sequential motion generation driven by natural language descriptions. Additionally, the learned motion primitive space allows for precise spatial motion control, which we formulate either as a latent noise optimization problem or as a Markov decision process addressed through reinforcement learning. We present effective algorithms for both approaches, demonstrating our model's versatility and superior performance in various motion synthesis tasks. Experiments show our method outperforms existing baselines in motion realism, efficiency, and controllability. Video results are available on the project page: https://zkf1997.github.io/DART/.
<div id='section'>Paperid: <span id='pid'>289, <a href='https://arxiv.org/pdf/2410.03441.pdf' target='_blank'>https://arxiv.org/pdf/2410.03441.pdf</a></span>   <span><a href='https://guytevet.github.io/CLoSD-page/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Guy Tevet, Sigal Raab, Setareh Cohan, Daniele Reda, Zhengyi Luo, Xue Bin Peng, Amit H. Bermano, Michiel van de Panne
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.03441">CLoSD: Closing the Loop between Simulation and Diffusion for multi-task character control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion diffusion models and Reinforcement Learning (RL) based control for physics-based simulations have complementary strengths for human motion generation. The former is capable of generating a wide variety of motions, adhering to intuitive control such as text, while the latter offers physically plausible motion and direct interaction with the environment. In this work, we present a method that combines their respective strengths. CLoSD is a text-driven RL physics-based controller, guided by diffusion generation for various tasks. Our key insight is that motion diffusion can serve as an on-the-fly universal planner for a robust RL controller. To this end, CLoSD maintains a closed-loop interaction between two modules -- a Diffusion Planner (DiP), and a tracking controller. DiP is a fast-responding autoregressive diffusion model, controlled by textual prompts and target locations, and the controller is a simple and robust motion imitator that continuously receives motion plans from DiP and provides feedback from the environment. CLoSD is capable of seamlessly performing a sequence of different tasks, including navigation to a goal location, striking an object with a hand or foot as specified in a text prompt, sitting down, and getting up. https://guytevet.github.io/CLoSD-page/
<div id='section'>Paperid: <span id='pid'>290, <a href='https://arxiv.org/pdf/2410.03311.pdf' target='_blank'>https://arxiv.org/pdf/2410.03311.pdf</a></span>   <span><a href='https://beingbeyond.github.io/Being-M0/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ye Wang, Sipeng Zheng, Bin Cao, Qianshan Wei, Weishuai Zeng, Qin Jin, Zongqing Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.03311">Scaling Large Motion Models with Million-Level Human Motions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Inspired by the recent success of LLMs, the field of human motion understanding has increasingly shifted toward developing large motion models. Despite some progress, current efforts remain far from achieving truly generalist models, primarily due to the lack of massive high-quality data. To address this gap, we present MotionLib, the first million-level dataset for motion generation, which is at least 15$\times$ larger than existing counterparts and enriched with hierarchical text descriptions. Using MotionLib, we train a large motion model named \projname, demonstrating robust performance across a wide range of human activities, including unseen ones. Through systematic investigation, for the first time, we highlight the importance of scaling both data and model size for advancing motion generation, along with key insights to achieve this goal. To better integrate the motion modality, we propose Motionbook, an innovative motion encoding approach including (1) a compact yet lossless feature to represent motions; (2) a novel 2D lookup-free motion tokenizer that preserves fine-grained motion details while expanding codebook capacity, significantly enhancing the representational power of motion tokens. We believe this work lays the groundwork for developing more versatile and powerful motion generation models in the future. For further details, visit https://beingbeyond.github.io/Being-M0/.
<div id='section'>Paperid: <span id='pid'>291, <a href='https://arxiv.org/pdf/2410.00464.pdf' target='_blank'>https://arxiv.org/pdf/2410.00464.pdf</a></span>   <span><a href='https://robinwitch.github.io/SynTalker-Page' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bohong Chen, Yumeng Li, Yao-Xiang Ding, Tianjia Shao, Kun Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.00464">Enabling Synergistic Full-Body Control in Prompt-Based Co-Speech Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current co-speech motion generation approaches usually focus on upper body gestures following speech contents only, while lacking supporting the elaborate control of synergistic full-body motion based on text prompts, such as talking while walking. The major challenges lie in 1) the existing speech-to-motion datasets only involve highly limited full-body motions, making a wide range of common human activities out of training distribution; 2) these datasets also lack annotated user prompts. To address these challenges, we propose SynTalker, which utilizes the off-the-shelf text-to-motion dataset as an auxiliary for supplementing the missing full-body motion and prompts. The core technical contributions are two-fold. One is the multi-stage training process which obtains an aligned embedding space of motion, speech, and prompts despite the significant distributional mismatch in motion between speech-to-motion and text-to-motion datasets. Another is the diffusion-based conditional inference process, which utilizes the separate-then-combine strategy to realize fine-grained control of local body parts. Extensive experiments are conducted to verify that our approach supports precise and flexible control of synergistic full-body motion generation based on both speeches and user prompts, which is beyond the ability of existing approaches.
<div id='section'>Paperid: <span id='pid'>292, <a href='https://arxiv.org/pdf/2409.19911.pdf' target='_blank'>https://arxiv.org/pdf/2409.19911.pdf</a></span>   <span><a href='https://github.com/ali-vilab/UniAnimate-DiT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiang Wang, Shiwei Zhang, Haonan Qiu, Ruihang Chu, Zekun Li, Yingya Zhang, Changxin Gao, Yuehuan Wang, Chunhua Shen, Nong Sang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.19911">Replace Anyone in Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The field of controllable human-centric video generation has witnessed remarkable progress, particularly with the advent of diffusion models. However, achieving precise and localized control over human motion in videos, such as replacing or inserting individuals while preserving desired motion patterns, still remains a formidable challenge. In this work, we present the ReplaceAnyone framework, which focuses on localized human replacement and insertion featuring intricate backgrounds. Specifically, we formulate this task as an image-conditioned video inpainting paradigm with pose guidance, utilizing a unified end-to-end video diffusion architecture that facilitates image-conditioned video inpainting within masked regions. To prevent shape leakage and enable granular local control, we introduce diverse mask forms involving both regular and irregular shapes. Furthermore, we implement an enriched visual guidance mechanism to enhance appearance alignment, a hybrid inpainting encoder to further preserve the detailed background information in the masked video, and a two-phase optimization methodology to simplify the training difficulty. ReplaceAnyone enables seamless replacement or insertion of characters while maintaining the desired pose motion and reference appearance within a single framework. Extensive experimental results demonstrate the effectiveness of our method in generating realistic and coherent video content. The proposed ReplaceAnyone can be seamlessly applied not only to traditional 3D-UNet base models but also to DiT-based video models such as Wan2.1. The code will be available at https://github.com/ali-vilab/UniAnimate-DiT.
<div id='section'>Paperid: <span id='pid'>293, <a href='https://arxiv.org/pdf/2409.18399.pdf' target='_blank'>https://arxiv.org/pdf/2409.18399.pdf</a></span>   <span><a href='https://github.com/LLsxyc/mine_motion_prediction.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lei Li, Zhifa Chen, Jian Wang, Bin Zhou, Guizhen Yu, Xiaoxuan Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.18399">Multimodal Trajectory Prediction for Autonomous Driving on Unstructured Roads using Deep Convolutional Network</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, the application of autonomous driving in open-pit mining has garnered increasing attention for achieving safe and efficient mineral transportation. Compared to urban structured roads, unstructured roads in mining sites have uneven boundaries and lack clearly defined lane markings. This leads to a lack of sufficient constraint information for predicting the trajectories of other human-driven vehicles, resulting in higher uncertainty in trajectory prediction problems. A method is proposed to predict multiple possible trajectories and their probabilities of the target vehicle. The surrounding environment and historical trajectories of the target vehicle are encoded as a rasterized image, which is used as input to our deep convolutional network to predict the target vehicle's multiple possible trajectories. The method underwent offline testing on a dataset specifically designed for autonomous driving scenarios in open-pit mining and was compared and evaluated against physics-based method. The open-source code and data are available at https://github.com/LLsxyc/mine_motion_prediction.git
<div id='section'>Paperid: <span id='pid'>294, <a href='https://arxiv.org/pdf/2409.18127.pdf' target='_blank'>https://arxiv.org/pdf/2409.18127.pdf</a></span>   <span><a href='https://hongfz16.github.io/projects/EgoLM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fangzhou Hong, Vladimir Guzov, Hyo Jin Kim, Yuting Ye, Richard Newcombe, Ziwei Liu, Lingni Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.18127">EgoLM: Multi-Modal Language Model of Egocentric Motions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As the prevalence of wearable devices, learning egocentric motions becomes essential to develop contextual AI. In this work, we present EgoLM, a versatile framework that tracks and understands egocentric motions from multi-modal inputs, e.g., egocentric videos and motion sensors. EgoLM exploits rich contexts for the disambiguation of egomotion tracking and understanding, which are ill-posed under single modality conditions. To facilitate the versatile and multi-modal framework, our key insight is to model the joint distribution of egocentric motions and natural languages using large language models (LLM). Multi-modal sensor inputs are encoded and projected to the joint latent space of language models, and used to prompt motion generation or text generation for egomotion tracking or understanding, respectively. Extensive experiments on large-scale multi-modal human motion dataset validate the effectiveness of EgoLM as a generalist model for universal egocentric learning.
<div id='section'>Paperid: <span id='pid'>295, <a href='https://arxiv.org/pdf/2409.16666.pdf' target='_blank'>https://arxiv.org/pdf/2409.16666.pdf</a></span>   <span><a href='https://aggelinacha.github.io/TalkinNeRF/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Aggelina Chatziagapi, Bindita Chaudhuri, Amit Kumar, Rakesh Ranjan, Dimitris Samaras, Nikolaos Sarafianos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.16666">TalkinNeRF: Animatable Neural Fields for Full-Body Talking Humans</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a novel framework that learns a dynamic neural radiance field (NeRF) for full-body talking humans from monocular videos. Prior work represents only the body pose or the face. However, humans communicate with their full body, combining body pose, hand gestures, as well as facial expressions. In this work, we propose TalkinNeRF, a unified NeRF-based network that represents the holistic 4D human motion. Given a monocular video of a subject, we learn corresponding modules for the body, face, and hands, that are combined together to generate the final result. To capture complex finger articulation, we learn an additional deformation field for the hands. Our multi-identity representation enables simultaneous training for multiple subjects, as well as robust animation under completely unseen poses. It can also generalize to novel identities, given only a short video as input. We demonstrate state-of-the-art performance for animating full-body talking humans, with fine-grained hand articulation and facial expressions.
<div id='section'>Paperid: <span id='pid'>296, <a href='https://arxiv.org/pdf/2409.15904.pdf' target='_blank'>https://arxiv.org/pdf/2409.15904.pdf</a></span>   <span><a href='https://coral79.github.io/uni-motion/' target='_blank'>  GitHub</a></span> <span><a href='https://coral79.github.io/uni-motion/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chuqiao Li, Julian Chibane, Yannan He, Naama Pearl, Andreas Geiger, Gerard Pons-moll
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.15904">Unimotion: Unifying 3D Human Motion Synthesis and Understanding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce Unimotion, the first unified multi-task human motion model capable of both flexible motion control and frame-level motion understanding. While existing works control avatar motion with global text conditioning, or with fine-grained per frame scripts, none can do both at once. In addition, none of the existing works can output frame-level text paired with the generated poses. In contrast, Unimotion allows to control motion with global text, or local frame-level text, or both at once, providing more flexible control for users. Importantly, Unimotion is the first model which by design outputs local text paired with the generated poses, allowing users to know what motion happens and when, which is necessary for a wide range of applications. We show Unimotion opens up new applications: 1.) Hierarchical control, allowing users to specify motion at different levels of detail, 2.) Obtaining motion text descriptions for existing MoCap data or YouTube videos 3.) Allowing for editability, generating motion from text, and editing the motion via text edits. Moreover, Unimotion attains state-of-the-art results for the frame-level text-to-motion task on the established HumanML3D dataset. The pre-trained model and code are available available on our project page at https://coral79.github.io/uni-motion/.
<div id='section'>Paperid: <span id='pid'>297, <a href='https://arxiv.org/pdf/2409.14101.pdf' target='_blank'>https://arxiv.org/pdf/2409.14101.pdf</a></span>   <span><a href='https://github.com/CaveSpiderLZJ/PoseAugment-ECCV2024' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhuojun Li, Chun Yu, Chen Liang, Yuanchun Shi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.14101">PoseAugment: Generative Human Pose Data Augmentation with Physical Plausibility for IMU-based Motion Capture</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The data scarcity problem is a crucial factor that hampers the model performance of IMU-based human motion capture. However, effective data augmentation for IMU-based motion capture is challenging, since it has to capture the physical relations and constraints of the human body, while maintaining the data distribution and quality. We propose PoseAugment, a novel pipeline incorporating VAE-based pose generation and physical optimization. Given a pose sequence, the VAE module generates infinite poses with both high fidelity and diversity, while keeping the data distribution. The physical module optimizes poses to satisfy physical constraints with minimal motion restrictions. High-quality IMU data are then synthesized from the augmented poses for training motion capture models. Experiments show that PoseAugment outperforms previous data augmentation and pose generation methods in terms of motion capture accuracy, revealing a strong potential of our method to alleviate the data collection burden for IMU-based motion capture and related tasks driven by human poses.
<div id='section'>Paperid: <span id='pid'>298, <a href='https://arxiv.org/pdf/2409.12202.pdf' target='_blank'>https://arxiv.org/pdf/2409.12202.pdf</a></span>   <span><a href='https://github.com/HanLingsgjk/CSCV' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Han Ling, Yinghui Sun, Quansen Sun, Yuhui Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.12202">ScaleFlow++: Robust and Accurate Estimation of 3D Motion from Video</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Perceiving and understanding 3D motion is a core technology in fields such as autonomous driving, robots, and motion prediction. This paper proposes a 3D motion perception method called ScaleFlow++ that is easy to generalize. With just a pair of RGB images, ScaleFlow++ can robustly estimate optical flow and motion-in-depth (MID). Most existing methods directly regress MID from two RGB frames or optical flow, resulting in inaccurate and unstable results. Our key insight is cross-scale matching, which extracts deep motion clues by matching objects in pairs of images at different scales. Unlike previous methods, ScaleFlow++ integrates optical flow and MID estimation into a unified architecture, estimating optical flow and MID end-to-end based on feature matching. Moreover, we also proposed modules such as global initialization network, global iterative optimizer, and hybrid training pipeline to integrate global motion information, reduce the number of iterations, and prevent overfitting during training. On KITTI, ScaleFlow++ achieved the best monocular scene flow estimation performance, reducing SF-all from 6.21 to 5.79. The evaluation of MID even surpasses RGBD-based methods. In addition, ScaleFlow++ has achieved stunning zero-shot generalization performance in both rigid and nonrigid scenes. Code is available at \url{https://github.com/HanLingsgjk/CSCV}.
<div id='section'>Paperid: <span id='pid'>299, <a href='https://arxiv.org/pdf/2409.12189.pdf' target='_blank'>https://arxiv.org/pdf/2409.12189.pdf</a></span>   <span><a href='https://github.com/felixbmuller/SAST' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Felix B Mueller, Julian Tanke, Juergen Gall
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.12189">Massively Multi-Person 3D Human Motion Forecasting with Scene Context</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Forecasting long-term 3D human motion is challenging: the stochasticity of human behavior makes it hard to generate realistic human motion from the input sequence alone. Information on the scene environment and the motion of nearby people can greatly aid the generation process. We propose a scene-aware social transformer model (SAST) to forecast long-term (10s) human motion motion. Unlike previous models, our approach can model interactions between both widely varying numbers of people and objects in a scene. We combine a temporal convolutional encoder-decoder architecture with a Transformer-based bottleneck that allows us to efficiently combine motion and scene information. We model the conditional motion distribution using denoising diffusion models. We benchmark our approach on the Humans in Kitchens dataset, which contains 1 to 16 persons and 29 to 50 objects that are visible simultaneously. Our model outperforms other approaches in terms of realism and diversity on different metrics and in a user study. Code is available at https://github.com/felixbmuller/SAST.
<div id='section'>Paperid: <span id='pid'>300, <a href='https://arxiv.org/pdf/2409.11676.pdf' target='_blank'>https://arxiv.org/pdf/2409.11676.pdf</a></span>   <span><a href='https://github.com/keshuw95/RHINO-Hypergraph-Motion-Generation' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Keshu Wu, Yang Zhou, Haotian Shi, Dominique Lord, Bin Ran, Xinyue Ye
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.11676">Hypergraph-based Motion Generation with Multi-modal Interaction Relational Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The intricate nature of real-world driving environments, characterized by dynamic and diverse interactions among multiple vehicles and their possible future states, presents considerable challenges in accurately predicting the motion states of vehicles and handling the uncertainty inherent in the predictions. Addressing these challenges requires comprehensive modeling and reasoning to capture the implicit relations among vehicles and the corresponding diverse behaviors. This research introduces an integrated framework for autonomous vehicles (AVs) motion prediction to address these complexities, utilizing a novel Relational Hypergraph Interaction-informed Neural mOtion generator (RHINO). RHINO leverages hypergraph-based relational reasoning by integrating a multi-scale hypergraph neural network to model group-wise interactions among multiple vehicles and their multi-modal driving behaviors, thereby enhancing motion prediction accuracy and reliability. Experimental validation using real-world datasets demonstrates the superior performance of this framework in improving predictive accuracy and fostering socially aware automated driving in dynamic traffic scenarios. The source code is publicly available at https://github.com/keshuw95/RHINO-Hypergraph-Motion-Generation.
<div id='section'>Paperid: <span id='pid'>301, <a href='https://arxiv.org/pdf/2409.11172.pdf' target='_blank'>https://arxiv.org/pdf/2409.11172.pdf</a></span>   <span><a href='https://github.com/valeoai/MF_aWTA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yihong Xu, Victor Letzelter, MickaÃ«l Chen, Ãloi Zablocki, Matthieu Cord
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.11172">Annealed Winner-Takes-All for Motion Forecasting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In autonomous driving, motion prediction aims at forecasting the future trajectories of nearby agents, helping the ego vehicle to anticipate behaviors and drive safely. A key challenge is generating a diverse set of future predictions, commonly addressed using data-driven models with Multiple Choice Learning (MCL) architectures and Winner-Takes-All (WTA) training objectives. However, these methods face initialization sensitivity and training instabilities. Additionally, to compensate for limited performance, some approaches rely on training with a large set of hypotheses, requiring a post-selection step during inference to significantly reduce the number of predictions. To tackle these issues, we take inspiration from annealed MCL, a recently introduced technique that improves the convergence properties of MCL methods through an annealed Winner-Takes-All loss (aWTA). In this paper, we demonstrate how the aWTA loss can be integrated with state-of-the-art motion forecasting models to enhance their performance using only a minimal set of hypotheses, eliminating the need for the cumbersome post-selection step. Our approach can be easily incorporated into any trajectory prediction model normally trained using WTA and yields significant improvements. To facilitate the application of our approach to future motion forecasting models, the code is made publicly available: https://github.com/valeoai/MF_aWTA.
<div id='section'>Paperid: <span id='pid'>302, <a href='https://arxiv.org/pdf/2409.10847.pdf' target='_blank'>https://arxiv.org/pdf/2409.10847.pdf</a></span>   <span><a href='https://github.com/RohollahHS/BAD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>S. Rohollah Hosseyni, Ali Ahmad Rahmani, S. Jamal Seyedmohammadi, Sanaz Seyedin, Arash Mohammadi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.10847">BAD: Bidirectional Auto-regressive Diffusion for Text-to-Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autoregressive models excel in modeling sequential dependencies by enforcing causal constraints, yet they struggle to capture complex bidirectional patterns due to their unidirectional nature. In contrast, mask-based models leverage bidirectional context, enabling richer dependency modeling. However, they often assume token independence during prediction, which undermines the modeling of sequential dependencies. Additionally, the corruption of sequences through masking or absorption can introduce unnatural distortions, complicating the learning process. To address these issues, we propose Bidirectional Autoregressive Diffusion (BAD), a novel approach that unifies the strengths of autoregressive and mask-based generative models. BAD utilizes a permutation-based corruption technique that preserves the natural sequence structure while enforcing causal dependencies through randomized ordering, enabling the effective capture of both sequential and bidirectional relationships. Comprehensive experiments show that BAD outperforms autoregressive and mask-based models in text-to-motion generation, suggesting a novel pre-training strategy for sequence modeling. The codebase for BAD is available on https://github.com/RohollahHS/BAD.
<div id='section'>Paperid: <span id='pid'>303, <a href='https://arxiv.org/pdf/2409.10308.pdf' target='_blank'>https://arxiv.org/pdf/2409.10308.pdf</a></span>   <span><a href='https://evm7.github.io/Self-AWare' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Esteve Valls Mascaro, Dongheui Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.10308">Know your limits! Optimize the robot's behavior through self-awareness</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As humanoid robots transition from labs to real-world environments, it is essential to democratize robot control for non-expert users. Recent human-robot imitation algorithms focus on following a reference human motion with high precision, but they are susceptible to the quality of the reference motion and require the human operator to simplify its movements to match the robot's capabilities. Instead, we consider that the robot should understand and adapt the reference motion to its own abilities, facilitating the operator's task. For that, we introduce a deep-learning model that anticipates the robot's performance when imitating a given reference. Then, our system can generate multiple references given a high-level task command, assign a score to each of them, and select the best reference to achieve the desired robot behavior. Our Self-AWare model (SAW) ranks potential robot behaviors based on various criteria, such as fall likelihood, adherence to the reference motion, and smoothness. We integrate advanced motion generation, robot control, and SAW in one unique system, ensuring optimal robot behavior for any task command. For instance, SAW can anticipate falls with 99.29% accuracy. For more information check our project page: https://evm7.github.io/Self-AWare
<div id='section'>Paperid: <span id='pid'>304, <a href='https://arxiv.org/pdf/2409.09177.pdf' target='_blank'>https://arxiv.org/pdf/2409.09177.pdf</a></span>   <span><a href='https://github.com/rd20karim/Synch-Transformer' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Karim Radouane, Sylvie Ranwez, Julien Lagarde, Andon Tchechmedjiev
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.09177">Transformer with Controlled Attention for Synchronous Motion Captioning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we address a challenging task, synchronous motion captioning, that aim to generate a language description synchronized with human motion sequences. This task pertains to numerous applications, such as aligned sign language transcription, unsupervised action segmentation and temporal grounding. Our method introduces mechanisms to control self- and cross-attention distributions of the Transformer, allowing interpretability and time-aligned text generation. We achieve this through masking strategies and structuring losses that push the model to maximize attention only on the most important frames contributing to the generation of a motion word. These constraints aim to prevent undesired mixing of information in attention maps and to provide a monotonic attention distribution across tokens. Thus, the cross attentions of tokens are used for progressive text generation in synchronization with human motion sequences. We demonstrate the superior performance of our approach through evaluation on the two available benchmark datasets, KIT-ML and HumanML3D. As visual evaluation is essential for this task, we provide a comprehensive set of animated visual illustrations in the code repository: https://github.com/rd20karim/Synch-Transformer.
<div id='section'>Paperid: <span id='pid'>305, <a href='https://arxiv.org/pdf/2409.08353.pdf' target='_blank'>https://arxiv.org/pdf/2409.08353.pdf</a></span>   <span><a href='https://nowheretrix.github.io/DualGS/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuheng Jiang, Zhehao Shen, Yu Hong, Chengcheng Guo, Yize Wu, Yingliang Zhang, Jingyi Yu, Lan Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.08353">Robust Dual Gaussian Splatting for Immersive Human-centric Volumetric Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Volumetric video represents a transformative advancement in visual media, enabling users to freely navigate immersive virtual experiences and narrowing the gap between digital and real worlds. However, the need for extensive manual intervention to stabilize mesh sequences and the generation of excessively large assets in existing workflows impedes broader adoption. In this paper, we present a novel Gaussian-based approach, dubbed \textit{DualGS}, for real-time and high-fidelity playback of complex human performance with excellent compression ratios. Our key idea in DualGS is to separately represent motion and appearance using the corresponding skin and joint Gaussians. Such an explicit disentanglement can significantly reduce motion redundancy and enhance temporal coherence. We begin by initializing the DualGS and anchoring skin Gaussians to joint Gaussians at the first frame. Subsequently, we employ a coarse-to-fine training strategy for frame-by-frame human performance modeling. It includes a coarse alignment phase for overall motion prediction as well as a fine-grained optimization for robust tracking and high-fidelity rendering. To integrate volumetric video seamlessly into VR environments, we efficiently compress motion using entropy encoding and appearance using codec compression coupled with a persistent codebook. Our approach achieves a compression ratio of up to 120 times, only requiring approximately 350KB of storage per frame. We demonstrate the efficacy of our representation through photo-realistic, free-view experiences on VR headsets, enabling users to immersively watch musicians in performance and feel the rhythm of the notes at the performers' fingertips.
<div id='section'>Paperid: <span id='pid'>306, <a href='https://arxiv.org/pdf/2409.07341.pdf' target='_blank'>https://arxiv.org/pdf/2409.07341.pdf</a></span>   <span><a href='https://rlodm.github.io/odm/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Luo Ji, Runji Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.07341">Online Decision MetaMorphFormer: A Casual Transformer-Based Reinforcement Learning Framework of Universal Embodied Intelligence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Interactive artificial intelligence in the motion control field is an interesting topic, especially when universal knowledge is adaptive to multiple tasks and universal environments. Despite there being increasing efforts in the field of Reinforcement Learning (RL) with the aid of transformers, most of them might be limited by the offline training pipeline, which prohibits exploration and generalization abilities. To address this limitation, we propose the framework of Online Decision MetaMorphFormer (ODM) which aims to achieve self-awareness, environment recognition, and action planning through a unified model architecture. Motivated by cognitive and behavioral psychology, an ODM agent is able to learn from others, recognize the world, and practice itself based on its own experience. ODM can also be applied to any arbitrary agent with a multi-joint body, located in different environments, and trained with different types of tasks using large-scale pre-trained datasets. Through the use of pre-trained datasets, ODM can quickly warm up and learn the necessary knowledge to perform the desired task, while the target environment continues to reinforce the universal policy. Extensive online experiments as well as few-shot and zero-shot environmental tests are used to verify ODM's performance and generalization ability. The results of our study contribute to the study of general artificial intelligence in embodied and cognitive fields. Code, results, and video examples can be found on the website \url{https://rlodm.github.io/odm/}.
<div id='section'>Paperid: <span id='pid'>307, <a href='https://arxiv.org/pdf/2409.06662.pdf' target='_blank'>https://arxiv.org/pdf/2409.06662.pdf</a></span>   <span><a href='https://zju3dv.github.io/gvhmr/' target='_blank'>  GitHub</a></span> <span><a href='https://zju3dv.github.io/gvhmr/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zehong Shen, Huaijin Pi, Yan Xia, Zhi Cen, Sida Peng, Zechen Hu, Hujun Bao, Ruizhen Hu, Xiaowei Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.06662">World-Grounded Human Motion Recovery via Gravity-View Coordinates</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a novel method for recovering world-grounded human motion from monocular video. The main challenge lies in the ambiguity of defining the world coordinate system, which varies between sequences. Previous approaches attempt to alleviate this issue by predicting relative motion in an autoregressive manner, but are prone to accumulating errors. Instead, we propose estimating human poses in a novel Gravity-View (GV) coordinate system, which is defined by the world gravity and the camera view direction. The proposed GV system is naturally gravity-aligned and uniquely defined for each video frame, largely reducing the ambiguity of learning image-pose mapping. The estimated poses can be transformed back to the world coordinate system using camera rotations, forming a global motion sequence. Additionally, the per-frame estimation avoids error accumulation in the autoregressive methods. Experiments on in-the-wild benchmarks demonstrate that our method recovers more realistic motion in both the camera space and world-grounded settings, outperforming state-of-the-art methods in both accuracy and speed. The code is available at https://zju3dv.github.io/gvhmr/.
<div id='section'>Paperid: <span id='pid'>308, <a href='https://arxiv.org/pdf/2409.06189.pdf' target='_blank'>https://arxiv.org/pdf/2409.06189.pdf</a></span>   <span><a href='https://metadrivescape.github.io/papers_project/MyGo/page.html' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yining Yao, Xi Guo, Chenjing Ding, Wei Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.06189">MyGo: Consistent and Controllable Multi-View Driving Video Generation with Camera Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>High-quality driving video generation is crucial for providing training data for autonomous driving models. However, current generative models rarely focus on enhancing camera motion control under multi-view tasks, which is essential for driving video generation. Therefore, we propose MyGo, an end-to-end framework for video generation, introducing motion of onboard cameras as conditions to make progress in camera controllability and multi-view consistency. MyGo employs additional plug-in modules to inject camera parameters into the pre-trained video diffusion model, which retains the extensive knowledge of the pre-trained model as much as possible. Furthermore, we use epipolar constraints and neighbor view information during the generation process of each view to enhance spatial-temporal consistency. Experimental results show that MyGo has achieved state-of-the-art results in both general camera-controlled video generation and multi-view driving video generation tasks, which lays the foundation for more accurate environment simulation in autonomous driving. Project page: https://metadrivescape.github.io/papers_project/MyGo/page.html
<div id='section'>Paperid: <span id='pid'>309, <a href='https://arxiv.org/pdf/2409.04440.pdf' target='_blank'>https://arxiv.org/pdf/2409.04440.pdf</a></span>   <span><a href='https://von31.github.io/synNsync' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Vongani Maluleke, Lea MÃ¼ller, Jathushan Rajasegaran, Georgios Pavlakos, Shiry Ginosar, Angjoo Kanazawa, Jitendra Malik
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.04440">Synergy and Synchrony in Couple Dances</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper asks to what extent social interaction influences one's behavior. We study this in the setting of two dancers dancing as a couple. We first consider a baseline in which we predict a dancer's future moves conditioned only on their past motion without regard to their partner. We then investigate the advantage of taking social information into account by conditioning also on the motion of their dancing partner. We focus our analysis on Swing, a dance genre with tight physical coupling for which we present an in-the-wild video dataset. We demonstrate that single-person future motion prediction in this context is challenging. Instead, we observe that prediction greatly benefits from considering the interaction partners' behavior, resulting in surprisingly compelling couple dance synthesis results (see supp. video). Our contributions are a demonstration of the advantages of socially conditioned future motion prediction and an in-the-wild, couple dance video dataset to enable future research in this direction. Video results are available on the project website: https://von31.github.io/synNsync
<div id='section'>Paperid: <span id='pid'>310, <a href='https://arxiv.org/pdf/2409.03944.pdf' target='_blank'>https://arxiv.org/pdf/2409.03944.pdf</a></span>   <span><a href='https://CarstenEpic.github.io/humos/' target='_blank'>  GitHub</a></span> <span><a href='https://CarstenEpic.github.io/humos/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shashank Tripathi, Omid Taheri, Christoph Lassner, Michael J. Black, Daniel Holden, Carsten Stoll
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.03944">HUMOS: Human Motion Model Conditioned on Body Shape</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating realistic human motion is essential for many computer vision and graphics applications. The wide variety of human body shapes and sizes greatly impacts how people move. However, most existing motion models ignore these differences, relying on a standardized, average body. This leads to uniform motion across different body types, where movements don't match their physical characteristics, limiting diversity. To solve this, we introduce a new approach to develop a generative motion model based on body shape. We show that it's possible to train this model using unpaired data by applying cycle consistency, intuitive physics, and stability constraints, which capture the relationship between identity and movement. The resulting model generates diverse, physically plausible, and dynamically stable human motions that are both quantitatively and qualitatively more realistic than current state-of-the-art methods. More details are available on our project page https://CarstenEpic.github.io/humos/.
<div id='section'>Paperid: <span id='pid'>311, <a href='https://arxiv.org/pdf/2409.02638.pdf' target='_blank'>https://arxiv.org/pdf/2409.02638.pdf</a></span>   <span><a href='https://irmvlab.github.io/madiff.github.io' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Junyi Ma, Xieyuanli Chen, Wentao Bao, Jingyi Xu, Hesheng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.02638">MADiff: Motion-Aware Mamba Diffusion Models for Hand Trajectory Prediction on Egocentric Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding human intentions and actions through egocentric videos is important on the path to embodied artificial intelligence. As a branch of egocentric vision techniques, hand trajectory prediction plays a vital role in comprehending human motion patterns, benefiting downstream tasks in extended reality and robot manipulation. However, capturing high-level human intentions consistent with reasonable temporal causality is challenging when only egocentric videos are available. This difficulty is exacerbated under camera egomotion interference and the absence of affordance labels to explicitly guide the optimization of hand waypoint distribution. In this work, we propose a novel hand trajectory prediction method dubbed MADiff, which forecasts future hand waypoints with diffusion models. The devised denoising operation in the latent space is achieved by our proposed motion-aware Mamba, where the camera wearer's egomotion is integrated to achieve motion-driven selective scan (MDSS). To discern the relationship between hands and scenarios without explicit affordance supervision, we leverage a foundation model that fuses visual and language features to capture high-level semantics from video clips. Comprehensive experiments conducted on five public datasets with the existing and our proposed new evaluation metrics demonstrate that MADiff predicts comparably reasonable hand trajectories compared to the state-of-the-art baselines, and achieves real-time performance. We will release our code and pretrained models of MADiff at the project page: https://irmvlab.github.io/madiff.github.io.
<div id='section'>Paperid: <span id='pid'>312, <a href='https://arxiv.org/pdf/2409.01522.pdf' target='_blank'>https://arxiv.org/pdf/2409.01522.pdf</a></span>   <span><a href='https://plyfager.github.io/LaMoG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifei Yang, Zikai Huang, Chenshu Xu, Shengfeng He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.01522">Lagrangian Motion Fields for Long-term Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Long-term motion generation is a challenging task that requires producing coherent and realistic sequences over extended durations. Current methods primarily rely on framewise motion representations, which capture only static spatial details and overlook temporal dynamics. This approach leads to significant redundancy across the temporal dimension, complicating the generation of effective long-term motion. To overcome these limitations, we introduce the novel concept of Lagrangian Motion Fields, specifically designed for long-term motion generation. By treating each joint as a Lagrangian particle with uniform velocity over short intervals, our approach condenses motion representations into a series of "supermotions" (analogous to superpixels). This method seamlessly integrates static spatial information with interpretable temporal dynamics, transcending the limitations of existing network architectures and motion sequence content types. Our solution is versatile and lightweight, eliminating the need for neural network preprocessing. Our approach excels in tasks such as long-term music-to-dance generation and text-to-motion generation, offering enhanced efficiency, superior generation quality, and greater diversity compared to existing methods. Additionally, the adaptability of Lagrangian Motion Fields extends to applications like infinite motion looping and fine-grained controlled motion generation, highlighting its broad utility. Video demonstrations are available at https://plyfager.github.io/LaMoG.
<div id='section'>Paperid: <span id='pid'>313, <a href='https://arxiv.org/pdf/2409.00774.pdf' target='_blank'>https://arxiv.org/pdf/2409.00774.pdf</a></span>   <span><a href='https://github.com/intelligolabs/SITUATE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Luigi Capogrosso, Andrea Toaiari, Andrea Avogaro, Uzair Khan, Aditya Jivoji, Franco Fummi, Marco Cristani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.00774">SITUATE: Indoor Human Trajectory Prediction through Geometric Features and Self-Supervised Vision Representation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Patterns of human motion in outdoor and indoor environments are substantially different due to the scope of the environment and the typical intentions of people therein. While outdoor trajectory forecasting has received significant attention, indoor forecasting is still an underexplored research area. This paper proposes SITUATE, a novel approach to cope with indoor human trajectory prediction by leveraging equivariant and invariant geometric features and a self-supervised vision representation. The geometric learning modules model the intrinsic symmetries and human movements inherent in indoor spaces. This concept becomes particularly important because self-loops at various scales and rapid direction changes often characterize indoor trajectories. On the other hand, the vision representation module is used to acquire spatial-semantic information about the environment to predict users' future locations more accurately. We evaluate our method through comprehensive experiments on the two most famous indoor trajectory forecasting datasets, i.e., THÃR and Supermarket, obtaining state-of-the-art performance. Furthermore, we also achieve competitive results in outdoor scenarios, showing that indoor-oriented forecasting models generalize better than outdoor-oriented ones. The source code is available at https://github.com/intelligolabs/SITUATE.
<div id='section'>Paperid: <span id='pid'>314, <a href='https://arxiv.org/pdf/2409.00487.pdf' target='_blank'>https://arxiv.org/pdf/2409.00487.pdf</a></span>   <span><a href='https://github.com/Xavier-Lin/TrackSSM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bin Hu, Run Luo, Zelin Liu, Cheng Wang, Wenyu Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.00487">TrackSSM: A General Motion Predictor by State-Space Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Temporal motion modeling has always been a key component in multiple object tracking (MOT) which can ensure smooth trajectory movement and provide accurate positional information to enhance association precision. However, current motion models struggle to be both efficient and effective across different application scenarios. To this end, we propose TrackSSM inspired by the recently popular state space models (SSM), a unified encoder-decoder motion framework that uses data-dependent state space model to perform temporal motion of trajectories. Specifically, we propose Flow-SSM, a module that utilizes the position and motion information from historical trajectories to guide the temporal state transition of object bounding boxes. Based on Flow-SSM, we design a flow decoder. It is composed of a cascaded motion decoding module employing Flow-SSM, which can use the encoded flow information to complete the temporal position prediction of trajectories. Additionally, we propose a Step-by-Step Linear (S$^2$L) training strategy. By performing linear interpolation between the positions of the object in the previous frame and the current frame, we construct the pseudo labels of step-by-step linear training, ensuring that the trajectory flow information can better guide the object bounding box in completing temporal transitions. TrackSSM utilizes a simple Mamba-Block to build a motion encoder for historical trajectories, forming a temporal motion model with an encoder-decoder structure in conjunction with the flow decoder. TrackSSM is applicable to various tracking scenarios and achieves excellent tracking performance across multiple benchmarks, further extending the potential of SSM-like temporal motion models in multi-object tracking tasks. Code and models are publicly available at \url{https://github.com/Xavier-Lin/TrackSSM}.
<div id='section'>Paperid: <span id='pid'>315, <a href='https://arxiv.org/pdf/2409.00343.pdf' target='_blank'>https://arxiv.org/pdf/2409.00343.pdf</a></span>   <span><a href='https://handiyin.github.io/EgoHDM/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bonan Liu, Handi Yin, Manuel Kaufmann, Jinhao He, Sammy Christen, Jie Song, Pan Hui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.00343">EgoHDM: An Online Egocentric-Inertial Human Motion Capture, Localization, and Dense Mapping System</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present EgoHDM, an online egocentric-inertial human motion capture (mocap), localization, and dense mapping system. Our system uses 6 inertial measurement units (IMUs) and a commodity head-mounted RGB camera. EgoHDM is the first human mocap system that offers dense scene mapping in near real-time. Further, it is fast and robust to initialize and fully closes the loop between physically plausible map-aware global human motion estimation and mocap-aware 3D scene reconstruction. Our key idea is integrating camera localization and mapping information with inertial human motion capture bidirectionally in our system. To achieve this, we design a tightly coupled mocap-aware dense bundle adjustment and physics-based body pose correction module leveraging a local body-centric elevation map. The latter introduces a novel terrain-aware contact PD controller, which enables characters to physically contact the given local elevation map thereby reducing human floating or penetration. We demonstrate the performance of our system on established synthetic and real-world benchmarks. The results show that our method reduces human localization, camera pose, and mapping accuracy error by 41%, 71%, 46%, respectively, compared to the state of the art. Our qualitative evaluations on newly captured data further demonstrate that EgoHDM can cover challenging scenarios in non-flat terrain including stepping over stairs and outdoor scenes in the wild.
<div id='section'>Paperid: <span id='pid'>316, <a href='https://arxiv.org/pdf/2408.17135.pdf' target='_blank'>https://arxiv.org/pdf/2408.17135.pdf</a></span>   <span><a href='https://aigc-explorer.github.io/TIMotion-page/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yabiao Wang, Shuo Wang, Jiangning Zhang, Ke Fan, Jiafu Wu, Zhucun Xue, Yong Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.17135">TIMotion: Temporal and Interactive Framework for Efficient Human-Human Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human-human motion generation is essential for understanding humans as social beings. Current methods fall into two main categories: single-person-based methods and separate modeling-based methods. To delve into this field, we abstract the overall generation process into a general framework MetaMotion, which consists of two phases: temporal modeling and interaction mixing. For temporal modeling, the single-person-based methods concatenate two people into a single one directly, while the separate modeling-based methods skip the modeling of interaction sequences. The inadequate modeling described above resulted in sub-optimal performance and redundant model parameters. In this paper, we introduce TIMotion (Temporal and Interactive Modeling), an efficient and effective framework for human-human motion generation. Specifically, we first propose Causal Interactive Injection to model two separate sequences as a causal sequence leveraging the temporal and causal properties. Then we present Role-Evolving Scanning to adjust to the change in the active and passive roles throughout the interaction. Finally, to generate smoother and more rational motion, we design Localized Pattern Amplification to capture short-term motion patterns. Extensive experiments on InterHuman and InterX demonstrate that our method achieves superior performance. Project page: https://aigc-explorer.github.io/TIMotion-page/
<div id='section'>Paperid: <span id='pid'>317, <a href='https://arxiv.org/pdf/2408.12885.pdf' target='_blank'>https://arxiv.org/pdf/2408.12885.pdf</a></span>   <span><a href='https://github.com/Gloria2tt/T3M.git' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/Gloria2tt/T3M.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenshuo Peng, Kaipeng Zhang, Sai Qian Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.12885">T3M: Text Guided 3D Human Motion Synthesis from Speech</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Speech-driven 3D motion synthesis seeks to create lifelike animations based on human speech, with potential uses in virtual reality, gaming, and the film production. Existing approaches reply solely on speech audio for motion generation, leading to inaccurate and inflexible synthesis results. To mitigate this problem, we introduce a novel text-guided 3D human motion synthesis method, termed \textit{T3M}. Unlike traditional approaches, T3M allows precise control over motion synthesis via textual input, enhancing the degree of diversity and user customization. The experiment results demonstrate that T3M can greatly outperform the state-of-the-art methods in both quantitative metrics and qualitative evaluations. We have publicly released our code at \href{https://github.com/Gloria2tt/T3M.git}{https://github.com/Gloria2tt/T3M.git}
<div id='section'>Paperid: <span id='pid'>318, <a href='https://arxiv.org/pdf/2408.11801.pdf' target='_blank'>https://arxiv.org/pdf/2408.11801.pdf</a></span>   <span><a href='https://yuzhou914.github.io/Story3D-Agent/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuzhou Huang, Yiran Qin, Shunlin Lu, Xintao Wang, Rui Huang, Ying Shan, Ruimao Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.11801">Story3D-Agent: Exploring 3D Storytelling Visualization with Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Traditional visual storytelling is complex, requiring specialized knowledge and substantial resources, yet often constrained by human creativity and creation precision. While Large Language Models (LLMs) enhance visual storytelling, current approaches often limit themselves to 2D visuals or oversimplify stories through motion synthesis and behavioral simulation, failing to create comprehensive, multi-dimensional narratives. To this end, we present Story3D-Agent, a pioneering approach that leverages the capabilities of LLMs to transform provided narratives into 3D-rendered visualizations. By integrating procedural modeling, our approach enables precise control over multi-character actions and motions, as well as diverse decorative elements, ensuring the long-range and dynamic 3D representation. Furthermore, our method supports narrative extension through logical reasoning, ensuring that generated content remains consistent with existing conditions. We have thoroughly evaluated our Story3D-Agent to validate its effectiveness, offering a basic framework to advance 3D story representation.
<div id='section'>Paperid: <span id='pid'>319, <a href='https://arxiv.org/pdf/2408.09397.pdf' target='_blank'>https://arxiv.org/pdf/2408.09397.pdf</a></span>   <span><a href='https://xc-csc101.github.io/combo/' target='_blank'>  GitHub</a></span> <span><a href='https://xc-csc101.github.io/combo/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chao Xu, Mingze Sun, Zhi-Qi Cheng, Fei Wang, Yang Liu, Baigui Sun, Ruqi Huang, Alexander Hauptmann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.09397">Combo: Co-speech holistic 3D human motion generation and efficient customizable adaptation in harmony</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose a novel framework, Combo, for harmonious co-speech holistic 3D human motion generation and efficient customizable adaption. In particular, we identify that one fundamental challenge as the multiple-input-multiple-output (MIMO) nature of the generative model of interest. More concretely, on the input end, the model typically consumes both speech signals and character guidance (e.g., identity and emotion), which not only poses challenge on learning capacity but also hinders further adaptation to varying guidance; on the output end, holistic human motions mainly consist of facial expressions and body movements, which are inherently correlated but non-trivial to coordinate in current data-driven generation process. In response to the above challenge, we propose tailored designs to both ends. For the former, we propose to pre-train on data regarding a fixed identity with neutral emotion, and defer the incorporation of customizable conditions (identity and emotion) to fine-tuning stage, which is boosted by our novel X-Adapter for parameter-efficient fine-tuning. For the latter, we propose a simple yet effective transformer design, DU-Trans, which first divides into two branches to learn individual features of face expression and body movements, and then unites those to learn a joint bi-directional distribution and directly predicts combined coefficients. Evaluated on BEAT2 and SHOW datasets, Combo is highly effective in generating high-quality motions but also efficient in transferring identity and emotion. Project website: \href{https://xc-csc101.github.io/combo/}{Combo}.
<div id='section'>Paperid: <span id='pid'>320, <a href='https://arxiv.org/pdf/2408.07295.pdf' target='_blank'>https://arxiv.org/pdf/2408.07295.pdf</a></span>   <span><a href='https://masked-humanoid.github.io/mhc/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Pranay Dugar, Aayam Shrestha, Fangzhou Yu, Bart van Marum, Alan Fern
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.07295">Learning Multi-Modal Whole-Body Control for Real-World Humanoid Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The foundational capabilities of humanoid robots should include robustly standing, walking, and mimicry of whole and partial-body motions. This work introduces the Masked Humanoid Controller (MHC), which supports all of these capabilities by tracking target trajectories over selected subsets of humanoid state variables while ensuring balance and robustness against disturbances. The MHC is trained in simulation using a carefully designed curriculum that imitates partially masked motions from a library of behaviors spanning standing, walking, optimized reference trajectories, re-targeted video clips, and human motion capture data. It also allows for combining joystick-based control with partial-body motion mimicry. We showcase simulation experiments validating the MHC's ability to execute a wide variety of behaviors from partially-specified target motions. Moreover, we demonstrate sim-to-real transfer on the real-world Digit V3 humanoid robot. To our knowledge, this is the first instance of a learned controller that can realize whole-body control of a real-world humanoid for such diverse multi-modal targets.
<div id='section'>Paperid: <span id='pid'>321, <a href='https://arxiv.org/pdf/2408.03499.pdf' target='_blank'>https://arxiv.org/pdf/2408.03499.pdf</a></span>   <span><a href='https://github.com/volatileee/FacialPulse' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruiqi Wang, Jinyang Huang, Jie Zhang, Xin Liu, Xiang Zhang, Zhi Liu, Peng Zhao, Sigui Chen, Xiao Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.03499">FacialPulse: An Efficient RNN-based Depression Detection via Temporal Facial Landmarks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Depression is a prevalent mental health disorder that significantly impacts individuals' lives and well-being. Early detection and intervention are crucial for effective treatment and management of depression. Recently, there are many end-to-end deep learning methods leveraging the facial expression features for automatic depression detection. However, most current methods overlook the temporal dynamics of facial expressions. Although very recent 3DCNN methods remedy this gap, they introduce more computational cost due to the selection of CNN-based backbones and redundant facial features.
  To address the above limitations, by considering the timing correlation of facial expressions, we propose a novel framework called FacialPulse, which recognizes depression with high accuracy and speed. By harnessing the bidirectional nature and proficiently addressing long-term dependencies, the Facial Motion Modeling Module (FMMM) is designed in FacialPulse to fully capture temporal features. Since the proposed FMMM has parallel processing capabilities and has the gate mechanism to mitigate gradient vanishing, this module can also significantly boost the training speed.
  Besides, to effectively use facial landmarks to replace original images to decrease information redundancy, a Facial Landmark Calibration Module (FLCM) is designed to eliminate facial landmark errors to further improve recognition accuracy. Extensive experiments on the AVEC2014 dataset and MMDA dataset (a depression dataset) demonstrate the superiority of FacialPulse on recognition accuracy and speed, with the average MAE (Mean Absolute Error) decreased by 21% compared to baselines, and the recognition speed increased by 100% compared to state-of-the-art methods. Codes are released at https://github.com/volatileee/FacialPulse.
<div id='section'>Paperid: <span id='pid'>322, <a href='https://arxiv.org/pdf/2408.02110.pdf' target='_blank'>https://arxiv.org/pdf/2408.02110.pdf</a></span>   <span><a href='https://eth-ait.github.io/AvatarPose/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Feichi Lu, Zijian Dong, Jie Song, Otmar Hilliges
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.02110">AvatarPose: Avatar-guided 3D Pose Estimation of Close Human Interaction from Sparse Multi-view Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite progress in human motion capture, existing multi-view methods often face challenges in estimating the 3D pose and shape of multiple closely interacting people. This difficulty arises from reliance on accurate 2D joint estimations, which are hard to obtain due to occlusions and body contact when people are in close interaction. To address this, we propose a novel method leveraging the personalized implicit neural avatar of each individual as a prior, which significantly improves the robustness and precision of this challenging pose estimation task. Concretely, the avatars are efficiently reconstructed via layered volume rendering from sparse multi-view videos. The reconstructed avatar prior allows for the direct optimization of 3D poses based on color and silhouette rendering loss, bypassing the issues associated with noisy 2D detections. To handle interpenetration, we propose a collision loss on the overlapping shape regions of avatars to add penetration constraints. Moreover, both 3D poses and avatars are optimized in an alternating manner. Our experimental results demonstrate state-of-the-art performance on several public datasets.
<div id='section'>Paperid: <span id='pid'>323, <a href='https://arxiv.org/pdf/2408.02091.pdf' target='_blank'>https://arxiv.org/pdf/2408.02091.pdf</a></span>   <span><a href='https://github.com/JunyuShi02/PMG-MRL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Junyu Shi, Baoxuan Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.02091">Past Movements-Guided Motion Representation Learning for Human Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion prediction based on 3D skeleton is a significant challenge in computer vision, primarily focusing on the effective representation of motion. In this paper, we propose a self-supervised learning framework designed to enhance motion representation. This framework consists of two stages: first, the network is pretrained through the self-reconstruction of past sequences, and the guided reconstruction of future sequences based on past movements. We design a velocity-based mask strategy to focus on the joints with large-scale moving. Subsequently, the pretrained network undergoes finetuning for specific tasks. Self-reconstruction, guided by patterns of past motion, substantially improves the model's ability to represent the spatiotemporal relationships among joints but also captures the latent relationships between past and future sequences. This capability is crucial for motion prediction tasks that solely depend on historical motion data. By employing this straightforward yet effective training paradigm, our method outperforms existing \textit{state-of-the-art} methods, reducing the average prediction errors by 8.8\% across Human3.6M, 3DPW, and AMASS datasets. The code is available at https://github.com/JunyuShi02/PMG-MRL.
<div id='section'>Paperid: <span id='pid'>324, <a href='https://arxiv.org/pdf/2407.19564.pdf' target='_blank'>https://arxiv.org/pdf/2407.19564.pdf</a></span>   <span><a href='https://github.com/csjfwang/Forecast-PEFT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jifeng Wang, Kaouther Messaoud, Yuejiang Liu, Juergen Gall, Alexandre Alahi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.19564">Forecast-PEFT: Parameter-Efficient Fine-Tuning for Pre-trained Motion Forecasting Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent progress in motion forecasting has been substantially driven by self-supervised pre-training. However, adapting pre-trained models for specific downstream tasks, especially motion prediction, through extensive fine-tuning is often inefficient. This inefficiency arises because motion prediction closely aligns with the masked pre-training tasks, and traditional full fine-tuning methods fail to fully leverage this alignment. To address this, we introduce Forecast-PEFT, a fine-tuning strategy that freezes the majority of the model's parameters, focusing adjustments on newly introduced prompts and adapters. This approach not only preserves the pre-learned representations but also significantly reduces the number of parameters that need retraining, thereby enhancing efficiency. This tailored strategy, supplemented by our method's capability to efficiently adapt to different datasets, enhances model efficiency and ensures robust performance across datasets without the need for extensive retraining. Our experiments show that Forecast-PEFT outperforms traditional full fine-tuning methods in motion prediction tasks, achieving higher accuracy with only 17% of the trainable parameters typically required. Moreover, our comprehensive adaptation, Forecast-FT, further improves prediction performance, evidencing up to a 9.6% enhancement over conventional baseline methods. Code will be available at https://github.com/csjfwang/Forecast-PEFT.
<div id='section'>Paperid: <span id='pid'>325, <a href='https://arxiv.org/pdf/2407.19244.pdf' target='_blank'>https://arxiv.org/pdf/2407.19244.pdf</a></span>   <span><a href='https://github.com/ph-w2000/SDM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Penghui Wen, Kun Hu, Dong Yuan, Zhiyuan Ning, Changyang Li, Zhiyong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.19244">Radio Frequency Signal based Human Silhouette Segmentation: A Sequential Diffusion Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Radio frequency (RF) signals have been proved to be flexible for human silhouette segmentation (HSS) under complex environments. Existing studies are mainly based on a one-shot approach, which lacks a coherent projection ability from the RF domain. Additionally, the spatio-temporal patterns have not been fully explored for human motion dynamics in HSS. Therefore, we propose a two-stage Sequential Diffusion Model (SDM) to progressively synthesize high-quality segmentation jointly with the considerations on motion dynamics. Cross-view transformation blocks are devised to guide the diffusion model in a multi-scale manner for comprehensively characterizing human related patterns in an individual frame such as directional projection from signal planes. Moreover, spatio-temporal blocks are devised to fine-tune the frame-level model to incorporate spatio-temporal contexts and motion dynamics, enhancing the consistency of the segmentation maps. Comprehensive experiments on a public benchmark -- HIBER demonstrate the state-of-the-art performance of our method with an IoU 0.732. Our code is available at https://github.com/ph-w2000/SDM.
<div id='section'>Paperid: <span id='pid'>326, <a href='https://arxiv.org/pdf/2407.17438.pdf' target='_blank'>https://arxiv.org/pdf/2407.17438.pdf</a></span>   <span><a href='https://humanvid.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenzhi Wang, Yixuan Li, Yanhong Zeng, Youqing Fang, Yuwei Guo, Wenran Liu, Jing Tan, Kai Chen, Tianfan Xue, Bo Dai, Dahua Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.17438">HumanVid: Demystifying Training Data for Camera-controllable Human Image Animation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human image animation involves generating videos from a character photo, allowing user control and unlocking the potential for video and movie production. While recent approaches yield impressive results using high-quality training data, the inaccessibility of these datasets hampers fair and transparent benchmarking. Moreover, these approaches prioritize 2D human motion and overlook the significance of camera motions in videos, leading to limited control and unstable video generation. To demystify the training data, we present HumanVid, the first large-scale high-quality dataset tailored for human image animation, which combines crafted real-world and synthetic data. For the real-world data, we compile a vast collection of real-world videos from the internet. We developed and applied careful filtering rules to ensure video quality, resulting in a curated collection of 20K high-resolution (1080P) human-centric videos. Human and camera motion annotation is accomplished using a 2D pose estimator and a SLAM-based method. To expand our synthetic dataset, we collected 10K 3D avatar assets and leveraged existing assets of body shapes, skin textures and clothings. Notably, we introduce a rule-based camera trajectory generation method, enabling the synthetic pipeline to incorporate diverse and precise camera motion annotation, which can rarely be found in real-world data. To verify the effectiveness of HumanVid, we establish a baseline model named CamAnimate, short for Camera-controllable Human Animation, that considers both human and camera motions as conditions. Through extensive experimentation, we demonstrate that such simple baseline training on our HumanVid achieves state-of-the-art performance in controlling both human pose and camera motions, setting a new benchmark. Demo, data and code could be found in the project website: https://humanvid.github.io/.
<div id='section'>Paperid: <span id='pid'>327, <a href='https://arxiv.org/pdf/2407.12783.pdf' target='_blank'>https://arxiv.org/pdf/2407.12783.pdf</a></span>   <span><a href='https://neu-vi.github.io/SMooDi/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lei Zhong, Yiming Xie, Varun Jampani, Deqing Sun, Huaizu Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.12783">SMooDi: Stylized Motion Diffusion Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a novel Stylized Motion Diffusion model, dubbed SMooDi, to generate stylized motion driven by content texts and style motion sequences. Unlike existing methods that either generate motion of various content or transfer style from one sequence to another, SMooDi can rapidly generate motion across a broad range of content and diverse styles. To this end, we tailor a pre-trained text-to-motion model for stylization. Specifically, we propose style guidance to ensure that the generated motion closely matches the reference style, alongside a lightweight style adaptor that directs the motion towards the desired style while ensuring realism. Experiments across various applications demonstrate that our proposed framework outperforms existing methods in stylized motion generation.
<div id='section'>Paperid: <span id='pid'>328, <a href='https://arxiv.org/pdf/2407.11962.pdf' target='_blank'>https://arxiv.org/pdf/2407.11962.pdf</a></span>   <span><a href='https://stevejaehyeok.github.io/publications/moco-nerf' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jaehyeok Kim, Dongyoon Wee, Dan Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.11962">Motion-Oriented Compositional Neural Radiance Fields for Monocular Dynamic Human Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces Motion-oriented Compositional Neural Radiance Fields (MoCo-NeRF), a framework designed to perform free-viewpoint rendering of monocular human videos via novel non-rigid motion modeling approach. In the context of dynamic clothed humans, complex cloth dynamics generate non-rigid motions that are intrinsically distinct from skeletal articulations and critically important for the rendering quality. The conventional approach models non-rigid motions as spatial (3D) deviations in addition to skeletal transformations. However, it is either time-consuming or challenging to achieve optimal quality due to its high learning complexity without a direct supervision. To target this problem, we propose a novel approach of modeling non-rigid motions as radiance residual fields to benefit from more direct color supervision in the rendering and utilize the rigid radiance fields as a prior to reduce the complexity of the learning process. Our approach utilizes a single multiresolution hash encoding (MHE) to concurrently learn the canonical T-pose representation from rigid skeletal motions and the radiance residual field for non-rigid motions. Additionally, to further improve both training efficiency and usability, we extend MoCo-NeRF to support simultaneous training of multiple subjects within a single framework, thanks to our effective design for modeling non-rigid motions. This scalability is achieved through the integration of a global MHE and learnable identity codes in addition to multiple local MHEs. We present extensive results on ZJU-MoCap and MonoCap, clearly demonstrating state-of-the-art performance in both single- and multi-subject settings. The code and model will be made publicly available at the project page: https://stevejaehyeok.github.io/publications/moco-nerf.
<div id='section'>Paperid: <span id='pid'>329, <a href='https://arxiv.org/pdf/2407.11915.pdf' target='_blank'>https://arxiv.org/pdf/2407.11915.pdf</a></span>   <span><a href='https://github.com/dingdingding60/Humanoids2024HRI' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bosong Ding, Murat Kirtay, Giacomo Spigler
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.11915">Imitation of human motion achieves natural head movements for humanoid robots in an active-speaker detection task</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Head movements are crucial for social human-human interaction. They can transmit important cues (e.g., joint attention, speaker detection) that cannot be achieved with verbal interaction alone. This advantage also holds for human-robot interaction. Even though modeling human motions through generative AI models has become an active research area within robotics in recent years, the use of these methods for producing head movements in human-robot interaction remains underexplored. In this work, we employed a generative AI pipeline to produce human-like head movements for a Nao humanoid robot. In addition, we tested the system on a real-time active-speaker tracking task in a group conversation setting. Overall, the results show that the Nao robot successfully imitates human head movements in a natural manner while actively tracking the speakers during the conversation. Code and data from this study are available at https://github.com/dingdingding60/Humanoids2024HRI
<div id='section'>Paperid: <span id='pid'>330, <a href='https://arxiv.org/pdf/2407.11494.pdf' target='_blank'>https://arxiv.org/pdf/2407.11494.pdf</a></span>   <span><a href='https://github.com/GuoweiXu368/SLD-HMP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Guowei Xu, Jiale Tao, Wen Li, Lixin Duan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.11494">Learning Semantic Latent Directions for Accurate and Controllable Human Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the realm of stochastic human motion prediction (SHMP), researchers have often turned to generative models like GANS, VAEs and diffusion models. However, most previous approaches have struggled to accurately predict motions that are both realistic and coherent with past motion due to a lack of guidance on the latent distribution. In this paper, we introduce Semantic Latent Directions (SLD) as a solution to this challenge, aiming to constrain the latent space to learn meaningful motion semantics and enhance the accuracy of SHMP. SLD defines a series of orthogonal latent directions and represents the hypothesis of future motion as a linear combination of these directions. By creating such an information bottleneck, SLD excels in capturing meaningful motion semantics, thereby improving the precision of motion predictions. Moreover, SLD offers controllable prediction capabilities by adjusting the coefficients of the latent directions during the inference phase. Expanding on SLD, we introduce a set of motion queries to enhance the diversity of predictions. By aligning these motion queries with the SLD space, SLD is further promoted to more accurate and coherent motion predictions. Through extensive experiments conducted on widely used benchmarks, we showcase the superiority of our method in accurately predicting motions while maintaining a balance of realism and diversity. Our code and pretrained models are available at https://github.com/GuoweiXu368/SLD-HMP.
<div id='section'>Paperid: <span id='pid'>331, <a href='https://arxiv.org/pdf/2407.11420.pdf' target='_blank'>https://arxiv.org/pdf/2407.11420.pdf</a></span>   <span><a href='https://github.com/Unsigned-Long/iKalibr' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuolong Chen, Xingxing Li, Shengyu Li, Yuxuan Zhou, Xiaoteng Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.11420">iKalibr: Unified Targetless Spatiotemporal Calibration for Resilient Integrated Inertial Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The integrated inertial system, typically integrating an IMU and an exteroceptive sensor such as radar, LiDAR, and camera, has been widely accepted and applied in modern robotic applications for ego-motion estimation, motion control, or autonomous exploration. To improve system accuracy, robustness, and further usability, both multiple and various sensors are generally resiliently integrated, which benefits the system performance regarding failure tolerance, perception capability, and environment compatibility. For such systems, accurate and consistent spatiotemporal calibration is required to maintain a unique spatiotemporal framework for multi-sensor fusion. Considering most existing calibration methods (i) are generally oriented to specific integrated inertial systems, (ii) often only focus on spatial determination, (iii) usually require artificial targets, lacking convenience and usability, we propose iKalibr: a unified targetless spatiotemporal calibration framework for resilient integrated inertial systems, which overcomes the above issues, and enables both accurate and consistent calibration. Altogether four commonly employed sensors are supported in iKalibr currently, namely IMU, radar, LiDAR, and camera. The proposed method starts with a rigorous and efficient dynamic initialization, where all parameters in the estimator would be accurately recovered. Subsequently, several continuous-time batch optimizations are conducted to refine the initialized parameters toward better states. Sufficient real-world experiments were conducted to verify the feasibility and evaluate the calibration performance of iKalibr. The results demonstrate that iKalibr can achieve accurate resilient spatiotemporal calibration. We open-source our implementations at (https://github.com/Unsigned-Long/iKalibr) to benefit the research community.
<div id='section'>Paperid: <span id='pid'>332, <a href='https://arxiv.org/pdf/2407.10528.pdf' target='_blank'>https://arxiv.org/pdf/2407.10528.pdf</a></span>   <span><a href='https://jpthu17.github.io/GuidedMotion-project/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Peng Jin, Hao Li, Zesen Cheng, Kehan Li, Runyi Yu, Chang Liu, Xiangyang Ji, Li Yuan, Jie Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.10528">Local Action-Guided Motion Diffusion Model for Text-to-Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-to-motion generation requires not only grounding local actions in language but also seamlessly blending these individual actions to synthesize diverse and realistic global motions. However, existing motion generation methods primarily focus on the direct synthesis of global motions while neglecting the importance of generating and controlling local actions. In this paper, we propose the local action-guided motion diffusion model, which facilitates global motion generation by utilizing local actions as fine-grained control signals. Specifically, we provide an automated method for reference local action sampling and leverage graph attention networks to assess the guiding weight of each local action in the overall motion synthesis. During the diffusion process for synthesizing global motion, we calculate the local-action gradient to provide conditional guidance. This local-to-global paradigm reduces the complexity associated with direct global motion generation and promotes motion diversity via sampling diverse actions as conditions. Extensive experiments on two human motion datasets, i.e., HumanML3D and KIT, demonstrate the effectiveness of our method. Furthermore, our method provides flexibility in seamlessly combining various local actions and continuous guiding weight adjustment, accommodating diverse user preferences, which may hold potential significance for the community. The project page is available at https://jpthu17.github.io/GuidedMotion-project/.
<div id='section'>Paperid: <span id='pid'>333, <a href='https://arxiv.org/pdf/2407.10061.pdf' target='_blank'>https://arxiv.org/pdf/2407.10061.pdf</a></span>   <span><a href='https://steve-zeyu-zhang.github.io/InfiniMotion/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zeyu Zhang, Akide Liu, Qi Chen, Feng Chen, Ian Reid, Richard Hartley, Bohan Zhuang, Hao Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.10061">InfiniMotion: Mamba Boosts Memory in Transformer for Arbitrary Long Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-to-motion generation holds potential for film, gaming, and robotics, yet current methods often prioritize short motion generation, making it challenging to produce long motion sequences effectively: (1) Current methods struggle to handle long motion sequences as a single input due to prohibitively high computational cost; (2) Breaking down the generation of long motion sequences into shorter segments can result in inconsistent transitions and requires interpolation or inpainting, which lacks entire sequence modeling. To solve these challenges, we propose InfiniMotion, a method that generates continuous motion sequences of arbitrary length within an autoregressive framework. We highlight its groundbreaking capability by generating a continuous 1-hour human motion with around 80,000 frames. Specifically, we introduce the Motion Memory Transformer with Bidirectional Mamba Memory, enhancing the transformer's memory to process long motion sequences effectively without overwhelming computational resources. Notably our method achieves over 30% improvement in FID and 6 times longer demonstration compared to previous state-of-the-art methods, showcasing significant advancements in long motion generation. See project webpage: https://steve-zeyu-zhang.github.io/InfiniMotion/
<div id='section'>Paperid: <span id='pid'>334, <a href='https://arxiv.org/pdf/2407.09797.pdf' target='_blank'>https://arxiv.org/pdf/2407.09797.pdf</a></span>   <span><a href='https://github.com/HanLingsgjk/CSCV' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Han Ling, Quansen Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.09797">ScaleFlow++: Robust and Accurate Estimation of 3D Motion from Video</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Perceiving and understanding 3D motion is a core technology in fields such as autonomous driving, robots, and motion prediction. This paper proposes a 3D motion perception method called ScaleFlow++ that is easy to generalize. With just a pair of RGB images, ScaleFlow++ can robustly estimate optical flow and motion-in-depth (MID). Most existing methods directly regress MID from two RGB frames or optical flow, resulting in inaccurate and unstable results. Our key insight is cross-scale matching, which extracts deep motion clues by matching objects in pairs of images at different scales. Unlike previous methods, ScaleFlow++ integrates optical flow and MID estimation into a unified architecture, estimating optical flow and MID end-to-end based on feature matching. Moreover, we also proposed modules such as global initialization network, global iterative optimizer, and hybrid training pipeline to integrate global motion information, reduce the number of iterations, and prevent overfitting during training. On KITTI, ScaleFlow++ achieved the best monocular scene flow estimation performance, reducing SF-all from 6.21 to 5.79. The evaluation of MID even surpasses RGBD-based methods. In addition, ScaleFlow++ has achieved stunning zero-shot generalization performance in both rigid and nonrigid scenes. Code is available at \url{https://github.com/HanLingsgjk/CSCV}.
<div id='section'>Paperid: <span id='pid'>335, <a href='https://arxiv.org/pdf/2407.08680.pdf' target='_blank'>https://arxiv.org/pdf/2407.08680.pdf</a></span>   <span><a href='https://gseancdat.github.io/projects/GIMMVFI' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zujin Guo, Wei Li, Chen Change Loy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.08680">Generalizable Implicit Motion Modeling for Video Frame Interpolation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion modeling is critical in flow-based Video Frame Interpolation (VFI). Existing paradigms either consider linear combinations of bidirectional flows or directly predict bilateral flows for given timestamps without exploring favorable motion priors, thus lacking the capability of effectively modeling spatiotemporal dynamics in real-world videos. To address this limitation, in this study, we introduce Generalizable Implicit Motion Modeling (GIMM), a novel and effective approach to motion modeling for VFI. Specifically, to enable GIMM as an effective motion modeling paradigm, we design a motion encoding pipeline to model spatiotemporal motion latent from bidirectional flows extracted from pre-trained flow estimators, effectively representing input-specific motion priors. Then, we implicitly predict arbitrary-timestep optical flows within two adjacent input frames via an adaptive coordinate-based neural network, with spatiotemporal coordinates and motion latent as inputs. Our GIMM can be easily integrated with existing flow-based VFI works by supplying accurately modeled motion. We show that GIMM performs better than the current state of the art on standard VFI benchmarks.
<div id='section'>Paperid: <span id='pid'>336, <a href='https://arxiv.org/pdf/2407.08443.pdf' target='_blank'>https://arxiv.org/pdf/2407.08443.pdf</a></span>   <span><a href='https://shuochengzhai.github.io/Infinite-motion.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mengtian Li, Chengshuo Zhai, Shengxiang Yao, Zhifeng Xie, Keyu Chen, Yu-Gang Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.08443">Infinite Motion: Extended Motion Generation via Long Text Instructions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the realm of motion generation, the creation of long-duration, high-quality motion sequences remains a significant challenge. This paper presents our groundbreaking work on "Infinite Motion", a novel approach that leverages long text to extended motion generation, effectively bridging the gap between short and long-duration motion synthesis. Our core insight is the strategic extension and reassembly of existing high-quality text-motion datasets, which has led to the creation of a novel benchmark dataset to facilitate the training of models for extended motion sequences. A key innovation of our model is its ability to accept arbitrary lengths of text as input, enabling the generation of motion sequences tailored to specific narratives or scenarios. Furthermore, we incorporate the timestamp design for text which allows precise editing of local segments within the generated sequences, offering unparalleled control and flexibility in motion synthesis. We further demonstrate the versatility and practical utility of "Infinite Motion" through three specific applications: natural language interactive editing, motion sequence editing within long sequences and splicing of independent motion sequences. Each application highlights the adaptability of our approach and broadens the spectrum of possibilities for research and development in motion generation. Through extensive experiments, we demonstrate the superior performance of our model in generating long sequence motions compared to existing methods.Project page: https://shuochengzhai.github.io/Infinite-motion.github.io/
<div id='section'>Paperid: <span id='pid'>337, <a href='https://arxiv.org/pdf/2407.06188.pdf' target='_blank'>https://arxiv.org/pdf/2407.06188.pdf</a></span>   <span><a href='https://yukangcao.github.io/CrowdMoGen' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yukang Cao, Xinying Guo, Mingyuan Zhang, Haozhe Xie, Chenyang Gu, Ziwei Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.06188">CrowdMoGen: Zero-Shot Text-Driven Collective Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While recent advances in text-to-motion generation have shown promising results, they typically assume all individuals are grouped as a single unit. Scaling these methods to handle larger crowds and ensuring that individuals respond appropriately to specific events remains a significant challenge. This is primarily due to the complexities of scene planning, which involves organizing groups, planning their activities, and coordinating interactions, and controllable motion generation. In this paper, we present CrowdMoGen, the first zero-shot framework for collective motion generation, which effectively groups individuals and generates event-aligned motion sequences from text prompts. 1) Being limited by the available datasets for training an effective scene planning module in a supervised manner, we instead propose a crowd scene planner that leverages pre-trained large language models (LLMs) to organize individuals into distinct groups. While LLMs offer high-level guidance for group divisions, they lack the low-level understanding of human motion. To address this, we further propose integrating an SMPL-based joint prior to generate context-appropriate activities, which consists of both joint trajectories and textual descriptions. 2) Secondly, to incorporate the assigned activities into the generative network, we introduce a collective motion generator that integrates the activities into a transformer-based network in a joint-wise manner, maintaining the spatial constraints during the multi-step denoising process. Extensive experiments demonstrate that CrowdMoGen significantly outperforms previous approaches, delivering realistic, event-driven motion sequences that are spatially coherent. As the first framework of collective motion generation, CrowdMoGen has the potential to advance applications in urban simulation, crowd planning, and other large-scale interactive environments.
<div id='section'>Paperid: <span id='pid'>338, <a href='https://arxiv.org/pdf/2407.05238.pdf' target='_blank'>https://arxiv.org/pdf/2407.05238.pdf</a></span>   <span><a href='https://github.com/haooozi/P2P' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiahao Nie, Fei Xie, Sifan Zhou, Xueyi Zhou, Dong-Kyu Chae, Zhiwei He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.05238">P2P: Part-to-Part Motion Cues Guide a Strong Tracking Framework for LiDAR Point Clouds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D single object tracking (SOT) methods based on appearance matching has long suffered from insufficient appearance information incurred by incomplete, textureless and semantically deficient LiDAR point clouds. While motion paradigm exploits motion cues instead of appearance matching for tracking, it incurs complex multi-stage processing and segmentation module. In this paper, we first provide in-depth explorations on motion paradigm, which proves that (\textbf{i}) it is feasible to directly infer target relative motion from point clouds across consecutive frames; (\textbf{ii}) fine-grained information comparison between consecutive point clouds facilitates target motion modeling. We thereby propose to perform part-to-part motion modeling for consecutive point clouds and introduce a novel tracking framework, termed \textbf{P2P}. The novel framework fuses each corresponding part information between consecutive point clouds, effectively exploring detailed information changes and thus modeling accurate target-related motion cues. Following this framework, we present P2P-point and P2P-voxel models, incorporating implicit and explicit part-to-part motion modeling by point- and voxel-based representation, respectively. Without bells and whistles, P2P-voxel sets a new state-of-the-art performance ($\sim$\textbf{89\%}, \textbf{72\%} and \textbf{63\%} precision on KITTI, NuScenes and Waymo Open Dataset, respectively). Moreover, under the same point-based representation, P2P-point outperforms the previous motion tracker M$^2$Track by \textbf{3.3\%} and \textbf{6.7\%} on the KITTI and NuScenes, while running at a considerably high speed of \textbf{107 Fps} on a single RTX3090 GPU. The source code and pre-trained models are available at https://github.com/haooozi/P2P.
<div id='section'>Paperid: <span id='pid'>339, <a href='https://arxiv.org/pdf/2407.02272.pdf' target='_blank'>https://arxiv.org/pdf/2407.02272.pdf</a></span>   <span><a href='https://motioncritic.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoru Wang, Wentao Zhu, Luyi Miao, Yishu Xu, Feng Gao, Qi Tian, Yizhou Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.02272">Aligning Human Motion Generation with Human Perceptions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion generation is a critical task with a wide range of applications. Achieving high realism in generated motions requires naturalness, smoothness, and plausibility. Despite rapid advancements in the field, current generation methods often fall short of these goals. Furthermore, existing evaluation metrics typically rely on ground-truth-based errors, simple heuristics, or distribution distances, which do not align well with human perceptions of motion quality. In this work, we propose a data-driven approach to bridge this gap by introducing a large-scale human perceptual evaluation dataset, MotionPercept, and a human motion critic model, MotionCritic, that capture human perceptual preferences. Our critic model offers a more accurate metric for assessing motion quality and could be readily integrated into the motion generation pipeline to enhance generation quality. Extensive experiments demonstrate the effectiveness of our approach in both evaluating and improving the quality of generated human motions by aligning with human perceptions. Code and data are publicly available at https://motioncritic.github.io/.
<div id='section'>Paperid: <span id='pid'>340, <a href='https://arxiv.org/pdf/2407.02129.pdf' target='_blank'>https://arxiv.org/pdf/2407.02129.pdf</a></span>   <span><a href='https://github.com/MIV-XJTU/ReliaAvatar' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bo Qian, Zhenhuan Wei, Jiashuo Li, Xing Wei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.02129">ReliaAvatar: A Robust Real-Time Avatar Animator with Integrated Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Efficiently estimating the full-body pose with minimal wearable devices presents a worthwhile research direction. Despite significant advancements in this field, most current research neglects to explore full-body avatar estimation under low-quality signal conditions, which is prevalent in practical usage. To bridge this gap, we summarize three scenarios that may be encountered in real-world applications: standard scenario, instantaneous data-loss scenario, and prolonged data-loss scenario, and propose a new evaluation benchmark. The solution we propose to address data-loss scenarios is integrating the full-body avatar pose estimation problem with motion prediction. Specifically, we present \textit{ReliaAvatar}, a real-time, \textbf{relia}ble \textbf{avatar} animator equipped with predictive modeling capabilities employing a dual-path architecture. ReliaAvatar operates effectively, with an impressive performance rate of 109 frames per second (fps). Extensive comparative evaluations on widely recognized benchmark datasets demonstrate Relia\-Avatar's superior performance in both standard and low data-quality conditions. The code is available at \url{https://github.com/MIV-XJTU/ReliaAvatar}.
<div id='section'>Paperid: <span id='pid'>341, <a href='https://arxiv.org/pdf/2407.00574.pdf' target='_blank'>https://arxiv.org/pdf/2407.00574.pdf</a></span>   <span><a href='https://martayang.github.io/HAC' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fengyuan Yang, Kerui Gu, Ha Linh Nguyen, Tze Ho Elden Tse, Angela Yao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.00574">Humans as Checkerboards: Calibrating Camera Motion Scale for World-Coordinate Human Mesh Recovery</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate camera motion estimation is essential for recovering global human motion in world coordinates from RGB video inputs. SLAM is widely used for estimating camera trajectory and point cloud, but monocular SLAM does so only up to an unknown scale factor. Previous works estimate the scale factor through optimization, but this is unreliable and time-consuming. This paper presents an optimization-free scale calibration framework, Human as Checkerboard (HAC). HAC innovatively leverages the human body predicted by human mesh recovery model as a calibration reference. Specifically, it uses the absolute depth of human-scene contact joints as references to calibrate the corresponding relative scene depth from SLAM. HAC benefits from geometric priors encoded in human mesh recovery models to estimate the SLAM scale and achieves precise global human motion estimation. Simple yet powerful, our method sets a new state-of-the-art performance for global human mesh estimation tasks, reducing motion errors by 50% over prior local-to-global methods while using 100$\times$ less inference time than optimization-based methods. Project page: https://martayang.github.io/HAC.
<div id='section'>Paperid: <span id='pid'>342, <a href='https://arxiv.org/pdf/2406.19680.pdf' target='_blank'>https://arxiv.org/pdf/2406.19680.pdf</a></span>   <span><a href='https://tencent.github.io/MimicMotion' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuang Zhang, Jiaxi Gu, Li-Wen Wang, Han Wang, Junqi Cheng, Yuefeng Zhu, Fangyuan Zou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.19680">MimicMotion: High-Quality Human Motion Video Generation with Confidence-aware Pose Guidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, generative artificial intelligence has achieved significant advancements in the field of image generation, spawning a variety of applications. However, video generation still faces considerable challenges in various aspects, such as controllability, video length, and richness of details, which hinder the application and popularization of this technology. In this work, we propose a controllable video generation framework, dubbed MimicMotion, which can generate high-quality videos of arbitrary length mimicking specific motion guidance. Compared with previous methods, our approach has several highlights. Firstly, we introduce confidence-aware pose guidance that ensures high frame quality and temporal smoothness. Secondly, we introduce regional loss amplification based on pose confidence, which significantly reduces image distortion. Lastly, for generating long and smooth videos, we propose a progressive latent fusion strategy. By this means, we can produce videos of arbitrary length with acceptable resource consumption. With extensive experiments and user studies, MimicMotion demonstrates significant improvements over previous approaches in various aspects. Detailed results and comparisons are available on our project page: https://tencent.github.io/MimicMotion .
<div id='section'>Paperid: <span id='pid'>343, <a href='https://arxiv.org/pdf/2406.17758.pdf' target='_blank'>https://arxiv.org/pdf/2406.17758.pdf</a></span>   <span><a href='https://jianzongwu.github.io/projects/motionbooth' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianzong Wu, Xiangtai Li, Yanhong Zeng, Jiangning Zhang, Qianyu Zhou, Yining Li, Yunhai Tong, Kai Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.17758">MotionBooth: Motion-Aware Customized Text-to-Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we present MotionBooth, an innovative framework designed for animating customized subjects with precise control over both object and camera movements. By leveraging a few images of a specific object, we efficiently fine-tune a text-to-video model to capture the object's shape and attributes accurately. Our approach presents subject region loss and video preservation loss to enhance the subject's learning performance, along with a subject token cross-attention loss to integrate the customized subject with motion control signals. Additionally, we propose training-free techniques for managing subject and camera motions during inference. In particular, we utilize cross-attention map manipulation to govern subject motion and introduce a novel latent shift module for camera movement control as well. MotionBooth excels in preserving the appearance of subjects while simultaneously controlling the motions in generated videos. Extensive quantitative and qualitative evaluations demonstrate the superiority and effectiveness of our method. Our project page is at https://jianzongwu.github.io/projects/motionbooth
<div id='section'>Paperid: <span id='pid'>344, <a href='https://arxiv.org/pdf/2406.15735.pdf' target='_blank'>https://arxiv.org/pdf/2406.15735.pdf</a></span>   <span><a href='https://cond-image-leak.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Min Zhao, Hongzhou Zhu, Chendong Xiang, Kaiwen Zheng, Chongxuan Li, Jun Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.15735">Identifying and Solving Conditional Image Leakage in Image-to-Video Diffusion Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion models have obtained substantial progress in image-to-video generation. However, in this paper, we find that these models tend to generate videos with less motion than expected. We attribute this to the issue called conditional image leakage, where the image-to-video diffusion models (I2V-DMs) tend to over-rely on the conditional image at large time steps. We further address this challenge from both inference and training aspects. First, we propose to start the generation process from an earlier time step to avoid the unreliable large-time steps of I2V-DMs, as well as an initial noise distribution with optimal analytic expressions (Analytic-Init) by minimizing the KL divergence between it and the actual marginal distribution to bridge the training-inference gap. Second, we design a time-dependent noise distribution (TimeNoise) for the conditional image during training, applying higher noise levels at larger time steps to disrupt it and reduce the model's dependency on it. We validate these general strategies on various I2V-DMs on our collected open-domain image benchmark and the UCF101 dataset. Extensive results show that our methods outperform baselines by producing higher motion scores with lower errors while maintaining image alignment and temporal consistency, thereby yielding superior overall performance and enabling more accurate motion control. The project page: \url{https://cond-image-leak.github.io/}.
<div id='section'>Paperid: <span id='pid'>345, <a href='https://arxiv.org/pdf/2406.14567.pdf' target='_blank'>https://arxiv.org/pdf/2406.14567.pdf</a></span>   <span><a href='https://upc-virvig.github.io/DragPoser/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jose Luis Ponton, Eduard Pujol, Andreas Aristidou, Carlos Andujar, Nuria Pelechano
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.14567">DragPoser: Motion Reconstruction from Variable Sparse Tracking Signals via Latent Space Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>High-quality motion reconstruction that follows the user's movements can be achieved by high-end mocap systems with many sensors. However, obtaining such animation quality with fewer input devices is gaining popularity as it brings mocap closer to the general public. The main challenges include the loss of end-effector accuracy in learning-based approaches, or the lack of naturalness and smoothness in IK-based solutions. In addition, such systems are often finely tuned to a specific number of trackers and are highly sensitive to missing data e.g., in scenarios where a sensor is occluded or malfunctions. In response to these challenges, we introduce DragPoser, a novel deep-learning-based motion reconstruction system that accurately represents hard and dynamic on-the-fly constraints, attaining real-time high end-effectors position accuracy. This is achieved through a pose optimization process within a structured latent space. Our system requires only one-time training on a large human motion dataset, and then constraints can be dynamically defined as losses, while the pose is iteratively refined by computing the gradients of these losses within the latent space. To further enhance our approach, we incorporate a Temporal Predictor network, which employs a Transformer architecture to directly encode temporality within the latent space. This network ensures the pose optimization is confined to the manifold of valid poses and also leverages past pose data to predict temporally coherent poses. Results demonstrate that DragPoser surpasses both IK-based and the latest data-driven methods in achieving precise end-effector positioning, while it produces natural poses and temporally coherent motion. In addition, our system showcases robustness against on-the-fly constraint modifications, and exhibits exceptional adaptability to various input configurations and changes.
<div id='section'>Paperid: <span id='pid'>346, <a href='https://arxiv.org/pdf/2406.10126.pdf' target='_blank'>https://arxiv.org/pdf/2406.10126.pdf</a></span>   <span><a href='https://lifedecoder.github.io/CamTrol/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chen Hou, Zhibo Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.10126">Training-free Camera Control for Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a training-free and robust solution to offer camera movement control for off-the-shelf video diffusion models. Unlike previous work, our method does not require any supervised finetuning on camera-annotated datasets or self-supervised training via data augmentation. Instead, it can be plug-and-play with most pretrained video diffusion models and generate camera-controllable videos with a single image or text prompt as input. The inspiration for our work comes from the layout prior that intermediate latents encode for the generated results, thus rearranging noisy pixels in them will cause the output content to relocate as well. As camera moving could also be seen as a type of pixel rearrangement caused by perspective change, videos can be reorganized following specific camera motion if their noisy latents change accordingly. Building on this, we propose CamTrol, which enables robust camera control for video diffusion models. It is achieved by a two-stage process. First, we model image layout rearrangement through explicit camera movement in 3D point cloud space. Second, we generate videos with camera motion by leveraging the layout prior of noisy latents formed by a series of rearranged images. Extensive experiments have demonstrated its superior performance in both video generation and camera motion alignment compared with other finetuned methods. Furthermore, we show the capability of CamTrol to generalize to various base models, as well as its impressive applications in scalable motion control, dealing with complicated trajectories and unsupervised 3D video generation. Videos available at https://lifedecoder.github.io/CamTrol/.
<div id='section'>Paperid: <span id='pid'>347, <a href='https://arxiv.org/pdf/2406.06508.pdf' target='_blank'>https://arxiv.org/pdf/2406.06508.pdf</a></span>   <span><a href='https://github.com/MonkeySeeDoCG/MoMo-code' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sigal Raab, Inbar Gat, Nathan Sala, Guy Tevet, Rotem Shalev-Arkushin, Ohad Fried, Amit H. Bermano, Daniel Cohen-Or
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.06508">Monkey See, Monkey Do: Harnessing Self-attention in Motion Diffusion for Zero-shot Motion Transfer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Given the remarkable results of motion synthesis with diffusion models, a natural question arises: how can we effectively leverage these models for motion editing? Existing diffusion-based motion editing methods overlook the profound potential of the prior embedded within the weights of pre-trained models, which enables manipulating the latent feature space; hence, they primarily center on handling the motion space. In this work, we explore the attention mechanism of pre-trained motion diffusion models. We uncover the roles and interactions of attention elements in capturing and representing intricate human motion patterns, and carefully integrate these elements to transfer a leader motion to a follower one while maintaining the nuanced characteristics of the follower, resulting in zero-shot motion transfer. Editing features associated with selected motions allows us to confront a challenge observed in prior motion diffusion approaches, which use general directives (e.g., text, music) for editing, ultimately failing to convey subtle nuances effectively. Our work is inspired by how a monkey closely imitates what it sees while maintaining its unique motion patterns; hence we call it Monkey See, Monkey Do, and dub it MoMo. Employing our technique enables accomplishing tasks such as synthesizing out-of-distribution motions, style transfer, and spatial editing. Furthermore, diffusion inversion is seldom employed for motions; as a result, editing efforts focus on generated motions, limiting the editability of real ones. MoMo harnesses motion inversion, extending its application to both real and generated motions. Experimental results show the advantage of our approach over the current art. In particular, unlike methods tailored for specific applications through training, our approach is applied at inference time, requiring no training. Our webpage is at https://monkeyseedocg.github.io.
<div id='section'>Paperid: <span id='pid'>348, <a href='https://arxiv.org/pdf/2406.05338.pdf' target='_blank'>https://arxiv.org/pdf/2406.05338.pdf</a></span>   <span><a href='https://bujiazi.github.io/motionclone.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Pengyang Ling, Jiazi Bu, Pan Zhang, Xiaoyi Dong, Yuhang Zang, Tong Wu, Huaian Chen, Jiaqi Wang, Yi Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.05338">MotionClone: Training-Free Motion Cloning for Controllable Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion-based controllable video generation offers the potential for creating captivating visual content. Existing methods typically necessitate model training to encode particular motion cues or incorporate fine-tuning to inject certain motion patterns, resulting in limited flexibility and generalization. In this work, we propose MotionClone, a training-free framework that enables motion cloning from reference videos to versatile motion-controlled video generation, including text-to-video and image-to-video. Based on the observation that the dominant components in temporal-attention maps drive motion synthesis, while the rest mainly capture noisy or very subtle motions, MotionClone utilizes sparse temporal attention weights as motion representations for motion guidance, facilitating diverse motion transfer across varying scenarios. Meanwhile, MotionClone allows for the direct extraction of motion representation through a single denoising step, bypassing the cumbersome inversion processes and thus promoting both efficiency and flexibility. Extensive experiments demonstrate that MotionClone exhibits proficiency in both global camera motion and local object motion, with notable superiority in terms of motion fidelity, textual alignment, and temporal consistency.
<div id='section'>Paperid: <span id='pid'>349, <a href='https://arxiv.org/pdf/2406.04765.pdf' target='_blank'>https://arxiv.org/pdf/2406.04765.pdf</a></span>   <span><a href='https://github.com/tianyuan168326/VideoSemanticCompression-Pytorch' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuan Tian, Guo Lu, Guangtao Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.04765">SMC++: Masked Learning of Unsupervised Video Semantic Compression</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Most video compression methods focus on human visual perception, neglecting semantic preservation. This leads to severe semantic loss during the compression, hampering downstream video analysis tasks. In this paper, we propose a Masked Video Modeling (MVM)-powered compression framework that particularly preserves video semantics, by jointly mining and compressing the semantics in a self-supervised manner. While MVM is proficient at learning generalizable semantics through the masked patch prediction task, it may also encode non-semantic information like trivial textural details, wasting bitcost and bringing semantic noises. To suppress this, we explicitly regularize the non-semantic entropy of the compressed video in the MVM token space. The proposed framework is instantiated as a simple Semantic-Mining-then-Compression (SMC) model. Furthermore, we extend SMC as an advanced SMC++ model from several aspects. First, we equip it with a masked motion prediction objective, leading to better temporal semantic learning ability. Second, we introduce a Transformer-based compression module, to improve the semantic compression efficacy. Considering that directly mining the complex redundancy among heterogeneous features in different coding stages is non-trivial, we introduce a compact blueprint semantic representation to align these features into a similar form, fully unleashing the power of the Transformer-based compression module. Extensive results demonstrate the proposed SMC and SMC++ models show remarkable superiority over previous traditional, learnable, and perceptual quality-oriented video codecs, on three video analysis tasks and seven datasets. \textit{Codes and model are available at: \url{https://github.com/tianyuan168326/VideoSemanticCompression-Pytorch}.
<div id='section'>Paperid: <span id='pid'>350, <a href='https://arxiv.org/pdf/2406.04649.pdf' target='_blank'>https://arxiv.org/pdf/2406.04649.pdf</a></span>   <span><a href='https://github.com/Inowlzy/SMART.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zengyuan Lai, Jiarui Yang, Songpengcheng Xia, Qi Wu, Zhen Sun, Wenxian Yu, Ling Pei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.04649">SMART: Scene-motion-aware human action recognition framework for mental disorder group</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Patients with mental disorders often exhibit risky abnormal actions, such as climbing walls or hitting windows, necessitating intelligent video behavior monitoring for smart healthcare with the rising Internet of Things (IoT) technology. However, the development of vision-based Human Action Recognition (HAR) for these actions is hindered by the lack of specialized algorithms and datasets. In this paper, we innovatively propose to build a vision-based HAR dataset including abnormal actions often occurring in the mental disorder group and then introduce a novel Scene-Motion-aware Action Recognition Technology framework, named SMART, consisting of two technical modules. First, we propose a scene perception module to extract human motion trajectory and human-scene interaction features, which introduces additional scene information for a supplementary semantic representation of the above actions. Second, the multi-stage fusion module fuses the skeleton motion, motion trajectory, and human-scene interaction features, enhancing the semantic association between the skeleton motion and the above supplementary representation, thus generating a comprehensive representation with both human motion and scene information. The effectiveness of our proposed method has been validated on our self-collected HAR dataset (MentalHAD), achieving 94.9% and 93.1% accuracy in un-seen subjects and scenes and outperforming state-of-the-art approaches by 6.5% and 13.2%, respectively. The demonstrated subject- and scene- generalizability makes it possible for SMART's migration to practical deployment in smart healthcare systems for mental disorder patients in medical settings. The code and dataset will be released publicly for further research: https://github.com/Inowlzy/SMART.git.
<div id='section'>Paperid: <span id='pid'>351, <a href='https://arxiv.org/pdf/2406.04629.pdf' target='_blank'>https://arxiv.org/pdf/2406.04629.pdf</a></span>   <span><a href='https://star-avatar.github.io' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zenghao Chai, Chen Tang, Yongkang Wong, Mohan Kankanhalli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.04629">STAR: Skeleton-aware Text-based 4D Avatar Generation with In-Network Motion Retargeting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The creation of 4D avatars (i.e., animated 3D avatars) from text description typically uses text-to-image (T2I) diffusion models to synthesize 3D avatars in the canonical space and subsequently applies animation with target motions. However, such an optimization-by-animation paradigm has several drawbacks. (1) For pose-agnostic optimization, the rendered images in canonical pose for naive Score Distillation Sampling (SDS) exhibit domain gap and cannot preserve view-consistency using only T2I priors, and (2) For post hoc animation, simply applying the source motions to target 3D avatars yields translation artifacts and misalignment. To address these issues, we propose Skeleton-aware Text-based 4D Avatar generation with in-network motion Retargeting (STAR). STAR considers the geometry and skeleton differences between the template mesh and target avatar, and corrects the mismatched source motion by resorting to the pretrained motion retargeting techniques. With the informatively retargeted and occlusion-aware skeleton, we embrace the skeleton-conditioned T2I and text-to-video (T2V) priors, and propose a hybrid SDS module to coherently provide multi-view and frame-consistent supervision signals. Hence, STAR can progressively optimize the geometry, texture, and motion in an end-to-end manner. The quantitative and qualitative experiments demonstrate our proposed STAR can synthesize high-quality 4D avatars with vivid animations that align well with the text description. Additional ablation studies shows the contributions of each component in STAR. The source code and demos are available at: \href{https://star-avatar.github.io}{https://star-avatar.github.io}.
<div id='section'>Paperid: <span id='pid'>352, <a href='https://arxiv.org/pdf/2406.01136.pdf' target='_blank'>https://arxiv.org/pdf/2406.01136.pdf</a></span>   <span><a href='https://moverseai.github.io/single-shot' target='_blank'>  GitHub</a></span> <span><a href='https://moverseai.github.io/single-shot' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Konstantinos Roditakis, Spyridon Thermos, Nikolaos Zioulis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.01136">Towards Practical Single-shot Motion Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite the recent advances in the so-called "cold start" generation from text prompts, their needs in data and computing resources, as well as the ambiguities around intellectual property and privacy concerns pose certain counterarguments for their utility. An interesting and relatively unexplored alternative has been the introduction of unconditional synthesis from a single sample, which has led to interesting generative applications. In this paper we focus on single-shot motion generation and more specifically on accelerating the training time of a Generative Adversarial Network (GAN). In particular, we tackle the challenge of GAN's equilibrium collapse when using mini-batch training by carefully annealing the weights of the loss functions that prevent mode collapse. Additionally, we perform statistical analysis in the generator and discriminator models to identify correlations between training stages and enable transfer learning. Our improved GAN achieves competitive quality and diversity on the Mixamo benchmark when compared to the original GAN architecture and a single-shot diffusion model, while being up to x6.8 faster in training time from the former and x1.75 from the latter. Finally, we demonstrate the ability of our improved GAN to mix and compose motion with a single forward pass. Project page available at https://moverseai.github.io/single-shot.
<div id='section'>Paperid: <span id='pid'>353, <a href='https://arxiv.org/pdf/2405.20991.pdf' target='_blank'>https://arxiv.org/pdf/2405.20991.pdf</a></span>   <span><a href='https://github.com/KTH-RPL/Detect_VLM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi Yang, Qingwen Zhang, Kei Ikemura, Nazre Batool, John Folkesson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.20991">Hard Cases Detection in Motion Prediction by Vision-Language Foundation Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Addressing hard cases in autonomous driving, such as anomalous road users, extreme weather conditions, and complex traffic interactions, presents significant challenges. To ensure safety, it is crucial to detect and manage these scenarios effectively for autonomous driving systems. However, the rarity and high-risk nature of these cases demand extensive, diverse datasets for training robust models. Vision-Language Foundation Models (VLMs) have shown remarkable zero-shot capabilities as being trained on extensive datasets. This work explores the potential of VLMs in detecting hard cases in autonomous driving. We demonstrate the capability of VLMs such as GPT-4v in detecting hard cases in traffic participant motion prediction on both agent and scenario levels. We introduce a feasible pipeline where VLMs, fed with sequential image frames with designed prompts, effectively identify challenging agents or scenarios, which are verified by existing prediction models. Moreover, by taking advantage of this detection of hard cases by VLMs, we further improve the training efficiency of the existing motion prediction pipeline by performing data selection for the training samples suggested by GPT. We show the effectiveness and feasibility of our pipeline incorporating VLMs with state-of-the-art methods on NuScenes datasets. The code is accessible at https://github.com/KTH-RPL/Detect_VLM.
<div id='section'>Paperid: <span id='pid'>354, <a href='https://arxiv.org/pdf/2405.20325.pdf' target='_blank'>https://arxiv.org/pdf/2405.20325.pdf</a></span>   <span><a href='https://francis-rings.github.io/MotionFollower/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuyuan Tu, Qi Dai, Zihao Zhang, Sicheng Xie, Zhi-Qi Cheng, Chong Luo, Xintong Han, Zuxuan Wu, Yu-Gang Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.20325">MotionFollower: Editing Video Motion via Lightweight Score-Guided Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite impressive advancements in diffusion-based video editing models in altering video attributes, there has been limited exploration into modifying motion information while preserving the original protagonist's appearance and background. In this paper, we propose MotionFollower, a lightweight score-guided diffusion model for video motion editing. To introduce conditional controls to the denoising process, MotionFollower leverages two of our proposed lightweight signal controllers, one for poses and the other for appearances, both of which consist of convolution blocks without involving heavy attention calculations. Further, we design a score guidance principle based on a two-branch architecture, including the reconstruction and editing branches, which significantly enhance the modeling capability of texture details and complicated backgrounds. Concretely, we enforce several consistency regularizers and losses during the score estimation. The resulting gradients thus inject appropriate guidance to the intermediate latents, forcing the model to preserve the original background details and protagonists' appearances without interfering with the motion modification. Experiments demonstrate the competitive motion editing ability of MotionFollower qualitatively and quantitatively. Compared with MotionEditor, the most advanced motion editing model, MotionFollower achieves an approximately 80% reduction in GPU memory while delivering superior motion editing performance and exclusively supporting large camera movements and actions.
<div id='section'>Paperid: <span id='pid'>355, <a href='https://arxiv.org/pdf/2405.19620.pdf' target='_blank'>https://arxiv.org/pdf/2405.19620.pdf</a></span>   <span><a href='https://github.com/swc-17/SparseDrive' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenchao Sun, Xuewu Lin, Yining Shi, Chuang Zhang, Haoran Wu, Sifa Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.19620">SparseDrive: End-to-End Autonomous Driving via Sparse Scene Representation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The well-established modular autonomous driving system is decoupled into different standalone tasks, e.g. perception, prediction and planning, suffering from information loss and error accumulation across modules. In contrast, end-to-end paradigms unify multi-tasks into a fully differentiable framework, allowing for optimization in a planning-oriented spirit. Despite the great potential of end-to-end paradigms, both the performance and efficiency of existing methods are not satisfactory, particularly in terms of planning safety. We attribute this to the computationally expensive BEV (bird's eye view) features and the straightforward design for prediction and planning. To this end, we explore the sparse representation and review the task design for end-to-end autonomous driving, proposing a new paradigm named SparseDrive. Concretely, SparseDrive consists of a symmetric sparse perception module and a parallel motion planner. The sparse perception module unifies detection, tracking and online mapping with a symmetric model architecture, learning a fully sparse representation of the driving scene. For motion prediction and planning, we review the great similarity between these two tasks, leading to a parallel design for motion planner. Based on this parallel design, which models planning as a multi-modal problem, we propose a hierarchical planning selection strategy , which incorporates a collision-aware rescore module, to select a rational and safe trajectory as the final planning output. With such effective designs, SparseDrive surpasses previous state-of-the-arts by a large margin in performance of all tasks, while achieving much higher training and inference efficiency. Code will be avaliable at https://github.com/swc-17/SparseDrive for facilitating future research.
<div id='section'>Paperid: <span id='pid'>356, <a href='https://arxiv.org/pdf/2405.19609.pdf' target='_blank'>https://arxiv.org/pdf/2405.19609.pdf</a></span>   <span><a href='https://alex-jyj.github.io/SMPLX-Lite/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yujiao Jiang, Qingmin Liao, Zhaolong Wang, Xiangru Lin, Zongqing Lu, Yuxi Zhao, Hanqing Wei, Jingrui Ye, Yu Zhang, Zhijing Shao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.19609">SMPLX-Lite: A Realistic and Drivable Avatar Benchmark with Rich Geometry and Texture Annotations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recovering photorealistic and drivable full-body avatars is crucial for numerous applications, including virtual reality, 3D games, and tele-presence. Most methods, whether reconstruction or generation, require large numbers of human motion sequences and corresponding textured meshes. To easily learn a drivable avatar, a reasonable parametric body model with unified topology is paramount. However, existing human body datasets either have images or textured models and lack parametric models which fit clothes well. We propose a new parametric model SMPLX-Lite-D, which can fit detailed geometry of the scanned mesh while maintaining stable geometry in the face, hand and foot regions. We present SMPLX-Lite dataset, the most comprehensive clothing avatar dataset with multi-view RGB sequences, keypoints annotations, textured scanned meshes, and textured SMPLX-Lite-D models. With the SMPLX-Lite dataset, we train a conditional variational autoencoder model that takes human pose and facial keypoints as input, and generates a photorealistic drivable human avatar.
<div id='section'>Paperid: <span id='pid'>357, <a href='https://arxiv.org/pdf/2405.19183.pdf' target='_blank'>https://arxiv.org/pdf/2405.19183.pdf</a></span>   <span><a href='https://github.com/TruongKhang/cLODE;' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Khang Truong Giang, Yongjae Kim, Andrea Finazzi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.19183">Conditional Latent ODEs for Motion Prediction in Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper addresses imitation learning for motion prediction problem in autonomous driving, especially in multi-agent setting. Different from previous methods based on GAN, we present the conditional latent ordinary differential equation (cLODE) to leverage both the generative strength of conditional VAE and the continuous representation of neural ODE. Our network architecture is inspired from the Latent-ODE model. The experiment shows that our method outperform the baseline methods in the simulation of multi-agent driving and is very efficient in term of GPU memory consumption. Our code and docker image are publicly available: https://github.com/TruongKhang/cLODE; https://hub.docker.com/r/kim4375731/clode.
<div id='section'>Paperid: <span id='pid'>358, <a href='https://arxiv.org/pdf/2405.18483.pdf' target='_blank'>https://arxiv.org/pdf/2405.18483.pdf</a></span>   <span><a href='https://shanmy.github.io/Multi-Motion/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mengyi Shan, Lu Dong, Yutao Han, Yuan Yao, Tao Liu, Ifeoma Nwogu, Guo-Jun Qi, Mitch Hill
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.18483">Towards Open Domain Text-Driven Synthesis of Multi-Person Motions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work aims to generate natural and diverse group motions of multiple humans from textual descriptions. While single-person text-to-motion generation is extensively studied, it remains challenging to synthesize motions for more than one or two subjects from in-the-wild prompts, mainly due to the lack of available datasets. In this work, we curate human pose and motion datasets by estimating pose information from large-scale image and video datasets. Our models use a transformer-based diffusion framework that accommodates multiple datasets with any number of subjects or frames. Experiments explore both generation of multi-person static poses and generation of multi-person motion sequences. To our knowledge, our method is the first to generate multi-subject motion sequences with high diversity and fidelity from a large variety of textual prompts.
<div id='section'>Paperid: <span id='pid'>359, <a href='https://arxiv.org/pdf/2405.17306.pdf' target='_blank'>https://arxiv.org/pdf/2405.17306.pdf</a></span>   <span><a href='https://wangqiang9.github.io/Controllable.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiang Wang, Minghua Liu, Junjun Hu, Fan Jiang, Mu Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.17306">Controllable Longer Image Animation with Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating realistic animated videos from static images is an important area of research in computer vision. Methods based on physical simulation and motion prediction have achieved notable advances, but they are often limited to specific object textures and motion trajectories, failing to exhibit highly complex environments and physical dynamics. In this paper, we introduce an open-domain controllable image animation method using motion priors with video diffusion models. Our method achieves precise control over the direction and speed of motion in the movable region by extracting the motion field information from videos and learning moving trajectories and strengths. Current pretrained video generation models are typically limited to producing very short videos, typically less than 30 frames. In contrast, we propose an efficient long-duration video generation method based on noise reschedule specifically tailored for image animation tasks, facilitating the creation of videos over 100 frames in length while maintaining consistency in content scenery and motion coordination. Specifically, we decompose the denoise process into two distinct phases: the shaping of scene contours and the refining of motion details. Then we reschedule the noise to control the generated frame sequences maintaining long-distance noise correlation. We conducted extensive experiments with 10 baselines, encompassing both commercial tools and academic methodologies, which demonstrate the superiority of our method. Our project page: https://wangqiang9.github.io/Controllable.github.io/
<div id='section'>Paperid: <span id='pid'>360, <a href='https://arxiv.org/pdf/2405.17013.pdf' target='_blank'>https://arxiv.org/pdf/2405.17013.pdf</a></span>   <span><a href='https://knoxzhao.github.io/Motion-Agent' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qi Wu, Yubo Zhao, Yifan Wang, Xinhang Liu, Yu-Wing Tai, Chi-Keung Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.17013">Motion-Agent: A Conversational Framework for Human Motion Generation with LLMs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While previous approaches to 3D human motion generation have achieved notable success, they often rely on extensive training and are limited to specific tasks. To address these challenges, we introduce Motion-Agent, an efficient conversational framework designed for general human motion generation, editing, and understanding. Motion-Agent employs an open-source pre-trained language model to develop a generative agent, MotionLLM, that bridges the gap between motion and text. This is accomplished by encoding and quantizing motions into discrete tokens that align with the language model's vocabulary. With only 1--3\% of the model's parameters fine-tuned using adapters, MotionLLM delivers performance on par with diffusion models and other transformer-based methods trained from scratch. By integrating MotionLLM with GPT-4 without additional training, Motion-Agent is able to generate highly complex motion sequences through multi-turn conversations, a capability that previous models have struggled to achieve. Motion-Agent supports a wide range of motion-language tasks, offering versatile capabilities for generating and customizing human motion through interactive conversational exchanges. Project page: https://knoxzhao.github.io/Motion-Agent
<div id='section'>Paperid: <span id='pid'>361, <a href='https://arxiv.org/pdf/2405.15758.pdf' target='_blank'>https://arxiv.org/pdf/2405.15758.pdf</a></span>   <span><a href='https://wangyuchi369.github.io/InstructAvatar/' target='_blank'>  GitHub</a></span> <span><a href='https://wangyuchi369.github.io/InstructAvatar/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuchi Wang, Junliang Guo, Jianhong Bai, Runyi Yu, Tianyu He, Xu Tan, Xu Sun, Jiang Bian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.15758">InstructAvatar: Text-Guided Emotion and Motion Control for Avatar Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent talking avatar generation models have made strides in achieving realistic and accurate lip synchronization with the audio, but often fall short in controlling and conveying detailed expressions and emotions of the avatar, making the generated video less vivid and controllable. In this paper, we propose a novel text-guided approach for generating emotionally expressive 2D avatars, offering fine-grained control, improved interactivity, and generalizability to the resulting video. Our framework, named InstructAvatar, leverages a natural language interface to control the emotion as well as the facial motion of avatars. Technically, we design an automatic annotation pipeline to construct an instruction-video paired training dataset, equipped with a novel two-branch diffusion-based generator to predict avatars with audio and text instructions at the same time. Experimental results demonstrate that InstructAvatar produces results that align well with both conditions, and outperforms existing methods in fine-grained emotion control, lip-sync quality, and naturalness. Our project page is https://wangyuchi369.github.io/InstructAvatar/.
<div id='section'>Paperid: <span id='pid'>362, <a href='https://arxiv.org/pdf/2405.14864.pdf' target='_blank'>https://arxiv.org/pdf/2405.14864.pdf</a></span>   <span><a href='https://xizaoqu.github.io/moft/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zeqi Xiao, Yifan Zhou, Shuai Yang, Xingang Pan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.14864">Video Diffusion Models are Training-free Motion Interpreter and Controller</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video generation primarily aims to model authentic and customized motion across frames, making understanding and controlling the motion a crucial topic. Most diffusion-based studies on video motion focus on motion customization with training-based paradigms, which, however, demands substantial training resources and necessitates retraining for diverse models. Crucially, these approaches do not explore how video diffusion models encode cross-frame motion information in their features, lacking interpretability and transparency in their effectiveness. To answer this question, this paper introduces a novel perspective to understand, localize, and manipulate motion-aware features in video diffusion models. Through analysis using Principal Component Analysis (PCA), our work discloses that robust motion-aware feature already exists in video diffusion models. We present a new MOtion FeaTure (MOFT) by eliminating content correlation information and filtering motion channels. MOFT provides a distinct set of benefits, including the ability to encode comprehensive motion information with clear interpretability, extraction without the need for training, and generalizability across diverse architectures. Leveraging MOFT, we propose a novel training-free video motion control framework. Our method demonstrates competitive performance in generating natural and faithful motion, providing architecture-agnostic insights and applicability in a variety of downstream tasks.
<div id='section'>Paperid: <span id='pid'>363, <a href='https://arxiv.org/pdf/2405.12970.pdf' target='_blank'>https://arxiv.org/pdf/2405.12970.pdf</a></span>   <span><a href='https://faceadapter.github.io/face-adapter.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yue Han, Junwei Zhu, Keke He, Xu Chen, Yanhao Ge, Wei Li, Xiangtai Li, Jiangning Zhang, Chengjie Wang, Yong Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.12970">Face Adapter for Pre-Trained Diffusion Models with Fine-Grained ID and Attribute Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current face reenactment and swapping methods mainly rely on GAN frameworks, but recent focus has shifted to pre-trained diffusion models for their superior generation capabilities. However, training these models is resource-intensive, and the results have not yet achieved satisfactory performance levels. To address this issue, we introduce Face-Adapter, an efficient and effective adapter designed for high-precision and high-fidelity face editing for pre-trained diffusion models. We observe that both face reenactment/swapping tasks essentially involve combinations of target structure, ID and attribute. We aim to sufficiently decouple the control of these factors to achieve both tasks in one model. Specifically, our method contains: 1) A Spatial Condition Generator that provides precise landmarks and background; 2) A Plug-and-play Identity Encoder that transfers face embeddings to the text space by a transformer decoder. 3) An Attribute Controller that integrates spatial conditions and detailed attributes. Face-Adapter achieves comparable or even superior performance in terms of motion control precision, ID retention capability, and generation quality compared to fully fine-tuned face reenactment/swapping models. Additionally, Face-Adapter seamlessly integrates with various StableDiffusion models.
<div id='section'>Paperid: <span id='pid'>364, <a href='https://arxiv.org/pdf/2405.09814.pdf' target='_blank'>https://arxiv.org/pdf/2405.09814.pdf</a></span>   <span><a href='https://pku-mocca.github.io/Semantic-Gesticulator-Page' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zeyi Zhang, Tenglong Ao, Yuyao Zhang, Qingzhe Gao, Chuan Lin, Baoquan Chen, Libin Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.09814">Semantic Gesticulator: Semantics-Aware Co-Speech Gesture Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we present Semantic Gesticulator, a novel framework designed to synthesize realistic gestures accompanying speech with strong semantic correspondence. Semantically meaningful gestures are crucial for effective non-verbal communication, but such gestures often fall within the long tail of the distribution of natural human motion. The sparsity of these movements makes it challenging for deep learning-based systems, trained on moderately sized datasets, to capture the relationship between the movements and the corresponding speech semantics. To address this challenge, we develop a generative retrieval framework based on a large language model. This framework efficiently retrieves suitable semantic gesture candidates from a motion library in response to the input speech. To construct this motion library, we summarize a comprehensive list of commonly used semantic gestures based on findings in linguistics, and we collect a high-quality motion dataset encompassing both body and hand movements. We also design a novel GPT-based model with strong generalization capabilities to audio, capable of generating high-quality gestures that match the rhythm of speech. Furthermore, we propose a semantic alignment mechanism to efficiently align the retrieved semantic gestures with the GPT's output, ensuring the naturalness of the final animation. Our system demonstrates robustness in generating gestures that are rhythmically coherent and semantically explicit, as evidenced by a comprehensive collection of examples. User studies confirm the quality and human-likeness of our results, and show that our system outperforms state-of-the-art systems in terms of semantic appropriateness by a clear margin.
<div id='section'>Paperid: <span id='pid'>365, <a href='https://arxiv.org/pdf/2405.07784.pdf' target='_blank'>https://arxiv.org/pdf/2405.07784.pdf</a></span>   <span><a href='https://zju3dv.github.io/text_scene_motion' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhi Cen, Huaijin Pi, Sida Peng, Zehong Shen, Minghui Yang, Shuai Zhu, Hujun Bao, Xiaowei Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.07784">Generating Human Motion in 3D Scenes from Text Descriptions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating human motions from textual descriptions has gained growing research interest due to its wide range of applications. However, only a few works consider human-scene interactions together with text conditions, which is crucial for visual and physical realism. This paper focuses on the task of generating human motions in 3D indoor scenes given text descriptions of the human-scene interactions. This task presents challenges due to the multi-modality nature of text, scene, and motion, as well as the need for spatial reasoning. To address these challenges, we propose a new approach that decomposes the complex problem into two more manageable sub-problems: (1) language grounding of the target object and (2) object-centric motion generation. For language grounding of the target object, we leverage the power of large language models. For motion generation, we design an object-centric scene representation for the generative model to focus on the target object, thereby reducing the scene complexity and facilitating the modeling of the relationship between human motions and the object. Experiments demonstrate the better motion quality of our approach compared to baselines and validate our design choices.
<div id='section'>Paperid: <span id='pid'>366, <a href='https://arxiv.org/pdf/2405.06088.pdf' target='_blank'>https://arxiv.org/pdf/2405.06088.pdf</a></span>   <span><a href='https://github.com/edshieh/motionprediction' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Edmund Shieh, Joshua Lee Franco, Kang Min Bae, Tej Lalvani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.06088">A Mixture of Experts Approach to 3D Human Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This project addresses the challenge of human motion prediction, a critical area for applications such as au- tonomous vehicle movement detection. Previous works have emphasized the need for low inference times to provide real time performance for applications like these. Our primary objective is to critically evaluate existing model ar- chitectures, identifying their advantages and opportunities for improvement by replicating the state-of-the-art (SOTA) Spatio-Temporal Transformer model as best as possible given computational con- straints. These models have surpassed the limitations of RNN-based models and have demonstrated the ability to generate plausible motion sequences over both short and long term horizons through the use of spatio-temporal rep- resentations. We also propose a novel architecture to ad- dress challenges of real time inference speed by incorpo- rating a Mixture of Experts (MoE) block within the Spatial- Temporal (ST) attention layer. The particular variation that is used is Soft MoE, a fully-differentiable sparse Transformer that has shown promising ability to enable larger model capacity at lower inference cost. We make out code publicly available at https://github.com/edshieh/motionprediction
<div id='section'>Paperid: <span id='pid'>367, <a href='https://arxiv.org/pdf/2405.05691.pdf' target='_blank'>https://arxiv.org/pdf/2405.05691.pdf</a></span>   <span><a href='https://h-y1heng.github.io/StableMoFusion-page/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiheng Huang, Hui Yang, Chuanchen Luo, Yuxi Wang, Shibiao Xu, Zhaoxiang Zhang, Man Zhang, Junran Peng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.05691">StableMoFusion: Towards Robust and Efficient Diffusion-based Motion Generation Framework</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Thanks to the powerful generative capacity of diffusion models, recent years have witnessed rapid progress in human motion generation. Existing diffusion-based methods employ disparate network architectures and training strategies. The effect of the design of each component is still unclear. In addition, the iterative denoising process consumes considerable computational overhead, which is prohibitive for real-time scenarios such as virtual characters and humanoid robots. For this reason, we first conduct a comprehensive investigation into network architectures, training strategies, and inference processs. Based on the profound analysis, we tailor each component for efficient high-quality human motion generation. Despite the promising performance, the tailored model still suffers from foot skating which is an ubiquitous issue in diffusion-based solutions. To eliminate footskate, we identify foot-ground contact and correct foot motions along the denoising process. By organically combining these well-designed components together, we present StableMoFusion, a robust and efficient framework for human motion generation. Extensive experimental results show that our StableMoFusion performs favorably against current state-of-the-art methods. Project page: https://h-y1heng.github.io/StableMoFusion-page/
<div id='section'>Paperid: <span id='pid'>368, <a href='https://arxiv.org/pdf/2405.04370.pdf' target='_blank'>https://arxiv.org/pdf/2405.04370.pdf</a></span>   <span><a href='https://github.com/IRMVLab/Diff-IP2D' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Junyi Ma, Jingyi Xu, Xieyuanli Chen, Hesheng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.04370">Diff-IP2D: Diffusion-Based Hand-Object Interaction Prediction on Egocentric Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding how humans would behave during hand-object interaction is vital for applications in service robot manipulation and extended reality. To achieve this, some recent works have been proposed to simultaneously forecast hand trajectories and object affordances on human egocentric videos. The joint prediction serves as a comprehensive representation of future hand-object interactions in 2D space, indicating potential human motion and motivation. However, the existing approaches mostly adopt the autoregressive paradigm for unidirectional prediction, which lacks mutual constraints within the holistic future sequence, and accumulates errors along the time axis. Meanwhile, these works basically overlook the effect of camera egomotion on first-person view predictions. To address these limitations, we propose a novel diffusion-based interaction prediction method, namely Diff-IP2D, to forecast future hand trajectories and object affordances concurrently in an iterative non-autoregressive manner. We transform the sequential 2D images into latent feature space and design a denoising diffusion model to predict future latent interaction features conditioned on past ones. Motion features are further integrated into the conditional denoising process to enable Diff-IP2D aware of the camera wearer's dynamics for more accurate interaction prediction. Extensive experiments demonstrate that our method significantly outperforms the state-of-the-art baselines on both the off-the-shelf metrics and our newly proposed evaluation protocol. This highlights the efficacy of leveraging a generative paradigm for 2D hand-object interaction prediction. The code of Diff-IP2D is released as open source at https://github.com/IRMVLab/Diff-IP2D.
<div id='section'>Paperid: <span id='pid'>369, <a href='https://arxiv.org/pdf/2405.03485.pdf' target='_blank'>https://arxiv.org/pdf/2405.03485.pdf</a></span>   <span><a href='https://github.com/L-Sun/LGTM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haowen Sun, Ruikun Zheng, Haibin Huang, Chongyang Ma, Hui Huang, Ruizhen Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.03485">LGTM: Local-to-Global Text-Driven Human Motion Diffusion Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we introduce LGTM, a novel Local-to-Global pipeline for Text-to-Motion generation. LGTM utilizes a diffusion-based architecture and aims to address the challenge of accurately translating textual descriptions into semantically coherent human motion in computer animation. Specifically, traditional methods often struggle with semantic discrepancies, particularly in aligning specific motions to the correct body parts. To address this issue, we propose a two-stage pipeline to overcome this challenge: it first employs large language models (LLMs) to decompose global motion descriptions into part-specific narratives, which are then processed by independent body-part motion encoders to ensure precise local semantic alignment. Finally, an attention-based full-body optimizer refines the motion generation results and guarantees the overall coherence. Our experiments demonstrate that LGTM gains significant improvements in generating locally accurate, semantically-aligned human motion, marking a notable advancement in text-to-motion applications. Code and data for this paper are available at https://github.com/L-Sun/LGTM
<div id='section'>Paperid: <span id='pid'>370, <a href='https://arxiv.org/pdf/2405.01434.pdf' target='_blank'>https://arxiv.org/pdf/2405.01434.pdf</a></span>   <span><a href='https://github.com/HVision-NKU/StoryDiffusion' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yupeng Zhou, Daquan Zhou, Ming-Ming Cheng, Jiashi Feng, Qibin Hou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.01434">StoryDiffusion: Consistent Self-Attention for Long-Range Image and Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>For recent diffusion-based generative models, maintaining consistent content across a series of generated images, especially those containing subjects and complex details, presents a significant challenge. In this paper, we propose a new way of self-attention calculation, termed Consistent Self-Attention, that significantly boosts the consistency between the generated images and augments prevalent pretrained diffusion-based text-to-image models in a zero-shot manner. To extend our method to long-range video generation, we further introduce a novel semantic space temporal motion prediction module, named Semantic Motion Predictor. It is trained to estimate the motion conditions between two provided images in the semantic spaces. This module converts the generated sequence of images into videos with smooth transitions and consistent subjects that are significantly more stable than the modules based on latent spaces only, especially in the context of long video generation. By merging these two novel components, our framework, referred to as StoryDiffusion, can describe a text-based story with consistent images or videos encompassing a rich variety of contents. The proposed StoryDiffusion encompasses pioneering explorations in visual story generation with the presentation of images and videos, which we hope could inspire more research from the aspect of architectural modifications. Our code is made publicly available at https://github.com/HVision-NKU/StoryDiffusion.
<div id='section'>Paperid: <span id='pid'>371, <a href='https://arxiv.org/pdf/2404.19722.pdf' target='_blank'>https://arxiv.org/pdf/2404.19722.pdf</a></span>   <span><a href='https://wangjingbo1219.github.io/papers/CVPR2024_PACER_PLUS/PACERPLUSPage.html' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingbo Wang, Zhengyi Luo, Ye Yuan, Yixuan Li, Bo Dai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.19722">PACER+: On-Demand Pedestrian Animation Controller in Driving Scenarios</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We address the challenge of content diversity and controllability in pedestrian simulation for driving scenarios. Recent pedestrian animation frameworks have a significant limitation wherein they primarily focus on either following trajectory [46] or the content of the reference video [57], consequently overlooking the potential diversity of human motion within such scenarios. This limitation restricts the ability to generate pedestrian behaviors that exhibit a wider range of variations and realistic motions and therefore restricts its usage to provide rich motion content for other components in the driving simulation system, e.g., suddenly changed motion to which the autonomous vehicle should respond. In our approach, we strive to surpass the limitation by showcasing diverse human motions obtained from various sources, such as generated human motions, in addition to following the given trajectory. The fundamental contribution of our framework lies in combining the motion tracking task with trajectory following, which enables the tracking of specific motion parts (e.g., upper body) while simultaneously following the given trajectory by a single policy. This way, we significantly enhance both the diversity of simulated human motion within the given scenario and the controllability of the content, including language-based control. Our framework facilitates the generation of a wide range of human motions, contributing to greater realism and adaptability in pedestrian simulations for driving scenarios. More information is on our project page https://wangjingbo1219.github.io/papers/CVPR2024_PACER_PLUS/PACERPLUSPage.html .
<div id='section'>Paperid: <span id='pid'>372, <a href='https://arxiv.org/pdf/2404.19619.pdf' target='_blank'>https://arxiv.org/pdf/2404.19619.pdf</a></span>   <span><a href='https://xinyu-yi.github.io/PNP/' target='_blank'>  GitHub</a></span> <span><a href='https://xinyu-yi.github.io/PNP/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinyu Yi, Yuxiao Zhou, Feng Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.19619">Physical Non-inertial Poser (PNP): Modeling Non-inertial Effects in Sparse-inertial Human Motion Capture</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing inertial motion capture techniques use the human root coordinate frame to estimate local poses and treat it as an inertial frame by default. We argue that when the root has linear acceleration or rotation, the root frame should be considered non-inertial theoretically. In this paper, we model the fictitious forces that are non-neglectable in a non-inertial frame by an auto-regressive estimator delicately designed following physics. With the fictitious forces, the force-related IMU measurement (accelerations) can be correctly compensated in the non-inertial frame and thus Newton's laws of motion are satisfied. In this case, the relationship between the accelerations and body motions is deterministic and learnable, and we train a neural network to model it for better motion capture. Furthermore, to train the neural network with synthetic data, we develop an IMU synthesis by simulation strategy to better model the noise model of IMU hardware and allow parameter tuning to fit different hardware. This strategy not only establishes the network training with synthetic data but also enables calibration error modeling to handle bad motion capture calibration, increasing the robustness of the system. Code is available at https://xinyu-yi.github.io/PNP/.
<div id='section'>Paperid: <span id='pid'>373, <a href='https://arxiv.org/pdf/2404.19541.pdf' target='_blank'>https://arxiv.org/pdf/2404.19541.pdf</a></span>   <span><a href='https://github.com/eth-siplab/UltraInertialPoser' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Rayan Armani, Changlin Qian, Jiaxi Jiang, Christian Holz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.19541">Ultra Inertial Poser: Scalable Motion Capture and Tracking from Sparse Inertial Sensors and Ultra-Wideband Ranging</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While camera-based capture systems remain the gold standard for recording human motion, learning-based tracking systems based on sparse wearable sensors are gaining popularity. Most commonly, they use inertial sensors, whose propensity for drift and jitter have so far limited tracking accuracy. In this paper, we propose Ultra Inertial Poser, a novel 3D full body pose estimation method that constrains drift and jitter in inertial tracking via inter-sensor distances. We estimate these distances across sparse sensor setups using a lightweight embedded tracker that augments inexpensive off-the-shelf 6D inertial measurement units with ultra-wideband radio-based ranging$-$dynamically and without the need for stationary reference anchors. Our method then fuses these inter-sensor distances with the 3D states estimated from each sensor Our graph-based machine learning model processes the 3D states and distances to estimate a person's 3D full body pose and translation. To train our model, we synthesize inertial measurements and distance estimates from the motion capture database AMASS. For evaluation, we contribute a novel motion dataset of 10 participants who performed 25 motion types, captured by 6 wearable IMU+UWB trackers and an optical motion capture system, totaling 200 minutes of synchronized sensor data (UIP-DB). Our extensive experiments show state-of-the-art performance for our method over PIP and TIP, reducing position error from $13.62$ to $10.65cm$ ($22\%$ better) and lowering jitter from $1.56$ to $0.055km/s^3$ (a reduction of $97\%$).
<div id='section'>Paperid: <span id='pid'>374, <a href='https://arxiv.org/pdf/2404.17269.pdf' target='_blank'>https://arxiv.org/pdf/2404.17269.pdf</a></span>   <span><a href='https://github.com/cztuda/semantic-feature-clustering' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Christoph Zelch, Jan Peters, Oskar von Stryk
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.17269">Clustering of Motion Trajectories by a Distance Measure Based on Semantic Features</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Clustering of motion trajectories is highly relevant for human-robot interactions as it allows the anticipation of human motions, fast reaction to those, as well as the recognition of explicit gestures. Further, it allows automated analysis of recorded motion data. Many clustering algorithms for trajectories build upon distance metrics that are based on pointwise Euclidean distances. However, our work indicates that focusing on salient characteristics is often sufficient. We present a novel distance measure for motion plans consisting of state and control trajectories that is based on a compressed representation built from their main features. This approach allows a flexible choice of feature classes relevant to the respective task. The distance measure is used in agglomerative hierarchical clustering. We compare our method with the widely used dynamic time warping algorithm on test sets of motion plans for the Furuta pendulum and the Manutec robot arm and on real-world data from a human motion dataset. The proposed method demonstrates slight advantages in clustering and strong advantages in runtime, especially for long trajectories.
<div id='section'>Paperid: <span id='pid'>375, <a href='https://arxiv.org/pdf/2404.13657.pdf' target='_blank'>https://arxiv.org/pdf/2404.13657.pdf</a></span>   <span><a href='https://github.com/eanson023/mlp' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sheng Yan, Mengyuan Liu, Yong Wang, Yang Liu, Chen Chen, Hong Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.13657">MLP: Motion Label Prior for Temporal Sentence Localization in Untrimmed 3D Human Motions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we address the unexplored question of temporal sentence localization in human motions (TSLM), aiming to locate a target moment from a 3D human motion that semantically corresponds to a text query. Considering that 3D human motions are captured using specialized motion capture devices, motions with only a few joints lack complex scene information like objects and lighting. Due to this character, motion data has low contextual richness and semantic ambiguity between frames, which limits the accuracy of predictions made by current video localization frameworks extended to TSLM to only a rough level. To refine this, we devise two novel label-prior-assisted training schemes: one embed prior knowledge of foreground and background to highlight the localization chances of target moments, and the other forces the originally rough predictions to overlap with the more accurate predictions obtained from the flipped start/end prior label sequences during recovery training. We show that injecting label-prior knowledge into the model is crucial for improving performance at high IoU. In our constructed TSLM benchmark, our model termed MLP achieves a recall of 44.13 at IoU@0.7 on the BABEL dataset and 71.17 on HumanML3D (Restore), outperforming prior works. Finally, we showcase the potential of our approach in corpus-level moment retrieval. Our source code is openly accessible at https://github.com/eanson023/mlp.
<div id='section'>Paperid: <span id='pid'>376, <a href='https://arxiv.org/pdf/2404.13026.pdf' target='_blank'>https://arxiv.org/pdf/2404.13026.pdf</a></span>   <span><a href='https://physdreamer.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianyuan Zhang, Hong-Xing Yu, Rundi Wu, Brandon Y. Feng, Changxi Zheng, Noah Snavely, Jiajun Wu, William T. Freeman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.13026">PhysDreamer: Physics-Based Interaction with 3D Objects via Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Realistic object interactions are crucial for creating immersive virtual experiences, yet synthesizing realistic 3D object dynamics in response to novel interactions remains a significant challenge. Unlike unconditional or text-conditioned dynamics generation, action-conditioned dynamics requires perceiving the physical material properties of objects and grounding the 3D motion prediction on these properties, such as object stiffness. However, estimating physical material properties is an open problem due to the lack of material ground-truth data, as measuring these properties for real objects is highly difficult. We present PhysDreamer, a physics-based approach that endows static 3D objects with interactive dynamics by leveraging the object dynamics priors learned by video generation models. By distilling these priors, PhysDreamer enables the synthesis of realistic object responses to novel interactions, such as external forces or agent manipulations. We demonstrate our approach on diverse examples of elastic objects and evaluate the realism of the synthesized interactions through a user study. PhysDreamer takes a step towards more engaging and realistic virtual experiences by enabling static 3D objects to dynamically respond to interactive stimuli in a physically plausible manner. See our project page at https://physdreamer.github.io/.
<div id='section'>Paperid: <span id='pid'>377, <a href='https://arxiv.org/pdf/2404.12867.pdf' target='_blank'>https://arxiv.org/pdf/2404.12867.pdf</a></span>   <span><a href='https://github.com/TabGuigui/FipTR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xingtai Gui, Tengteng Huang, Haonan Shao, Haotian Yao, Chi Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.12867">FipTR: A Simple yet Effective Transformer Framework for Future Instance Prediction in Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The future instance prediction from a Bird's Eye View(BEV) perspective is a vital component in autonomous driving, which involves future instance segmentation and instance motion prediction. Existing methods usually rely on a redundant and complex pipeline which requires multiple auxiliary outputs and post-processing procedures. Moreover, estimated errors on each of the auxiliary predictions will lead to degradation of the prediction performance. In this paper, we propose a simple yet effective fully end-to-end framework named Future Instance Prediction Transformer(FipTR), which views the task as BEV instance segmentation and prediction for future frames. We propose to adopt instance queries representing specific traffic participants to directly estimate the corresponding future occupied masks, and thus get rid of complex post-processing procedures. Besides, we devise a flow-aware BEV predictor for future BEV feature prediction composed of a flow-aware deformable attention that takes backward flow guiding the offset sampling. A novel future instance matching strategy is also proposed to further improve the temporal coherence. Extensive experiments demonstrate the superiority of FipTR and its effectiveness under different temporal BEV encoders. The code is available at https://github.com/TabGuigui/FipTR .
<div id='section'>Paperid: <span id='pid'>378, <a href='https://arxiv.org/pdf/2404.11327.pdf' target='_blank'>https://arxiv.org/pdf/2404.11327.pdf</a></span>   <span><a href='https://github.com/L-Scofano/SDA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Luca Scofano, Alessio Sampieri, Tommaso Campari, Valentino Sacco, Indro Spinelli, Lamberto Ballan, Fabio Galasso
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.11327">Following the Human Thread in Social Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The success of collaboration between humans and robots in shared environments relies on the robot's real-time adaptation to human motion. Specifically, in Social Navigation, the agent should be close enough to assist but ready to back up to let the human move freely, avoiding collisions. Human trajectories emerge as crucial cues in Social Navigation, but they are partially observable from the robot's egocentric view and computationally complex to process.
  We present the first Social Dynamics Adaptation model (SDA) based on the robot's state-action history to infer the social dynamics. We propose a two-stage Reinforcement Learning framework: the first learns to encode the human trajectories into social dynamics and learns a motion policy conditioned on this encoded information, the current status, and the previous action. Here, the trajectories are fully visible, i.e., assumed as privileged information. In the second stage, the trained policy operates without direct access to trajectories. Instead, the model infers the social dynamics solely from the history of previous actions and statuses in real-time. Tested on the novel Habitat 3.0 platform, SDA sets a novel state-of-the-art (SotA) performance in finding and following humans.
  The code can be found at https://github.com/L-Scofano/SDA.
<div id='section'>Paperid: <span id='pid'>379, <a href='https://arxiv.org/pdf/2404.10879.pdf' target='_blank'>https://arxiv.org/pdf/2404.10879.pdf</a></span>   <span><a href='https://github.com/TUMFTM/FlexMap_Fusion' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Maximilian Leitenstern, Florian Sauerbeck, Dominik Kulmer, Johannes Betz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.10879">FlexMap Fusion: Georeferencing and Automated Conflation of HD Maps with OpenStreetMap</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Today's software stacks for autonomous vehicles rely on HD maps to enable sufficient localization, accurate path planning, and reliable motion prediction. Recent developments have resulted in pipelines for the automated generation of HD maps to reduce manual efforts for creating and updating these HD maps. We present FlexMap Fusion, a methodology to automatically update and enhance existing HD vector maps using OpenStreetMap. Our approach is designed to enable the use of HD maps created from LiDAR and camera data within Autoware. The pipeline provides different functionalities: It provides the possibility to georeference both the point cloud map and the vector map using an RTK-corrected GNSS signal. Moreover, missing semantic attributes can be conflated from OpenStreetMap into the vector map. Differences between the HD map and OpenStreetMap are visualized for manual refinement by the user. In general, our findings indicate that our approach leads to reduced human labor during HD map generation, increases the scalability of the mapping pipeline, and improves the completeness and usability of the maps. The methodological choices may have resulted in limitations that arise especially at complex street structures, e.g., traffic islands. Therefore, more research is necessary to create efficient preprocessing algorithms and advancements in the dynamic adjustment of matching parameters. In order to build upon our work, our source code is available at https://github.com/TUMFTM/FlexMap_Fusion.
<div id='section'>Paperid: <span id='pid'>380, <a href='https://arxiv.org/pdf/2404.09988.pdf' target='_blank'>https://arxiv.org/pdf/2404.09988.pdf</a></span>   <span><a href='https://pabloruizponce.github.io/in2IN/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Pablo Ruiz Ponce, German Barquero, Cristina Palmero, Sergio Escalera, Jose Garcia-Rodriguez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.09988">in2IN: Leveraging individual Information to Generate Human INteractions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating human-human motion interactions conditioned on textual descriptions is a very useful application in many areas such as robotics, gaming, animation, and the metaverse. Alongside this utility also comes a great difficulty in modeling the highly dimensional inter-personal dynamics. In addition, properly capturing the intra-personal diversity of interactions has a lot of challenges. Current methods generate interactions with limited diversity of intra-person dynamics due to the limitations of the available datasets and conditioning strategies. For this, we introduce in2IN, a novel diffusion model for human-human motion generation which is conditioned not only on the textual description of the overall interaction but also on the individual descriptions of the actions performed by each person involved in the interaction. To train this model, we use a large language model to extend the InterHuman dataset with individual descriptions. As a result, in2IN achieves state-of-the-art performance in the InterHuman dataset. Furthermore, in order to increase the intra-personal diversity on the existing interaction datasets, we propose DualMDM, a model composition technique that combines the motions generated with in2IN and the motions generated by a single-person motion prior pre-trained on HumanML3D. As a result, DualMDM generates motions with higher individual diversity and improves control over the intra-person dynamics while maintaining inter-personal coherence.
<div id='section'>Paperid: <span id='pid'>381, <a href='https://arxiv.org/pdf/2404.09445.pdf' target='_blank'>https://arxiv.org/pdf/2404.09445.pdf</a></span>   <span><a href='https://github.com/THU-LYJ-Lab/InstructMotion' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jenny Sheng, Matthieu Lin, Andrew Zhao, Kevin Pruvost, Yu-Hui Wen, Yangguang Li, Gao Huang, Yong-Jin Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.09445">Exploring Text-to-Motion Generation with Human Preference</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents an exploration of preference learning in text-to-motion generation. We find that current improvements in text-to-motion generation still rely on datasets requiring expert labelers with motion capture systems. Instead, learning from human preference data does not require motion capture systems; a labeler with no expertise simply compares two generated motions. This is particularly efficient because evaluating the model's output is easier than gathering the motion that performs a desired task (e.g. backflip). To pioneer the exploration of this paradigm, we annotate 3,528 preference pairs generated by MotionGPT, marking the first effort to investigate various algorithms for learning from preference data. In particular, our exploration highlights important design choices when using preference data. Additionally, our experimental results show that preference learning has the potential to greatly improve current text-to-motion generative models. Our code and dataset are publicly available at https://github.com/THU-LYJ-Lab/InstructMotion}{https://github.com/THU-LYJ-Lab/InstructMotion to further facilitate research in this area.
<div id='section'>Paperid: <span id='pid'>382, <a href='https://arxiv.org/pdf/2404.05218.pdf' target='_blank'>https://arxiv.org/pdf/2404.05218.pdf</a></span>   <span><a href='https://github.com/Jaewoo97/T2P' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jaewoo Jeong, Daehee Park, Kuk-Jin Yoon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.05218">Multi-agent Long-term 3D Human Pose Forecasting via Interaction-aware Trajectory Conditioning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human pose forecasting garners attention for its diverse applications. However, challenges in modeling the multi-modal nature of human motion and intricate interactions among agents persist, particularly with longer timescales and more agents. In this paper, we propose an interaction-aware trajectory-conditioned long-term multi-agent human pose forecasting model, utilizing a coarse-to-fine prediction approach: multi-modal global trajectories are initially forecasted, followed by respective local pose forecasts conditioned on each mode. In doing so, our Trajectory2Pose model introduces a graph-based agent-wise interaction module for a reciprocal forecast of local motion-conditioned global trajectory and trajectory-conditioned local pose. Our model effectively handles the multi-modality of human motion and the complexity of long-term multi-agent interactions, improving performance in complex environments. Furthermore, we address the lack of long-term (6s+) multi-agent (5+) datasets by constructing a new dataset from real-world images and 2D annotations, enabling a comprehensive evaluation of our proposed model. State-of-the-art prediction performance on both complex and simpler datasets confirms the generalized effectiveness of our method. The code is available at https://github.com/Jaewoo97/T2P.
<div id='section'>Paperid: <span id='pid'>383, <a href='https://arxiv.org/pdf/2404.03790.pdf' target='_blank'>https://arxiv.org/pdf/2404.03790.pdf</a></span>   <span><a href='https://github.com/stevens-armlab/uvms_bimanual_sim' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Justin Sitler, Srikarran Sowrirajan, Brendan Englot, Long Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.03790">A Bimanual Teleoperation Framework for Light Duty Underwater Vehicle-Manipulator Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In an effort to lower the barrier to entry in underwater manipulation, this paper presents an open-source, user-friendly framework for bimanual teleoperation of a light-duty underwater vehicle-manipulator system (UVMS). This framework allows for the control of the vehicle along with two manipulators and their end-effectors using two low-cost haptic devices.
  The UVMS kinematics are derived in order to create an independent resolved motion rate controller for each manipulator, which optimally controls the joint positions to achieve a desired end-effector pose. This desired pose is computed in real-time using a teleoperation controller developed to process the dual haptic device input from the user. A physics-based simulation environment is used to implement this framework for two example tasks as well as provide data for error analysis of user commands. The first task illustrates the functionality of the framework through motion control of the vehicle and manipulators using only the haptic devices. The second task is to grasp an object using both manipulators simultaneously, demonstrating precision and coordination using the framework. The framework code is available at https://github.com/stevens-armlab/uvms_bimanual_sim.
<div id='section'>Paperid: <span id='pid'>384, <a href='https://arxiv.org/pdf/2404.03789.pdf' target='_blank'>https://arxiv.org/pdf/2404.03789.pdf</a></span>   <span><a href='https://github.com/PurdueDigitalTwin/seneva' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Juanwu Lu, Can Cui, Yunsheng Ma, Aniket Bera, Ziran Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.03789">Quantifying Uncertainty in Motion Prediction with Variational Bayesian Mixture</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Safety and robustness are crucial factors in developing trustworthy autonomous vehicles. One essential aspect of addressing these factors is to equip vehicles with the capability to predict future trajectories for all moving objects in the surroundings and quantify prediction uncertainties. In this paper, we propose the Sequential Neural Variational Agent (SeNeVA), a generative model that describes the distribution of future trajectories for a single moving object. Our approach can distinguish Out-of-Distribution data while quantifying uncertainty and achieving competitive performance compared to state-of-the-art methods on the Argoverse 2 and INTERACTION datasets. Specifically, a 0.446 meters minimum Final Displacement Error, a 0.203 meters minimum Average Displacement Error, and a 5.35% Miss Rate are achieved on the INTERACTION test set. Extensive qualitative and quantitative analysis is also provided to evaluate the proposed model. Our open-source code is available at https://github.com/PurdueDigitalTwin/seneva.
<div id='section'>Paperid: <span id='pid'>385, <a href='https://arxiv.org/pdf/2404.03736.pdf' target='_blank'>https://arxiv.org/pdf/2404.03736.pdf</a></span>   <span><a href='https://github.com/JarrentWu1031/SC4D' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zijie Wu, Chaohui Yu, Yanqin Jiang, Chenjie Cao, Fan Wang, Xiang Bai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.03736">SC4D: Sparse-Controlled Video-to-4D Generation and Motion Transfer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in 2D/3D generative models enable the generation of dynamic 3D objects from a single-view video. Existing approaches utilize score distillation sampling to form the dynamic scene as dynamic NeRF or dense 3D Gaussians. However, these methods struggle to strike a balance among reference view alignment, spatio-temporal consistency, and motion fidelity under single-view conditions due to the implicit nature of NeRF or the intricate dense Gaussian motion prediction. To address these issues, this paper proposes an efficient, sparse-controlled video-to-4D framework named SC4D, that decouples motion and appearance to achieve superior video-to-4D generation. Moreover, we introduce Adaptive Gaussian (AG) initialization and Gaussian Alignment (GA) loss to mitigate shape degeneration issue, ensuring the fidelity of the learned motion and shape. Comprehensive experimental results demonstrate that our method surpasses existing methods in both quality and efficiency. In addition, facilitated by the disentangled modeling of motion and appearance of SC4D, we devise a novel application that seamlessly transfers the learned motion onto a diverse array of 4D entities according to textual descriptions.
<div id='section'>Paperid: <span id='pid'>386, <a href='https://arxiv.org/pdf/2404.01284.pdf' target='_blank'>https://arxiv.org/pdf/2404.01284.pdf</a></span>   <span><a href='https://mingyuan-zhang.github.io/projects/LMM.html' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingyuan Zhang, Daisheng Jin, Chenyang Gu, Fangzhou Hong, Zhongang Cai, Jingfang Huang, Chongzhi Zhang, Xinying Guo, Lei Yang, Ying He, Ziwei Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.01284">Large Motion Model for Unified Multi-Modal Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion generation, a cornerstone technique in animation and video production, has widespread applications in various tasks like text-to-motion and music-to-dance. Previous works focus on developing specialist models tailored for each task without scalability. In this work, we present Large Motion Model (LMM), a motion-centric, multi-modal framework that unifies mainstream motion generation tasks into a generalist model. A unified motion model is appealing since it can leverage a wide range of motion data to achieve broad generalization beyond a single task. However, it is also challenging due to the heterogeneous nature of substantially different motion data and tasks. LMM tackles these challenges from three principled aspects: 1) Data: We consolidate datasets with different modalities, formats and tasks into a comprehensive yet unified motion generation dataset, MotionVerse, comprising 10 tasks, 16 datasets, a total of 320k sequences, and 100 million frames. 2) Architecture: We design an articulated attention mechanism ArtAttention that incorporates body part-aware modeling into Diffusion Transformer backbone. 3) Pre-Training: We propose a novel pre-training strategy for LMM, which employs variable frame rates and masking forms, to better exploit knowledge from diverse training data. Extensive experiments demonstrate that our generalist LMM achieves competitive performance across various standard motion generation tasks over state-of-the-art specialist models. Notably, LMM exhibits strong generalization capabilities and emerging properties across many unseen tasks. Additionally, our ablation studies reveal valuable insights about training and scaling up large motion models for future research.
<div id='section'>Paperid: <span id='pid'>387, <a href='https://arxiv.org/pdf/2404.01225.pdf' target='_blank'>https://arxiv.org/pdf/2404.01225.pdf</a></span>   <span><a href='https://taohuumd.github.io/projects/SurMo/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tao Hu, Fangzhou Hong, Ziwei Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.01225">SurMo: Surface-based 4D Motion Modeling for Dynamic Human Rendering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dynamic human rendering from video sequences has achieved remarkable progress by formulating the rendering as a mapping from static poses to human images. However, existing methods focus on the human appearance reconstruction of every single frame while the temporal motion relations are not fully explored. In this paper, we propose a new 4D motion modeling paradigm, SurMo, that jointly models the temporal dynamics and human appearances in a unified framework with three key designs: 1) Surface-based motion encoding that models 4D human motions with an efficient compact surface-based triplane. It encodes both spatial and temporal motion relations on the dense surface manifold of a statistical body template, which inherits body topology priors for generalizable novel view synthesis with sparse training observations. 2) Physical motion decoding that is designed to encourage physical motion learning by decoding the motion triplane features at timestep t to predict both spatial derivatives and temporal derivatives at the next timestep t+1 in the training stage. 3) 4D appearance decoding that renders the motion triplanes into images by an efficient volumetric surface-conditioned renderer that focuses on the rendering of body surfaces with motion learning conditioning. Extensive experiments validate the state-of-the-art performance of our new paradigm and illustrate the expressiveness of surface-based motion triplanes for rendering high-fidelity view-consistent humans with fast motions and even motion-dependent shadows. Our project page is at: https://taohuumd.github.io/projects/SurMo/
<div id='section'>Paperid: <span id='pid'>388, <a href='https://arxiv.org/pdf/2403.19652.pdf' target='_blank'>https://arxiv.org/pdf/2403.19652.pdf</a></span>   <span><a href='https://sirui-xu.github.io/InterDreamer/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sirui Xu, Ziyin Wang, Yu-Xiong Wang, Liang-Yan Gui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.19652">InterDreamer: Zero-Shot Text to 3D Dynamic Human-Object Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-conditioned human motion generation has experienced significant advancements with diffusion models trained on extensive motion capture data and corresponding textual annotations. However, extending such success to 3D dynamic human-object interaction (HOI) generation faces notable challenges, primarily due to the lack of large-scale interaction data and comprehensive descriptions that align with these interactions. This paper takes the initiative and showcases the potential of generating human-object interactions without direct training on text-interaction pair data. Our key insight in achieving this is that interaction semantics and dynamics can be decoupled. Being unable to learn interaction semantics through supervised training, we instead leverage pre-trained large models, synergizing knowledge from a large language model and a text-to-motion model. While such knowledge offers high-level control over interaction semantics, it cannot grasp the intricacies of low-level interaction dynamics. To overcome this issue, we further introduce a world model designed to comprehend simple physics, modeling how human actions influence object motion. By integrating these components, our novel framework, InterDreamer, is able to generate text-aligned 3D HOI sequences in a zero-shot manner. We apply InterDreamer to the BEHAVE and CHAIRS datasets, and our comprehensive experimental analysis demonstrates its capability to generate realistic and coherent interaction sequences that seamlessly align with the text directives.
<div id='section'>Paperid: <span id='pid'>389, <a href='https://arxiv.org/pdf/2403.19435.pdf' target='_blank'>https://arxiv.org/pdf/2403.19435.pdf</a></span>   <span><a href='https://exitudio.github.io/BAMM-page' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ekkasit Pinyoanuntapong, Muhammad Usama Saleem, Pu Wang, Minwoo Lee, Srijan Das, Chen Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.19435">BAMM: Bidirectional Autoregressive Motion Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating human motion from text has been dominated by denoising motion models either through diffusion or generative masking process. However, these models face great limitations in usability by requiring prior knowledge of the motion length. Conversely, autoregressive motion models address this limitation by adaptively predicting motion endpoints, at the cost of degraded generation quality and editing capabilities. To address these challenges, we propose Bidirectional Autoregressive Motion Model (BAMM), a novel text-to-motion generation framework. BAMM consists of two key components: (1) a motion tokenizer that transforms 3D human motion into discrete tokens in latent space, and (2) a masked self-attention transformer that autoregressively predicts randomly masked tokens via a hybrid attention masking strategy. By unifying generative masked modeling and autoregressive modeling, BAMM captures rich and bidirectional dependencies among motion tokens, while learning the probabilistic mapping from textual inputs to motion outputs with dynamically-adjusted motion sequence length. This feature enables BAMM to simultaneously achieving high-quality motion generation with enhanced usability and built-in motion editability. Extensive experiments on HumanML3D and KIT-ML datasets demonstrate that BAMM surpasses current state-of-the-art methods in both qualitative and quantitative measures. Our project page is available at https://exitudio.github.io/BAMM-page
<div id='section'>Paperid: <span id='pid'>390, <a href='https://arxiv.org/pdf/2403.18512.pdf' target='_blank'>https://arxiv.org/pdf/2403.18512.pdf</a></span>   <span><a href='https://github.com/qrzou/ParCo' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiran Zou, Shangyuan Yuan, Shian Du, Yu Wang, Chang Liu, Yi Xu, Jie Chen, Xiangyang Ji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.18512">ParCo: Part-Coordinating Text-to-Motion Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We study a challenging task: text-to-motion synthesis, aiming to generate motions that align with textual descriptions and exhibit coordinated movements. Currently, the part-based methods introduce part partition into the motion synthesis process to achieve finer-grained generation. However, these methods encounter challenges such as the lack of coordination between different part motions and difficulties for networks to understand part concepts. Moreover, introducing finer-grained part concepts poses computational complexity challenges. In this paper, we propose Part-Coordinating Text-to-Motion Synthesis (ParCo), endowed with enhanced capabilities for understanding part motions and communication among different part motion generators, ensuring a coordinated and fined-grained motion synthesis. Specifically, we discretize whole-body motion into multiple part motions to establish the prior concept of different parts. Afterward, we employ multiple lightweight generators designed to synthesize different part motions and coordinate them through our part coordination module. Our approach demonstrates superior performance on common benchmarks with economic computations, including HumanML3D and KIT-ML, providing substantial evidence of its effectiveness. Code is available at https://github.com/qrzou/ParCo .
<div id='section'>Paperid: <span id='pid'>391, <a href='https://arxiv.org/pdf/2403.17694.pdf' target='_blank'>https://arxiv.org/pdf/2403.17694.pdf</a></span>   <span><a href='https://github.com/scutzzj/AniPortrait' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Huawei Wei, Zejun Yang, Zhisheng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.17694">AniPortrait: Audio-Driven Synthesis of Photorealistic Portrait Animation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this study, we propose AniPortrait, a novel framework for generating high-quality animation driven by audio and a reference portrait image. Our methodology is divided into two stages. Initially, we extract 3D intermediate representations from audio and project them into a sequence of 2D facial landmarks. Subsequently, we employ a robust diffusion model, coupled with a motion module, to convert the landmark sequence into photorealistic and temporally consistent portrait animation. Experimental results demonstrate the superiority of AniPortrait in terms of facial naturalness, pose diversity, and visual quality, thereby offering an enhanced perceptual experience. Moreover, our methodology exhibits considerable potential in terms of flexibility and controllability, which can be effectively applied in areas such as facial motion editing or face reenactment. We release code and model weights at https://github.com/scutzzj/AniPortrait
<div id='section'>Paperid: <span id='pid'>392, <a href='https://arxiv.org/pdf/2403.17610.pdf' target='_blank'>https://arxiv.org/pdf/2403.17610.pdf</a></span>   <span><a href='https://metaverse-ai-lab-thu.github.io/MMVP-Dataset/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>He Zhang, Shenghao Ren, Haolei Yuan, Jianhui Zhao, Fan Li, Shuangpeng Sun, Zhenghao Liang, Tao Yu, Qiu Shen, Xun Cao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.17610">MMVP: A Multimodal MoCap Dataset with Vision and Pressure Sensors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Foot contact is an important cue for human motion capture, understanding, and generation. Existing datasets tend to annotate dense foot contact using visual matching with thresholding or incorporating pressure signals. However, these approaches either suffer from low accuracy or are only designed for small-range and slow motion. There is still a lack of a vision-pressure multimodal dataset with large-range and fast human motion, as well as accurate and dense foot-contact annotation. To fill this gap, we propose a Multimodal MoCap Dataset with Vision and Pressure sensors, named MMVP. MMVP provides accurate and dense plantar pressure signals synchronized with RGBD observations, which is especially useful for both plausible shape estimation, robust pose fitting without foot drifting, and accurate global translation tracking. To validate the dataset, we propose an RGBD-P SMPL fitting method and also a monocular-video-based baseline framework, VP-MoCap, for human motion capture. Experiments demonstrate that our RGBD-P SMPL Fitting results significantly outperform pure visual motion capture. Moreover, VP-MoCap outperforms SOTA methods in foot-contact and global translation estimation accuracy. We believe the configuration of the dataset and the baseline frameworks will stimulate the research in this direction and also provide a good reference for MoCap applications in various domains. Project page: https://metaverse-ai-lab-thu.github.io/MMVP-Dataset/.
<div id='section'>Paperid: <span id='pid'>393, <a href='https://arxiv.org/pdf/2403.15709.pdf' target='_blank'>https://arxiv.org/pdf/2403.15709.pdf</a></span>   <span><a href='https://xymsh.github.io/RICH-CAT/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sihan Ma, Qiong Cao, Jing Zhang, Dacheng Tao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.15709">Contact-aware Human Motion Generation from Textual Descriptions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper addresses the problem of generating 3D interactive human motion from text. Given a textual description depicting the actions of different body parts in contact with static objects, we synthesize sequences of 3D body poses that are visually natural and physically plausible. Yet, this task poses a significant challenge due to the inadequate consideration of interactions by physical contacts in both motion and textual descriptions, leading to unnatural and implausible sequences. To tackle this challenge, we create a novel dataset named RICH-CAT, representing "Contact-Aware Texts" constructed from the RICH dataset. RICH-CAT comprises high-quality motion, accurate human-object contact labels, and detailed textual descriptions, encompassing over 8,500 motion-text pairs across 26 indoor/outdoor actions. Leveraging RICH-CAT, we propose a novel approach named CATMO for text-driven interactive human motion synthesis that explicitly integrates human body contacts as evidence. We employ two VQ-VAE models to encode motion and body contact sequences into distinct yet complementary latent spaces and an intertwined GPT for generating human motions and contacts in a mutually conditioned manner. Additionally, we introduce a pre-trained text encoder to learn textual embeddings that better discriminate among various contact types, allowing for more precise control over synthesized motions and contacts. Our experiments demonstrate the superior performance of our approach compared to existing text-to-motion methods, producing stable, contact-aware motion sequences. Code and data will be available for research purposes at https://xymsh.github.io/RICH-CAT/
<div id='section'>Paperid: <span id='pid'>394, <a href='https://arxiv.org/pdf/2403.14173.pdf' target='_blank'>https://arxiv.org/pdf/2403.14173.pdf</a></span>   <span><a href='https://github.com/kafeiyin00/HCTO' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/kafeiyin00/HCTO' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianping Li, Shenghai Yuan, Muqing Cao, Thien-Minh Nguyen, Kun Cao, Lihua Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.14173">HCTO: Optimality-Aware LiDAR Inertial Odometry with Hybrid Continuous Time Optimization for Compact Wearable Mapping System</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Compact wearable mapping system (WMS) has gained significant attention due to their convenience in various applications. Specifically, it provides an efficient way to collect prior maps for 3D structure inspection and robot-based "last-mile delivery" in complex environments. However, vibrations in human motion and the uneven distribution of point cloud features in complex environments often lead to rapid drift, which is a prevalent issue when applying existing LiDAR Inertial Odometry (LIO) methods on low-cost WMS. To address these limitations, we propose a novel LIO for WMSs based on Hybrid Continuous Time Optimization (HCTO) considering the optimality of Lidar correspondences. First, HCTO recognizes patterns in human motion (high-frequency part, low-frequency part, and constant velocity part) by analyzing raw IMU measurements. Second, HCTO constructs hybrid IMU factors according to different motion states, which enables robust and accurate estimation against vibration-induced noise in the IMU measurements. Third, the best point correspondences are selected using optimal design to achieve real-time performance and better odometry accuracy. We conduct experiments on head-mounted WMS datasets to evaluate the performance of our system, demonstrating significant advantages over state-of-the-art methods. Video recordings of experiments can be found on the project page of HCTO: \href{https://github.com/kafeiyin00/HCTO}{https://github.com/kafeiyin00/HCTO}.
<div id='section'>Paperid: <span id='pid'>395, <a href='https://arxiv.org/pdf/2403.14104.pdf' target='_blank'>https://arxiv.org/pdf/2403.14104.pdf</a></span>   <span><a href='https://github.com/Motionpre/Adaptive-Salient-Loss-SAGGB' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhihao Wang, Yulin Zhou, Ningyu Zhang, Xiaosong Yang, Jun Xiao, Zhao Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.14104">Existence Is Chaos: Enhancing 3D Human Motion Prediction with Uncertainty Consideration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion prediction is consisting in forecasting future body poses from historically observed sequences. It is a longstanding challenge due to motion's complex dynamics and uncertainty. Existing methods focus on building up complicated neural networks to model the motion dynamics. The predicted results are required to be strictly similar to the training samples with L2 loss in current training pipeline. However, little attention has been paid to the uncertainty property which is crucial to the prediction task. We argue that the recorded motion in training data could be an observation of possible future, rather than a predetermined result. In addition, existing works calculate the predicted error on each future frame equally during training, while recent work indicated that different frames could play different roles. In this work, a novel computationally efficient encoder-decoder model with uncertainty consideration is proposed, which could learn proper characteristics for future frames by a dynamic function. Experimental results on benchmark datasets demonstrate that our uncertainty consideration approach has obvious advantages both in quantity and quality. Moreover, the proposed method could produce motion sequences with much better quality that avoids the intractable shaking artefacts. We believe our work could provide a novel perspective to consider the uncertainty quality for the general motion prediction task and encourage the studies in this field. The code will be available in https://github.com/Motionpre/Adaptive-Salient-Loss-SAGGB.
<div id='section'>Paperid: <span id='pid'>396, <a href='https://arxiv.org/pdf/2403.13570.pdf' target='_blank'>https://arxiv.org/pdf/2403.13570.pdf</a></span>   <span><a href='https://yudeng.github.io/Portrait4D-v2/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Deng, Duomin Wang, Baoyuan Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.13570">Portrait4D-v2: Pseudo Multi-View Data Creates Better 4D Head Synthesizer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose a novel learning approach for feed-forward one-shot 4D head avatar synthesis. Different from existing methods that often learn from reconstructing monocular videos guided by 3DMM, we employ pseudo multi-view videos to learn a 4D head synthesizer in a data-driven manner, avoiding reliance on inaccurate 3DMM reconstruction that could be detrimental to the synthesis performance. The key idea is to first learn a 3D head synthesizer using synthetic multi-view images to convert monocular real videos into multi-view ones, and then utilize the pseudo multi-view videos to learn a 4D head synthesizer via cross-view self-reenactment. By leveraging a simple vision transformer backbone with motion-aware cross-attentions, our method exhibits superior performance compared to previous methods in terms of reconstruction fidelity, geometry consistency, and motion control accuracy. We hope our method offers novel insights into integrating 3D priors with 2D supervisions for improved 4D head avatar creation.
<div id='section'>Paperid: <span id='pid'>397, <a href='https://arxiv.org/pdf/2403.13518.pdf' target='_blank'>https://arxiv.org/pdf/2403.13518.pdf</a></span>   <span><a href='https://github.com/KunhangL/finemotiondiffuse' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kunhang Li, Yansong Feng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.13518">Motion Generation from Fine-grained Textual Descriptions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The task of text2motion is to generate human motion sequences from given textual descriptions, where the model explores diverse mappings from natural language instructions to human body movements. While most existing works are confined to coarse-grained motion descriptions, e.g., "A man squats.", fine-grained descriptions specifying movements of relevant body parts are barely explored. Models trained with coarse-grained texts may not be able to learn mappings from fine-grained motion-related words to motion primitives, resulting in the failure to generate motions from unseen descriptions. In this paper, we build a large-scale language-motion dataset specializing in fine-grained textual descriptions, FineHumanML3D, by feeding GPT-3.5-turbo with step-by-step instructions with pseudo-code compulsory checks. Accordingly, we design a new text2motion model, FineMotionDiffuse, making full use of fine-grained textual information. Our quantitative evaluation shows that FineMotionDiffuse trained on FineHumanML3D improves FID by a large margin of 0.38, compared with competitive baselines. According to the qualitative evaluation and case study, our model outperforms MotionDiffuse in generating spatially or chronologically composite motions, by learning the implicit mappings from fine-grained descriptions to the corresponding basic motions. We release our data at https://github.com/KunhangL/finemotiondiffuse.
<div id='section'>Paperid: <span id='pid'>398, <a href='https://arxiv.org/pdf/2403.13294.pdf' target='_blank'>https://arxiv.org/pdf/2403.13294.pdf</a></span>   <span><a href='https://qingyuan-jiang.github.io/iros2024_poseForecasting/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qingyuan Jiang, Burak Susam, Jun-Jee Chao, Volkan Isler
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.13294">Map-Aware Human Pose Prediction for Robot Follow-Ahead</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the robot follow-ahead task, a mobile robot is tasked to maintain its relative position in front of a moving human actor while keeping the actor in sight. To accomplish this task, it is important that the robot understand the full 3D pose of the human (since the head orientation can be different than the torso) and predict future human poses so as to plan accordingly. This prediction task is especially tricky in a complex environment with junctions and multiple corridors. In this work, we address the problem of forecasting the full 3D trajectory of a human in such environments. Our main insight is to show that one can first predict the 2D trajectory and then estimate the full 3D trajectory by conditioning the estimator on the predicted 2D trajectory. With this approach, we achieve results comparable or better than the state-of-the-art methods three times faster. As part of our contribution, we present a new dataset where, in contrast to existing datasets, the human motion is in a much larger area than a single room. We also present a complete robot system that integrates our human pose forecasting network on the mobile robot to enable real-time robot follow-ahead and present results from real-world experiments in multiple buildings on campus. Our project page, including supplementary material and videos, can be found at: https://qingyuan-jiang.github.io/iros2024_poseForecasting/
<div id='section'>Paperid: <span id='pid'>399, <a href='https://arxiv.org/pdf/2403.12670.pdf' target='_blank'>https://arxiv.org/pdf/2403.12670.pdf</a></span>   <span><a href='https://github.com/library87/OpenRoboExp' target='_blank'>  GitHub</a></span> <span><a href='https://library87.github.io/animatronic-face-iros24' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Boren Li, Hang Li, Hangxin Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.12670">Driving Animatronic Robot Facial Expression From Speech</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Animatronic robots hold the promise of enabling natural human-robot interaction through lifelike facial expressions. However, generating realistic, speech-synchronized robot expressions poses significant challenges due to the complexities of facial biomechanics and the need for responsive motion synthesis. This paper introduces a novel, skinning-centric approach to drive animatronic robot facial expressions from speech input. At its core, the proposed approach employs linear blend skinning (LBS) as a unifying representation, guiding innovations in both embodiment design and motion synthesis. LBS informs the actuation topology, facilitates human expression retargeting, and enables efficient speech-driven facial motion generation. This approach demonstrates the capability to produce highly realistic facial expressions on an animatronic face in real-time at over 4000 fps on a single Nvidia RTX 4090, significantly advancing robots' ability to replicate nuanced human expressions for natural interaction. To foster further research and development in this field, the code has been made publicly available at: \url{https://github.com/library87/OpenRoboExp}.
<div id='section'>Paperid: <span id='pid'>400, <a href='https://arxiv.org/pdf/2403.11589.pdf' target='_blank'>https://arxiv.org/pdf/2403.11589.pdf</a></span>   <span><a href='https://alex-jyj.github.io/UV-Gaussians/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yujiao Jiang, Qingmin Liao, Xiaoyu Li, Li Ma, Qi Zhang, Chaopeng Zhang, Zongqing Lu, Ying Shan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.11589">UV Gaussians: Joint Learning of Mesh Deformation and Gaussian Textures for Human Avatar Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reconstructing photo-realistic drivable human avatars from multi-view image sequences has been a popular and challenging topic in the field of computer vision and graphics. While existing NeRF-based methods can achieve high-quality novel view rendering of human models, both training and inference processes are time-consuming. Recent approaches have utilized 3D Gaussians to represent the human body, enabling faster training and rendering. However, they undermine the importance of the mesh guidance and directly predict Gaussians in 3D space with coarse mesh guidance. This hinders the learning procedure of the Gaussians and tends to produce blurry textures. Therefore, we propose UV Gaussians, which models the 3D human body by jointly learning mesh deformations and 2D UV-space Gaussian textures. We utilize the embedding of UV map to learn Gaussian textures in 2D space, leveraging the capabilities of powerful 2D networks to extract features. Additionally, through an independent Mesh network, we optimize pose-dependent geometric deformations, thereby guiding Gaussian rendering and significantly enhancing rendering quality. We collect and process a new dataset of human motion, which includes multi-view images, scanned models, parametric model registration, and corresponding texture maps. Experimental results demonstrate that our method achieves state-of-the-art synthesis of novel view and novel pose. The code and data will be made available on the homepage https://alex-jyj.github.io/UV-Gaussians/ once the paper is accepted.
<div id='section'>Paperid: <span id='pid'>401, <a href='https://arxiv.org/pdf/2403.11492.pdf' target='_blank'>https://arxiv.org/pdf/2403.11492.pdf</a></span>   <span><a href='https://github.com/opendilab/SmartRefine/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yang Zhou, Hao Shao, Letian Wang, Steven L. Waslander, Hongsheng Li, Yu Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.11492">SmartRefine: A Scenario-Adaptive Refinement Framework for Efficient Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Predicting the future motion of surrounding agents is essential for autonomous vehicles (AVs) to operate safely in dynamic, human-robot-mixed environments. Context information, such as road maps and surrounding agents' states, provides crucial geometric and semantic information for motion behavior prediction. To this end, recent works explore two-stage prediction frameworks where coarse trajectories are first proposed, and then used to select critical context information for trajectory refinement. However, they either incur a large amount of computation or bring limited improvement, if not both. In this paper, we introduce a novel scenario-adaptive refinement strategy, named SmartRefine, to refine prediction with minimal additional computation. Specifically, SmartRefine can comprehensively adapt refinement configurations based on each scenario's properties, and smartly chooses the number of refinement iterations by introducing a quality score to measure the prediction quality and remaining refinement potential of each scenario. SmartRefine is designed as a generic and flexible approach that can be seamlessly integrated into most state-of-the-art motion prediction models. Experiments on Argoverse (1 & 2) show that our method consistently improves the prediction accuracy of multiple state-of-the-art prediction models. Specifically, by adding SmartRefine to QCNet, we outperform all published ensemble-free works on the Argoverse 2 leaderboard (single agent track) at submission. Comprehensive studies are also conducted to ablate design choices and explore the mechanism behind multi-iteration refinement. Codes are available at https://github.com/opendilab/SmartRefine/
<div id='section'>Paperid: <span id='pid'>402, <a href='https://arxiv.org/pdf/2403.11057.pdf' target='_blank'>https://arxiv.org/pdf/2403.11057.pdf</a></span>   <span><a href='https://github.com/AIR-DISCOVER/LLM-Augmented-MTR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoji Zheng, Lixiu Wu, Zhijie Yan, Yuanrong Tang, Hao Zhao, Chen Zhong, Bokui Chen, Jiangtao Gong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.11057">Large Language Models Powered Context-aware Motion Prediction in Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion prediction is among the most fundamental tasks in autonomous driving. Traditional methods of motion forecasting primarily encode vector information of maps and historical trajectory data of traffic participants, lacking a comprehensive understanding of overall traffic semantics, which in turn affects the performance of prediction tasks. In this paper, we utilized Large Language Models (LLMs) to enhance the global traffic context understanding for motion prediction tasks. We first conducted systematic prompt engineering, visualizing complex traffic environments and historical trajectory information of traffic participants into image prompts -- Transportation Context Map (TC-Map), accompanied by corresponding text prompts. Through this approach, we obtained rich traffic context information from the LLM. By integrating this information into the motion prediction model, we demonstrate that such context can enhance the accuracy of motion predictions. Furthermore, considering the cost associated with LLMs, we propose a cost-effective deployment strategy: enhancing the accuracy of motion prediction tasks at scale with 0.7\% LLM-augmented datasets. Our research offers valuable insights into enhancing the understanding of traffic scenes of LLMs and the motion prediction performance of autonomous driving. The source code is available at \url{https://github.com/AIR-DISCOVER/LLM-Augmented-MTR} and \url{https://aistudio.baidu.com/projectdetail/7809548}.
<div id='section'>Paperid: <span id='pid'>403, <a href='https://arxiv.org/pdf/2403.09805.pdf' target='_blank'>https://arxiv.org/pdf/2403.09805.pdf</a></span>   <span><a href='https://s-shamil.github.io/HandFormer/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Md Salman Shamil, Dibyadip Chatterjee, Fadime Sener, Shugao Ma, Angela Yao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.09805">On the Utility of 3D Hand Poses for Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D hand pose is an underexplored modality for action recognition. Poses are compact yet informative and can greatly benefit applications with limited compute budgets. However, poses alone offer an incomplete understanding of actions, as they cannot fully capture objects and environments with which humans interact. We propose HandFormer, a novel multimodal transformer, to efficiently model hand-object interactions. HandFormer combines 3D hand poses at a high temporal resolution for fine-grained motion modeling with sparsely sampled RGB frames for encoding scene semantics. Observing the unique characteristics of hand poses, we temporally factorize hand modeling and represent each joint by its short-term trajectories. This factorized pose representation combined with sparse RGB samples is remarkably efficient and highly accurate. Unimodal HandFormer with only hand poses outperforms existing skeleton-based methods at 5x fewer FLOPs. With RGB, we achieve new state-of-the-art performance on Assembly101 and H2O with significant improvements in egocentric action recognition.
<div id='section'>Paperid: <span id='pid'>404, <a href='https://arxiv.org/pdf/2403.07487.pdf' target='_blank'>https://arxiv.org/pdf/2403.07487.pdf</a></span>   <span><a href='https://steve-zeyu-zhang.github.io/MotionMamba/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zeyu Zhang, Akide Liu, Ian Reid, Richard Hartley, Bohan Zhuang, Hao Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.07487">Motion Mamba: Efficient and Long Sequence Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion generation stands as a significant pursuit in generative computer vision, while achieving long-sequence and efficient motion generation remains challenging. Recent advancements in state space models (SSMs), notably Mamba, have showcased considerable promise in long sequence modeling with an efficient hardware-aware design, which appears to be a promising direction to build motion generation model upon it. Nevertheless, adapting SSMs to motion generation faces hurdles since the lack of a specialized design architecture to model motion sequence. To address these challenges, we propose Motion Mamba, a simple and efficient approach that presents the pioneering motion generation model utilized SSMs. Specifically, we design a Hierarchical Temporal Mamba (HTM) block to process temporal data by ensemble varying numbers of isolated SSM modules across a symmetric U-Net architecture aimed at preserving motion consistency between frames. We also design a Bidirectional Spatial Mamba (BSM) block to bidirectionally process latent poses, to enhance accurate motion generation within a temporal frame. Our proposed method achieves up to 50% FID improvement and up to 4 times faster on the HumanML3D and KIT-ML datasets compared to the previous best diffusion-based method, which demonstrates strong capabilities of high-quality long sequence motion modeling and real-time human motion generation. See project website https://steve-zeyu-zhang.github.io/MotionMamba/
<div id='section'>Paperid: <span id='pid'>405, <a href='https://arxiv.org/pdf/2403.07420.pdf' target='_blank'>https://arxiv.org/pdf/2403.07420.pdf</a></span>   <span><a href='https://github.com/showlab/DragAnything' target='_blank'>  GitHub</a></span> <span><a href='https://weijiawu.github.io/draganything_page/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Weijia Wu, Zhuang Li, Yuchao Gu, Rui Zhao, Yefei He, David Junhao Zhang, Mike Zheng Shou, Yan Li, Tingting Gao, Di Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.07420">DragAnything: Motion Control for Anything using Entity Representation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce DragAnything, which utilizes a entity representation to achieve motion control for any object in controllable video generation. Comparison to existing motion control methods, DragAnything offers several advantages. Firstly, trajectory-based is more userfriendly for interaction, when acquiring other guidance signals (e.g., masks, depth maps) is labor-intensive. Users only need to draw a line (trajectory) during interaction. Secondly, our entity representation serves as an open-domain embedding capable of representing any object, enabling the control of motion for diverse entities, including background. Lastly, our entity representation allows simultaneous and distinct motion control for multiple objects. Extensive experiments demonstrate that our DragAnything achieves state-of-the-art performance for FVD, FID, and User Study, particularly in terms of object motion control, where our method surpasses the previous methods (e.g., DragNUWA) by 26% in human voting.
<div id='section'>Paperid: <span id='pid'>406, <a href='https://arxiv.org/pdf/2403.06828.pdf' target='_blank'>https://arxiv.org/pdf/2403.06828.pdf</a></span>   <span><a href='https://hanruihua.github.io/neupan_project/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruihua Han, Shuai Wang, Shuaijun Wang, Zeqing Zhang, Jianjun Chen, Shijie Lin, Chengyang Li, Chengzhong Xu, Yonina C. Eldar, Qi Hao, Jia Pan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.06828">NeuPAN: Direct Point Robot Navigation with End-to-End Model-based Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Navigating a nonholonomic robot in a cluttered, unknown environment requires accurate perception and precise motion control for real-time collision avoidance. This paper presents NeuPAN: a real-time, highly accurate, map-free, easy-to-deploy, and environment-invariant robot motion planner. Leveraging a tightly coupled perception-to-control framework, NeuPAN has two key innovations compared to existing approaches: 1) it directly maps raw point cloud data to a latent distance feature space for collision-free motion generation, avoiding error propagation from the perception to control pipeline; 2) it is interpretable from an end-to-end model-based learning perspective. The crux of NeuPAN is solving an end-to-end mathematical model with numerous point-level constraints using a plug-and-play (PnP) proximal alternating-minimization network (PAN), incorporating neurons in the loop. This allows NeuPAN to generate real-time, physically interpretable motions. It seamlessly integrates data and knowledge engines, and its network parameters can be fine-tuned via backpropagation. We evaluate NeuPAN on a ground mobile robot, a wheel-legged robot, and an autonomous vehicle, in extensive simulated and real-world environments. Results demonstrate that NeuPAN outperforms existing baselines in terms of accuracy, efficiency, robustness, and generalization capabilities across various environments, including the cluttered sandbox, office, corridor, and parking lot. We show that NeuPAN works well in unknown and unstructured environments with arbitrarily shaped objects, transforming impassable paths into passable ones.
<div id='section'>Paperid: <span id='pid'>407, <a href='https://arxiv.org/pdf/2403.05489.pdf' target='_blank'>https://arxiv.org/pdf/2403.05489.pdf</a></span>   <span><a href='https://github.com/kit-mrt/future-motion' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Royden Wagner, Omer Sahin Tas, Marvin Klemp, Carlos Fernandez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.05489">JointMotion: Joint Self-Supervision for Joint Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present JointMotion, a self-supervised pre-training method for joint motion prediction in self-driving vehicles. Our method jointly optimizes a scene-level objective connecting motion and environments, and an instance-level objective to refine learned representations. Scene-level representations are learned via non-contrastive similarity learning of past motion sequences and environment context. At the instance level, we use masked autoencoding to refine multimodal polyline representations. We complement this with an adaptive pre-training decoder that enables JointMotion to generalize across different environment representations, fusion mechanisms, and dataset characteristics. Notably, our method reduces the joint final displacement error of Wayformer, HPTR, and Scene Transformer models by 3\%, 8\%, and 12\%, respectively; and enables transfer learning between the Waymo Open Motion and the Argoverse 2 Motion Forecasting datasets. Code: https://github.com/kit-mrt/future-motion
<div id='section'>Paperid: <span id='pid'>408, <a href='https://arxiv.org/pdf/2403.03561.pdf' target='_blank'>https://arxiv.org/pdf/2403.03561.pdf</a></span>   <span><a href='https://pico-ai-team.github.io/hmd-poser' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Peng Dai, Yang Zhang, Tao Liu, Zhen Fan, Tianyuan Du, Zhuo Su, Xiaozheng Zheng, Zeming Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.03561">HMD-Poser: On-Device Real-time Human Motion Tracking from Scalable Sparse Observations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>It is especially challenging to achieve real-time human motion tracking on a standalone VR Head-Mounted Display (HMD) such as Meta Quest and PICO. In this paper, we propose HMD-Poser, the first unified approach to recover full-body motions using scalable sparse observations from HMD and body-worn IMUs. In particular, it can support a variety of input scenarios, such as HMD, HMD+2IMUs, HMD+3IMUs, etc. The scalability of inputs may accommodate users' choices for both high tracking accuracy and easy-to-wear. A lightweight temporal-spatial feature learning network is proposed in HMD-Poser to guarantee that the model runs in real-time on HMDs. Furthermore, HMD-Poser presents online body shape estimation to improve the position accuracy of body joints. Extensive experimental results on the challenging AMASS dataset show that HMD-Poser achieves new state-of-the-art results in both accuracy and real-time performance. We also build a new free-dancing motion dataset to evaluate HMD-Poser's on-device performance and investigate the performance gap between synthetic data and real-captured sensor data. Finally, we demonstrate our HMD-Poser with a real-time Avatar-driving application on a commercial HMD. Our code and free-dancing motion dataset are available https://pico-ai-team.github.io/hmd-poser
<div id='section'>Paperid: <span id='pid'>409, <a href='https://arxiv.org/pdf/2402.19237.pdf' target='_blank'>https://arxiv.org/pdf/2402.19237.pdf</a></span>   <span><a href='https://github.com/QualityMinds/cistgcn' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Edgar Medina, Leyong Loh, Namrata Gurung, Kyung Hun Oh, Niels Heller
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.19237">Context-based Interpretable Spatio-Temporal Graph Convolutional Network for Human Motion Forecasting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion prediction is still an open problem extremely important for autonomous driving and safety applications. Due to the complex spatiotemporal relation of motion sequences, this remains a challenging problem not only for movement prediction but also to perform a preliminary interpretation of the joint connections. In this work, we present a Context-based Interpretable Spatio-Temporal Graph Convolutional Network (CIST-GCN), as an efficient 3D human pose forecasting model based on GCNs that encompasses specific layers, aiding model interpretability and providing information that might be useful when analyzing motion distribution and body behavior. Our architecture extracts meaningful information from pose sequences, aggregates displacements and accelerations into the input model, and finally predicts the output displacements. Extensive experiments on Human 3.6M, AMASS, 3DPW, and ExPI datasets demonstrate that CIST-GCN outperforms previous methods in human motion prediction and robustness. Since the idea of enhancing interpretability for motion prediction has its merits, we showcase experiments towards it and provide preliminary evaluations of such insights here. available code: https://github.com/QualityMinds/cistgcn
<div id='section'>Paperid: <span id='pid'>410, <a href='https://arxiv.org/pdf/2402.18796.pdf' target='_blank'>https://arxiv.org/pdf/2402.18796.pdf</a></span>   <span><a href='https://portal-cornell.github.io/MOSAIC/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Huaxiaoyue Wang, Kushal Kedia, Juntao Ren, Rahma Abdullah, Atiksh Bhardwaj, Angela Chao, Kelly Y Chen, Nathaniel Chin, Prithwish Dan, Xinyi Fan, Gonzalo Gonzalez-Pumariega, Aditya Kompella, Maximus Adrian Pace, Yash Sharma, Xiangwan Sun, Neha Sunkara, Sanjiban Choudhury
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.18796">MOSAIC: A Modular System for Assistive and Interactive Cooking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present MOSAIC, a modular architecture for home robots to perform complex collaborative tasks, such as cooking with everyday users. MOSAIC tightly collaborates with humans, interacts with users using natural language, coordinates multiple robots, and manages an open vocabulary of everyday objects. At its core, MOSAIC employs modularity: it leverages multiple large-scale pre-trained models for general tasks like language and image recognition, while using streamlined modules designed for task-specific control. We extensively evaluate MOSAIC on 60 end-to-end trials where two robots collaborate with a human user to cook a combination of 6 recipes. We also extensively test individual modules with 180 episodes of visuomotor picking, 60 episodes of human motion forecasting, and 46 online user evaluations of the task planner. We show that MOSAIC is able to efficiently collaborate with humans by running the overall system end-to-end with a real human user, completing 68.3% (41/60) collaborative cooking trials of 6 different recipes with a subtask completion rate of 91.6%. Finally, we discuss the limitations of the current system and exciting open challenges in this domain. The project's website is at https://portal-cornell.github.io/MOSAIC/
<div id='section'>Paperid: <span id='pid'>411, <a href='https://arxiv.org/pdf/2402.15509.pdf' target='_blank'>https://arxiv.org/pdf/2402.15509.pdf</a></span>   <span><a href='https://barquerogerman.github.io/FlowMDM/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>German Barquero, Sergio Escalera, Cristina Palmero
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.15509">Seamless Human Motion Composition with Blended Positional Encodings</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Conditional human motion generation is an important topic with many applications in virtual reality, gaming, and robotics. While prior works have focused on generating motion guided by text, music, or scenes, these typically result in isolated motions confined to short durations. Instead, we address the generation of long, continuous sequences guided by a series of varying textual descriptions. In this context, we introduce FlowMDM, the first diffusion-based model that generates seamless Human Motion Compositions (HMC) without any postprocessing or redundant denoising steps. For this, we introduce the Blended Positional Encodings, a technique that leverages both absolute and relative positional encodings in the denoising chain. More specifically, global motion coherence is recovered at the absolute stage, whereas smooth and realistic transitions are built at the relative stage. As a result, we achieve state-of-the-art results in terms of accuracy, realism, and smoothness on the Babel and HumanML3D datasets. FlowMDM excels when trained with only a single description per motion sequence thanks to its Pose-Centric Cross-ATtention, which makes it robust against varying text descriptions at inference time. Finally, to address the limitations of existing HMC metrics, we propose two new metrics: the Peak Jerk and the Area Under the Jerk, to detect abrupt transitions.
<div id='section'>Paperid: <span id='pid'>412, <a href='https://arxiv.org/pdf/2402.13185.pdf' target='_blank'>https://arxiv.org/pdf/2402.13185.pdf</a></span>   <span><a href='https://jianhongbai.github.io/UniEdit/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianhong Bai, Tianyu He, Yuchi Wang, Junliang Guo, Haoji Hu, Zuozhu Liu, Jiang Bian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.13185">UniEdit: A Unified Tuning-Free Framework for Video Motion and Appearance Editing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in text-guided video editing have showcased promising results in appearance editing (e.g., stylization). However, video motion editing in the temporal dimension (e.g., from eating to waving), which distinguishes video editing from image editing, is underexplored. In this work, we present UniEdit, a tuning-free framework that supports both video motion and appearance editing by harnessing the power of a pre-trained text-to-video generator within an inversion-then-generation framework. To realize motion editing while preserving source video content, based on the insights that temporal and spatial self-attention layers encode inter-frame and intra-frame dependency respectively, we introduce auxiliary motion-reference and reconstruction branches to produce text-guided motion and source features respectively. The obtained features are then injected into the main editing path via temporal and spatial self-attention layers. Extensive experiments demonstrate that UniEdit covers video motion editing and various appearance editing scenarios, and surpasses the state-of-the-art methods. Our code will be publicly available.
<div id='section'>Paperid: <span id='pid'>413, <a href='https://arxiv.org/pdf/2402.11502.pdf' target='_blank'>https://arxiv.org/pdf/2402.11502.pdf</a></span>   <span><a href='https://github.com/wzzheng/GenAD' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/wzzheng/GenAD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenzhao Zheng, Ruiqi Song, Xianda Guo, Chenming Zhang, Long Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.11502">GenAD: Generative End-to-End Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Directly producing planning results from raw sensors has been a long-desired solution for autonomous driving and has attracted increasing attention recently. Most existing end-to-end autonomous driving methods factorize this problem into perception, motion prediction, and planning. However, we argue that the conventional progressive pipeline still cannot comprehensively model the entire traffic evolution process, e.g., the future interaction between the ego car and other traffic participants and the structural trajectory prior. In this paper, we explore a new paradigm for end-to-end autonomous driving, where the key is to predict how the ego car and the surroundings evolve given past scenes. We propose GenAD, a generative framework that casts autonomous driving into a generative modeling problem. We propose an instance-centric scene tokenizer that first transforms the surrounding scenes into map-aware instance tokens. We then employ a variational autoencoder to learn the future trajectory distribution in a structural latent space for trajectory prior modeling. We further adopt a temporal model to capture the agent and ego movements in the latent space to generate more effective future trajectories. GenAD finally simultaneously performs motion prediction and planning by sampling distributions in the learned structural latent space conditioned on the instance tokens and using the learned temporal model to generate futures. Extensive experiments on the widely used nuScenes benchmark show that the proposed GenAD achieves state-of-the-art performance on vision-centric end-to-end autonomous driving with high efficiency. Code: https://github.com/wzzheng/GenAD.
<div id='section'>Paperid: <span id='pid'>414, <a href='https://arxiv.org/pdf/2402.04324.pdf' target='_blank'>https://arxiv.org/pdf/2402.04324.pdf</a></span>   <span><a href='https://tiger-ai-lab.github.io/ConsistI2V/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Weiming Ren, Huan Yang, Ge Zhang, Cong Wei, Xinrun Du, Wenhao Huang, Wenhu Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.04324">ConsistI2V: Enhancing Visual Consistency for Image-to-Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image-to-video (I2V) generation aims to use the initial frame (alongside a text prompt) to create a video sequence. A grand challenge in I2V generation is to maintain visual consistency throughout the video: existing methods often struggle to preserve the integrity of the subject, background, and style from the first frame, as well as ensure a fluid and logical progression within the video narrative. To mitigate these issues, we propose ConsistI2V, a diffusion-based method to enhance visual consistency for I2V generation. Specifically, we introduce (1) spatiotemporal attention over the first frame to maintain spatial and motion consistency, (2) noise initialization from the low-frequency band of the first frame to enhance layout consistency. These two approaches enable ConsistI2V to generate highly consistent videos. We also extend the proposed approaches to show their potential to improve consistency in auto-regressive long video generation and camera motion control. To verify the effectiveness of our method, we propose I2V-Bench, a comprehensive evaluation benchmark for I2V generation. Our automatic and human evaluation results demonstrate the superiority of ConsistI2V over existing methods.
<div id='section'>Paperid: <span id='pid'>415, <a href='https://arxiv.org/pdf/2402.02519.pdf' target='_blank'>https://arxiv.org/pdf/2402.02519.pdf</a></span>   <span><a href='https://github.com/HKUST-Aerial-Robotics/SIMPL' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/HKUST-Aerial-Robotics/SIMPL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lu Zhang, Peiliang Li, Sikang Liu, Shaojie Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.02519">SIMPL: A Simple and Efficient Multi-agent Motion Prediction Baseline for Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a Simple and effIcient Motion Prediction baseLine (SIMPL) for autonomous vehicles. Unlike conventional agent-centric methods with high accuracy but repetitive computations and scene-centric methods with compromised accuracy and generalizability, SIMPL delivers real-time, accurate motion predictions for all relevant traffic participants. To achieve improvements in both accuracy and inference speed, we propose a compact and efficient global feature fusion module that performs directed message passing in a symmetric manner, enabling the network to forecast future motion for all road users in a single feed-forward pass and mitigating accuracy loss caused by viewpoint shifting. Additionally, we investigate the continuous trajectory parameterization using Bernstein basis polynomials in trajectory decoding, allowing evaluations of states and their higher-order derivatives at any desired time point, which is valuable for downstream planning tasks. As a strong baseline, SIMPL exhibits highly competitive performance on Argoverse 1 & 2 motion forecasting benchmarks compared with other state-of-the-art methods. Furthermore, its lightweight design and low inference latency make SIMPL highly extensible and promising for real-world onboard deployment. We open-source the code at https://github.com/HKUST-Aerial-Robotics/SIMPL.
<div id='section'>Paperid: <span id='pid'>416, <a href='https://arxiv.org/pdf/2401.15977.pdf' target='_blank'>https://arxiv.org/pdf/2401.15977.pdf</a></span>   <span><a href='https://xiaoyushi97.github.io/Motion-I2V/' target='_blank'>  GitHub</a></span> <span><a href='https://xiaoyushi97.github.io/Motion-I2V/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoyu Shi, Zhaoyang Huang, Fu-Yun Wang, Weikang Bian, Dasong Li, Yi Zhang, Manyuan Zhang, Ka Chun Cheung, Simon See, Hongwei Qin, Jifeng Dai, Hongsheng Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.15977">Motion-I2V: Consistent and Controllable Image-to-Video Generation with Explicit Motion Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce Motion-I2V, a novel framework for consistent and controllable image-to-video generation (I2V). In contrast to previous methods that directly learn the complicated image-to-video mapping, Motion-I2V factorizes I2V into two stages with explicit motion modeling. For the first stage, we propose a diffusion-based motion field predictor, which focuses on deducing the trajectories of the reference image's pixels. For the second stage, we propose motion-augmented temporal attention to enhance the limited 1-D temporal attention in video latent diffusion models. This module can effectively propagate reference image's feature to synthesized frames with the guidance of predicted trajectories from the first stage. Compared with existing methods, Motion-I2V can generate more consistent videos even at the presence of large motion and viewpoint variation. By training a sparse trajectory ControlNet for the first stage, Motion-I2V can support users to precisely control motion trajectories and motion regions with sparse trajectory and region annotations. This offers more controllability of the I2V process than solely relying on textual instructions. Additionally, Motion-I2V's second stage naturally supports zero-shot video-to-video translation. Both qualitative and quantitative comparisons demonstrate the advantages of Motion-I2V over prior approaches in consistent and controllable image-to-video generation. Please see our project page at https://xiaoyushi97.github.io/Motion-I2V/.
<div id='section'>Paperid: <span id='pid'>417, <a href='https://arxiv.org/pdf/2401.15318.pdf' target='_blank'>https://arxiv.org/pdf/2401.15318.pdf</a></span>   <span><a href='https://gaussiansplashing.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yutao Feng, Xiang Feng, Yintong Shang, Ying Jiang, Chang Yu, Zeshun Zong, Tianjia Shao, Hongzhi Wu, Kun Zhou, Chenfanfu Jiang, Yin Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.15318">Gaussian Splashing: Unified Particles for Versatile Motion Synthesis and Rendering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We demonstrate the feasibility of integrating physics-based animations of solids and fluids with 3D Gaussian Splatting (3DGS) to create novel effects in virtual scenes reconstructed using 3DGS. Leveraging the coherence of the Gaussian Splatting and Position-Based Dynamics (PBD) in the underlying representation, we manage rendering, view synthesis, and the dynamics of solids and fluids in a cohesive manner. Similar to GaussianShader, we enhance each Gaussian kernel with an added normal, aligning the kernel's orientation with the surface normal to refine the PBD simulation. This approach effectively eliminates spiky noises that arise from rotational deformation in solids. It also allows us to integrate physically based rendering to augment the dynamic surface reflections on fluids. Consequently, our framework is capable of realistically reproducing surface highlights on dynamic fluids and facilitating interactions between scene objects and fluids from new views. For more information, please visit our project page at \url{https://gaussiansplashing.github.io/}.
<div id='section'>Paperid: <span id='pid'>418, <a href='https://arxiv.org/pdf/2401.13505.pdf' target='_blank'>https://arxiv.org/pdf/2401.13505.pdf</a></span>   <span><a href='https://murrol.github.io/GenMoStyle' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chuan Guo, Yuxuan Mu, Xinxin Zuo, Peng Dai, Youliang Yan, Juwei Lu, Li Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.13505">Generative Human Motion Stylization in Latent Space</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion stylization aims to revise the style of an input motion while keeping its content unaltered. Unlike existing works that operate directly in pose space, we leverage the latent space of pretrained autoencoders as a more expressive and robust representation for motion extraction and infusion. Building upon this, we present a novel generative model that produces diverse stylization results of a single motion (latent) code. During training, a motion code is decomposed into two coding components: a deterministic content code, and a probabilistic style code adhering to a prior distribution; then a generator massages the random combination of content and style codes to reconstruct the corresponding motion codes. Our approach is versatile, allowing the learning of probabilistic style space from either style labeled or unlabeled motions, providing notable flexibility in stylization as well. In inference, users can opt to stylize a motion using style cues from a reference motion or a label. Even in the absence of explicit style input, our model facilitates novel re-stylization by sampling from the unconditional style prior distribution. Experimental results show that our proposed stylization models, despite their lightweight design, outperform the state-of-the-art in style reenactment, content preservation, and generalization across various applications and settings. Project Page: https://murrol.github.io/GenMoStyle
<div id='section'>Paperid: <span id='pid'>419, <a href='https://arxiv.org/pdf/2401.11115.pdf' target='_blank'>https://arxiv.org/pdf/2401.11115.pdf</a></span>   <span><a href='https://nhathoang2002.github.io/MotionMix-page/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Nhat M. Hoang, Kehong Gong, Chuan Guo, Michael Bi Mi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.11115">MotionMix: Weakly-Supervised Diffusion for Controllable Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Controllable generation of 3D human motions becomes an important topic as the world embraces digital transformation. Existing works, though making promising progress with the advent of diffusion models, heavily rely on meticulously captured and annotated (e.g., text) high-quality motion corpus, a resource-intensive endeavor in the real world. This motivates our proposed MotionMix, a simple yet effective weakly-supervised diffusion model that leverages both noisy and unannotated motion sequences. Specifically, we separate the denoising objectives of a diffusion model into two stages: obtaining conditional rough motion approximations in the initial $T-T^*$ steps by learning the noisy annotated motions, followed by the unconditional refinement of these preliminary motions during the last $T^*$ steps using unannotated motions. Notably, though learning from two sources of imperfect data, our model does not compromise motion generation quality compared to fully supervised approaches that access gold data. Extensive experiments on several benchmarks demonstrate that our MotionMix, as a versatile framework, consistently achieves state-of-the-art performances on text-to-motion, action-to-motion, and music-to-dance tasks. Project page: https://nhathoang2002.github.io/MotionMix-page/
<div id='section'>Paperid: <span id='pid'>420, <a href='https://arxiv.org/pdf/2401.11037.pdf' target='_blank'>https://arxiv.org/pdf/2401.11037.pdf</a></span>   <span><a href='https://github.com/MinkaiXu/egno' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Minkai Xu, Jiaqi Han, Aaron Lou, Jean Kossaifi, Arvind Ramanathan, Kamyar Azizzadenesheli, Jure Leskovec, Stefano Ermon, Anima Anandkumar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.11037">Equivariant Graph Neural Operator for Modeling 3D Dynamics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modeling the complex three-dimensional (3D) dynamics of relational systems is an important problem in the natural sciences, with applications ranging from molecular simulations to particle mechanics. Machine learning methods have achieved good success by learning graph neural networks to model spatial interactions. However, these approaches do not faithfully capture temporal correlations since they only model next-step predictions. In this work, we propose Equivariant Graph Neural Operator (EGNO), a novel and principled method that directly models dynamics as trajectories instead of just next-step prediction. Different from existing methods, EGNO explicitly learns the temporal evolution of 3D dynamics where we formulate the dynamics as a function over time and learn neural operators to approximate it. To capture the temporal correlations while keeping the intrinsic SE(3)-equivariance, we develop equivariant temporal convolutions parameterized in the Fourier space and build EGNO by stacking the Fourier layers over equivariant networks. EGNO is the first operator learning framework that is capable of modeling solution dynamics functions over time while retaining 3D equivariance. Comprehensive experiments in multiple domains, including particle simulations, human motion capture, and molecular dynamics, demonstrate the significantly superior performance of EGNO against existing methods, thanks to the equivariant temporal modeling. Our code is available at https://github.com/MinkaiXu/egno.
<div id='section'>Paperid: <span id='pid'>421, <a href='https://arxiv.org/pdf/2401.08739.pdf' target='_blank'>https://arxiv.org/pdf/2401.08739.pdf</a></span>   <span><a href='https://ego-gen.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Gen Li, Kaifeng Zhao, Siwei Zhang, Xiaozhong Lyu, Mihai Dusmanu, Yan Zhang, Marc Pollefeys, Siyu Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.08739">EgoGen: An Egocentric Synthetic Data Generator</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding the world in first-person view is fundamental in Augmented Reality (AR). This immersive perspective brings dramatic visual changes and unique challenges compared to third-person views. Synthetic data has empowered third-person-view vision models, but its application to embodied egocentric perception tasks remains largely unexplored. A critical challenge lies in simulating natural human movements and behaviors that effectively steer the embodied cameras to capture a faithful egocentric representation of the 3D world. To address this challenge, we introduce EgoGen, a new synthetic data generator that can produce accurate and rich ground-truth training data for egocentric perception tasks. At the heart of EgoGen is a novel human motion synthesis model that directly leverages egocentric visual inputs of a virtual human to sense the 3D environment. Combined with collision-avoiding motion primitives and a two-stage reinforcement learning approach, our motion synthesis model offers a closed-loop solution where the embodied perception and movement of the virtual human are seamlessly coupled. Compared to previous works, our model eliminates the need for a pre-defined global path, and is directly applicable to dynamic environments. Combined with our easy-to-use and scalable data generation pipeline, we demonstrate EgoGen's efficacy in three tasks: mapping and localization for head-mounted cameras, egocentric camera tracking, and human mesh recovery from egocentric views. EgoGen will be fully open-sourced, offering a practical solution for creating realistic egocentric training data and aiming to serve as a useful tool for egocentric computer vision research. Refer to our project page: https://ego-gen.github.io/.
<div id='section'>Paperid: <span id='pid'>422, <a href='https://arxiv.org/pdf/2401.08570.pdf' target='_blank'>https://arxiv.org/pdf/2401.08570.pdf</a></span>   <span><a href='https://sanweiliti.github.io/ROHM/ROHM.html' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Siwei Zhang, Bharat Lal Bhatnagar, Yuanlu Xu, Alexander Winkler, Petr Kadlecek, Siyu Tang, Federica Bogo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.08570">RoHM: Robust Human Motion Reconstruction via Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose RoHM, an approach for robust 3D human motion reconstruction from monocular RGB(-D) videos in the presence of noise and occlusions. Most previous approaches either train neural networks to directly regress motion in 3D or learn data-driven motion priors and combine them with optimization at test time. The former do not recover globally coherent motion and fail under occlusions; the latter are time-consuming, prone to local minima, and require manual tuning. To overcome these shortcomings, we exploit the iterative, denoising nature of diffusion models. RoHM is a novel diffusion-based motion model that, conditioned on noisy and occluded input data, reconstructs complete, plausible motions in consistent global coordinates. Given the complexity of the problem -- requiring one to address different tasks (denoising and infilling) in different solution spaces (local and global motion) -- we decompose it into two sub-tasks and learn two models, one for global trajectory and one for local motion. To capture the correlations between the two, we then introduce a novel conditioning module, combining it with an iterative inference scheme. We apply RoHM to a variety of tasks -- from motion reconstruction and denoising to spatial and temporal infilling. Extensive experiments on three popular datasets show that our method outperforms state-of-the-art approaches qualitatively and quantitatively, while being faster at test time. The code is available at https://sanweiliti.github.io/ROHM/ROHM.html.
<div id='section'>Paperid: <span id='pid'>423, <a href='https://arxiv.org/pdf/2401.08398.pdf' target='_blank'>https://arxiv.org/pdf/2401.08398.pdf</a></span>   <span><a href='https://github.com/grignarder/high-quality-blendshape-generation' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xin Ming, Jiawei Li, Jingwang Ling, Libo Zhang, Feng Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.08398">High-Quality Mesh Blendshape Generation from Face Videos via Neural Inverse Rendering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Readily editable mesh blendshapes have been widely used in animation pipelines, while recent advancements in neural geometry and appearance representations have enabled high-quality inverse rendering. Building upon these observations, we introduce a novel technique that reconstructs mesh-based blendshape rigs from single or sparse multi-view videos, leveraging state-of-the-art neural inverse rendering. We begin by constructing a deformation representation that parameterizes vertex displacements into differential coordinates with tetrahedral connections, allowing for high-quality vertex deformation on high-resolution meshes. By constructing a set of semantic regulations in this representation, we achieve joint optimization of blendshapes and expression coefficients. Furthermore, to enable a user-friendly multi-view setup with unsynchronized cameras, we propose a neural regressor to model time-varying motion parameters. This approach implicitly considers the time difference across multiple cameras, enhancing the accuracy of motion modeling. Experiments demonstrate that, with the flexible input of single or sparse multi-view videos, we reconstruct personalized high-fidelity blendshapes. These blendshapes are both geometrically and semantically accurate, and they are compatible with industrial animation pipelines. Code and data are available at https://github.com/grignarder/high-quality-blendshape-generation.
<div id='section'>Paperid: <span id='pid'>424, <a href='https://arxiv.org/pdf/2401.02142.pdf' target='_blank'>https://arxiv.org/pdf/2401.02142.pdf</a></span>   <span><a href='https://github.com/Xuehao-Gao/GUESS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuehao Gao, Yang Yang, Zhenyu Xie, Shaoyi Du, Zhongqian Sun, Yang Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.02142">GUESS:GradUally Enriching SyntheSis for Text-Driven Human Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose a novel cascaded diffusion-based generative framework for text-driven human motion synthesis, which exploits a strategy named GradUally Enriching SyntheSis (GUESS as its abbreviation). The strategy sets up generation objectives by grouping body joints of detailed skeletons in close semantic proximity together and then replacing each of such joint group with a single body-part node. Such an operation recursively abstracts a human pose to coarser and coarser skeletons at multiple granularity levels. With gradually increasing the abstraction level, human motion becomes more and more concise and stable, significantly benefiting the cross-modal motion synthesis task. The whole text-driven human motion synthesis problem is then divided into multiple abstraction levels and solved with a multi-stage generation framework with a cascaded latent diffusion model: an initial generator first generates the coarsest human motion guess from a given text description; then, a series of successive generators gradually enrich the motion details based on the textual description and the previous synthesized results. Notably, we further integrate GUESS with the proposed dynamic multi-condition fusion mechanism to dynamically balance the cooperative effects of the given textual condition and synthesized coarse motion prompt in different generation stages. Extensive experiments on large-scale datasets verify that GUESS outperforms existing state-of-the-art methods by large margins in terms of accuracy, realisticness, and diversity. Code is available at https://github.com/Xuehao-Gao/GUESS.
<div id='section'>Paperid: <span id='pid'>425, <a href='https://arxiv.org/pdf/2401.01730.pdf' target='_blank'>https://arxiv.org/pdf/2401.01730.pdf</a></span>   <span><a href='https://yw0208.github.io/staf/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Yao, Hongwen Zhang, Yunlian Sun, Jinhui Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.01730">STAF: 3D Human Mesh Recovery from Video with Spatio-Temporal Alignment Fusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The recovery of 3D human mesh from monocular images has significantly been developed in recent years. However, existing models usually ignore spatial and temporal information, which might lead to mesh and image misalignment and temporal discontinuity. For this reason, we propose a novel Spatio-Temporal Alignment Fusion (STAF) model. As a video-based model, it leverages coherence clues from human motion by an attention-based Temporal Coherence Fusion Module (TCFM). As for spatial mesh-alignment evidence, we extract fine-grained local information through predicted mesh projection on the feature maps. Based on the spatial features, we further introduce a multi-stage adjacent Spatial Alignment Fusion Module (SAFM) to enhance the feature representation of the target frame. In addition to the above, we propose an Average Pooling Module (APM) to allow the model to focus on the entire input sequence rather than just the target frame. This method can remarkably improve the smoothness of recovery results from video. Extensive experiments on 3DPW, MPII3D, and H36M demonstrate the superiority of STAF. We achieve a state-of-the-art trade-off between precision and smoothness. Our code and more video results are on the project page https://yw0208.github.io/staf/
<div id='section'>Paperid: <span id='pid'>426, <a href='https://arxiv.org/pdf/2410.06513.pdf' target='_blank'>https://arxiv.org/pdf/2410.06513.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoyang Liu, Yunyao Mao, Wengang Zhou, Houqiang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.06513">MotionRL: Align Text-to-Motion Generation to Human Preferences with Multi-Reward Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce MotionRL, the first approach to utilize Multi-Reward Reinforcement Learning (RL) for optimizing text-to-motion generation tasks and aligning them with human preferences. Previous works focused on improving numerical performance metrics on the given datasets, often neglecting the variability and subjectivity of human feedback. In contrast, our novel approach uses reinforcement learning to fine-tune the motion generator based on human preferences prior knowledge of the human perception model, allowing it to generate motions that better align human preferences. In addition, MotionRL introduces a novel multi-objective optimization strategy to approximate Pareto optimality between text adherence, motion quality, and human preferences. Extensive experiments and user studies demonstrate that MotionRL not only allows control over the generated results across different objectives but also significantly enhances performance across these metrics compared to other algorithms.
<div id='section'>Paperid: <span id='pid'>427, <a href='https://arxiv.org/pdf/2405.15763.pdf' target='_blank'>https://arxiv.org/pdf/2405.15763.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ke Fan, Junshu Tang, Weijian Cao, Ran Yi, Moran Li, Jingyu Gong, Jiangning Zhang, Yabiao Wang, Chengjie Wang, Lizhuang Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.15763">FreeMotion: A Unified Framework for Number-free Text-to-Motion Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-to-motion synthesis is a crucial task in computer vision. Existing methods are limited in their universality, as they are tailored for single-person or two-person scenarios and can not be applied to generate motions for more individuals. To achieve the number-free motion synthesis, this paper reconsiders motion generation and proposes to unify the single and multi-person motion by the conditional motion distribution. Furthermore, a generation module and an interaction module are designed for our FreeMotion framework to decouple the process of conditional motion generation and finally support the number-free motion synthesis. Besides, based on our framework, the current single-person motion spatial control method could be seamlessly integrated, achieving precise control of multi-person motion. Extensive experiments demonstrate the superior performance of our method and our capability to infer single and multi-human motions simultaneously.
<div id='section'>Paperid: <span id='pid'>428, <a href='https://arxiv.org/pdf/2405.15541.pdf' target='_blank'>https://arxiv.org/pdf/2405.15541.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunyao Mao, Xiaoyang Liu, Wengang Zhou, Zhenbo Lu, Houqiang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.15541">Learning Generalizable Human Motion Generator with Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-driven human motion generation, as one of the vital tasks in computer-aided content creation, has recently attracted increasing attention. While pioneering research has largely focused on improving numerical performance metrics on given datasets, practical applications reveal a common challenge: existing methods often overfit specific motion expressions in the training data, hindering their ability to generalize to novel descriptions like unseen combinations of motions. This limitation restricts their broader applicability. We argue that the aforementioned problem primarily arises from the scarcity of available motion-text pairs, given the many-to-many nature of text-driven motion generation. To tackle this problem, we formulate text-to-motion generation as a Markov decision process and present \textbf{InstructMotion}, which incorporate the trail and error paradigm in reinforcement learning for generalizable human motion generation. Leveraging contrastive pre-trained text and motion encoders, we delve into optimizing reward design to enable InstructMotion to operate effectively on both paired data, enhancing global semantic level text-motion alignment, and synthetic text-only data, facilitating better generalization to novel prompts without the need for ground-truth motion supervision. Extensive experiments on prevalent benchmarks and also our synthesized unpaired dataset demonstrate that the proposed InstructMotion achieves outstanding performance both quantitatively and qualitatively.
<div id='section'>Paperid: <span id='pid'>429, <a href='https://arxiv.org/pdf/2505.04317.pdf' target='_blank'>https://arxiv.org/pdf/2505.04317.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruize Zhang, Sirui Xiang, Zelai Xu, Feng Gao, Shilong Ji, Wenhao Tang, Wenbo Ding, Chao Yu, Yu Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.04317">Mastering Multi-Drone Volleyball through Hierarchical Co-Self-Play Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we tackle the problem of learning to play 3v3 multi-drone volleyball, a new embodied competitive task that requires both high-level strategic coordination and low-level agile control. The task is turn-based, multi-agent, and physically grounded, posing significant challenges due to its long-horizon dependencies, tight inter-agent coupling, and the underactuated dynamics of quadrotors. To address this, we propose Hierarchical Co-Self-Play (HCSP), a hierarchical reinforcement learning framework that separates centralized high-level strategic decision-making from decentralized low-level motion control. We design a three-stage population-based training pipeline to enable both strategy and skill to emerge from scratch without expert demonstrations: (I) training diverse low-level skills, (II) learning high-level strategy via self-play with fixed low-level skills, and (III) joint fine-tuning through co-self-play. Experiments show that HCSP achieves superior performance, outperforming non-hierarchical self-play and rule-based hierarchical baselines with an average 82.9% win rate and a 71.5% win rate against the two-stage variant. Moreover, co-self-play leads to emergent team behaviors such as role switching and coordinated formations, demonstrating the effectiveness of our hierarchical design and training scheme. The project page is at https://sites.google.com/view/hi-co-self-play.
<div id='section'>Paperid: <span id='pid'>430, <a href='https://arxiv.org/pdf/2504.13582.pdf' target='_blank'>https://arxiv.org/pdf/2504.13582.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zongyuan Chen, Yan Xia, Jiayuan Liu, Jijia Liu, Wenhao Tang, Jiayu Chen, Feng Gao, Longfei Ma, Hongen Liao, Yu Wang, Chao Yu, Boyu Zhang, Fei Xing
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.13582">Hysteresis-Aware Neural Network Modeling and Whole-Body Reinforcement Learning Control of Soft Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Soft robots exhibit inherent compliance and safety, which makes them particularly suitable for applications requiring direct physical interaction with humans, such as surgical procedures. However, their nonlinear and hysteretic behavior, resulting from the properties of soft materials, presents substantial challenges for accurate modeling and control. In this study, we present a soft robotic system designed for surgical applications and propose a hysteresis-aware whole-body neural network model that accurately captures and predicts the soft robot's whole-body motion, including its hysteretic behavior. Building upon the high-precision dynamic model, we construct a highly parallel simulation environment for soft robot control and apply an on-policy reinforcement learning algorithm to efficiently train whole-body motion control strategies. Based on the trained control policy, we developed a soft robotic system for surgical applications and validated it through phantom-based laser ablation experiments in a physical environment. The results demonstrate that the hysteresis-aware modeling reduces the Mean Squared Error (MSE) by 84.95 percent compared to traditional modeling methods. The deployed control algorithm achieved a trajectory tracking error ranging from 0.126 to 0.250 mm on the real soft robot, highlighting its precision in real-world conditions. The proposed method showed strong performance in phantom-based surgical experiments and demonstrates its potential for complex scenarios, including future real-world clinical applications.
<div id='section'>Paperid: <span id='pid'>431, <a href='https://arxiv.org/pdf/2502.01932.pdf' target='_blank'>https://arxiv.org/pdf/2502.01932.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zelai Xu, Ruize Zhang, Chao Yu, Huining Yuan, Xiangmin Yi, Shilong Ji, Chuqi Wang, Wenhao Tang, Feng Gao, Wenbo Ding, Xinlei Chen, Yu Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.01932">VolleyBots: A Testbed for Multi-Drone Volleyball Game Combining Motion Control and Strategic Play</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robot sports, characterized by well-defined objectives, explicit rules, and dynamic interactions, present ideal scenarios for demonstrating embodied intelligence. In this paper, we present VolleyBots, a novel robot sports testbed where multiple drones cooperate and compete in the sport of volleyball under physical dynamics. VolleyBots integrates three features within a unified platform: competitive and cooperative gameplay, turn-based interaction structure, and agile 3D maneuvering. Competitive and cooperative gameplay challenges each drone to coordinate with its teammates while anticipating and countering opposing teams' tactics. Turn-based interaction demands precise timing, accurate state prediction, and management of long-horizon temporal dependencies. Agile 3D maneuvering requires rapid accelerations, sharp turns, and precise 3D positioning despite the quadrotor's underactuated dynamics. These intertwined features yield a complex problem combining motion control and strategic play, with no available expert demonstrations. We provide a comprehensive suite of tasks ranging from single-drone drills to multi-drone cooperative and competitive tasks, accompanied by baseline evaluations of representative multi-agent reinforcement learning (MARL) and game-theoretic algorithms. Simulation results show that on-policy reinforcement learning (RL) methods outperform off-policy methods in single-agent tasks, but both approaches struggle in complex tasks that combine motion control and strategic play. We additionally design a hierarchical policy which achieves a 69.5% percent win rate against the strongest baseline in the 3 vs 3 task, underscoring its potential as an effective solution for tackling the complex interplay between low-level control and high-level strategy. The project page is at https://sites.google.com/view/thu-volleybots.
<div id='section'>Paperid: <span id='pid'>432, <a href='https://arxiv.org/pdf/2511.13032.pdf' target='_blank'>https://arxiv.org/pdf/2511.13032.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sheng Liu, Yuanzhi Liang, Jiepeng Wang, Sidan Du, Chi Zhang, Xuelong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.13032">Uni-Inter: Unifying 3D Human Motion Synthesis Across Diverse Interaction Contexts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present Uni-Inter, a unified framework for human motion generation that supports a wide range of interaction scenarios: including human-human, human-object, and human-scene-within a single, task-agnostic architecture. In contrast to existing methods that rely on task-specific designs and exhibit limited generalization, Uni-Inter introduces the Unified Interactive Volume (UIV), a volumetric representation that encodes heterogeneous interactive entities into a shared spatial field. This enables consistent relational reasoning and compound interaction modeling. Motion generation is formulated as joint-wise probabilistic prediction over the UIV, allowing the model to capture fine-grained spatial dependencies and produce coherent, context-aware behaviors. Experiments across three representative interaction tasks demonstrate that Uni-Inter achieves competitive performance and generalizes well to novel combinations of entities. These results suggest that unified modeling of compound interactions offers a promising direction for scalable motion synthesis in complex environments.
<div id='section'>Paperid: <span id='pid'>433, <a href='https://arxiv.org/pdf/2508.10297.pdf' target='_blank'>https://arxiv.org/pdf/2508.10297.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiyi Ma, Yuanzhi Liang, Xiu Li, Chi Zhang, Xuelong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10297">InterSyn: Interleaved Learning for Dynamic Motion Synthesis in the Wild</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present Interleaved Learning for Motion Synthesis (InterSyn), a novel framework that targets the generation of realistic interaction motions by learning from integrated motions that consider both solo and multi-person dynamics. Unlike previous methods that treat these components separately, InterSyn employs an interleaved learning strategy to capture the natural, dynamic interactions and nuanced coordination inherent in real-world scenarios. Our framework comprises two key modules: the Interleaved Interaction Synthesis (INS) module, which jointly models solo and interactive behaviors in a unified paradigm from a first-person perspective to support multiple character interactions, and the Relative Coordination Refinement (REC) module, which refines mutual dynamics and ensures synchronized motions among characters. Experimental results show that the motion sequences generated by InterSyn exhibit higher text-to-motion alignment and improved diversity compared with recent methods, setting a new benchmark for robust and natural motion synthesis. Additionally, our code will be open-sourced in the future to promote further research and development in this area.
<div id='section'>Paperid: <span id='pid'>434, <a href='https://arxiv.org/pdf/2503.23284.pdf' target='_blank'>https://arxiv.org/pdf/2503.23284.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Feng-Lin Liu, Hongbo Fu, Xintao Wang, Weicai Ye, Pengfei Wan, Di Zhang, Lin Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.23284">SketchVideo: Sketch-based Video Generation and Editing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video generation and editing conditioned on text prompts or images have undergone significant advancements. However, challenges remain in accurately controlling global layout and geometry details solely by texts, and supporting motion control and local modification through images. In this paper, we aim to achieve sketch-based spatial and motion control for video generation and support fine-grained editing of real or synthetic videos. Based on the DiT video generation model, we propose a memory-efficient control structure with sketch control blocks that predict residual features of skipped DiT blocks. Sketches are drawn on one or two keyframes (at arbitrary time points) for easy interaction. To propagate such temporally sparse sketch conditions across all frames, we propose an inter-frame attention mechanism to analyze the relationship between the keyframes and each video frame. For sketch-based video editing, we design an additional video insertion module that maintains consistency between the newly edited content and the original video's spatial feature and dynamic motion. During inference, we use latent fusion for the accurate preservation of unedited regions. Extensive experiments demonstrate that our SketchVideo achieves superior performance in controllable video generation and editing.
<div id='section'>Paperid: <span id='pid'>435, <a href='https://arxiv.org/pdf/2509.14915.pdf' target='_blank'>https://arxiv.org/pdf/2509.14915.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shenghai Yuan, Jason Wai Hao Yee, Weixiang Guo, Zhongyuan Liu, Thien-Minh Nguyen, Lihua Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14915">PERAL: Perception-Aware Motion Control for Passive LiDAR Excitation in Spherical Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous mobile robots increasingly rely on LiDAR-IMU odometry for navigation and mapping, yet horizontally mounted LiDARs such as the MID360 capture few near-ground returns, limiting terrain awareness and degrading performance in feature-scarce environments. Prior solutions - static tilt, active rotation, or high-density sensors - either sacrifice horizontal perception or incur added actuators, cost, and power. We introduce PERAL, a perception-aware motion control framework for spherical robots that achieves passive LiDAR excitation without dedicated hardware. By modeling the coupling between internal differential-drive actuation and sensor attitude, PERAL superimposes bounded, non-periodic oscillations onto nominal goal- or trajectory-tracking commands, enriching vertical scan diversity while preserving navigation accuracy. Implemented on a compact spherical robot, PERAL is validated across laboratory, corridor, and tactical environments. Experiments demonstrate up to 96 percent map completeness, a 27 percent reduction in trajectory tracking error, and robust near-ground human detection, all at lower weight, power, and cost compared with static tilt, active rotation, and fixed horizontal baselines. The design and code will be open-sourced upon acceptance.
<div id='section'>Paperid: <span id='pid'>436, <a href='https://arxiv.org/pdf/2408.16426.pdf' target='_blank'>https://arxiv.org/pdf/2408.16426.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiefeng Li, Ye Yuan, Davis Rempe, Haotian Zhang, Pavlo Molchanov, Cewu Lu, Jan Kautz, Umar Iqbal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.16426">COIN: Control-Inpainting Diffusion Prior for Human and Camera Motion Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Estimating global human motion from moving cameras is challenging due to the entanglement of human and camera motions. To mitigate the ambiguity, existing methods leverage learned human motion priors, which however often result in oversmoothed motions with misaligned 2D projections. To tackle this problem, we propose COIN, a control-inpainting motion diffusion prior that enables fine-grained control to disentangle human and camera motions. Although pre-trained motion diffusion models encode rich motion priors, we find it non-trivial to leverage such knowledge to guide global motion estimation from RGB videos. COIN introduces a novel control-inpainting score distillation sampling method to ensure well-aligned, consistent, and high-quality motion from the diffusion prior within a joint optimization framework. Furthermore, we introduce a new human-scene relation loss to alleviate the scale ambiguity by enforcing consistency among the humans, camera, and scene. Experiments on three challenging benchmarks demonstrate the effectiveness of COIN, which outperforms the state-of-the-art methods in terms of global human motion estimation and camera motion estimation. As an illustrative example, COIN outperforms the state-of-the-art method by 33% in world joint position error (W-MPJPE) on the RICH dataset.
<div id='section'>Paperid: <span id='pid'>437, <a href='https://arxiv.org/pdf/2404.15789.pdf' target='_blank'>https://arxiv.org/pdf/2404.15789.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Teng Hu, Jiangning Zhang, Ran Yi, Yating Wang, Hongrui Huang, Jieyu Weng, Yabiao Wang, Lizhuang Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.15789">MotionMaster: Training-free Camera Motion Transfer For Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The emergence of diffusion models has greatly propelled the progress in image and video generation. Recently, some efforts have been made in controllable video generation, including text-to-video generation and video motion control, among which camera motion control is an important topic. However, existing camera motion control methods rely on training a temporal camera module, and necessitate substantial computation resources due to the large amount of parameters in video generation models. Moreover, existing methods pre-define camera motion types during training, which limits their flexibility in camera control. Therefore, to reduce training costs and achieve flexible camera control, we propose COMD, a novel training-free video motion transfer model, which disentangles camera motions and object motions in source videos and transfers the extracted camera motions to new videos. We first propose a one-shot camera motion disentanglement method to extract camera motion from a single source video, which separates the moving objects from the background and estimates the camera motion in the moving objects region based on the motion in the background by solving a Poisson equation. Furthermore, we propose a few-shot camera motion disentanglement method to extract the common camera motion from multiple videos with similar camera motions, which employs a window-based clustering technique to extract the common features in temporal attention maps of multiple videos. Finally, we propose a motion combination method to combine different types of camera motions together, enabling our model a more controllable and flexible camera control. Extensive experiments demonstrate that our training-free approach can effectively decouple camera-object motion and apply the decoupled camera motion to a wide range of controllable video generation tasks, achieving flexible and diverse camera motion control.
<div id='section'>Paperid: <span id='pid'>438, <a href='https://arxiv.org/pdf/2405.15325.pdf' target='_blank'>https://arxiv.org/pdf/2405.15325.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zijian Li, Yifan Shen, Kaitao Zheng, Ruichu Cai, Xiangchen Song, Mingming Gong, Zhengmao Zhu, Guangyi Chen, Kun Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.15325">On the Identification of Temporally Causal Representation with Instantaneous Dependence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Temporally causal representation learning aims to identify the latent causal process from time series observations, but most methods require the assumption that the latent causal processes do not have instantaneous relations. Although some recent methods achieve identifiability in the instantaneous causality case, they require either interventions on the latent variables or grouping of the observations, which are in general difficult to obtain in real-world scenarios. To fill this gap, we propose an \textbf{ID}entification framework for instantane\textbf{O}us \textbf{L}atent dynamics (\textbf{IDOL}) by imposing a sparse influence constraint that the latent causal processes have sparse time-delayed and instantaneous relations. Specifically, we establish identifiability results of the latent causal process based on sufficient variability and the sparse influence constraint by employing contextual information of time series data. Based on these theories, we incorporate a temporally variational inference architecture to estimate the latent variables and a gradient-based sparsity regularization to identify the latent causal process. Experimental results on simulation datasets illustrate that our method can identify the latent causal process. Furthermore, evaluations on multiple human motion forecasting benchmarks with instantaneous dependencies indicate the effectiveness of our method in real-world settings.
<div id='section'>Paperid: <span id='pid'>439, <a href='https://arxiv.org/pdf/2503.06499.pdf' target='_blank'>https://arxiv.org/pdf/2503.06499.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xukun Zhou, Fengxin Li, Ming Chen, Yan Zhou, Pengfei Wan, Di Zhang, Yeying Jin, Zhaoxin Fan, Hongyan Liu, Jun He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.06499">ExGes: Expressive Human Motion Retrieval and Modulation for Audio-Driven Gesture Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Audio-driven human gesture synthesis is a crucial task with broad applications in virtual avatars, human-computer interaction, and creative content generation. Despite notable progress, existing methods often produce gestures that are coarse, lack expressiveness, and fail to fully align with audio semantics. To address these challenges, we propose ExGes, a novel retrieval-enhanced diffusion framework with three key designs: (1) a Motion Base Construction, which builds a gesture library using training dataset; (2) a Motion Retrieval Module, employing constrative learning and momentum distillation for fine-grained reference poses retreiving; and (3) a Precision Control Module, integrating partial masking and stochastic masking to enable flexible and fine-grained control. Experimental evaluations on BEAT2 demonstrate that ExGes reduces FrÃ©chet Gesture Distance by 6.2\% and improves motion diversity by 5.3\% over EMAGE, with user studies revealing a 71.3\% preference for its naturalness and semantic relevance. Code will be released upon acceptance.
<div id='section'>Paperid: <span id='pid'>440, <a href='https://arxiv.org/pdf/2508.00362.pdf' target='_blank'>https://arxiv.org/pdf/2508.00362.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenghan Chen, Haodong Zhang, Dongqi Wang, Jiyu Yu, Haocheng Xu, Yue Wang, Rong Xiong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.00362">A Whole-Body Motion Imitation Framework from Human Data for Full-Size Humanoid Robot</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion imitation is a pivotal and effective approach for humanoid robots to achieve a more diverse range of complex and expressive movements, making their performances more human-like. However, the significant differences in kinematics and dynamics between humanoid robots and humans present a major challenge in accurately imitating motion while maintaining balance. In this paper, we propose a novel whole-body motion imitation framework for a full-size humanoid robot. The proposed method employs contact-aware whole-body motion retargeting to mimic human motion and provide initial values for reference trajectories, and the non-linear centroidal model predictive controller ensures the motion accuracy while maintaining balance and overcoming external disturbances in real time. The assistance of the whole-body controller allows for more precise torque control. Experiments have been conducted to imitate a variety of human motions both in simulation and in a real-world humanoid robot. These experiments demonstrate the capability of performing with accuracy and adaptability, which validates the effectiveness of our approach.
<div id='section'>Paperid: <span id='pid'>441, <a href='https://arxiv.org/pdf/2503.09015.pdf' target='_blank'>https://arxiv.org/pdf/2503.09015.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haodong Zhang, Liang Zhang, Zhenghan Chen, Lu Chen, Yue Wang, Rong Xiong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.09015">Natural Humanoid Robot Locomotion with Generative Motion Prior</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Natural and lifelike locomotion remains a fundamental challenge for humanoid robots to interact with human society. However, previous methods either neglect motion naturalness or rely on unstable and ambiguous style rewards. In this paper, we propose a novel Generative Motion Prior (GMP) that provides fine-grained motion-level supervision for the task of natural humanoid robot locomotion. To leverage natural human motions, we first employ whole-body motion retargeting to effectively transfer them to the robot. Subsequently, we train a generative model offline to predict future natural reference motions for the robot based on a conditional variational auto-encoder. During policy training, the generative motion prior serves as a frozen online motion generator, delivering precise and comprehensive supervision at the trajectory level, including joint angles and keypoint positions. The generative motion prior significantly enhances training stability and improves interpretability by offering detailed and dense guidance throughout the learning process. Experimental results in both simulation and real-world environments demonstrate that our method achieves superior motion naturalness compared to existing approaches. Project page can be found at https://sites.google.com/view/humanoid-gmp
<div id='section'>Paperid: <span id='pid'>442, <a href='https://arxiv.org/pdf/2502.14795.pdf' target='_blank'>https://arxiv.org/pdf/2502.14795.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pengxiang Ding, Jianfei Ma, Xinyang Tong, Binghong Zou, Xinxin Luo, Yiguo Fan, Ting Wang, Hongchao Lu, Panzhong Mo, Jinxin Liu, Yuefan Wang, Huaicheng Zhou, Wenshuo Feng, Jiacheng Liu, Siteng Huang, Donglin Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.14795">Humanoid-VLA: Towards Universal Humanoid Control with Visual Integration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper addresses the limitations of current humanoid robot control frameworks, which primarily rely on reactive mechanisms and lack autonomous interaction capabilities due to data scarcity. We propose Humanoid-VLA, a novel framework that integrates language understanding, egocentric scene perception, and motion control, enabling universal humanoid control. Humanoid-VLA begins with language-motion pre-alignment using non-egocentric human motion datasets paired with textual descriptions, allowing the model to learn universal motion patterns and action semantics. We then incorporate egocentric visual context through a parameter efficient video-conditioned fine-tuning, enabling context-aware motion generation. Furthermore, we introduce a self-supervised data augmentation strategy that automatically generates pseudoannotations directly derived from motion data. This process converts raw motion sequences into informative question-answer pairs, facilitating the effective use of large-scale unlabeled video data. Built upon whole-body control architectures, extensive experiments show that Humanoid-VLA achieves object interaction and environment exploration tasks with enhanced contextual awareness, demonstrating a more human-like capacity for adaptive and intelligent engagement.
<div id='section'>Paperid: <span id='pid'>443, <a href='https://arxiv.org/pdf/2508.13013.pdf' target='_blank'>https://arxiv.org/pdf/2508.13013.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingqiao Xiu, Fangzhou Hong, Yicong Li, Mengze Li, Wentao Wang, Sirui Han, Liang Pan, Ziwei Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.13013">EgoTwin: Dreaming Body and View in First Person</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While exocentric video synthesis has achieved great progress, egocentric video generation remains largely underexplored, which requires modeling first-person view content along with camera motion patterns induced by the wearer's body movements. To bridge this gap, we introduce a novel task of joint egocentric video and human motion generation, characterized by two key challenges: 1) Viewpoint Alignment: the camera trajectory in the generated video must accurately align with the head trajectory derived from human motion; 2) Causal Interplay: the synthesized human motion must causally align with the observed visual dynamics across adjacent video frames. To address these challenges, we propose EgoTwin, a joint video-motion generation framework built on the diffusion transformer architecture. Specifically, EgoTwin introduces a head-centric motion representation that anchors the human motion to the head joint and incorporates a cybernetics-inspired interaction mechanism that explicitly captures the causal interplay between video and motion within attention operations. For comprehensive evaluation, we curate a large-scale real-world dataset of synchronized text-video-motion triplets and design novel metrics to assess video-motion consistency. Extensive experiments demonstrate the effectiveness of the EgoTwin framework.
<div id='section'>Paperid: <span id='pid'>444, <a href='https://arxiv.org/pdf/2503.00948.pdf' target='_blank'>https://arxiv.org/pdf/2503.00948.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jie Tian, Xiaoye Qu, Zhenyi Lu, Wei Wei, Sichen Liu, Yu Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.00948">Extrapolating and Decoupling Image-to-Video Generation Models: Motion Modeling is Easier Than You Think</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image-to-Video (I2V) generation aims to synthesize a video clip according to a given image and condition (e.g., text). The key challenge of this task lies in simultaneously generating natural motions while preserving the original appearance of the images. However, current I2V diffusion models (I2V-DMs) often produce videos with limited motion degrees or exhibit uncontrollable motion that conflicts with the textual condition. To address these limitations, we propose a novel Extrapolating and Decoupling framework, which introduces model merging techniques to the I2V domain for the first time. Specifically, our framework consists of three separate stages: (1) Starting with a base I2V-DM, we explicitly inject the textual condition into the temporal module using a lightweight, learnable adapter and fine-tune the integrated model to improve motion controllability. (2) We introduce a training-free extrapolation strategy to amplify the dynamic range of the motion, effectively reversing the fine-tuning process to enhance the motion degree significantly. (3) With the above two-stage models excelling in motion controllability and degree, we decouple the relevant parameters associated with each type of motion ability and inject them into the base I2V-DM. Since the I2V-DM handles different levels of motion controllability and dynamics at various denoising time steps, we adjust the motion-aware parameters accordingly over time. Extensive qualitative and quantitative experiments have been conducted to demonstrate the superiority of our framework over existing methods.
<div id='section'>Paperid: <span id='pid'>445, <a href='https://arxiv.org/pdf/2406.14558.pdf' target='_blank'>https://arxiv.org/pdf/2406.14558.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiawei Gao, Ziqin Wang, Zeqi Xiao, Jingbo Wang, Tai Wang, Jinkun Cao, Xiaolin Hu, Si Liu, Jifeng Dai, Jiangmiao Pang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.14558">CooHOI: Learning Cooperative Human-Object Interaction with Manipulated Object Dynamics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Enabling humanoid robots to clean rooms has long been a pursued dream within humanoid research communities. However, many tasks require multi-humanoid collaboration, such as carrying large and heavy furniture together. Given the scarcity of motion capture data on multi-humanoid collaboration and the efficiency challenges associated with multi-agent learning, these tasks cannot be straightforwardly addressed using training paradigms designed for single-agent scenarios. In this paper, we introduce Cooperative Human-Object Interaction (CooHOI), a framework designed to tackle the challenge of multi-humanoid object transportation problem through a two-phase learning paradigm: individual skill learning and subsequent policy transfer. First, a single humanoid character learns to interact with objects through imitation learning from human motion priors. Then, the humanoid learns to collaborate with others by considering the shared dynamics of the manipulated object using centralized training and decentralized execution (CTDE) multi-agent RL algorithms. When one agent interacts with the object, resulting in specific object dynamics changes, the other agents learn to respond appropriately, thereby achieving implicit communication and coordination between teammates. Unlike previous approaches that relied on tracking-based methods for multi-humanoid HOI, CooHOI is inherently efficient, does not depend on motion capture data of multi-humanoid interactions, and can be seamlessly extended to include more participants and a wide range of object types.
<div id='section'>Paperid: <span id='pid'>446, <a href='https://arxiv.org/pdf/2512.13093.pdf' target='_blank'>https://arxiv.org/pdf/2512.13093.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingqi Yuan, Tao Yu, Haolin Song, Bo Li, Xin Jin, Hua Chen, Wenjun Zeng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.13093">PvP: Data-Efficient Humanoid Robot Learning with Proprioceptive-Privileged Contrastive Representations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Achieving efficient and robust whole-body control (WBC) is essential for enabling humanoid robots to perform complex tasks in dynamic environments. Despite the success of reinforcement learning (RL) in this domain, its sample inefficiency remains a significant challenge due to the intricate dynamics and partial observability of humanoid robots. To address this limitation, we propose PvP, a Proprioceptive-Privileged contrastive learning framework that leverages the intrinsic complementarity between proprioceptive and privileged states. PvP learns compact and task-relevant latent representations without requiring hand-crafted data augmentations, enabling faster and more stable policy learning. To support systematic evaluation, we develop SRL4Humanoid, the first unified and modular framework that provides high-quality implementations of representative state representation learning (SRL) methods for humanoid robot learning. Extensive experiments on the LimX Oli robot across velocity tracking and motion imitation tasks demonstrate that PvP significantly improves sample efficiency and final performance compared to baseline SRL methods. Our study further provides practical insights into integrating SRL with RL for humanoid WBC, offering valuable guidance for data-efficient humanoid robot learning.
<div id='section'>Paperid: <span id='pid'>447, <a href='https://arxiv.org/pdf/2503.03774.pdf' target='_blank'>https://arxiv.org/pdf/2503.03774.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenmin Huang, Ce Hao, Wei Zhan, Jun Ma, Masayoshi Tomizuka
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.03774">Fair Play in the Fast Lane: Integrating Sportsmanship into Autonomous Racing Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous racing has gained significant attention as a platform for high-speed decision-making and motion control. While existing methods primarily focus on trajectory planning and overtaking strategies, the role of sportsmanship in ensuring fair competition remains largely unexplored. In human racing, rules such as the one-motion rule and the enough-space rule prevent dangerous and unsportsmanlike behavior. However, autonomous racing systems often lack mechanisms to enforce these principles, potentially leading to unsafe maneuvers. This paper introduces a bi-level game-theoretic framework to integrate sportsmanship (SPS) into versus racing. At the high level, we model racing intentions using a Stackelberg game, where Monte Carlo Tree Search (MCTS) is employed to derive optimal strategies. At the low level, vehicle interactions are formulated as a Generalized Nash Equilibrium Problem (GNEP), ensuring that all agents follow sportsmanship constraints while optimizing their trajectories. Simulation results demonstrate the effectiveness of the proposed approach in enforcing sportsmanship rules while maintaining competitive performance. We analyze different scenarios where attackers and defenders adhere to or disregard sportsmanship rules and show how knowledge of these constraints influences strategic decision-making. This work highlights the importance of balancing competition and fairness in autonomous racing and provides a foundation for developing ethical and safe AI-driven racing systems.
<div id='section'>Paperid: <span id='pid'>448, <a href='https://arxiv.org/pdf/2409.10032.pdf' target='_blank'>https://arxiv.org/pdf/2409.10032.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weiliang Tang, Jia-Hui Pan, Wei Zhan, Jianshu Zhou, Huaxiu Yao, Yun-Hui Liu, Masayoshi Tomizuka, Mingyu Ding, Chi-Wing Fu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.10032">Embodiment-Agnostic Action Planning via Object-Part Scene Flow</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Observing that the key for robotic action planning is to understand the target-object motion when its associated part is manipulated by the end effector, we propose to generate the 3D object-part scene flow and extract its transformations to solve the action trajectories for diverse embodiments. The advantage of our approach is that it derives the robot action explicitly from object motion prediction, yielding a more robust policy by understanding the object motions. Also, beyond policies trained on embodiment-centric data, our method is embodiment-agnostic, generalizable across diverse embodiments, and being able to learn from human demonstrations. Our method comprises three components: an object-part predictor to locate the part for the end effector to manipulate, an RGBD video generator to predict future RGBD videos, and a trajectory planner to extract embodiment-agnostic transformation sequences and solve the trajectory for diverse embodiments. Trained on videos even without trajectory data, our method still outperforms existing works significantly by 27.7% and 26.2% on the prevailing virtual environments MetaWorld and Franka-Kitchen, respectively. Furthermore, we conducted real-world experiments, showing that our policy, trained only with human demonstration, can be deployed to various embodiments.
<div id='section'>Paperid: <span id='pid'>449, <a href='https://arxiv.org/pdf/2406.11253.pdf' target='_blank'>https://arxiv.org/pdf/2406.11253.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuan Wang, Zhao Wang, Junhao Gong, Di Huang, Tong He, Wanli Ouyang, Jile Jiao, Xuetao Feng, Qi Dou, Shixiang Tang, Dan Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.11253">Holistic-Motion2D: Scalable Whole-body Human Motion Generation in 2D Space</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we introduce a novel path to $\textit{general}$ human motion generation by focusing on 2D space. Traditional methods have primarily generated human motions in 3D, which, while detailed and realistic, are often limited by the scope of available 3D motion data in terms of both the size and the diversity. To address these limitations, we exploit extensive availability of 2D motion data. We present $\textbf{Holistic-Motion2D}$, the first comprehensive and large-scale benchmark for 2D whole-body motion generation, which includes over 1M in-the-wild motion sequences, each paired with high-quality whole-body/partial pose annotations and textual descriptions. Notably, Holistic-Motion2D is ten times larger than the previously largest 3D motion dataset. We also introduce a baseline method, featuring innovative $\textit{whole-body part-aware attention}$ and $\textit{confidence-aware modeling}$ techniques, tailored for 2D $\underline{\text T}$ext-driv$\underline{\text{EN}}$ whole-bo$\underline{\text D}$y motion gen$\underline{\text{ER}}$ation, namely $\textbf{Tender}$. Extensive experiments demonstrate the effectiveness of $\textbf{Holistic-Motion2D}$ and $\textbf{Tender}$ in generating expressive, diverse, and realistic human motions. We also highlight the utility of 2D motion for various downstream applications and its potential for lifting to 3D motion. The page link is: https://holistic-motion2d.github.io.
<div id='section'>Paperid: <span id='pid'>450, <a href='https://arxiv.org/pdf/2403.06086.pdf' target='_blank'>https://arxiv.org/pdf/2403.06086.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Juanwu Lu, Wei Zhan, Masayoshi Tomizuka, Yeping Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.06086">Towards Generalizable and Interpretable Motion Prediction: A Deep Variational Bayes Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Estimating the potential behavior of the surrounding human-driven vehicles is crucial for the safety of autonomous vehicles in a mixed traffic flow. Recent state-of-the-art achieved accurate prediction using deep neural networks. However, these end-to-end models are usually black boxes with weak interpretability and generalizability. This paper proposes the Goal-based Neural Variational Agent (GNeVA), an interpretable generative model for motion prediction with robust generalizability to out-of-distribution cases. For interpretability, the model achieves target-driven motion prediction by estimating the spatial distribution of long-term destinations with a variational mixture of Gaussians. We identify a causal structure among maps and agents' histories and derive a variational posterior to enhance generalizability. Experiments on motion prediction datasets validate that the fitted model can be interpretable and generalizable and can achieve comparable performance to state-of-the-art results.
<div id='section'>Paperid: <span id='pid'>451, <a href='https://arxiv.org/pdf/2507.11949.pdf' target='_blank'>https://arxiv.org/pdf/2507.11949.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuyang Xu, Zhiyang Dou, Mingyi Shi, Liang Pan, Leo Ho, Jingbo Wang, Yuan Liu, Cheng Lin, Yuexin Ma, Wenping Wang, Taku Komura
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.11949">MOSPA: Human Motion Generation Driven by Spatial Audio</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Enabling virtual humans to dynamically and realistically respond to diverse auditory stimuli remains a key challenge in character animation, demanding the integration of perceptual modeling and motion synthesis. Despite its significance, this task remains largely unexplored. Most previous works have primarily focused on mapping modalities like speech, audio, and music to generate human motion. As of yet, these models typically overlook the impact of spatial features encoded in spatial audio signals on human motion. To bridge this gap and enable high-quality modeling of human movements in response to spatial audio, we introduce the first comprehensive Spatial Audio-Driven Human Motion (SAM) dataset, which contains diverse and high-quality spatial audio and motion data. For benchmarking, we develop a simple yet effective diffusion-based generative framework for human MOtion generation driven by SPatial Audio, termed MOSPA, which faithfully captures the relationship between body motion and spatial audio through an effective fusion mechanism. Once trained, MOSPA could generate diverse realistic human motions conditioned on varying spatial audio inputs. We perform a thorough investigation of the proposed dataset and conduct extensive experiments for benchmarking, where our method achieves state-of-the-art performance on this task. Our model and dataset will be open-sourced upon acceptance. Please refer to our supplementary video for more details.
<div id='section'>Paperid: <span id='pid'>452, <a href='https://arxiv.org/pdf/2502.05996.pdf' target='_blank'>https://arxiv.org/pdf/2502.05996.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gaurav Shetty, Mahya Ramezani, Hamed Habibi, Holger Voos, Jose Luis Sanchez-Lopez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.05996">Motion Control in Multi-Rotor Aerial Robots Using Deep Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper investigates the application of Deep Reinforcement (DRL) Learning to address motion control challenges in drones for additive manufacturing (AM). Drone-based additive manufacturing promises flexible and autonomous material deposition in large-scale or hazardous environments. However, achieving robust real-time control of a multi-rotor aerial robot under varying payloads and potential disturbances remains challenging. Traditional controllers like PID often require frequent parameter re-tuning, limiting their applicability in dynamic scenarios. We propose a DRL framework that learns adaptable control policies for multi-rotor drones performing waypoint navigation in AM tasks. We compare Deep Deterministic Policy Gradient (DDPG) and Twin Delayed Deep Deterministic Policy Gradient (TD3) within a curriculum learning scheme designed to handle increasing complexity. Our experiments show TD3 consistently balances training stability, accuracy, and success, particularly when mass variability is introduced. These findings provide a scalable path toward robust, autonomous drone control in additive manufacturing.
<div id='section'>Paperid: <span id='pid'>453, <a href='https://arxiv.org/pdf/2505.16055.pdf' target='_blank'>https://arxiv.org/pdf/2505.16055.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Patanjali Maithani, Aliasghar Arab, Farshad Khorrami, Prashanth Krishnamurthy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.16055">Proactive Hierarchical Control Barrier Function-Based Safety Prioritization in Close Human-Robot Interaction Scenarios</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In collaborative human-robot environments, the unpredictable and dynamic nature of human motion can lead to situations where collisions become unavoidable. In such cases, it is essential for the robotic system to proactively mitigate potential harm through intelligent control strategies. This paper presents a hierarchical control framework based on Control Barrier Functions (CBFs) designed to ensure safe and adaptive operation of autonomous robotic manipulators during close-proximity human-robot interaction. The proposed method introduces a relaxation variable that enables real-time prioritization of safety constraints, allowing the robot to dynamically manage collision risks based on the criticality of different parts of the human body. A secondary constraint mechanism is incorporated to resolve infeasibility by increasing the priority of imminent threats. The framework is experimentally validated on a Franka Research 3 robot equipped with a ZED2i AI camera for real-time human pose and body detection. Experimental results confirm that the CBF-based controller, integrated with depth sensing, facilitates responsive and safe human-robot collaboration, while providing detailed risk analysis and maintaining robust performance in highly dynamic settings.
<div id='section'>Paperid: <span id='pid'>454, <a href='https://arxiv.org/pdf/2512.04532.pdf' target='_blank'>https://arxiv.org/pdf/2512.04532.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu-Wei Zhan, Xin Wang, Hong Chen, Tongtong Feng, Wei Feng, Ren Wang, Guangyao Li, Qing Li, Wenwu Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.04532">PhyVLLM: Physics-Guided Video Language Model with Motion-Appearance Disentanglement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video Large Language Models (Video LLMs) have shown impressive performance across a wide range of video-language tasks. However, they often fail in scenarios requiring a deeper understanding of physical dynamics. This limitation primarily arises from their reliance on appearance-based matching. Incorporating physical motion modeling is crucial for deeper video understanding, but presents three key challenges: (1) motion signals are often entangled with appearance variations, making it difficult to extract clean physical cues; (2) effective motion modeling requires not only continuous-time motion representations but also capturing physical dynamics; and (3) collecting accurate annotations for physical attributes is costly and often impractical. To address these issues, we propose PhyVLLM, a physical-guided video-language framework that explicitly incorporates physical motion into Video LLMs. Specifically, PhyVLLM disentangles visual appearance and object motion through a dual-branch encoder. To model physical dynamics over time, we incorporate a Neural Ordinary Differential Equation (Neural ODE) module, which generates differentiable physical dynamic representations. The resulting motion-aware representations are projected into the token space of a pretrained LLM, enabling physics reasoning without compromising the model's original multimodal capabilities. To circumvent the need for explicit physical labels, PhyVLLM employs a self-supervised manner to model the continuous evolution of object motion. Experimental results demonstrate that PhyVLLM significantly outperforms state-of-the-art Video LLMs on both physical reasoning and general video understanding tasks, highlighting the advantages of incorporating explicit physical modeling.
<div id='section'>Paperid: <span id='pid'>455, <a href='https://arxiv.org/pdf/2502.02936.pdf' target='_blank'>https://arxiv.org/pdf/2502.02936.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junkun Jiang, Jie Chen, Ho Yin Au, Mingyuan Chen, Wei Xue, Yike Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.02936">Every Angle Is Worth A Second Glance: Mining Kinematic Skeletal Structures from Multi-view Joint Cloud</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-person motion capture over sparse angular observations is a challenging problem under interference from both self- and mutual-occlusions. Existing works produce accurate 2D joint detection, however, when these are triangulated and lifted into 3D, available solutions all struggle in selecting the most accurate candidates and associating them to the correct joint type and target identity. As such, in order to fully utilize all accurate 2D joint location information, we propose to independently triangulate between all same-typed 2D joints from all camera views regardless of their target ID, forming the Joint Cloud. Joint Cloud consist of both valid joints lifted from the same joint type and target ID, as well as falsely constructed ones that are from different 2D sources. These redundant and inaccurate candidates are processed over the proposed Joint Cloud Selection and Aggregation Transformer (JCSAT) involving three cascaded encoders which deeply explore the trajectile, skeletal structural, and view-dependent correlations among all 3D point candidates in the cross-embedding space. An Optimal Token Attention Path (OTAP) module is proposed which subsequently selects and aggregates informative features from these redundant observations for the final prediction of human motion. To demonstrate the effectiveness of JCSAT, we build and publish a new multi-person motion capture dataset BUMocap-X with complex interactions and severe occlusions. Comprehensive experiments over the newly presented as well as benchmark datasets validate the effectiveness of the proposed framework, which outperforms all existing state-of-the-art methods, especially under challenging occlusion scenarios.
<div id='section'>Paperid: <span id='pid'>456, <a href='https://arxiv.org/pdf/2508.02106.pdf' target='_blank'>https://arxiv.org/pdf/2508.02106.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaiyang Ji, Ye Shi, Zichen Jin, Kangyi Chen, Lan Xu, Yuexin Ma, Jingyi Yu, Jingya Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02106">Towards Immersive Human-X Interaction: A Real-Time Framework for Physically Plausible Motion Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Real-time synthesis of physically plausible human interactions remains a critical challenge for immersive VR/AR systems and humanoid robotics. While existing methods demonstrate progress in kinematic motion generation, they often fail to address the fundamental tension between real-time responsiveness, physical feasibility, and safety requirements in dynamic human-machine interactions. We introduce Human-X, a novel framework designed to enable immersive and physically plausible human interactions across diverse entities, including human-avatar, human-humanoid, and human-robot systems. Unlike existing approaches that focus on post-hoc alignment or simplified physics, our method jointly predicts actions and reactions in real-time using an auto-regressive reaction diffusion planner, ensuring seamless synchronization and context-aware responses. To enhance physical realism and safety, we integrate an actor-aware motion tracking policy trained with reinforcement learning, which dynamically adapts to interaction partners' movements while avoiding artifacts like foot sliding and penetration. Extensive experiments on the Inter-X and InterHuman datasets demonstrate significant improvements in motion quality, interaction continuity, and physical plausibility over state-of-the-art methods. Our framework is validated in real-world applications, including virtual reality interface for human-robot interaction, showcasing its potential for advancing human-robot collaboration.
<div id='section'>Paperid: <span id='pid'>457, <a href='https://arxiv.org/pdf/2404.04890.pdf' target='_blank'>https://arxiv.org/pdf/2404.04890.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiangnan Tang, Jingya Wang, Kaiyang Ji, Lan Xu, Jingyi Yu, Ye Shi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.04890">A Unified Diffusion Framework for Scene-aware Human Motion Estimation from Sparse Signals</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Estimating full-body human motion via sparse tracking signals from head-mounted displays and hand controllers in 3D scenes is crucial to applications in AR/VR. One of the biggest challenges to this task is the one-to-many mapping from sparse observations to dense full-body motions, which endowed inherent ambiguities. To help resolve this ambiguous problem, we introduce a new framework to combine rich contextual information provided by scenes to benefit full-body motion tracking from sparse observations. To estimate plausible human motions given sparse tracking signals and 3D scenes, we develop $\text{S}^2$Fusion, a unified framework fusing \underline{S}cene and sparse \underline{S}ignals with a conditional dif\underline{Fusion} model. $\text{S}^2$Fusion first extracts the spatial-temporal relations residing in the sparse signals via a periodic autoencoder, and then produces time-alignment feature embedding as additional inputs. Subsequently, by drawing initial noisy motion from a pre-trained prior, $\text{S}^2$Fusion utilizes conditional diffusion to fuse scene geometry and sparse tracking signals to generate full-body scene-aware motions. The sampling procedure of $\text{S}^2$Fusion is further guided by a specially designed scene-penetration loss and phase-matching loss, which effectively regularizes the motion of the lower body even in the absence of any tracking signals, making the generated motion much more plausible and coherent. Extensive experimental results have demonstrated that our $\text{S}^2$Fusion outperforms the state-of-the-art in terms of estimation quality and smoothness.
<div id='section'>Paperid: <span id='pid'>458, <a href='https://arxiv.org/pdf/2403.11208.pdf' target='_blank'>https://arxiv.org/pdf/2403.11208.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qianyang Wu, Ye Shi, Xiaoshui Huang, Jingyi Yu, Lan Xu, Jingya Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.11208">THOR: Text to Human-Object Interaction Diffusion via Relation Intervention</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper addresses new methodologies to deal with the challenging task of generating dynamic Human-Object Interactions from textual descriptions (Text2HOI). While most existing works assume interactions with limited body parts or static objects, our task involves addressing the variation in human motion, the diversity of object shapes, and the semantic vagueness of object motion simultaneously. To tackle this, we propose a novel Text-guided Human-Object Interaction diffusion model with Relation Intervention (THOR). THOR is a cohesive diffusion model equipped with a relation intervention mechanism. In each diffusion step, we initiate text-guided human and object motion and then leverage human-object relations to intervene in object motion. This intervention enhances the spatial-temporal relations between humans and objects, with human-centric interaction representation providing additional guidance for synthesizing consistent motion from text. To achieve more reasonable and realistic results, interaction losses is introduced at different levels of motion granularity. Moreover, we construct Text-BEHAVE, a Text2HOI dataset that seamlessly integrates textual descriptions with the currently largest publicly available 3D HOI dataset. Both quantitative and qualitative experiments demonstrate the effectiveness of our proposed model.
<div id='section'>Paperid: <span id='pid'>459, <a href='https://arxiv.org/pdf/2402.17171.pdf' target='_blank'>https://arxiv.org/pdf/2402.17171.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiming Ren, Xiao Han, Chengfeng Zhao, Jingya Wang, Lan Xu, Jingyi Yu, Yuexin Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.17171">LiveHPS: LiDAR-based Scene-level Human Pose and Shape Estimation in Free Environment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>For human-centric large-scale scenes, fine-grained modeling for 3D human global pose and shape is significant for scene understanding and can benefit many real-world applications. In this paper, we present LiveHPS, a novel single-LiDAR-based approach for scene-level human pose and shape estimation without any limitation of light conditions and wearable devices. In particular, we design a distillation mechanism to mitigate the distribution-varying effect of LiDAR point clouds and exploit the temporal-spatial geometric and dynamic information existing in consecutive frames to solve the occlusion and noise disturbance. LiveHPS, with its efficient configuration and high-quality output, is well-suited for real-world applications. Moreover, we propose a huge human motion dataset, named FreeMotion, which is collected in various scenarios with diverse human poses, shapes and translations. It consists of multi-modal and multi-view acquisition data from calibrated and synchronized LiDARs, cameras, and IMUs. Extensive experiments on our new dataset and other public datasets demonstrate the SOTA performance and robustness of our approach. We will release our code and dataset soon.
<div id='section'>Paperid: <span id='pid'>460, <a href='https://arxiv.org/pdf/2411.04399.pdf' target='_blank'>https://arxiv.org/pdf/2411.04399.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongsheng Wang, Zehui Feng, Tong Xiao, Genfan Yang, Shengyu Zhang, Fei Wu, Feng Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.04399">ProGraph: Temporally-alignable Probability Guided Graph Topological Modeling for 3D Human Reconstruction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current 3D human motion reconstruction methods from monocular videos rely on features within the current reconstruction window, leading to distortion and deformations in the human structure under local occlusions or blurriness in video frames. To estimate realistic 3D human mesh sequences based on incomplete features, we propose Temporally-alignable Probability Guided Graph Topological Modeling for 3D Human Reconstruction (ProGraph). For missing parts recovery, we exploit the explicit topological-aware probability distribution across the entire motion sequence. To restore the complete human, Graph Topological Modeling (GTM) learns the underlying topological structure, focusing on the relationships inherent in the individual parts. Next, to generate blurred motion parts, Temporal-alignable Probability Distribution (TPDist) utilizes the GTM to predict features based on distribution. This interactive mechanism facilitates motion consistency, allowing the restoration of human parts. Furthermore, Hierarchical Human Loss (HHLoss) constrains the probability distribution errors of inter-frame features during topological structure variation. Our Method achieves superior results than other SOTA methods in addressing occlusions and blurriness on 3DPW.
<div id='section'>Paperid: <span id='pid'>461, <a href='https://arxiv.org/pdf/2511.05124.pdf' target='_blank'>https://arxiv.org/pdf/2511.05124.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Felix Divo, Maurice Kraus, Anh Q. Nguyen, Hao Xue, Imran Razzak, Flora D. Salim, Kristian Kersting, Devendra Singh Dhami
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.05124">QuAnTS: Question Answering on Time Series</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text offers intuitive access to information. This can, in particular, complement the density of numerical time series, thereby allowing improved interactions with time series models to enhance accessibility and decision-making. While the creation of question-answering datasets and models has recently seen remarkable growth, most research focuses on question answering (QA) on vision and text, with time series receiving minute attention. To bridge this gap, we propose a challenging novel time series QA (TSQA) dataset, QuAnTS, for Question Answering on Time Series data. Specifically, we pose a wide variety of questions and answers about human motion in the form of tracked skeleton trajectories. We verify that the large-scale QuAnTS dataset is well-formed and comprehensive through extensive experiments. Thoroughly evaluating existing and newly proposed baselines then lays the groundwork for a deeper exploration of TSQA using QuAnTS. Additionally, we provide human performances as a key reference for gauging the practical usability of such models. We hope to encourage future research on interacting with time series models through text, enabling better decision-making and more transparent systems.
<div id='section'>Paperid: <span id='pid'>462, <a href='https://arxiv.org/pdf/2512.23649.pdf' target='_blank'>https://arxiv.org/pdf/2512.23649.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhe Li, Cheng Chi, Boan Zhu, Yangyang Wei, Shuanghao Bai, Yuheng Ji, Yibo Peng, Tao Huang, Pengwei Wang, Zhongyuan Wang, S. -H. Gary Chan, Chang Xu, Shanghang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.23649">RoboMirror: Understand Before You Imitate for Video to Humanoid Locomotion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humans learn locomotion through visual observation, interpreting visual content first before imitating actions. However, state-of-the-art humanoid locomotion systems rely on either curated motion capture trajectories or sparse text commands, leaving a critical gap between visual understanding and control. Text-to-motion methods suffer from semantic sparsity and staged pipeline errors, while video-based approaches only perform mechanical pose mimicry without genuine visual understanding. We propose RoboMirror, the first retargeting-free video-to-locomotion framework embodying "understand before you imitate". Leveraging VLMs, it distills raw egocentric/third-person videos into visual motion intents, which directly condition a diffusion-based policy to generate physically plausible, semantically aligned locomotion without explicit pose reconstruction or retargeting. Extensive experiments validate the effectiveness of RoboMirror, it enables telepresence via egocentric videos, drastically reduces third-person control latency by 80%, and achieves a 3.7% higher task success rate than baselines. By reframing humanoid control around video understanding, we bridge the visual understanding and action gap.
<div id='section'>Paperid: <span id='pid'>463, <a href='https://arxiv.org/pdf/2510.14952.pdf' target='_blank'>https://arxiv.org/pdf/2510.14952.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhe Li, Cheng Chi, Yangyang Wei, Boan Zhu, Yibo Peng, Tao Huang, Pengwei Wang, Zhongyuan Wang, Shanghang Zhang, Chang Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.14952">From Language to Locomotion: Retargeting-free Humanoid Control via Motion Latent Guidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Natural language offers a natural interface for humanoid robots, but existing language-guided humanoid locomotion pipelines remain cumbersome and unreliable. They typically decode human motion, retarget it to robot morphology, and then track it with a physics-based controller. However, this multi-stage process is prone to cumulative errors, introduces high latency, and yields weak coupling between semantics and control. These limitations call for a more direct pathway from language to action, one that eliminates fragile intermediate stages. Therefore, we present RoboGhost, a retargeting-free framework that directly conditions humanoid policies on language-grounded motion latents. By bypassing explicit motion decoding and retargeting, RoboGhost enables a diffusion-based policy to denoise executable actions directly from noise, preserving semantic intent and supporting fast, reactive control. A hybrid causal transformer-diffusion motion generator further ensures long-horizon consistency while maintaining stability and diversity, yielding rich latent representations for precise humanoid behavior. Extensive experiments demonstrate that RoboGhost substantially reduces deployment latency, improves success rates and tracking accuracy, and produces smooth, semantically aligned locomotion on real humanoids. Beyond text, the framework naturally extends to other modalities such as images, audio, and music, providing a general foundation for vision-language-action humanoid systems.
<div id='section'>Paperid: <span id='pid'>464, <a href='https://arxiv.org/pdf/2503.09985.pdf' target='_blank'>https://arxiv.org/pdf/2503.09985.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiang Zhang, Jiahang Cao, Jingkai Sun, Yecheng Shao, Gang Han, Wen Zhao, Yijie Guo, Renjing Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.09985">ES-Parkour: Advanced Robot Parkour with Bio-inspired Event Camera and Spiking Neural Network</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, quadruped robotics has advanced significantly, particularly in perception and motion control via reinforcement learning, enabling complex motions in challenging environments. Visual sensors like depth cameras enhance stability and robustness but face limitations, such as low operating frequencies relative to joint control and sensitivity to lighting, which hinder outdoor deployment. Additionally, deep neural networks in sensor and control systems increase computational demands. To address these issues, we introduce spiking neural networks (SNNs) and event cameras to perform a challenging quadruped parkour task. Event cameras capture dynamic visual data, while SNNs efficiently process spike sequences, mimicking biological perception. Experimental results demonstrate that this approach significantly outperforms traditional models, achieving excellent parkour performance with just 11.7% of the energy consumption of an artificial neural network (ANN)-based model, yielding an 88.3% energy reduction. By integrating event cameras with SNNs, our work advances robotic reinforcement learning and opens new possibilities for applications in demanding environments.
<div id='section'>Paperid: <span id='pid'>465, <a href='https://arxiv.org/pdf/2503.08338.pdf' target='_blank'>https://arxiv.org/pdf/2503.08338.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingkai Sun, Qiang Zhang, Gang Han, Wen Zhao, Zhe Yong, Yan He, Jiaxu Wang, Jiahang Cao, Yijie Guo, Renjing Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.08338">Trinity: A Modular Humanoid Robot AI System</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, research on humanoid robots has garnered increasing attention. With breakthroughs in various types of artificial intelligence algorithms, embodied intelligence, exemplified by humanoid robots, has been highly anticipated. The advancements in reinforcement learning (RL) algorithms have significantly improved the motion control and generalization capabilities of humanoid robots. Simultaneously, the groundbreaking progress in large language models (LLM) and visual language models (VLM) has brought more possibilities and imagination to humanoid robots. LLM enables humanoid robots to understand complex tasks from language instructions and perform long-term task planning, while VLM greatly enhances the robots' understanding and interaction with their environment. This paper introduces \textcolor{magenta}{Trinity}, a novel AI system for humanoid robots that integrates RL, LLM, and VLM. By combining these technologies, Trinity enables efficient control of humanoid robots in complex environments. This innovative approach not only enhances the capabilities but also opens new avenues for future research and applications of humanoid robotics.
<div id='section'>Paperid: <span id='pid'>466, <a href='https://arxiv.org/pdf/2407.05679.pdf' target='_blank'>https://arxiv.org/pdf/2407.05679.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yumeng Zhang, Shi Gong, Kaixin Xiong, Xiaoqing Ye, Xiaofan Li, Xiao Tan, Fan Wang, Jizhou Huang, Hua Wu, Haifeng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.05679">BEVWorld: A Multimodal World Simulator for Autonomous Driving via Scene-Level BEV Latents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>World models have attracted increasing attention in autonomous driving for their ability to forecast potential future scenarios. In this paper, we propose BEVWorld, a novel framework that transforms multimodal sensor inputs into a unified and compact Bird's Eye View (BEV) latent space for holistic environment modeling. The proposed world model consists of two main components: a multi-modal tokenizer and a latent BEV sequence diffusion model. The multi-modal tokenizer first encodes heterogeneous sensory data, and its decoder reconstructs the latent BEV tokens into LiDAR and surround-view image observations via ray-casting rendering in a self-supervised manner. This enables joint modeling and bidirectional encoding-decoding of panoramic imagery and point cloud data within a shared spatial representation. On top of this, the latent BEV sequence diffusion model performs temporally consistent forecasting of future scenes, conditioned on high-level action tokens, enabling scene-level reasoning over time. Extensive experiments demonstrate the effectiveness of BEVWorld on autonomous driving benchmarks, showcasing its capability in realistic future scene generation and its benefits for downstream tasks such as perception and motion prediction.
<div id='section'>Paperid: <span id='pid'>467, <a href='https://arxiv.org/pdf/2512.13465.pdf' target='_blank'>https://arxiv.org/pdf/2512.13465.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruiyan Wang, Teng Hu, Kaihui Huang, Zihan Su, Ran Yi, Lizhuang Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.13465">PoseAnything: Universal Pose-guided Video Generation with Part-aware Temporal Coherence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pose-guided video generation refers to controlling the motion of subjects in generated video through a sequence of poses. It enables precise control over subject motion and has important applications in animation. However, current pose-guided video generation methods are limited to accepting only human poses as input, thus generalizing poorly to pose of other subjects. To address this issue, we propose PoseAnything, the first universal pose-guided video generation framework capable of handling both human and non-human characters, supporting arbitrary skeletal inputs. To enhance consistency preservation during motion, we introduce Part-aware Temporal Coherence Module, which divides the subject into different parts, establishes part correspondences, and computes cross-attention between corresponding parts across frames to achieve fine-grained part-level consistency. Additionally, we propose Subject and Camera Motion Decoupled CFG, a novel guidance strategy that, for the first time, enables independent camera movement control in pose-guided video generation, by separately injecting subject and camera motion control information into the positive and negative anchors of CFG. Furthermore, we present XPose, a high-quality public dataset containing 50,000 non-human pose-video pairs, along with an automated pipeline for annotation and filtering. Extensive experiments demonstrate that Pose-Anything significantly outperforms state-of-the-art methods in both effectiveness and generalization.
<div id='section'>Paperid: <span id='pid'>468, <a href='https://arxiv.org/pdf/2510.13208.pdf' target='_blank'>https://arxiv.org/pdf/2510.13208.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lianlian Liu, YongKang He, Zhaojie Chu, Xiaofen Xing, Xiangmin Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.13208">MimicParts: Part-aware Style Injection for Speech-Driven 3D Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating stylized 3D human motion from speech signals presents substantial challenges, primarily due to the intricate and fine-grained relationships among speech signals, individual styles, and the corresponding body movements. Current style encoding approaches either oversimplify stylistic diversity or ignore regional motion style differences (e.g., upper vs. lower body), limiting motion realism. Additionally, motion style should dynamically adapt to changes in speech rhythm and emotion, but existing methods often overlook this. To address these issues, we propose MimicParts, a novel framework designed to enhance stylized motion generation based on part-aware style injection and part-aware denoising network. It divides the body into different regions to encode localized motion styles, enabling the model to capture fine-grained regional differences. Furthermore, our part-aware attention block allows rhythm and emotion cues to guide each body region precisely, ensuring that the generated motion aligns with variations in speech rhythm and emotional state. Experimental results show that our method outperforming existing methods showcasing naturalness and expressive 3D human motion sequences.
<div id='section'>Paperid: <span id='pid'>469, <a href='https://arxiv.org/pdf/2508.08991.pdf' target='_blank'>https://arxiv.org/pdf/2508.08991.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zan Wang, Jingze Zhang, Yixin Chen, Baoxiong Jia, Wei Liang, Siyuan Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08991">Spatial-Temporal Multi-Scale Quantization for Flexible Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite significant advancements in human motion generation, current motion representations, typically formulated as discrete frame sequences, still face two critical limitations: (i) they fail to capture motion from a multi-scale perspective, limiting the capability in complex patterns modeling; (ii) they lack compositional flexibility, which is crucial for model's generalization in diverse generation tasks. To address these challenges, we introduce MSQ, a novel quantization method that compresses the motion sequence into multi-scale discrete tokens across spatial and temporal dimensions. MSQ employs distinct encoders to capture body parts at varying spatial granularities and temporally interpolates the encoded features into multiple scales before quantizing them into discrete tokens. Building on this representation, we establish a generative mask modeling model to effectively support motion editing, motion control, and conditional motion generation. Through quantitative and qualitative analysis, we show that our quantization method enables the seamless composition of motion tokens without requiring specialized design or re-training. Furthermore, extensive evaluations demonstrate that our approach outperforms existing baseline methods on various benchmarks.
<div id='section'>Paperid: <span id='pid'>470, <a href='https://arxiv.org/pdf/2507.23305.pdf' target='_blank'>https://arxiv.org/pdf/2507.23305.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yixuan Dang, Qinyang Xu, Yu Zhang, Xiangtong Yao, Liding Zhang, Zhenshan Bing, Florian Roehrbein, Alois Knoll
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.23305">Whisker-based Active Tactile Perception for Contour Reconstruction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Perception using whisker-inspired tactile sensors currently faces a major challenge: the lack of active control in robots based on direct contact information from the whisker. To accurately reconstruct object contours, it is crucial for the whisker sensor to continuously follow and maintain an appropriate relative touch pose on the surface. This is especially important for localization based on tip contact, which has a low tolerance for sharp surfaces and must avoid slipping into tangential contact. In this paper, we first construct a magnetically transduced whisker sensor featuring a compact and robust suspension system composed of three flexible spiral arms. We develop a method that leverages a characterized whisker deflection profile to directly extract the tip contact position using gradient descent, with a Bayesian filter applied to reduce fluctuations. We then propose an active motion control policy to maintain the optimal relative pose of the whisker sensor against the object surface. A B-Spline curve is employed to predict the local surface curvature and determine the sensor orientation. Results demonstrate that our algorithm can effectively track objects and reconstruct contours with sub-millimeter accuracy. Finally, we validate the method in simulations and real-world experiments where a robot arm drives the whisker sensor to follow the surfaces of three different objects.
<div id='section'>Paperid: <span id='pid'>471, <a href='https://arxiv.org/pdf/2506.18680.pdf' target='_blank'>https://arxiv.org/pdf/2506.18680.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anindita Ghosh, Bing Zhou, Rishabh Dabral, Jian Wang, Vladislav Golyanik, Christian Theobalt, Philipp Slusallek, Chuan Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.18680">DuetGen: Music Driven Two-Person Dance Generation via Hierarchical Masked Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present DuetGen, a novel framework for generating interactive two-person dances from music. The key challenge of this task lies in the inherent complexities of two-person dance interactions, where the partners need to synchronize both with each other and with the music. Inspired by the recent advances in motion synthesis, we propose a two-stage solution: encoding two-person motions into discrete tokens and then generating these tokens from music. To effectively capture intricate interactions, we represent both dancers' motions as a unified whole to learn the necessary motion tokens, and adopt a coarse-to-fine learning strategy in both the stages. Our first stage utilizes a VQ-VAE that hierarchically separates high-level semantic features at a coarse temporal resolution from low-level details at a finer resolution, producing two discrete token sequences at different abstraction levels. Subsequently, in the second stage, two generative masked transformers learn to map music signals to these dance tokens: the first producing high-level semantic tokens, and the second, conditioned on music and these semantic tokens, producing the low-level tokens. We train both transformers to learn to predict randomly masked tokens within the sequence, enabling them to iteratively generate motion tokens by filling an empty token sequence during inference. Through the hierarchical masked modeling and dedicated interaction representation, DuetGen achieves the generation of synchronized and interactive two-person dances across various genres. Extensive experiments and user studies on a benchmark duet dance dataset demonstrate state-of-the-art performance of DuetGen in motion realism, music-dance alignment, and partner coordination.
<div id='section'>Paperid: <span id='pid'>472, <a href='https://arxiv.org/pdf/2506.10353.pdf' target='_blank'>https://arxiv.org/pdf/2506.10353.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Runqi Ouyang, Haoyun Li, Zhenyuan Zhang, Xiaofeng Wang, Zheng Zhu, Guan Huang, Xingang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.10353">Motion-R1: Chain-of-Thought Reasoning and Reinforcement Learning for Human Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in large language models, especially in natural language understanding and reasoning, have opened new possibilities for text-to-motion generation. Although existing approaches have made notable progress in semantic alignment and motion synthesis, they often rely on end-to-end mapping strategies that fail to capture deep linguistic structures and logical reasoning. Consequently, generated motions tend to lack controllability, consistency, and diversity. To address these limitations, we propose Motion-R1, a unified motion-language modeling framework that integrates a Chain-of-Thought mechanism. By explicitly decomposing complex textual instructions into logically structured action paths, Motion-R1 provides high-level semantic guidance for motion generation, significantly enhancing the model's ability to interpret and execute multi-step, long-horizon, and compositionally rich commands. To train our model, we adopt Group Relative Policy Optimization, a reinforcement learning algorithm designed for large models, which leverages motion quality feedback to optimize reasoning chains and motion synthesis jointly. Extensive experiments across multiple benchmark datasets demonstrate that Motion-R1 achieves competitive or superior performance compared to state-of-the-art methods, particularly in scenarios requiring nuanced semantic understanding and long-term temporal coherence. The code, model and data will be publicly available.
<div id='section'>Paperid: <span id='pid'>473, <a href='https://arxiv.org/pdf/2503.24026.pdf' target='_blank'>https://arxiv.org/pdf/2503.24026.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Boyuan Wang, Xiaofeng Wang, Chaojun Ni, Guosheng Zhao, Zhiqin Yang, Zheng Zhu, Muyang Zhang, Yukun Zhou, Xinze Chen, Guan Huang, Lihong Liu, Xingang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.24026">HumanDreamer: Generating Controllable Human-Motion Videos via Decoupled Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human-motion video generation has been a challenging task, primarily due to the difficulty inherent in learning human body movements. While some approaches have attempted to drive human-centric video generation explicitly through pose control, these methods typically rely on poses derived from existing videos, thereby lacking flexibility. To address this, we propose HumanDreamer, a decoupled human video generation framework that first generates diverse poses from text prompts and then leverages these poses to generate human-motion videos. Specifically, we propose MotionVid, the largest dataset for human-motion pose generation. Based on the dataset, we present MotionDiT, which is trained to generate structured human-motion poses from text prompts. Besides, a novel LAMA loss is introduced, which together contribute to a significant improvement in FID by 62.4%, along with respective enhancements in R-precision for top1, top2, and top3 by 41.8%, 26.3%, and 18.3%, thereby advancing both the Text-to-Pose control accuracy and FID metrics. Our experiments across various Pose-to-Video baselines demonstrate that the poses generated by our method can produce diverse and high-quality human-motion videos. Furthermore, our model can facilitate other downstream tasks, such as pose sequence prediction and 2D-3D motion lifting.
<div id='section'>Paperid: <span id='pid'>474, <a href='https://arxiv.org/pdf/2502.07869.pdf' target='_blank'>https://arxiv.org/pdf/2502.07869.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Christen Millerdurai, Hiroyasu Akada, Jian Wang, Diogo Luvizon, Alain Pagani, Didier Stricker, Christian Theobalt, Vladislav Golyanik
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.07869">EventEgo3D++: 3D Human Motion Capture from a Head-Mounted Event Camera</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Monocular egocentric 3D human motion capture remains a significant challenge, particularly under conditions of low lighting and fast movements, which are common in head-mounted device applications. Existing methods that rely on RGB cameras often fail under these conditions. To address these limitations, we introduce EventEgo3D++, the first approach that leverages a monocular event camera with a fisheye lens for 3D human motion capture. Event cameras excel in high-speed scenarios and varying illumination due to their high temporal resolution, providing reliable cues for accurate 3D human motion capture. EventEgo3D++ leverages the LNES representation of event streams to enable precise 3D reconstructions. We have also developed a mobile head-mounted device (HMD) prototype equipped with an event camera, capturing a comprehensive dataset that includes real event observations from both controlled studio environments and in-the-wild settings, in addition to a synthetic dataset. Additionally, to provide a more holistic dataset, we include allocentric RGB streams that offer different perspectives of the HMD wearer, along with their corresponding SMPL body model. Our experiments demonstrate that EventEgo3D++ achieves superior 3D accuracy and robustness compared to existing solutions, even in challenging conditions. Moreover, our method supports real-time 3D pose updates at a rate of 140Hz. This work is an extension of the EventEgo3D approach (CVPR 2024) and further advances the state of the art in egocentric 3D human motion capture. For more details, visit the project page at https://eventego3d.mpi-inf.mpg.de.
<div id='section'>Paperid: <span id='pid'>475, <a href='https://arxiv.org/pdf/2404.08640.pdf' target='_blank'>https://arxiv.org/pdf/2404.08640.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Christen Millerdurai, Hiroyasu Akada, Jian Wang, Diogo Luvizon, Christian Theobalt, Vladislav Golyanik
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.08640">EventEgo3D: 3D Human Motion Capture from Egocentric Event Streams</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Monocular egocentric 3D human motion capture is a challenging and actively researched problem. Existing methods use synchronously operating visual sensors (e.g. RGB cameras) and often fail under low lighting and fast motions, which can be restricting in many applications involving head-mounted devices. In response to the existing limitations, this paper 1) introduces a new problem, i.e., 3D human motion capture from an egocentric monocular event camera with a fisheye lens, and 2) proposes the first approach to it called EventEgo3D (EE3D). Event streams have high temporal resolution and provide reliable cues for 3D human motion capture under high-speed human motions and rapidly changing illumination. The proposed EE3D framework is specifically tailored for learning with event streams in the LNES representation, enabling high 3D reconstruction accuracy. We also design a prototype of a mobile head-mounted device with an event camera and record a real dataset with event observations and the ground-truth 3D human poses (in addition to the synthetic dataset). Our EE3D demonstrates robustness and superior 3D accuracy compared to existing solutions across various challenging experiments while supporting real-time 3D pose update rates of 140Hz.
<div id='section'>Paperid: <span id='pid'>476, <a href='https://arxiv.org/pdf/2403.18036.pdf' target='_blank'>https://arxiv.org/pdf/2403.18036.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zan Wang, Yixin Chen, Baoxiong Jia, Puhao Li, Jinlu Zhang, Jingze Zhang, Tengyu Liu, Yixin Zhu, Wei Liang, Siyuan Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.18036">Move as You Say, Interact as You Can: Language-guided Human Motion Generation with Scene Affordance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite significant advancements in text-to-motion synthesis, generating language-guided human motion within 3D environments poses substantial challenges. These challenges stem primarily from (i) the absence of powerful generative models capable of jointly modeling natural language, 3D scenes, and human motion, and (ii) the generative models' intensive data requirements contrasted with the scarcity of comprehensive, high-quality, language-scene-motion datasets. To tackle these issues, we introduce a novel two-stage framework that employs scene affordance as an intermediate representation, effectively linking 3D scene grounding and conditional motion generation. Our framework comprises an Affordance Diffusion Model (ADM) for predicting explicit affordance map and an Affordance-to-Motion Diffusion Model (AMDM) for generating plausible human motions. By leveraging scene affordance maps, our method overcomes the difficulty in generating human motion under multimodal condition signals, especially when training with limited data lacking extensive language-scene-motion pairs. Our extensive experiments demonstrate that our approach consistently outperforms all baselines on established benchmarks, including HumanML3D and HUMANISE. Additionally, we validate our model's exceptional generalization capabilities on a specially curated evaluation set featuring previously unseen descriptions and scenes.
<div id='section'>Paperid: <span id='pid'>477, <a href='https://arxiv.org/pdf/2507.17406.pdf' target='_blank'>https://arxiv.org/pdf/2507.17406.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ayce Idil Aytekin, Chuqiao Li, Diogo Luvizon, Rishabh Dabral, Martin Oswald, Marc Habermann, Christian Theobalt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.17406">Physics-based Human Pose Estimation from a Single Moving RGB Camera</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Most monocular and physics-based human pose tracking methods, while achieving state-of-the-art results, suffer from artifacts when the scene does not have a strictly flat ground plane or when the camera is moving. Moreover, these methods are often evaluated on in-the-wild real world videos without ground-truth data or on synthetic datasets, which fail to model the real world light transport, camera motion, and pose-induced appearance and geometry changes. To tackle these two problems, we introduce MoviCam, the first non-synthetic dataset containing ground-truth camera trajectories of a dynamically moving monocular RGB camera, scene geometry, and 3D human motion with human-scene contact labels. Additionally, we propose PhysDynPose, a physics-based method that incorporates scene geometry and physical constraints for more accurate human motion tracking in case of camera motion and non-flat scenes. More precisely, we use a state-of-the-art kinematics estimator to obtain the human pose and a robust SLAM method to capture the dynamic camera trajectory, enabling the recovery of the human pose in the world frame. We then refine the kinematic pose estimate using our scene-aware physics optimizer. From our new benchmark, we found that even state-of-the-art methods struggle with this inherently challenging setting, i.e. a moving camera and non-planar environments, while our method robustly estimates both human and camera poses in world coordinates.
<div id='section'>Paperid: <span id='pid'>478, <a href='https://arxiv.org/pdf/2507.06405.pdf' target='_blank'>https://arxiv.org/pdf/2507.06405.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lala Shakti Swarup Ray, Mengxi Liu, Deepika Gurung, Bo Zhou, Sungho Suh, Paul Lukowicz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06405">SImpHAR: Advancing impedance-based human activity recognition using 3D simulation and text-to-motion models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human Activity Recognition (HAR) with wearable sensors is essential for applications in healthcare, fitness, and human-computer interaction. Bio-impedance sensing offers unique advantages for fine-grained motion capture but remains underutilized due to the scarcity of labeled data. We introduce SImpHAR, a novel framework addressing this limitation through two core contributions. First, we propose a simulation pipeline that generates realistic bio-impedance signals from 3D human meshes using shortest-path estimation, soft-body physics, and text-to-motion generation serving as a digital twin for data augmentation. Second, we design a two-stage training strategy with decoupled approach that enables broader activity coverage without requiring label-aligned synthetic data. We evaluate SImpHAR on our collected ImpAct dataset and two public benchmarks, showing consistent improvements over state-of-the-art methods, with gains of up to 22.3% and 21.8%, in terms of accuracy and macro F1 score, respectively. Our results highlight the promise of simulation-driven augmentation and modular training for impedance-based HAR.
<div id='section'>Paperid: <span id='pid'>479, <a href='https://arxiv.org/pdf/2506.09995.pdf' target='_blank'>https://arxiv.org/pdf/2506.09995.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuanpeng Tu, Hao Luo, Xi Chen, Xiang Bai, Fan Wang, Hengshuang Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.09995">PlayerOne: Egocentric World Simulator</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce PlayerOne, the first egocentric realistic world simulator, facilitating immersive and unrestricted exploration within vividly dynamic environments. Given an egocentric scene image from the user, PlayerOne can accurately construct the corresponding world and generate egocentric videos that are strictly aligned with the real scene human motion of the user captured by an exocentric camera. PlayerOne is trained in a coarse-to-fine pipeline that first performs pretraining on large-scale egocentric text-video pairs for coarse-level egocentric understanding, followed by finetuning on synchronous motion-video data extracted from egocentric-exocentric video datasets with our automatic construction pipeline. Besides, considering the varying importance of different components, we design a part-disentangled motion injection scheme, enabling precise control of part-level movements. In addition, we devise a joint reconstruction framework that progressively models both the 4D scene and video frames, ensuring scene consistency in the long-form video generation. Experimental results demonstrate its great generalization ability in precise control of varying human movements and worldconsistent modeling of diverse scenarios. It marks the first endeavor into egocentric real-world simulation and can pave the way for the community to delve into fresh frontiers of world modeling and its diverse applications.
<div id='section'>Paperid: <span id='pid'>480, <a href='https://arxiv.org/pdf/2506.00043.pdf' target='_blank'>https://arxiv.org/pdf/2506.00043.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jusheng Zhang, Jinzhou Tang, Sidi Liu, Mingyan Li, Sheng Zhang, Jian Wang, Keze Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.00043">From Motion to Behavior: Hierarchical Modeling of Humanoid Generative Behavior Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion generative modeling or synthesis aims to characterize complicated human motions of daily activities in diverse real-world environments. However, current research predominantly focuses on either low-level, short-period motions or high-level action planning, without taking into account the hierarchical goal-oriented nature of human activities. In this work, we take a step forward from human motion generation to human behavior modeling, which is inspired by cognitive science. We present a unified framework, dubbed Generative Behavior Control (GBC), to model diverse human motions driven by various high-level intentions by aligning motions with hierarchical behavior plans generated by large language models (LLMs). Our insight is that human motions can be jointly controlled by task and motion planning in robotics, but guided by LLMs to achieve improved motion diversity and physical fidelity. Meanwhile, to overcome the limitations of existing benchmarks, i.e., lack of behavioral plans, we propose GBC-100K dataset annotated with a hierarchical granularity of semantic and motion plans driven by target goals. Our experiments demonstrate that GBC can generate more diverse and purposeful high-quality human motions with 10* longer horizons compared with existing methods when trained on GBC-100K, laying a foundation for future research on behavioral modeling of human motions. Our dataset and source code will be made publicly available.
<div id='section'>Paperid: <span id='pid'>481, <a href='https://arxiv.org/pdf/2504.04338.pdf' target='_blank'>https://arxiv.org/pdf/2504.04338.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alexander Naumann, Xunjiang Gu, Tolga Dimlioglu, Mariusz Bojarski, Alperen Degirmenci, Alexander Popov, Devansh Bisla, Marco Pavone, Urs MÃ¼ller, Boris Ivanovic
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.04338">Data Scaling Laws for End-to-End Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous vehicle (AV) stacks have traditionally relied on decomposed approaches, with separate modules handling perception, prediction, and planning. However, this design introduces information loss during inter-module communication, increases computational overhead, and can lead to compounding errors. To address these challenges, recent works have proposed architectures that integrate all components into an end-to-end differentiable model, enabling holistic system optimization. This shift emphasizes data engineering over software integration, offering the potential to enhance system performance by simply scaling up training resources. In this work, we evaluate the performance of a simple end-to-end driving architecture on internal driving datasets ranging in size from 16 to 8192 hours with both open-loop metrics and closed-loop simulations. Specifically, we investigate how much additional training data is needed to achieve a target performance gain, e.g., a 5% improvement in motion prediction accuracy. By understanding the relationship between model performance and training dataset size, we aim to provide insights for data-driven decision-making in autonomous driving development.
<div id='section'>Paperid: <span id='pid'>482, <a href='https://arxiv.org/pdf/2503.17978.pdf' target='_blank'>https://arxiv.org/pdf/2503.17978.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dominique Nshimyimana, Vitor Fortes Rey, Sungho Suh, Bo Zhou, Paul Lukowicz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.17978">PIM: Physics-Informed Multi-task Pre-training for Improving Inertial Sensor-Based Human Activity Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human activity recognition (HAR) with deep learning models relies on large amounts of labeled data, often challenging to obtain due to associated cost, time, and labor. Self-supervised learning (SSL) has emerged as an effective approach to leverage unlabeled data through pretext tasks, such as masked reconstruction and multitask learning with signal processing-based data augmentations, to pre-train encoder models. However, such methods are often derived from computer vision approaches that disregard physical mechanisms and constraints that govern wearable sensor data and the phenomena they reflect. In this paper, we propose a physics-informed multi-task pre-training (PIM) framework for IMU-based HAR. PIM generates pre-text tasks based on the understanding of basic physical aspects of human motion: including movement speed, angles of movement, and symmetry between sensor placements. Given a sensor signal, we calculate corresponding features using physics-based equations and use them as pretext tasks for SSL. This enables the model to capture fundamental physical characteristics of human activities, which is especially relevant for multi-sensor systems. Experimental evaluations on four HAR benchmark datasets demonstrate that the proposed method outperforms existing state-of-the-art methods, including data augmentation and masked reconstruction, in terms of accuracy and F1 score. We have observed gains of almost 10\% in macro f1 score and accuracy with only 2 to 8 labeled examples per class and up to 3% when there is no reduction in the amount of training data.
<div id='section'>Paperid: <span id='pid'>483, <a href='https://arxiv.org/pdf/2501.01427.pdf' target='_blank'>https://arxiv.org/pdf/2501.01427.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuanpeng Tu, Hao Luo, Xi Chen, Sihui Ji, Xiang Bai, Hengshuang Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.01427">VideoAnydoor: High-fidelity Video Object Insertion with Precise Motion Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite significant advancements in video generation, inserting a given object into videos remains a challenging task. The difficulty lies in preserving the appearance details of the reference object and accurately modeling coherent motions at the same time. In this paper, we propose VideoAnydoor, a zero-shot video object insertion framework with high-fidelity detail preservation and precise motion control. Starting from a text-to-video model, we utilize an ID extractor to inject the global identity and leverage a box sequence to control the overall motion. To preserve the detailed appearance and meanwhile support fine-grained motion control, we design a pixel warper. It takes the reference image with arbitrary key-points and the corresponding key-point trajectories as inputs. It warps the pixel details according to the trajectories and fuses the warped features with the diffusion U-Net, thus improving detail preservation and supporting users in manipulating the motion trajectories. In addition, we propose a training strategy involving both videos and static images with a weighted loss to enhance insertion quality. VideoAnydoor demonstrates significant superiority over existing methods and naturally supports various downstream applications (e.g., talking head generation, video virtual try-on, multi-region editing) without task-specific fine-tuning.
<div id='section'>Paperid: <span id='pid'>484, <a href='https://arxiv.org/pdf/2406.01867.pdf' target='_blank'>https://arxiv.org/pdf/2406.01867.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kengo Uchida, Takashi Shibuya, Yuhta Takida, Naoki Murata, Julian Tanke, Shusuke Takahashi, Yuki Mitsufuji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.01867">MoLA: Motion Generation and Editing with Latent Diffusion Enhanced by Adversarial Training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In text-to-motion generation, controllability as well as generation quality and speed has become increasingly critical. The controllability challenges include generating a motion of a length that matches the given textual description and editing the generated motions according to control signals, such as the start-end positions and the pelvis trajectory. In this paper, we propose MoLA, which provides fast, high-quality, variable-length motion generation and can also deal with multiple editing tasks in a single framework. Our approach revisits the motion representation used as inputs and outputs in the model, incorporating an activation variable to enable variable-length motion generation. Additionally, we integrate a variational autoencoder and a latent diffusion model, further enhanced through adversarial training, to achieve high-quality and fast generation. Moreover, we apply a training-free guided generation framework to achieve various editing tasks with motion control inputs. We quantitatively show the effectiveness of adversarial learning in text-to-motion generation, and demonstrate the applicability of our editing framework to multiple editing tasks in the motion domain.
<div id='section'>Paperid: <span id='pid'>485, <a href='https://arxiv.org/pdf/2403.17936.pdf' target='_blank'>https://arxiv.org/pdf/2403.17936.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Muhammad Hamza Mughal, Rishabh Dabral, Ikhsanul Habibie, Lucia Donatelli, Marc Habermann, Christian Theobalt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.17936">ConvoFusion: Multi-Modal Conversational Diffusion for Co-Speech Gesture Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Gestures play a key role in human communication. Recent methods for co-speech gesture generation, while managing to generate beat-aligned motions, struggle generating gestures that are semantically aligned with the utterance. Compared to beat gestures that align naturally to the audio signal, semantically coherent gestures require modeling the complex interactions between the language and human motion, and can be controlled by focusing on certain words. Therefore, we present ConvoFusion, a diffusion-based approach for multi-modal gesture synthesis, which can not only generate gestures based on multi-modal speech inputs, but can also facilitate controllability in gesture synthesis. Our method proposes two guidance objectives that allow the users to modulate the impact of different conditioning modalities (e.g. audio vs text) as well as to choose certain words to be emphasized during gesturing. Our method is versatile in that it can be trained either for generating monologue gestures or even the conversational gestures. To further advance the research on multi-party interactive gestures, the DnD Group Gesture dataset is released, which contains 6 hours of gesture data showing 5 people interacting with one another. We compare our method with several recent works and demonstrate effectiveness of our method on a variety of tasks. We urge the reader to watch our supplementary video at our website.
<div id='section'>Paperid: <span id='pid'>486, <a href='https://arxiv.org/pdf/2501.04325.pdf' target='_blank'>https://arxiv.org/pdf/2501.04325.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhi-Lin Huang, Yixuan Liu, Chujun Qin, Zhongdao Wang, Dong Zhou, Dong Li, Emad Barsoum
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.04325">Edit as You See: Image-guided Video Editing via Masked Motion Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in diffusion models have significantly facilitated text-guided video editing. However, there is a relative scarcity of research on image-guided video editing, a method that empowers users to edit videos by merely indicating a target object in the initial frame and providing an RGB image as reference, without relying on the text prompts. In this paper, we propose a novel Image-guided Video Editing Diffusion model, termed IVEDiff for the image-guided video editing. IVEDiff is built on top of image editing models, and is equipped with learnable motion modules to maintain the temporal consistency of edited video. Inspired by self-supervised learning concepts, we introduce a masked motion modeling fine-tuning strategy that empowers the motion module's capabilities for capturing inter-frame motion dynamics, while preserving the capabilities for intra-frame semantic correlations modeling of the base image editing model. Moreover, an optical-flow-guided motion reference network is proposed to ensure the accurate propagation of information between edited video frames, alleviating the misleading effects of invalid information. We also construct a benchmark to facilitate further research. The comprehensive experiments demonstrate that our method is able to generate temporally smooth edited videos while robustly dealing with various editing objects with high quality.
<div id='section'>Paperid: <span id='pid'>487, <a href='https://arxiv.org/pdf/2405.13865.pdf' target='_blank'>https://arxiv.org/pdf/2405.13865.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chong Mou, Mingdeng Cao, Xintao Wang, Zhaoyang Zhang, Ying Shan, Jian Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.13865">ReVideo: Remake a Video with Motion and Content Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite significant advancements in video generation and editing using diffusion models, achieving accurate and localized video editing remains a substantial challenge. Additionally, most existing video editing methods primarily focus on altering visual content, with limited research dedicated to motion editing. In this paper, we present a novel attempt to Remake a Video (ReVideo) which stands out from existing methods by allowing precise video editing in specific areas through the specification of both content and motion. Content editing is facilitated by modifying the first frame, while the trajectory-based motion control offers an intuitive user interaction experience. ReVideo addresses a new task involving the coupling and training imbalance between content and motion control. To tackle this, we develop a three-stage training strategy that progressively decouples these two aspects from coarse to fine. Furthermore, we propose a spatiotemporal adaptive fusion module to integrate content and motion control across various sampling steps and spatial locations. Extensive experiments demonstrate that our ReVideo has promising performance on several accurate video editing applications, i.e., (1) locally changing video content while keeping the motion constant, (2) keeping content unchanged and customizing new motion trajectories, (3) modifying both content and motion trajectories. Our method can also seamlessly extend these applications to multi-area editing without specific training, demonstrating its flexibility and robustness.
<div id='section'>Paperid: <span id='pid'>488, <a href='https://arxiv.org/pdf/2505.09827.pdf' target='_blank'>https://arxiv.org/pdf/2505.09827.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Julian Tanke, Takashi Shibuya, Kengo Uchida, Koichi Saito, Yuki Mitsufuji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.09827">Dyadic Mamba: Long-term Dyadic Human Motion Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating realistic dyadic human motion from text descriptions presents significant challenges, particularly for extended interactions that exceed typical training sequence lengths. While recent transformer-based approaches have shown promising results for short-term dyadic motion synthesis, they struggle with longer sequences due to inherent limitations in positional encoding schemes. In this paper, we introduce Dyadic Mamba, a novel approach that leverages State-Space Models (SSMs) to generate high-quality dyadic human motion of arbitrary length. Our method employs a simple yet effective architecture that facilitates information flow between individual motion sequences through concatenation, eliminating the need for complex cross-attention mechanisms. We demonstrate that Dyadic Mamba achieves competitive performance on standard short-term benchmarks while significantly outperforming transformer-based approaches on longer sequences. Additionally, we propose a new benchmark for evaluating long-term motion synthesis quality, providing a standardized framework for future research. Our results demonstrate that SSM-based architectures offer a promising direction for addressing the challenging task of long-term dyadic human motion synthesis from text descriptions.
<div id='section'>Paperid: <span id='pid'>489, <a href='https://arxiv.org/pdf/2511.17961.pdf' target='_blank'>https://arxiv.org/pdf/2511.17961.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Wang, Xiaobao Wei, Ying Li, Qingpo Wuwu, Dongli Wu, Jiajun Cao, Ming Lu, Wenzhao Zheng, Shanghang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.17961">RoboArmGS: High-Quality Robotic Arm Splatting via Bézier Curve Refinement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Building high-quality digital assets of robotic arms is crucial yet challenging for the Real2Sim2Real pipeline. Current approaches naively bind static 3D Gaussians according to URDF links, forcing them to follow an URDF-rigged motion passively. However, real-world arm motion is noisy, and the idealized URDF-rigged motion cannot accurately model it, leading to severe rendering artifacts in 3D Gaussians. To address these challenges, we propose RoboArmGS, a novel hybrid representation that refines the URDF-rigged motion with learnable Bézier curves, enabling more accurate real-world motion modeling. To be more specific, we present a learnable Bézier Curve motion refiner that corrects per-joint residuals to address mismatches between real-world motion and URDF-rigged motion. RoboArmGS enables the learning of more accurate real-world motion while achieving a coherent binding of 3D Gaussians across arm parts. To support future research, we contribute a carefully collected dataset named RoboArm4D, which comprises several widely used robotic arms for evaluating the quality of building high-quality digital assets. We evaluate our approach on RoboArm4D, and RoboArmGS achieves state-of-the-art performance in real-world motion modeling and rendering quality. The code and dataset will be released.
<div id='section'>Paperid: <span id='pid'>490, <a href='https://arxiv.org/pdf/2505.18780.pdf' target='_blank'>https://arxiv.org/pdf/2505.18780.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yahao Fan, Tianxiang Gui, Kaiyang Ji, Shutong Ding, Chixuan Zhang, Jiayuan Gu, Jingyi Yu, Jingya Wang, Ye Shi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.18780">One Policy but Many Worlds: A Scalable Unified Policy for Versatile Humanoid Locomotion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humanoid locomotion faces a critical scalability challenge: traditional reinforcement learning (RL) methods require task-specific rewards and struggle to leverage growing datasets, even as more training terrains are introduced. We propose DreamPolicy, a unified framework that enables a single policy to master diverse terrains and generalize zero-shot to unseen scenarios by systematically integrating offline data and diffusion-driven motion synthesis. At its core, DreamPolicy introduces Humanoid Motion Imagery (HMI) - future state predictions synthesized through an autoregressive terrain-aware diffusion planner curated by aggregating rollouts from specialized policies across various distinct terrains. Unlike human motion datasets requiring laborious retargeting, our data directly captures humanoid kinematics, enabling the diffusion planner to synthesize "dreamed" trajectories that encode terrain-specific physical constraints. These trajectories act as dynamic objectives for our HMI-conditioned policy, bypassing manual reward engineering and enabling cross-terrain generalization. DreamPolicy addresses the scalability limitations of prior methods: while traditional RL fails to exploit growing datasets, our framework scales seamlessly with more offline data. As the dataset expands, the diffusion prior learns richer locomotion skills, which the policy leverages to master new terrains without retraining. Experiments demonstrate that DreamPolicy achieves average 90% success rates in training environments and an average of 20% higher success on unseen terrains than the prevalent method. It also generalizes to perturbed and composite scenarios where prior approaches collapse. By unifying offline data, diffusion-based trajectory synthesis, and policy optimization, DreamPolicy overcomes the "one task, one policy" bottleneck, establishing a paradigm for scalable, data-driven humanoid control.
<div id='section'>Paperid: <span id='pid'>491, <a href='https://arxiv.org/pdf/2410.02141.pdf' target='_blank'>https://arxiv.org/pdf/2410.02141.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiqun Duan, Qiang Zhang, Jinzhao Zhou, Jingkai Sun, Xiaowei Jiang, Jiahang Cao, Jiaxu Wang, Yiqian Yang, Wen Zhao, Gang Han, Yijie Guo, Chin-Teng Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.02141">E2H: A Two-Stage Non-Invasive Neural Signal Driven Humanoid Robotic Whole-Body Control Framework</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in humanoid robotics, including the integration of hierarchical reinforcement learning-based control and the utilization of LLM planning, have significantly enhanced the ability of robots to perform complex tasks. In contrast to the highly developed humanoid robots, the human factors involved remain relatively unexplored. Directly controlling humanoid robots with the brain has already appeared in many science fiction novels, such as Pacific Rim and Gundam. In this work, we present E2H (EEG-to-Humanoid), an innovative framework that pioneers the control of humanoid robots using high-frequency non-invasive neural signals. As the none-invasive signal quality remains low in decoding precise spatial trajectory, we decompose the E2H framework in an innovative two-stage formation: 1) decoding neural signals (EEG) into semantic motion keywords, 2) utilizing LLM facilitated motion generation with a precise motion imitation control policy to realize humanoid robotics control. The method of directly driving robots with brainwave commands offers a novel approach to human-machine collaboration, especially in situations where verbal commands are impractical, such as in cases of speech impairments, space exploration, or underwater exploration, unlocking significant potential. E2H offers an exciting glimpse into the future, holding immense potential for human-computer interaction.
<div id='section'>Paperid: <span id='pid'>492, <a href='https://arxiv.org/pdf/2403.01740.pdf' target='_blank'>https://arxiv.org/pdf/2403.01740.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingyu Gong, Min Wang, Wentao Liu, Chen Qian, Zhizhong Zhang, Yuan Xie, Lizhuang Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.01740">DEMOS: Dynamic Environment Motion Synthesis in 3D Scenes via Local Spherical-BEV Perception</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion synthesis in real-world 3D scenes has recently attracted much attention. However, the static environment assumption made by most current methods usually cannot be satisfied especially for real-time motion synthesis in scanned point cloud scenes, if multiple dynamic objects exist, e.g., moving persons or vehicles. To handle this problem, we propose the first Dynamic Environment MOtion Synthesis framework (DEMOS) to predict future motion instantly according to the current scene, and use it to dynamically update the latent motion for final motion synthesis. Concretely, we propose a Spherical-BEV perception method to extract local scene features that are specifically designed for instant scene-aware motion prediction. Then, we design a time-variant motion blending to fuse the new predicted motions into the latent motion, and the final motion is derived from the updated latent motions, benefitting both from motion-prior and iterative methods. We unify the data format of two prevailing datasets, PROX and GTA-IM, and take them for motion synthesis evaluation in 3D scenes. We also assess the effectiveness of the proposed method in dynamic environments from GTA-IM and Semantic3D to check the responsiveness. The results show our method outperforms previous works significantly and has great performance in handling dynamic environments.
<div id='section'>Paperid: <span id='pid'>493, <a href='https://arxiv.org/pdf/2511.05038.pdf' target='_blank'>https://arxiv.org/pdf/2511.05038.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhengxuan Li, Qinhui Yang, Yiyu Zhuang, Chuan Guo, Xinxin Zuo, Xiaoxiao Long, Yao Yao, Xun Cao, Qiu Shen, Hao Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.05038">Pressure2Motion: Hierarchical Motion Synthesis from Ground Pressure with Text Guidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present Pressure2Motion, a novel motion capture algorithm that synthesizes human motion from a ground pressure sequence and text prompt. It eliminates the need for specialized lighting setups, cameras, or wearable devices, making it suitable for privacy-preserving, low-light, and low-cost motion capture scenarios. Such a task is severely ill-posed due to the indeterminate nature of the pressure signals to full-body motion. To address this issue, we introduce Pressure2Motion, a generative model that leverages pressure features as input and utilizes a text prompt as a high-level guiding constraint. Specifically, our model utilizes a dual-level feature extractor that accurately interprets pressure data, followed by a hierarchical diffusion model that discerns broad-scale movement trajectories and subtle posture adjustments. Both the physical cues gained from the pressure sequence and the semantic guidance derived from descriptive texts are leveraged to guide the motion generation with precision. To the best of our knowledge, Pressure2Motion is a pioneering work in leveraging both pressure data and linguistic priors for motion generation, and the established MPL benchmark is the first benchmark for this task. Experiments show our method generates high-fidelity, physically plausible motions, establishing a new state-of-the-art for this task. The codes and benchmarks will be publicly released upon publication.
<div id='section'>Paperid: <span id='pid'>494, <a href='https://arxiv.org/pdf/2505.22977.pdf' target='_blank'>https://arxiv.org/pdf/2505.22977.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuolin Xu, Siming Zheng, Ziyi Wang, HC Yu, Jinwei Chen, Huaqi Zhang, Bo Li, Peng-Tao Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.22977">HyperMotion: DiT-Based Pose-Guided Human Image Animation of Complex Motions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in diffusion models have significantly improved conditional video generation, particularly in the pose-guided human image animation task. Although existing methods are capable of generating high-fidelity and time-consistent animation sequences in regular motions and static scenes, there are still obvious limitations when facing complex human body motions (Hypermotion) that contain highly dynamic, non-standard motions, and the lack of a high-quality benchmark for evaluation of complex human motion animations. To address this challenge, we introduce the \textbf{Open-HyperMotionX Dataset} and \textbf{HyperMotionX Bench}, which provide high-quality human pose annotations and curated video clips for evaluating and improving pose-guided human image animation models under complex human motion conditions. Furthermore, we propose a simple yet powerful DiT-based video generation baseline and design spatial low-frequency enhanced RoPE, a novel module that selectively enhances low-frequency spatial feature modeling by introducing learnable frequency scaling. Our method significantly improves structural stability and appearance consistency in highly dynamic human motion sequences. Extensive experiments demonstrate the effectiveness of our dataset and proposed approach in advancing the generation quality of complex human motion image animations. Code and dataset will be made publicly available.
<div id='section'>Paperid: <span id='pid'>495, <a href='https://arxiv.org/pdf/2505.21325.pdf' target='_blank'>https://arxiv.org/pdf/2505.21325.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guangyuan Li, Siming Zheng, Hao Zhang, Jinwei Chen, Junsheng Luan, Binkai Ou, Lei Zhao, Bo Li, Peng-Tao Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.21325">MagicTryOn: Harnessing Diffusion Transformer for Garment-Preserving Video Virtual Try-on</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video Virtual Try-On (VVT) aims to simulate the natural appearance of garments across consecutive video frames, capturing their dynamic variations and interactions with human body motion. However, current VVT methods still face challenges in terms of spatiotemporal consistency and garment content preservation. First, they use diffusion models based on the U-Net, which are limited in their expressive capability and struggle to reconstruct complex details. Second, they adopt a separative modeling approach for spatial and temporal attention, which hinders the effective capture of structural relationships and dynamic consistency across frames. Third, their expression of garment details remains insufficient, affecting the realism and stability of the overall synthesized results, especially during human motion. To address the above challenges, we propose MagicTryOn, a video virtual try-on framework built upon the large-scale video diffusion Transformer. We replace the U-Net architecture with a diffusion Transformer and combine full self-attention to jointly model the spatiotemporal consistency of videos. We design a coarse-to-fine garment preservation strategy. The coarse strategy integrates garment tokens during the embedding stage, while the fine strategy incorporates multiple garment-based conditions, such as semantics, textures, and contour lines during the denoising stage. Moreover, we introduce a mask-aware loss to further optimize garment region fidelity. Extensive experiments on both image and video try-on datasets demonstrate that our method outperforms existing SOTA methods in comprehensive evaluations and generalizes to in-the-wild scenarios.
<div id='section'>Paperid: <span id='pid'>496, <a href='https://arxiv.org/pdf/2505.04917.pdf' target='_blank'>https://arxiv.org/pdf/2505.04917.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenxu Peng, Chenxu Wang, Minrui Zou, Danyang Li, Zhengpeng Yang, Yimian Dai, Ming-Ming Cheng, Xiang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.04917">A Simple Detector with Frame Dynamics is a Strong Tracker</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Infrared object tracking plays a crucial role in Anti-Unmanned Aerial Vehicle (Anti-UAV) applications. Existing trackers often depend on cropped template regions and have limited motion modeling capabilities, which pose challenges when dealing with tiny targets. To address this, we propose a simple yet effective infrared tiny-object tracker that enhances tracking performance by integrating global detection and motion-aware learning with temporal priors. Our method is based on object detection and achieves significant improvements through two key innovations. First, we introduce frame dynamics, leveraging frame difference and optical flow to encode both prior target features and motion characteristics at the input level, enabling the model to better distinguish the target from background clutter. Second, we propose a trajectory constraint filtering strategy in the post-processing stage, utilizing spatio-temporal priors to suppress false positives and enhance tracking robustness. Extensive experiments show that our method consistently outperforms existing approaches across multiple metrics in challenging infrared UAV tracking scenarios. Notably, we achieve state-of-the-art performance in the 4th Anti-UAV Challenge, securing 1st place in Track 1 and 2nd place in Track 2.
<div id='section'>Paperid: <span id='pid'>497, <a href='https://arxiv.org/pdf/2411.01805.pdf' target='_blank'>https://arxiv.org/pdf/2411.01805.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fuming You, Minghui Fang, Li Tang, Rongjie Huang, Yongqi Wang, Zhou Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.01805">MoMu-Diffusion: On Learning Long-Term Motion-Music Synchronization and Correspondence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion-to-music and music-to-motion have been studied separately, each attracting substantial research interest within their respective domains. The interaction between human motion and music is a reflection of advanced human intelligence, and establishing a unified relationship between them is particularly important. However, to date, there has been no work that considers them jointly to explore the modality alignment within. To bridge this gap, we propose a novel framework, termed MoMu-Diffusion, for long-term and synchronous motion-music generation. Firstly, to mitigate the huge computational costs raised by long sequences, we propose a novel Bidirectional Contrastive Rhythmic Variational Auto-Encoder (BiCoR-VAE) that extracts the modality-aligned latent representations for both motion and music inputs. Subsequently, leveraging the aligned latent spaces, we introduce a multi-modal Transformer-based diffusion model and a cross-guidance sampling strategy to enable various generation tasks, including cross-modal, multi-modal, and variable-length generation. Extensive experiments demonstrate that MoMu-Diffusion surpasses recent state-of-the-art methods both qualitatively and quantitatively, and can synthesize realistic, diverse, long-term, and beat-matched music or motion sequences. The generated samples and codes are available at https://momu-diffusion.github.io/
<div id='section'>Paperid: <span id='pid'>498, <a href='https://arxiv.org/pdf/2509.25304.pdf' target='_blank'>https://arxiv.org/pdf/2509.25304.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haozhe Jia, Wenshuo Chen, Yuqi Lin, Yang Yang, Lei Wang, Mang Ning, Bowen Tian, Songning Lai, Nanqian Jia, Yifan Chen, Yutao Yue
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25304">LUMA: Low-Dimension Unified Motion Alignment with Dual-Path Anchoring for Text-to-Motion Diffusion Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While current diffusion-based models, typically built on U-Net architectures, have shown promising results on the text-to-motion generation task, they still suffer from semantic misalignment and kinematic artifacts. Through analysis, we identify severe gradient attenuation in the deep layers of the network as a key bottleneck, leading to insufficient learning of high-level features. To address this issue, we propose \textbf{LUMA} (\textit{\textbf{L}ow-dimension \textbf{U}nified \textbf{M}otion \textbf{A}lignment}), a text-to-motion diffusion model that incorporates dual-path anchoring to enhance semantic alignment. The first path incorporates a lightweight MoCLIP model trained via contrastive learning without relying on external data, offering semantic supervision in the temporal domain. The second path introduces complementary alignment signals in the frequency domain, extracted from low-frequency DCT components known for their rich semantic content. These two anchors are adaptively fused through a temporal modulation mechanism, allowing the model to progressively transition from coarse alignment to fine-grained semantic refinement throughout the denoising process. Experimental results on HumanML3D and KIT-ML demonstrate that LUMA achieves state-of-the-art performance, with FID scores of 0.035 and 0.123, respectively. Furthermore, LUMA accelerates convergence by 1.4$\times$ compared to the baseline, making it an efficient and scalable solution for high-fidelity text-to-motion generation.
<div id='section'>Paperid: <span id='pid'>499, <a href='https://arxiv.org/pdf/2506.02452.pdf' target='_blank'>https://arxiv.org/pdf/2506.02452.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenshuo Chen, Kuimou Yu, Haozhe Jia, Kaishen Yuan, Zexu Huang, Bowen Tian, Songning Lai, Hongru Xiao, Erhang Zhang, Lei Wang, Yutao Yue
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.02452">ANT: Adaptive Neural Temporal-Aware Text-to-Motion Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While diffusion models advance text-to-motion generation, their static semantic conditioning ignores temporal-frequency demands: early denoising requires structural semantics for motion foundations while later stages need localized details for text alignment. This mismatch mirrors biological morphogenesis where developmental phases demand distinct genetic programs. Inspired by epigenetic regulation governing morphological specialization, we propose **(ANT)**, an **A**daptive **N**eural **T**emporal-Aware architecture. ANT orchestrates semantic granularity through: **(i) Semantic Temporally Adaptive (STA) Module:** Automatically partitions denoising into low-frequency structural planning and high-frequency refinement via spectral analysis. **(ii) Dynamic Classifier-Free Guidance scheduling (DCFG):** Adaptively adjusts conditional to unconditional ratio enhancing efficiency while maintaining fidelity. Extensive experiments show that ANT can be applied to various baselines, significantly improving model performance, and achieving state-of-the-art semantic alignment on StableMoFusion.
<div id='section'>Paperid: <span id='pid'>500, <a href='https://arxiv.org/pdf/2501.18232.pdf' target='_blank'>https://arxiv.org/pdf/2501.18232.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenshuo Chen, Haozhe Jia, Songning Lai, Keming Wu, Hongru Xiao, Lijie Hu, Yutao Yue
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.18232">Free-T2M: Frequency Enhanced Text-to-Motion Diffusion Model With Consistency Loss</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Rapid progress in text-to-motion generation has been largely driven by diffusion models. However, existing methods focus solely on temporal modeling, thereby overlooking frequency-domain analysis. We identify two key phases in motion denoising: the **semantic planning stage** and the **fine-grained improving stage**. To address these phases effectively, we propose **Fre**quency **e**nhanced **t**ext-**to**-**m**otion diffusion model (**Free-T2M**), incorporating stage-specific consistency losses that enhance the robustness of static features and improve fine-grained accuracy. Extensive experiments demonstrate the effectiveness of our method. Specifically, on StableMoFusion, our method reduces the FID from **0.189** to **0.051**, establishing a new SOTA performance within the diffusion architecture. These findings highlight the importance of incorporating frequency-domain insights into text-to-motion generation for more precise and robust results.
<div id='section'>Paperid: <span id='pid'>501, <a href='https://arxiv.org/pdf/2411.04428.pdf' target='_blank'>https://arxiv.org/pdf/2411.04428.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuqi Zhao, Xinghao Zhu, Yuxin Chen, Chenran Li, Xiang Zhang, Mingyu Ding, Masayoshi Tomizuka
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.04428">DexH2R: Task-oriented Dexterous Manipulation from Human to Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dexterous manipulation is a critical aspect of human capability, enabling interaction with a wide variety of objects. Recent advancements in learning from human demonstrations and teleoperation have enabled progress for robots in such ability. However, these approaches either require complex data collection such as costly human effort for eye-robot contact, or suffer from poor generalization when faced with novel scenarios. To solve both challenges, we propose a framework, DexH2R, that combines human hand motion retargeting with a task-oriented residual action policy, improving task performance by bridging the embodiment gap between human and robotic dexterous hands. Specifically, DexH2R learns the residual policy directly from retargeted primitive actions and task-oriented rewards, eliminating the need for labor-intensive teleoperation systems. Moreover, we incorporate test-time guidance for novel scenarios by taking in desired trajectories of human hands and objects, allowing the dexterous hand to acquire new skills with high generalizability. Extensive experiments in both simulation and real-world environments demonstrate the effectiveness of our work, outperforming prior state-of-the-arts by 40% across various settings.
<div id='section'>Paperid: <span id='pid'>502, <a href='https://arxiv.org/pdf/2407.10625.pdf' target='_blank'>https://arxiv.org/pdf/2407.10625.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zijian He, Peixin Chen, Guangrun Wang, Guanbin Li, Philip H. S. Torr, Liang Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.10625">WildVidFit: Video Virtual Try-On in the Wild via Image-Based Controlled Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video virtual try-on aims to generate realistic sequences that maintain garment identity and adapt to a person's pose and body shape in source videos. Traditional image-based methods, relying on warping and blending, struggle with complex human movements and occlusions, limiting their effectiveness in video try-on applications. Moreover, video-based models require extensive, high-quality data and substantial computational resources. To tackle these issues, we reconceptualize video try-on as a process of generating videos conditioned on garment descriptions and human motion. Our solution, WildVidFit, employs image-based controlled diffusion models for a streamlined, one-stage approach. This model, conditioned on specific garments and individuals, is trained on still images rather than videos. It leverages diffusion guidance from pre-trained models including a video masked autoencoder for segment smoothness improvement and a self-supervised model for feature alignment of adjacent frame in the latent space. This integration markedly boosts the model's ability to maintain temporal coherence, enabling more effective video try-on within an image-based framework. Our experiments on the VITON-HD and DressCode datasets, along with tests on the VVT and TikTok datasets, demonstrate WildVidFit's capability to generate fluid and coherent videos. The project page website is at wildvidfit-project.github.io.
<div id='section'>Paperid: <span id='pid'>503, <a href='https://arxiv.org/pdf/2509.00403.pdf' target='_blank'>https://arxiv.org/pdf/2509.00403.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yushuo Chen, Ruizhi Shao, Youxin Pang, Hongwen Zhang, Xinyi Wu, Rihui Wu, Yebin Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.00403">DevilSight: Augmenting Monocular Human Avatar Reconstruction through a Virtual Perspective</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a novel framework to reconstruct human avatars from monocular videos. Recent approaches have struggled either to capture the fine-grained dynamic details from the input or to generate plausible details at novel viewpoints, which mainly stem from the limited representational capacity of the avatar model and insufficient observational data. To overcome these challenges, we propose to leverage the advanced video generative model, Human4DiT, to generate the human motions from alternative perspective as an additional supervision signal. This approach not only enriches the details in previously unseen regions but also effectively regularizes the avatar representation to mitigate artifacts. Furthermore, we introduce two complementary strategies to enhance video generation: To ensure consistent reproduction of human motion, we inject the physical identity into the model through video fine-tuning. For higher-resolution outputs with finer details, a patch-based denoising algorithm is employed. Experimental results demonstrate that our method outperforms recent state-of-the-art approaches and validate the effectiveness of our proposed strategies.
<div id='section'>Paperid: <span id='pid'>504, <a href='https://arxiv.org/pdf/2410.13830.pdf' target='_blank'>https://arxiv.org/pdf/2410.13830.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yujie Wei, Shiwei Zhang, Hangjie Yuan, Xiang Wang, Haonan Qiu, Rui Zhao, Yutong Feng, Feng Liu, Zhizhong Huang, Jiaxin Ye, Yingya Zhang, Hongming Shan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.13830">DreamVideo-2: Zero-Shot Subject-Driven Video Customization with Precise Motion Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in customized video generation have enabled users to create videos tailored to both specific subjects and motion trajectories. However, existing methods often require complicated test-time fine-tuning and struggle with balancing subject learning and motion control, limiting their real-world applications. In this paper, we present DreamVideo-2, a zero-shot video customization framework capable of generating videos with a specific subject and motion trajectory, guided by a single image and a bounding box sequence, respectively, and without the need for test-time fine-tuning. Specifically, we introduce reference attention, which leverages the model's inherent capabilities for subject learning, and devise a mask-guided motion module to achieve precise motion control by fully utilizing the robust motion signal of box masks derived from bounding boxes. While these two components achieve their intended functions, we empirically observe that motion control tends to dominate over subject learning. To address this, we propose two key designs: 1) the masked reference attention, which integrates a blended latent mask modeling scheme into reference attention to enhance subject representations at the desired positions, and 2) a reweighted diffusion loss, which differentiates the contributions of regions inside and outside the bounding boxes to ensure a balance between subject and motion control. Extensive experimental results on a newly curated dataset demonstrate that DreamVideo-2 outperforms state-of-the-art methods in both subject customization and motion control. The dataset, code, and models will be made publicly available.
<div id='section'>Paperid: <span id='pid'>505, <a href='https://arxiv.org/pdf/2403.05834.pdf' target='_blank'>https://arxiv.org/pdf/2403.05834.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiaochu Huang, Xu He, Boshi Tang, Haolin Zhuang, Liyang Chen, Shuochen Gao, Zhiyong Wu, Haozhi Huang, Helen Meng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.05834">Enhancing Expressiveness in Dance Generation via Integrating Frequency and Music Style Information</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dance generation, as a branch of human motion generation, has attracted increasing attention. Recently, a few works attempt to enhance dance expressiveness, which includes genre matching, beat alignment, and dance dynamics, from certain aspects. However, the enhancement is quite limited as they lack comprehensive consideration of the aforementioned three factors. In this paper, we propose ExpressiveBailando, a novel dance generation method designed to generate expressive dances, concurrently taking all three factors into account. Specifically, we mitigate the issue of speed homogenization by incorporating frequency information into VQ-VAE, thus improving dance dynamics. Additionally, we integrate music style information by extracting genre- and beat-related features with a pre-trained music model, hence achieving improvements in the other two factors. Extensive experimental results demonstrate that our proposed method can generate dances with high expressiveness and outperforms existing methods both qualitatively and quantitatively.
<div id='section'>Paperid: <span id='pid'>506, <a href='https://arxiv.org/pdf/2504.05649.pdf' target='_blank'>https://arxiv.org/pdf/2504.05649.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yining Shi, Kun Jiang, Xin Zhao, Kangan Qian, Chuchu Xie, Tuopu Wen, Mengmeng Yang, Diange Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.05649">POD: Predictive Object Detection with Single-Frame FMCW LiDAR Point Cloud</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>LiDAR-based 3D object detection is a fundamental task in the field of autonomous driving. This paper explores the unique advantage of Frequency Modulated Continuous Wave (FMCW) LiDAR in autonomous perception. Given a single frame FMCW point cloud with radial velocity measurements, we expect that our object detector can detect the short-term future locations of objects using only the current frame sensor data and demonstrate a fast ability to respond to intermediate danger. To achieve this, we extend the standard object detection task to a novel task named predictive object detection (POD), which aims to predict the short-term future location and dimensions of objects based solely on current observations. Typically, a motion prediction task requires historical sensor information to process the temporal contexts of each object, while our detector's avoidance of multi-frame historical information enables a much faster response time to potential dangers. The core advantage of FMCW LiDAR lies in the radial velocity associated with every reflected point. We propose a novel POD framework, the core idea of which is to generate a virtual future point using a ray casting mechanism, create virtual two-frame point clouds with the current and virtual future frames, and encode these two-frame voxel features with a sparse 4D encoder. Subsequently, the 4D voxel features are separated by temporal indices and remapped into two Bird's Eye View (BEV) features: one decoded for standard current frame object detection and the other for future predictive object detection. Extensive experiments on our in-house dataset demonstrate the state-of-the-art standard and predictive detection performance of the proposed POD framework.
<div id='section'>Paperid: <span id='pid'>507, <a href='https://arxiv.org/pdf/2503.07367.pdf' target='_blank'>https://arxiv.org/pdf/2503.07367.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kangan Qian, Jinyu Miao, Ziang Luo, Zheng Fu, and Jinchen Li, Yining Shi, Yunlong Wang, Kun Jiang, Mengmeng Yang, Diange Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.07367">LEGO-Motion: Learning-Enhanced Grids with Occupancy Instance Modeling for Class-Agnostic Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate and reliable spatial and motion information plays a pivotal role in autonomous driving systems. However, object-level perception models struggle with handling open scenario categories and lack precise intrinsic geometry. On the other hand, occupancy-based class-agnostic methods excel in representing scenes but fail to ensure physics consistency and ignore the importance of interactions between traffic participants, hindering the model's ability to learn accurate and reliable motion. In this paper, we introduce a novel occupancy-instance modeling framework for class-agnostic motion prediction tasks, named LEGO-Motion, which incorporates instance features into Bird's Eye View (BEV) space. Our model comprises (1) a BEV encoder, (2) an Interaction-Augmented Instance Encoder, and (3) an Instance-Enhanced BEV Encoder, improving both interaction relationships and physics consistency within the model, thereby ensuring a more accurate and robust understanding of the environment. Extensive experiments on the nuScenes dataset demonstrate that our method achieves state-of-the-art performance, outperforming existing approaches. Furthermore, the effectiveness of our framework is validated on the advanced FMCW LiDAR benchmark, showcasing its practical applicability and generalization capabilities. The code will be made publicly available to facilitate further research.
<div id='section'>Paperid: <span id='pid'>508, <a href='https://arxiv.org/pdf/2501.07039.pdf' target='_blank'>https://arxiv.org/pdf/2501.07039.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Subrata Kumer Paul, Abu Saleh Musa Miah, Rakhi Rani Paul, Md. Ekramul Hamid, Jungpil Shin, Md Abdur Rahim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.07039">IoT-Based Real-Time Medical-Related Human Activity Recognition Using Skeletons and Multi-Stage Deep Learning for Healthcare</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Internet of Things (IoT) and mobile technology have significantly transformed healthcare by enabling real-time monitoring and diagnosis of patients. Recognizing medical-related human activities (MRHA) is pivotal for healthcare systems, particularly for identifying actions that are critical to patient well-being. However, challenges such as high computational demands, low accuracy, and limited adaptability persist in Human Motion Recognition (HMR). While some studies have integrated HMR with IoT for real-time healthcare applications, limited research has focused on recognizing MRHA as essential for effective patient monitoring. This study proposes a novel HMR method for MRHA detection, leveraging multi-stage deep learning techniques integrated with IoT. The approach employs EfficientNet to extract optimized spatial features from skeleton frame sequences using seven Mobile Inverted Bottleneck Convolutions (MBConv) blocks, followed by ConvLSTM to capture spatio-temporal patterns. A classification module with global average pooling, a fully connected layer, and a dropout layer generates the final predictions. The model is evaluated on the NTU RGB+D 120 and HMDB51 datasets, focusing on MRHA, such as sneezing, falling, walking, sitting, etc. It achieves 94.85% accuracy for cross-subject evaluations and 96.45% for cross-view evaluations on NTU RGB+D 120, along with 89.00% accuracy on HMDB51. Additionally, the system integrates IoT capabilities using a Raspberry Pi and GSM module, delivering real-time alerts via Twilios SMS service to caregivers and patients. This scalable and efficient solution bridges the gap between HMR and IoT, advancing patient monitoring, improving healthcare outcomes, and reducing costs.
<div id='section'>Paperid: <span id='pid'>509, <a href='https://arxiv.org/pdf/2407.05890.pdf' target='_blank'>https://arxiv.org/pdf/2407.05890.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaqi Chen, Bingqian Lin, Xinmin Liu, Lin Ma, Xiaodan Liang, Kwan-Yee K. Wong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.05890">Affordances-Oriented Planning using Foundation Models for Continuous Vision-Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>LLM-based agents have demonstrated impressive zero-shot performance in vision-language navigation (VLN) task. However, existing LLM-based methods often focus only on solving high-level task planning by selecting nodes in predefined navigation graphs for movements, overlooking low-level control in navigation scenarios. To bridge this gap, we propose AO-Planner, a novel Affordances-Oriented Planner for continuous VLN task. Our AO-Planner integrates various foundation models to achieve affordances-oriented low-level motion planning and high-level decision-making, both performed in a zero-shot setting. Specifically, we employ a Visual Affordances Prompting (VAP) approach, where the visible ground is segmented by SAM to provide navigational affordances, based on which the LLM selects potential candidate waypoints and plans low-level paths towards selected waypoints. We further propose a high-level PathAgent which marks planned paths into the image input and reasons the most probable path by comprehending all environmental information. Finally, we convert the selected path into 3D coordinates using camera intrinsic parameters and depth information, avoiding challenging 3D predictions for LLMs. Experiments on the challenging R2R-CE and RxR-CE datasets show that AO-Planner achieves state-of-the-art zero-shot performance (8.8% improvement on SPL). Our method can also serve as a data annotator to obtain pseudo-labels, distilling its waypoint prediction ability into a learning-based predictor. This new predictor does not require any waypoint data from the simulator and achieves 47% SR competing with supervised methods. We establish an effective connection between LLM and 3D world, presenting novel prospects for employing foundation models in low-level motion control.
<div id='section'>Paperid: <span id='pid'>510, <a href='https://arxiv.org/pdf/2509.23169.pdf' target='_blank'>https://arxiv.org/pdf/2509.23169.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bolin Chen, Ru-Ling Liao, Yan Ye, Jie Chen, Shanzhi Yin, Xinrui Ju, Shiqi Wang, Yibo Fan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23169">Sparse2Dense: A Keypoint-driven Generative Framework for Human Video Compression and Vertex Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>For bandwidth-constrained multimedia applications, simultaneously achieving ultra-low bitrate human video compression and accurate vertex prediction remains a critical challenge, as it demands the harmonization of dynamic motion modeling, detailed appearance synthesis, and geometric consistency. To address this challenge, we propose Sparse2Dense, a keypoint-driven generative framework that leverages extremely sparse 3D keypoints as compact transmitted symbols to enable ultra-low bitrate human video compression and precise human vertex prediction. The key innovation is the multi-task learning-based and keypoint-aware deep generative model, which could encode complex human motion via compact 3D keypoints and leverage these sparse keypoints to estimate dense motion for video synthesis with temporal coherence and realistic textures. Additionally, a vertex predictor is integrated to learn human vertex geometry through joint optimization with video generation, ensuring alignment between visual content and geometric structure. Extensive experiments demonstrate that the proposed Sparse2Dense framework achieves competitive compression performance for human video over traditional/generative video codecs, whilst enabling precise human vertex prediction for downstream geometry applications. As such, Sparse2Dense is expected to facilitate bandwidth-efficient human-centric media transmission, such as real-time motion analysis, virtual human animation, and immersive entertainment.
<div id='section'>Paperid: <span id='pid'>511, <a href='https://arxiv.org/pdf/2506.03753.pdf' target='_blank'>https://arxiv.org/pdf/2506.03753.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Caiyi Sun, Yujing Sun, Xiao Han, Zemin Yang, Jiawei Liu, Xinge Zhu, Siu Ming Yiu, Yuexin Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.03753">HUMOF: Human Motion Forecasting in Interactive Social Scenes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Complex scenes present significant challenges for predicting human behaviour due to the abundance of interaction information, such as human-human and humanenvironment interactions. These factors complicate the analysis and understanding of human behaviour, thereby increasing the uncertainty in forecasting human motions. Existing motion prediction methods thus struggle in these complex scenarios. In this paper, we propose an effective method for human motion forecasting in interactive scenes. To achieve a comprehensive representation of interactions, we design a hierarchical interaction feature representation so that high-level features capture the overall context of the interactions, while low-level features focus on fine-grained details. Besides, we propose a coarse-to-fine interaction reasoning module that leverages both spatial and frequency perspectives to efficiently utilize hierarchical features, thereby enhancing the accuracy of motion predictions. Our method achieves state-of-the-art performance across four public datasets. Code will be released when this paper is published.
<div id='section'>Paperid: <span id='pid'>512, <a href='https://arxiv.org/pdf/2403.13307.pdf' target='_blank'>https://arxiv.org/pdf/2403.13307.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Peishan Cong, Ziyi Wang, Zhiyang Dou, Yiming Ren, Wei Yin, Kai Cheng, Yujing Sun, Xiaoxiao Long, Xinge Zhu, Yuexin Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.13307">LaserHuman: Language-guided Scene-aware Human Motion Generation in Free Environment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Language-guided scene-aware human motion generation has great significance for entertainment and robotics. In response to the limitations of existing datasets, we introduce LaserHuman, a pioneering dataset engineered to revolutionize Scene-Text-to-Motion research. LaserHuman stands out with its inclusion of genuine human motions within 3D environments, unbounded free-form natural language descriptions, a blend of indoor and outdoor scenarios, and dynamic, ever-changing scenes. Diverse modalities of capture data and rich annotations present great opportunities for the research of conditional motion generation, and can also facilitate the development of real-life applications. Moreover, to generate semantically consistent and physically plausible human motions, we propose a multi-conditional diffusion model, which is simple but effective, achieving state-of-the-art performance on existing datasets.
<div id='section'>Paperid: <span id='pid'>513, <a href='https://arxiv.org/pdf/2506.12851.pdf' target='_blank'>https://arxiv.org/pdf/2506.12851.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weiji Xie, Jinrui Han, Jiakun Zheng, Huanyu Li, Xinzhe Liu, Jiyuan Shi, Weinan Zhang, Chenjia Bai, Xuelong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.12851">KungfuBot: Physics-Based Humanoid Whole-Body Control for Learning Highly-Dynamic Skills</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humanoid robots are promising to acquire various skills by imitating human behaviors. However, existing algorithms are only capable of tracking smooth, low-speed human motions, even with delicate reward and curriculum design. This paper presents a physics-based humanoid control framework, aiming to master highly-dynamic human behaviors such as Kungfu and dancing through multi-steps motion processing and adaptive motion tracking. For motion processing, we design a pipeline to extract, filter out, correct, and retarget motions, while ensuring compliance with physical constraints to the maximum extent. For motion imitation, we formulate a bi-level optimization problem to dynamically adjust the tracking accuracy tolerance based on the current tracking error, creating an adaptive curriculum mechanism. We further construct an asymmetric actor-critic framework for policy training. In experiments, we train whole-body control policies to imitate a set of highly-dynamic motions. Our method achieves significantly lower tracking errors than existing approaches and is successfully deployed on the Unitree G1 robot, demonstrating stable and expressive behaviors. The project page is https://kungfu-bot.github.io.
<div id='section'>Paperid: <span id='pid'>514, <a href='https://arxiv.org/pdf/2506.08840.pdf' target='_blank'>https://arxiv.org/pdf/2506.08840.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dewei Wang, Xinmiao Wang, Xinzhe Liu, Jiyuan Shi, Yingnan Zhao, Chenjia Bai, Xuelong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.08840">MoRE: Mixture of Residual Experts for Humanoid Lifelike Gaits Learning on Complex Terrains</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humanoid robots have demonstrated robust locomotion capabilities using Reinforcement Learning (RL)-based approaches. Further, to obtain human-like behaviors, existing methods integrate human motion-tracking or motion prior in the RL framework. However, these methods are limited in flat terrains with proprioception only, restricting their abilities to traverse challenging terrains with human-like gaits. In this work, we propose a novel framework using a mixture of latent residual experts with multi-discriminators to train an RL policy, which is capable of traversing complex terrains in controllable lifelike gaits with exteroception. Our two-stage training pipeline first teaches the policy to traverse complex terrains using a depth camera, and then enables gait-commanded switching between human-like gait patterns. We also design gait rewards to adjust human-like behaviors like robot base height. Simulation and real-world experiments demonstrate that our framework exhibits exceptional performance in traversing complex terrains, and achieves seamless transitions between multiple human-like gait patterns.
<div id='section'>Paperid: <span id='pid'>515, <a href='https://arxiv.org/pdf/2510.26794.pdf' target='_blank'>https://arxiv.org/pdf/2510.26794.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jing Lin, Ruisi Wang, Junzhe Lu, Ziqi Huang, Guorui Song, Ailing Zeng, Xian Liu, Chen Wei, Wanqi Yin, Qingping Sun, Zhongang Cai, Lei Yang, Ziwei Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.26794">The Quest for Generalizable Motion Generation: Data, Model, and Evaluation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite recent advances in 3D human motion generation (MoGen) on standard benchmarks, existing models still face a fundamental bottleneck in their generalization capability. In contrast, adjacent generative fields, most notably video generation (ViGen), have demonstrated remarkable generalization in modeling human behaviors, highlighting transferable insights that MoGen can leverage. Motivated by this observation, we present a comprehensive framework that systematically transfers knowledge from ViGen to MoGen across three key pillars: data, modeling, and evaluation. First, we introduce ViMoGen-228K, a large-scale dataset comprising 228,000 high-quality motion samples that integrates high-fidelity optical MoCap data with semantically annotated motions from web videos and synthesized samples generated by state-of-the-art ViGen models. The dataset includes both text-motion pairs and text-video-motion triplets, substantially expanding semantic diversity. Second, we propose ViMoGen, a flow-matching-based diffusion transformer that unifies priors from MoCap data and ViGen models through gated multimodal conditioning. To enhance efficiency, we further develop ViMoGen-light, a distilled variant that eliminates video generation dependencies while preserving strong generalization. Finally, we present MBench, a hierarchical benchmark designed for fine-grained evaluation across motion quality, prompt fidelity, and generalization ability. Extensive experiments show that our framework significantly outperforms existing approaches in both automatic and human evaluations. The code, data, and benchmark will be made publicly available.
<div id='section'>Paperid: <span id='pid'>516, <a href='https://arxiv.org/pdf/2509.23852.pdf' target='_blank'>https://arxiv.org/pdf/2509.23852.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiheng Huang, Junran Peng, Silei Shen, Jingwei Yang, ZeJi Wei, ChenCheng Bai, Yonghao He, Wei Sui, Muyi Sun, Yan Liu, Xu-Cheng Yin, Man Zhang, Zhaoxiang Zhang, Chuanchen Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23852">SIG-Chat: Spatial Intent-Guided Conversational Gesture Generation Involving How, When and Where</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The accompanying actions and gestures in dialogue are often closely linked to interactions with the environment, such as looking toward the interlocutor or using gestures to point to the described target at appropriate moments. Speech and semantics guide the production of gestures by determining their timing (WHEN) and style (HOW), while the spatial locations of interactive objects dictate their directional execution (WHERE). Existing approaches either rely solely on descriptive language to generate motions or utilize audio to produce non-interactive gestures, thereby lacking the characterization of interactive timing and spatial intent. This significantly limits the applicability of conversational gesture generation, whether in robotics or in the fields of game and animation production. To address this gap, we present a full-stack solution. We first established a unique data collection method to simultaneously capture high-precision human motion and spatial intent. We then developed a generation model driven by audio, language, and spatial data, alongside dedicated metrics for evaluating interaction timing and spatial accuracy. Finally, we deployed the solution on a humanoid robot, enabling rich, context-aware physical interactions.
<div id='section'>Paperid: <span id='pid'>517, <a href='https://arxiv.org/pdf/2508.06205.pdf' target='_blank'>https://arxiv.org/pdf/2508.06205.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruiyan Wang, Lin Zuo, Zonghao Lin, Qiang Wang, Zhengxue Cheng, Rong Xie, Jun Ling, Li Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.06205">PA-HOI: A Physics-Aware Human and Object Interaction Dataset</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Human-Object Interaction (HOI) task explores the dynamic interactions between humans and objects in physical environments, providing essential biomechanical and cognitive-behavioral foundations for fields such as robotics, virtual reality, and human-computer interaction. However, existing HOI data sets focus on details of affordance, often neglecting the influence of physical properties of objects on human long-term motion. To bridge this gap, we introduce the PA-HOI Motion Capture dataset, which highlights the impact of objects' physical attributes on human motion dynamics, including human posture, moving velocity, and other motion characteristics. The dataset comprises 562 motion sequences of human-object interactions, with each sequence performed by subjects of different genders interacting with 35 3D objects that vary in size, shape, and weight. This dataset stands out by significantly extending the scope of existing ones for understanding how the physical attributes of different objects influence human posture, speed, motion scale, and interacting strategies. We further demonstrate the applicability of the PA-HOI dataset by integrating it with existing motion generation methods, validating its capacity to transfer realistic physical awareness.
<div id='section'>Paperid: <span id='pid'>518, <a href='https://arxiv.org/pdf/2501.13335.pdf' target='_blank'>https://arxiv.org/pdf/2501.13335.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xianrui Luo, Juewen Peng, Zhongang Cai, Lei Yang, Fan Yang, Zhiguo Cao, Guosheng Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.13335">Deblur-Avatar: Animatable Avatars from Motion-Blurred Monocular Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a novel framework for modeling high-fidelity, animatable 3D human avatars from motion-blurred monocular video inputs. Motion blur is prevalent in real-world dynamic video capture, especially due to human movements in 3D human avatar modeling. Existing methods either (1) assume sharp image inputs, failing to address the detail loss introduced by motion blur, or (2) mainly consider blur by camera movements, neglecting the human motion blur which is more common in animatable avatars. Our proposed approach integrates a human movement-based motion blur model into 3D Gaussian Splatting (3DGS). By explicitly modeling human motion trajectories during exposure time, we jointly optimize the trajectories and 3D Gaussians to reconstruct sharp, high-quality human avatars. We employ a pose-dependent fusion mechanism to distinguish moving body regions, optimizing both blurred and sharp areas effectively. Extensive experiments on synthetic and real-world datasets demonstrate that our method significantly outperforms existing methods in rendering quality and quantitative metrics, producing sharp avatar reconstructions and enabling real-time rendering under challenging motion blur conditions.
<div id='section'>Paperid: <span id='pid'>519, <a href='https://arxiv.org/pdf/2401.02916.pdf' target='_blank'>https://arxiv.org/pdf/2401.02916.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxin Yang, Pengfei Zhu, Mengshi Qi, Huadong Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.02916">Uncovering the human motion pattern: Pattern Memory-based Diffusion Model for Trajectory Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human trajectory forecasting is a critical challenge in fields such as robotics and autonomous driving. Due to the inherent uncertainty of human actions and intentions in real-world scenarios, various unexpected occurrences may arise. To uncover latent motion patterns in human behavior, we introduce a novel memory-based method, named Motion Pattern Priors Memory Network. Our method involves constructing a memory bank derived from clustered prior knowledge of motion patterns observed in the training set trajectories. We introduce an addressing mechanism to retrieve the matched pattern and the potential target distributions for each prediction from the memory bank, which enables the identification and retrieval of natural motion patterns exhibited by agents, subsequently using the target priors memory token to guide the diffusion model to generate predictions. Extensive experiments validate the effectiveness of our approach, achieving state-of-the-art trajectory prediction accuracy. The code will be made publicly available.
<div id='section'>Paperid: <span id='pid'>520, <a href='https://arxiv.org/pdf/2503.20724.pdf' target='_blank'>https://arxiv.org/pdf/2503.20724.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nan Jiang, Hongjie Li, Ziye Yuan, Zimo He, Yixin Chen, Tengyu Liu, Yixin Zhu, Siyuan Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.20724">Dynamic Motion Blending for Versatile Motion Editing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-guided motion editing enables high-level semantic control and iterative modifications beyond traditional keyframe animation. Existing methods rely on limited pre-collected training triplets, which severely hinders their versatility in diverse editing scenarios. We introduce MotionCutMix, an online data augmentation technique that dynamically generates training triplets by blending body part motions based on input text. While MotionCutMix effectively expands the training distribution, the compositional nature introduces increased randomness and potential body part incoordination. To model such a rich distribution, we present MotionReFit, an auto-regressive diffusion model with a motion coordinator. The auto-regressive architecture facilitates learning by decomposing long sequences, while the motion coordinator mitigates the artifacts of motion composition. Our method handles both spatial and temporal motion edits directly from high-level human instructions, without relying on additional specifications or Large Language Models. Through extensive experiments, we show that MotionReFit achieves state-of-the-art performance in text-guided motion editing.
<div id='section'>Paperid: <span id='pid'>521, <a href='https://arxiv.org/pdf/2501.04541.pdf' target='_blank'>https://arxiv.org/pdf/2501.04541.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ching-Chun Chang, Yijie Lin, Isao Echizen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.04541">Cyber-Physical Steganography in Robotic Motion Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Steganography, the art of information hiding, has continually evolved across visual, auditory and linguistic domains, adapting to the ceaseless interplay between steganographic concealment and steganalytic revelation. This study seeks to extend the horizons of what constitutes a viable steganographic medium by introducing a steganographic paradigm in robotic motion control. Based on the observation of the robot's inherent sensitivity to changes in its environment, we propose a methodology to encode messages as environmental stimuli influencing the motions of the robotic agent and to decode messages from the resulting motion trajectory. The constraints of maximal robot integrity and minimal motion deviation are established as fundamental principles underlying secrecy. As a proof of concept, we conduct experiments in simulated environments across various manipulation tasks, incorporating robotic embodiments equipped with generalist multimodal policies.
<div id='section'>Paperid: <span id='pid'>522, <a href='https://arxiv.org/pdf/2403.08629.pdf' target='_blank'>https://arxiv.org/pdf/2403.08629.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nan Jiang, Zhiyuan Zhang, Hongjie Li, Xiaoxuan Ma, Zan Wang, Yixin Chen, Tengyu Liu, Yixin Zhu, Siyuan Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.08629">Scaling Up Dynamic Human-Scene Interaction Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Confronting the challenges of data scarcity and advanced motion synthesis in human-scene interaction modeling, we introduce the TRUMANS dataset alongside a novel HSI motion synthesis method. TRUMANS stands as the most comprehensive motion-captured HSI dataset currently available, encompassing over 15 hours of human interactions across 100 indoor scenes. It intricately captures whole-body human motions and part-level object dynamics, focusing on the realism of contact. This dataset is further scaled up by transforming physical environments into exact virtual models and applying extensive augmentations to appearance and motion for both humans and objects while maintaining interaction fidelity. Utilizing TRUMANS, we devise a diffusion-based autoregressive model that efficiently generates HSI sequences of any length, taking into account both scene context and intended actions. In experiments, our approach shows remarkable zero-shot generalizability on a range of 3D scene datasets (e.g., PROX, Replica, ScanNet, ScanNet++), producing motions that closely mimic original motion-captured sequences, as confirmed by quantitative experiments and human studies.
<div id='section'>Paperid: <span id='pid'>523, <a href='https://arxiv.org/pdf/2512.18211.pdf' target='_blank'>https://arxiv.org/pdf/2512.18211.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yudong Liu, Spencer Hallyburton, Jiwoo Kim, Yueqian Lin, Yiming Li, Qinsi Wang, Hui Ye, Jingwei Sun, Miroslav Pajic, Yiran Chen, Hai Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.18211">LLaViDA: A Large Language Vision Driving Assistant for Explicit Reasoning and Enhanced Trajectory Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Trajectory planning is a fundamental yet challenging component of autonomous driving. End-to-end planners frequently falter under adverse weather, unpredictable human behavior, or complex road layouts, primarily because they lack strong generalization or few-shot capabilities beyond their training data. We propose LLaViDA, a Large Language Vision Driving Assistant that leverages a Vision-Language Model (VLM) for object motion prediction, semantic grounding, and chain-of-thought reasoning for trajectory planning in autonomous driving. A two-stage training pipeline--supervised fine-tuning followed by Trajectory Preference Optimization (TPO)--enhances scene understanding and trajectory planning by injecting regression-based supervision, produces a powerful "VLM Trajectory Planner for Autonomous Driving." On the NuScenes benchmark, LLaViDA surpasses state-of-the-art end-to-end and other recent VLM/LLM-based baselines in open-loop trajectory planning task, achieving an average L2 trajectory error of 0.31 m and a collision rate of 0.10% on the NuScenes test set. The code for this paper is available at GitHub.
<div id='section'>Paperid: <span id='pid'>524, <a href='https://arxiv.org/pdf/2510.14954.pdf' target='_blank'>https://arxiv.org/pdf/2510.14954.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhe Li, Weihao Yuan, Weichao Shen, Siyu Zhu, Zilong Dong, Chang Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.14954">OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Whole-body multi-modal human motion generation poses two primary challenges: creating an effective motion generation mechanism and integrating various modalities, such as text, speech, and music, into a cohesive framework. Unlike previous methods that usually employ discrete masked modeling or autoregressive modeling, we develop a continuous masked autoregressive motion transformer, where a causal attention is performed considering the sequential nature within the human motion. Within this transformer, we introduce a gated linear attention and an RMSNorm module, which drive the transformer to pay attention to the key actions and suppress the instability caused by either the abnormal movements or the heterogeneous distributions within multi-modalities. To further enhance both the motion generation and the multimodal generalization, we employ the DiT structure to diffuse the conditions from the transformer towards the targets. To fuse different modalities, AdaLN and cross-attention are leveraged to inject the text, speech, and music signals. Experimental results demonstrate that our framework outperforms previous methods across all modalities, including text-to-motion, speech-to-gesture, and music-to-dance. The code of our method will be made public.
<div id='section'>Paperid: <span id='pid'>525, <a href='https://arxiv.org/pdf/2410.07093.pdf' target='_blank'>https://arxiv.org/pdf/2410.07093.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhe Li, Weihao Yuan, Yisheng He, Lingteng Qiu, Shenhao Zhu, Xiaodong Gu, Weichao Shen, Yuan Dong, Zilong Dong, Laurence T. Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.07093">LaMP: Language-Motion Pretraining for Motion Generation, Retrieval, and Captioning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Language plays a vital role in the realm of human motion. Existing methods have largely depended on CLIP text embeddings for motion generation, yet they fall short in effectively aligning language and motion due to CLIP's pretraining on static image-text pairs. This work introduces LaMP, a novel Language-Motion Pretraining model, which transitions from a language-vision to a more suitable language-motion latent space. It addresses key limitations by generating motion-informative text embeddings, significantly enhancing the relevance and semantics of generated motion sequences. With LaMP, we advance three key tasks: text-to-motion generation, motion-text retrieval, and motion captioning through aligned language-motion representation learning. For generation, we utilize LaMP to provide the text condition instead of CLIP, and an autoregressive masked prediction is designed to achieve mask modeling without rank collapse in transformers. For retrieval, motion features from LaMP's motion transformer interact with query tokens to retrieve text features from the text transformer, and vice versa. For captioning, we finetune a large language model with the language-informative motion features to develop a strong motion captioning model. In addition, we introduce the LaMP-BertScore metric to assess the alignment of generated motions with textual descriptions. Extensive experimental results on multiple datasets demonstrate substantial improvements over previous methods across all three tasks. The code of our method will be made public.
<div id='section'>Paperid: <span id='pid'>526, <a href='https://arxiv.org/pdf/2511.09141.pdf' target='_blank'>https://arxiv.org/pdf/2511.09141.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuetao Li, Wenke Huang, Nengyuan Pan, Kaiyan Zhao, Songhua Yang, Yiming Wang, Mengde Li, Mang Ye, Jifeng Xuan, Miao Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.09141">RGMP: Recurrent Geometric-prior Multimodal Policy for Generalizable Humanoid Robot Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humanoid robots exhibit significant potential in executing diverse human-level skills. However, current research predominantly relies on data-driven approaches that necessitate extensive training datasets to achieve robust multimodal decision-making capabilities and generalizable visuomotor control. These methods raise concerns due to the neglect of geometric reasoning in unseen scenarios and the inefficient modeling of robot-target relationships within the training data, resulting in significant waste of training resources. To address these limitations, we present the Recurrent Geometric-prior Multimodal Policy (RGMP), an end-to-end framework that unifies geometric-semantic skill reasoning with data-efficient visuomotor control. For perception capabilities, we propose the Geometric-prior Skill Selector, which infuses geometric inductive biases into a vision language model, producing adaptive skill sequences for unseen scenes with minimal spatial common sense tuning. To achieve data-efficient robotic motion synthesis, we introduce the Adaptive Recursive Gaussian Network, which parameterizes robot-object interactions as a compact hierarchy of Gaussian processes that recursively encode multi-scale spatial relationships, yielding dexterous, data-efficient motion synthesis even from sparse demonstrations. Evaluated on both our humanoid robot and desktop dual-arm robot, the RGMP framework achieves 87% task success in generalization tests and exhibits 5x greater data efficiency than the state-of-the-art model. This performance underscores its superior cross-domain generalization, enabled by geometric-semantic reasoning and recursive-Gaussion adaptation.
<div id='section'>Paperid: <span id='pid'>527, <a href='https://arxiv.org/pdf/2502.03207.pdf' target='_blank'>https://arxiv.org/pdf/2502.03207.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinyao Liao, Xianfang Zeng, Liao Wang, Gang Yu, Guosheng Lin, Chi Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.03207">MotionAgent: Fine-grained Controllable Video Generation via Motion Field Agent</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose MotionAgent, enabling fine-grained motion control for text-guided image-to-video generation. The key technique is the motion field agent that converts motion information in text prompts into explicit motion fields, providing flexible and precise motion guidance. Specifically, the agent extracts the object movement and camera motion described in the text and converts them into object trajectories and camera extrinsics, respectively. An analytical optical flow composition module integrates these motion representations in 3D space and projects them into a unified optical flow. An optical flow adapter takes the flow to control the base image-to-video diffusion model for generating fine-grained controlled videos. The significant improvement in the Video-Text Camera Motion metrics on VBench indicates that our method achieves precise control over camera motion. We construct a subset of VBench to evaluate the alignment of motion information in the text and the generated video, outperforming other advanced models on motion generation accuracy.
<div id='section'>Paperid: <span id='pid'>528, <a href='https://arxiv.org/pdf/2411.08656.pdf' target='_blank'>https://arxiv.org/pdf/2411.08656.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaxu Zhang, Xianfang Zeng, Xin Chen, Wei Zuo, Gang Yu, Zhigang Tu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.08656">MikuDance: Animating Character Art with Mixed Motion Dynamics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose MikuDance, a diffusion-based pipeline incorporating mixed motion dynamics to animate stylized character art. MikuDance consists of two key techniques: Mixed Motion Modeling and Mixed-Control Diffusion, to address the challenges of high-dynamic motion and reference-guidance misalignment in character art animation. Specifically, a Scene Motion Tracking strategy is presented to explicitly model the dynamic camera in pixel-wise space, enabling unified character-scene motion modeling. Building on this, the Mixed-Control Diffusion implicitly aligns the scale and body shape of diverse characters with motion guidance, allowing flexible control of local character motion. Subsequently, a Motion-Adaptive Normalization module is incorporated to effectively inject global scene motion, paving the way for comprehensive character art animation. Through extensive experiments, we demonstrate the effectiveness and generalizability of MikuDance across various character art and motion guidance, consistently producing high-quality animations with remarkable motion dynamics.
<div id='section'>Paperid: <span id='pid'>529, <a href='https://arxiv.org/pdf/2508.10566.pdf' target='_blank'>https://arxiv.org/pdf/2508.10566.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shiyu Liu, Kui Jiang, Xianming Liu, Hongxun Yao, Xiaocheng Feng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10566">HM-Talker: Hybrid Motion Modeling for High-Fidelity Talking Head Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Audio-driven talking head video generation enhances user engagement in human-computer interaction. However, current methods frequently produce videos with motion blur and lip jitter, primarily due to their reliance on implicit modeling of audio-facial motion correlations--an approach lacking explicit articulatory priors (i.e., anatomical guidance for speech-related facial movements). To overcome this limitation, we propose HM-Talker, a novel framework for generating high-fidelity, temporally coherent talking heads. HM-Talker leverages a hybrid motion representation combining both implicit and explicit motion cues. Explicit cues use Action Units (AUs), anatomically defined facial muscle movements, alongside implicit features to minimize phoneme-viseme misalignment. Specifically, our Cross-Modal Disentanglement Module (CMDM) extracts complementary implicit/explicit motion features while predicting AUs directly from audio input aligned to visual cues. To mitigate identity-dependent biases in explicit features and enhance cross-subject generalization, we introduce the Hybrid Motion Modeling Module (HMMM). This module dynamically merges randomly paired implicit/explicit features, enforcing identity-agnostic learning. Together, these components enable robust lip synchronization across diverse identities, advancing personalized talking head synthesis. Extensive experiments demonstrate HM-Talker's superiority over state-of-the-art methods in visual quality and lip-sync accuracy.
<div id='section'>Paperid: <span id='pid'>530, <a href='https://arxiv.org/pdf/2506.12723.pdf' target='_blank'>https://arxiv.org/pdf/2506.12723.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ye Li, Yuan Meng, Zewen Sun, Kangye Ji, Chen Tang, Jiajun Fan, Xinzhu Ma, Shutao Xia, Zhi Wang, Wenwu Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.12723">SP-VLA: A Joint Model Scheduling and Token Pruning Approach for VLA Model Acceleration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models have attracted increasing attention for their strong control capabilities. However, their high computational cost and low execution frequency hinder their suitability for real-time tasks such as robotic manipulation and autonomous navigation. Existing VLA acceleration methods primarily focus on structural optimization, overlooking the fact that these models operate in sequential decision-making environments. As a result, temporal redundancy in sequential action generation and spatial redundancy in visual input remain unaddressed. To this end, we propose SP-VLA, a unified framework that accelerates VLA models by jointly scheduling models and pruning tokens. Specifically, we design an action-aware model scheduling mechanism that reduces temporal redundancy by dynamically switching between VLA model and a lightweight generator. Inspired by the human motion pattern of focusing on key decision points while relying on intuition for other actions, we categorize VLA actions into deliberative and intuitive, assigning the former to the VLA model and the latter to the lightweight generator, enabling frequency-adaptive execution through collaborative model scheduling. To address spatial redundancy, we further develop a spatio-semantic dual-aware token pruning method. Tokens are classified into spatial and semantic types and pruned based on their dual-aware importance to accelerate VLA inference. These two mechanisms work jointly to guide the VLA in focusing on critical actions and salient visual information, achieving effective acceleration while maintaining high accuracy. Experimental results demonstrate that our method achieves up to 1.5$\times$ acceleration with less than 3% drop in accuracy, outperforming existing approaches in multiple tasks.
<div id='section'>Paperid: <span id='pid'>531, <a href='https://arxiv.org/pdf/2506.02618.pdf' target='_blank'>https://arxiv.org/pdf/2506.02618.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jialiang Zhang, Haoran Geng, Yang You, Congyue Deng, Pieter Abbeel, Jitendra Malik, Leonidas Guibas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.02618">Rodrigues Network for Learning Robot Actions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding and predicting articulated actions is important in robot learning. However, common architectures such as MLPs and Transformers lack inductive biases that reflect the underlying kinematic structure of articulated systems. To this end, we propose the Neural Rodrigues Operator, a learnable generalization of the classical forward kinematics operation, designed to inject kinematics-aware inductive bias into neural computation. Building on this operator, we design the Rodrigues Network (RodriNet), a novel neural architecture specialized for processing actions. We evaluate the expressivity of our network on two synthetic tasks on kinematic and motion prediction, showing significant improvements compared to standard backbones. We further demonstrate its effectiveness in two realistic applications: (i) imitation learning on robotic benchmarks with the Diffusion Policy, and (ii) single-image 3D hand reconstruction. Our results suggest that integrating structured kinematic priors into the network architecture improves action learning in various domains.
<div id='section'>Paperid: <span id='pid'>532, <a href='https://arxiv.org/pdf/2505.03729.pdf' target='_blank'>https://arxiv.org/pdf/2505.03729.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Arthur Allshire, Hongsuk Choi, Junyi Zhang, David McAllister, Anthony Zhang, Chung Min Kim, Trevor Darrell, Pieter Abbeel, Jitendra Malik, Angjoo Kanazawa
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.03729">Visual Imitation Enables Contextual Humanoid Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>How can we teach humanoids to climb staircases and sit on chairs using the surrounding environment context? Arguably, the simplest way is to just show them-casually capture a human motion video and feed it to humanoids. We introduce VIDEOMIMIC, a real-to-sim-to-real pipeline that mines everyday videos, jointly reconstructs the humans and the environment, and produces whole-body control policies for humanoid robots that perform the corresponding skills. We demonstrate the results of our pipeline on real humanoid robots, showing robust, repeatable contextual control such as staircase ascents and descents, sitting and standing from chairs and benches, as well as other dynamic whole-body skills-all from a single policy, conditioned on the environment and global root commands. VIDEOMIMIC offers a scalable path towards teaching humanoids to operate in diverse real-world environments.
<div id='section'>Paperid: <span id='pid'>533, <a href='https://arxiv.org/pdf/2407.10485.pdf' target='_blank'>https://arxiv.org/pdf/2407.10485.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mufeng Yao, Jinlong Peng, Qingdong He, Bo Peng, Hao Chen, Mingmin Chi, Chao Liu, Jon Atli Benediktsson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.10485">MM-Tracker: Motion Mamba with Margin Loss for UAV-platform Multiple Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multiple object tracking (MOT) from unmanned aerial vehicle (UAV) platforms requires efficient motion modeling. This is because UAV-MOT faces both local object motion and global camera motion. Motion blur also increases the difficulty of detecting large moving objects. Previous UAV motion modeling approaches either focus only on local motion or ignore motion blurring effects, thus limiting their tracking performance and speed. To address these issues, we propose the Motion Mamba Module, which explores both local and global motion features through cross-correlation and bi-directional Mamba Modules for better motion modeling. To address the detection difficulties caused by motion blur, we also design motion margin loss to effectively improve the detection accuracy of motion blurred objects. Based on the Motion Mamba module and motion margin loss, our proposed MM-Tracker surpasses the state-of-the-art in two widely open-source UAV-MOT datasets. Code will be available.
<div id='section'>Paperid: <span id='pid'>534, <a href='https://arxiv.org/pdf/2511.19320.pdf' target='_blank'>https://arxiv.org/pdf/2511.19320.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaming Zhang, Shengming Cao, Rui Li, Xiaotong Zhao, Yutao Cui, Xinglin Hou, Gangshan Wu, Haolan Chen, Yu Xu, Limin Wang, Kai Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.19320">SteadyDancer: Harmonized and Coherent Human Image Animation with First-Frame Preservation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Preserving first-frame identity while ensuring precise motion control is a fundamental challenge in human image animation. The Image-to-Motion Binding process of the dominant Reference-to-Video (R2V) paradigm overlooks critical spatio-temporal misalignments common in real-world applications, leading to failures such as identity drift and visual artifacts. We introduce SteadyDancer, an Image-to-Video (I2V) paradigm-based framework that achieves harmonized and coherent animation and is the first to ensure first-frame preservation robustly. Firstly, we propose a Condition-Reconciliation Mechanism to harmonize the two conflicting conditions, enabling precise control without sacrificing fidelity. Secondly, we design Synergistic Pose Modulation Modules to generate an adaptive and coherent pose representation that is highly compatible with the reference image. Finally, we employ a Staged Decoupled-Objective Training Pipeline that hierarchically optimizes the model for motion fidelity, visual quality, and temporal coherence. Experiments demonstrate that SteadyDancer achieves state-of-the-art performance in both appearance fidelity and motion control, while requiring significantly fewer training resources than comparable methods.
<div id='section'>Paperid: <span id='pid'>535, <a href='https://arxiv.org/pdf/2511.17373.pdf' target='_blank'>https://arxiv.org/pdf/2511.17373.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yixuan Pan, Ruoyi Qiao, Li Chen, Kashyap Chitta, Liang Pan, Haoguang Mai, Qingwen Bu, Hao Zhao, Cunyuan Zheng, Ping Luo, Hongyang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.17373">Agility Meets Stability: Versatile Humanoid Control with Heterogeneous Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humanoid robots are envisioned to perform a wide range of tasks in human-centered environments, requiring controllers that combine agility with robust balance. Recent advances in locomotion and whole-body tracking have enabled impressive progress in either agile dynamic skills or stability-critical behaviors, but existing methods remain specialized, focusing on one capability while compromising the other. In this work, we introduce AMS (Agility Meets Stability), the first framework that unifies both dynamic motion tracking and extreme balance maintenance in a single policy. Our key insight is to leverage heterogeneous data sources: human motion capture datasets that provide rich, agile behaviors, and physically constrained synthetic balance motions that capture stability configurations. To reconcile the divergent optimization goals of agility and stability, we design a hybrid reward scheme that applies general tracking objectives across all data while injecting balance-specific priors only into synthetic motions. Further, an adaptive learning strategy with performance-driven sampling and motion-specific reward shaping enables efficient training across diverse motion distributions. We validate AMS extensively in simulation and on a real Unitree G1 humanoid. Experiments demonstrate that a single policy can execute agile skills such as dancing and running, while also performing zero-shot extreme balance motions like Ip Man's Squat, highlighting AMS as a versatile control paradigm for future humanoid applications.
<div id='section'>Paperid: <span id='pid'>536, <a href='https://arxiv.org/pdf/2509.20322.pdf' target='_blank'>https://arxiv.org/pdf/2509.20322.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shaofeng Yin, Yanjie Ze, Hong-Xing Yu, C. Karen Liu, Jiajun Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20322">VisualMimic: Visual Humanoid Loco-Manipulation via Motion Tracking and Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humanoid loco-manipulation in unstructured environments demands tight integration of egocentric perception and whole-body control. However, existing approaches either depend on external motion capture systems or fail to generalize across diverse tasks. We introduce VisualMimic, a visual sim-to-real framework that unifies egocentric vision with hierarchical whole-body control for humanoid robots. VisualMimic combines a task-agnostic low-level keypoint tracker -- trained from human motion data via a teacher-student scheme -- with a task-specific high-level policy that generates keypoint commands from visual and proprioceptive input. To ensure stable training, we inject noise into the low-level policy and clip high-level actions using human motion statistics. VisualMimic enables zero-shot transfer of visuomotor policies trained in simulation to real humanoid robots, accomplishing a wide range of loco-manipulation tasks such as box lifting, pushing, football dribbling, and kicking. Beyond controlled laboratory settings, our policies also generalize robustly to outdoor environments. Videos are available at: https://visualmimic.github.io .
<div id='section'>Paperid: <span id='pid'>537, <a href='https://arxiv.org/pdf/2509.15536.pdf' target='_blank'>https://arxiv.org/pdf/2509.15536.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sen Wang, Jingyi Tian, Le Wang, Zhimin Liao, Jiayi Li, Huaiyi Dong, Kun Xia, Sanping Zhou, Wei Tang, Hua Gang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15536">SAMPO:Scale-wise Autoregression with Motion PrOmpt for generative world models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>World models allow agents to simulate the consequences of actions in imagined environments for planning, control, and long-horizon decision-making. However, existing autoregressive world models struggle with visually coherent predictions due to disrupted spatial structure, inefficient decoding, and inadequate motion modeling. In response, we propose \textbf{S}cale-wise \textbf{A}utoregression with \textbf{M}otion \textbf{P}r\textbf{O}mpt (\textbf{SAMPO}), a hybrid framework that combines visual autoregressive modeling for intra-frame generation with causal modeling for next-frame generation. Specifically, SAMPO integrates temporal causal decoding with bidirectional spatial attention, which preserves spatial locality and supports parallel decoding within each scale. This design significantly enhances both temporal consistency and rollout efficiency. To further improve dynamic scene understanding, we devise an asymmetric multi-scale tokenizer that preserves spatial details in observed frames and extracts compact dynamic representations for future frames, optimizing both memory usage and model performance. Additionally, we introduce a trajectory-aware motion prompt module that injects spatiotemporal cues about object and robot trajectories, focusing attention on dynamic regions and improving temporal consistency and physical realism. Extensive experiments show that SAMPO achieves competitive performance in action-conditioned video prediction and model-based control, improving generation quality with 4.4$\times$ faster inference. We also evaluate SAMPO's zero-shot generalization and scaling behavior, demonstrating its ability to generalize to unseen tasks and benefit from larger model sizes.
<div id='section'>Paperid: <span id='pid'>538, <a href='https://arxiv.org/pdf/2508.20604.pdf' target='_blank'>https://arxiv.org/pdf/2508.20604.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zheng Qin, Yabing Wang, Minghui Yang, Sanping Zhou, Ming Yang, Le Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.20604">Embracing Aleatoric Uncertainty: Generating Diverse 3D Human Motion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating 3D human motions from text is a challenging yet valuable task. The key aspects of this task are ensuring text-motion consistency and achieving generation diversity. Although recent advancements have enabled the generation of precise and high-quality human motions from text, achieving diversity in the generated motions remains a significant challenge. In this paper, we aim to overcome the above challenge by designing a simple yet effective text-to-motion generation method, \textit{i.e.}, Diverse-T2M. Our method introduces uncertainty into the generation process, enabling the generation of highly diverse motions while preserving the semantic consistency of the text. Specifically, we propose a novel perspective that utilizes noise signals as carriers of diversity information in transformer-based methods, facilitating a explicit modeling of uncertainty. Moreover, we construct a latent space where text is projected into a continuous representation, instead of a rigid one-to-one mapping, and integrate a latent space sampler to introduce stochastic sampling into the generation process, thereby enhancing the diversity and uncertainty of the outputs. Our results on text-to-motion generation benchmark datasets~(HumanML3D and KIT-ML) demonstrate that our method significantly enhances diversity while maintaining state-of-the-art performance in text consistency.
<div id='section'>Paperid: <span id='pid'>539, <a href='https://arxiv.org/pdf/2508.04049.pdf' target='_blank'>https://arxiv.org/pdf/2508.04049.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiayi He, Xu Wang, Shengeng Tang, Yaxiong Wang, Lechao Cheng, Dan Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.04049">Motion is the Choreographer: Learning Latent Pose Dynamics for Seamless Sign Language Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Sign language video generation requires producing natural signing motions with realistic appearances under precise semantic control, yet faces two critical challenges: excessive signer-specific data requirements and poor generalization. We propose a new paradigm for sign language video generation that decouples motion semantics from signer identity through a two-phase synthesis framework. First, we construct a signer-independent multimodal motion lexicon, where each gloss is stored as identity-agnostic pose, gesture, and 3D mesh sequences, requiring only one recording per sign. This compact representation enables our second key innovation: a discrete-to-continuous motion synthesis stage that transforms retrieved gloss sequences into temporally coherent motion trajectories, followed by identity-aware neural rendering to produce photorealistic videos of arbitrary signers. Unlike prior work constrained by signer-specific datasets, our method treats motion as a first-class citizen: the learned latent pose dynamics serve as a portable "choreography layer" that can be visually realized through different human appearances. Extensive experiments demonstrate that disentangling motion from identity is not just viable but advantageous - enabling both high-quality synthesis and unprecedented flexibility in signer personalization.
<div id='section'>Paperid: <span id='pid'>540, <a href='https://arxiv.org/pdf/2506.01941.pdf' target='_blank'>https://arxiv.org/pdf/2506.01941.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Longyan Wu, Checheng Yu, Jieji Ren, Li Chen, Ran Huang, Guoying Gu, Hongyang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.01941">FreeTacMan: Robot-free Visuo-Tactile Data Collection System for Contact-rich Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Enabling robots with contact-rich manipulation remains a pivotal challenge in robot learning, which is substantially hindered by the data collection gap, including its inefficiency and limited sensor setup. While prior work has explored handheld paradigms, their rod-based mechanical structures remain rigid and unintuitive, providing limited tactile feedback and posing challenges for human operators. Motivated by the dexterity and force feedback of human motion, we propose FreeTacMan, a human-centric and robot-free data collection system for accurate and efficient robot manipulation. Concretely, we design a wearable data collection device with dual visuo-tactile grippers, which can be worn by human fingers for intuitive and natural control. A high-precision optical tracking system is introduced to capture end-effector poses, while synchronizing visual and tactile feedback simultaneously. FreeTacMan achieves multiple improvements in data collection performance compared to prior works, and enables effective policy learning for contact-rich manipulation tasks with the help of the visuo-tactile information. We will release the work to facilitate reproducibility and accelerate research in visuo-tactile manipulation.
<div id='section'>Paperid: <span id='pid'>541, <a href='https://arxiv.org/pdf/2502.17822.pdf' target='_blank'>https://arxiv.org/pdf/2502.17822.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Peng Zhang, Xin Li, Xin Lin, Liang He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.17822">Easy-Poly: A Easy Polyhedral Framework For 3D Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in 3D multi-object tracking (3D MOT) have predominantly relied on tracking-by-detection pipelines. However, these approaches often neglect potential enhancements in 3D detection processes, leading to high false positives (FP), missed detections (FN), and identity switches (IDS), particularly in challenging scenarios such as crowded scenes, small-object configurations, and adverse weather conditions. Furthermore, limitations in data preprocessing, association mechanisms, motion modeling, and life-cycle management hinder overall tracking robustness. To address these issues, we present Easy-Poly, a real-time, filter-based 3D MOT framework for multiple object categories. Our contributions include: (1) An Augmented Proposal Generator utilizing multi-modal data augmentation and refined SpConv operations, significantly improving mAP and NDS on nuScenes; (2) A Dynamic Track-Oriented (DTO) data association algorithm that effectively manages uncertainties and occlusions through optimal assignment and multiple hypothesis handling; (3) A Dynamic Motion Modeling (DMM) incorporating a confidence-weighted Kalman filter and adaptive noise covariances, enhancing MOTA and AMOTA in challenging conditions; and (4) An extended life-cycle management system with adjustive thresholds to reduce ID switches and false terminations. Experimental results show that Easy-Poly outperforms state-of-the-art methods such as Poly-MOT and Fast-Poly, achieving notable gains in mAP (e.g., from 63.30% to 64.96% with LargeKernel3D) and AMOTA (e.g., from 73.1% to 74.5%), while also running in real-time. These findings highlight Easy-Poly's adaptability and robustness in diverse scenarios, making it a compelling choice for autonomous driving and related 3D MOT applications. The source code of this paper will be published upon acceptance.
<div id='section'>Paperid: <span id='pid'>542, <a href='https://arxiv.org/pdf/2406.09905.pdf' target='_blank'>https://arxiv.org/pdf/2406.09905.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lingni Ma, Yuting Ye, Fangzhou Hong, Vladimir Guzov, Yifeng Jiang, Rowan Postyeni, Luis Pesqueira, Alexander Gamino, Vijay Baiyya, Hyo Jin Kim, Kevin Bailey, David Soriano Fosas, C. Karen Liu, Ziwei Liu, Jakob Engel, Renzo De Nardi, Richard Newcombe
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.09905">Nymeria: A Massive Collection of Multimodal Egocentric Daily Motion in the Wild</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce Nymeria - a large-scale, diverse, richly annotated human motion dataset collected in the wild with multiple multimodal egocentric devices. The dataset comes with a) full-body ground-truth motion; b) multiple multimodal egocentric data from Project Aria devices with videos, eye tracking, IMUs and etc; and c) a third-person perspective by an additional observer. All devices are precisely synchronized and localized in on metric 3D world. We derive hierarchical protocol to add in-context language descriptions of human motion, from fine-grain motion narration, to simplified atomic action and high-level activity summarization. To the best of our knowledge, Nymeria dataset is the world's largest collection of human motion in the wild; first of its kind to provide synchronized and localized multi-device multimodal egocentric data; and the world's largest motion-language dataset. It provides 300 hours of daily activities from 264 participants across 50 locations, total travelling distance over 399Km. The language descriptions contain 301.5K sentences in 8.64M words from a vocabulary size of 6545. To demonstrate the potential of the dataset, we evaluate several SOTA algorithms for egocentric body tracking, motion synthesis, and action recognition. Data and code are open-sourced for research (c.f. https://www.projectaria.com/datasets/nymeria).
<div id='section'>Paperid: <span id='pid'>543, <a href='https://arxiv.org/pdf/2401.14159.pdf' target='_blank'>https://arxiv.org/pdf/2401.14159.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang Chen, Feng Yan, Zhaoyang Zeng, Hao Zhang, Feng Li, Jie Yang, Hongyang Li, Qing Jiang, Lei Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.14159">Grounded SAM: Assembling Open-World Models for Diverse Visual Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce Grounded SAM, which uses Grounding DINO as an open-set object detector to combine with the segment anything model (SAM). This integration enables the detection and segmentation of any regions based on arbitrary text inputs and opens a door to connecting various vision models. As shown in Fig.1, a wide range of vision tasks can be achieved by using the versatile Grounded SAM pipeline. For example, an automatic annotation pipeline based solely on input images can be realized by incorporating models such as BLIP and Recognize Anything. Additionally, incorporating Stable-Diffusion allows for controllable image editing, while the integration of OSX facilitates promptable 3D human motion analysis. Grounded SAM also shows superior performance on open-vocabulary benchmarks, achieving 48.7 mean AP on SegInW (Segmentation in the wild) zero-shot benchmark with the combination of Grounding DINO-Base and SAM-Huge models.
<div id='section'>Paperid: <span id='pid'>544, <a href='https://arxiv.org/pdf/2512.05094.pdf' target='_blank'>https://arxiv.org/pdf/2512.05094.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>James Ni, Zekai Wang, Wei Lin, Amir Bar, Yann LeCun, Trevor Darrell, Jitendra Malik, Roei Herzig
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.05094">From Generated Human Videos to Physically Plausible Robot Trajectories</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video generation models are rapidly improving in their ability to synthesize human actions in novel contexts, holding the potential to serve as high-level planners for contextual robot control. To realize this potential, a key research question remains open: how can a humanoid execute the human actions from generated videos in a zero-shot manner? This challenge arises because generated videos are often noisy and exhibit morphological distortions that make direct imitation difficult compared to real video. To address this, we introduce a two-stage pipeline. First, we lift video pixels into a 4D human representation and then retarget to the humanoid morphology. Second, we propose GenMimic-a physics-aware reinforcement learning policy conditioned on 3D keypoints, and trained with symmetry regularization and keypoint-weighted tracking rewards. As a result, GenMimic can mimic human actions from noisy, generated videos. We curate GenMimicBench, a synthetic human-motion dataset generated using two video generation models across a spectrum of actions and contexts, establishing a benchmark for assessing zero-shot generalization and policy robustness. Extensive experiments demonstrate improvements over strong baselines in simulation and confirm coherent, physically stable motion tracking on a Unitree G1 humanoid robot without fine-tuning. This work offers a promising path to realizing the potential of video generation models as high-level policies for robot control.
<div id='section'>Paperid: <span id='pid'>545, <a href='https://arxiv.org/pdf/2511.20275.pdf' target='_blank'>https://arxiv.org/pdf/2511.20275.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenhui Dong, Haozhe Xu, Wenhao Feng, Zhipeng Wang, Yanmin Zhou, Yifei Zhao, Bin He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.20275">HAFO: A Force-Adaptive Control Framework for Humanoid Robots in Intense Interaction Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reinforcement learning (RL) controllers have made impressive progress in humanoid locomotion and light-weight object manipulation. However, achieving robust and precise motion control with intense force interaction remains a significant challenge. To address these limitations, this paper proposes HAFO, a dual-agent reinforcement learning framework that concurrently optimizes both a robust locomotion strategy and a precise upper-body manipulation strategy via coupled training in environments with external disturbances. The external pulling disturbances are explicitly modeled using a spring-damper system, allowing for fine-grained force control through manipulation of the virtual spring. In this process, the reinforcement learning policy autonomously generates a disturbance-rejection response by utilizing environmental feedback. Furthermore, HAFO employs an asymmetric Actor-Critic framework in which the Critic network's access to privileged external forces guides the actor network to acquire generalizable force adaptation for resisting external disturbances. The experimental results demonstrate that HAFO achieves whole-body control for humanoid robots across diverse force-interaction environments, delivering outstanding performance in load-bearing tasks and maintaining stable operation even under rope suspension state.
<div id='section'>Paperid: <span id='pid'>546, <a href='https://arxiv.org/pdf/2508.10522.pdf' target='_blank'>https://arxiv.org/pdf/2508.10522.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Quang Nguyen, Nhat Le, Baoru Huang, Minh Nhat Vu, Chengcheng Tang, Van Nguyen, Ngan Le, Thieu Vo, Anh Nguyen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10522">EgoMusic-driven Human Dance Motion Estimation with Skeleton Mamba</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Estimating human dance motion is a challenging task with various industrial applications. Recently, many efforts have focused on predicting human dance motion using either egocentric video or music as input. However, the task of jointly estimating human motion from both egocentric video and music remains largely unexplored. In this paper, we aim to develop a new method that predicts human dance motion from both egocentric video and music. In practice, the egocentric view often obscures much of the body, making accurate full-pose estimation challenging. Additionally, incorporating music requires the generated head and body movements to align well with both visual and musical inputs. We first introduce EgoAIST++, a new large-scale dataset that combines both egocentric views and music with more than 36 hours of dancing motion. Drawing on the success of diffusion models and Mamba on modeling sequences, we develop an EgoMusic Motion Network with a core Skeleton Mamba that explicitly captures the skeleton structure of the human body. We illustrate that our approach is theoretically supportive. Intensive experiments show that our method clearly outperforms state-of-the-art approaches and generalizes effectively to real-world data.
<div id='section'>Paperid: <span id='pid'>547, <a href='https://arxiv.org/pdf/2506.15483.pdf' target='_blank'>https://arxiv.org/pdf/2506.15483.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shujia Li, Haiyu Zhang, Xinyuan Chen, Yaohui Wang, Yutong Ban
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.15483">GenHOI: Generalizing Text-driven 4D Human-Object Interaction Synthesis for Unseen Objects</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While diffusion models and large-scale motion datasets have advanced text-driven human motion synthesis, extending these advances to 4D human-object interaction (HOI) remains challenging, mainly due to the limited availability of large-scale 4D HOI datasets. In our study, we introduce GenHOI, a novel two-stage framework aimed at achieving two key objectives: 1) generalization to unseen objects and 2) the synthesis of high-fidelity 4D HOI sequences. In the initial stage of our framework, we employ an Object-AnchorNet to reconstruct sparse 3D HOI keyframes for unseen objects, learning solely from 3D HOI datasets, thereby mitigating the dependence on large-scale 4D HOI datasets. Subsequently, we introduce a Contact-Aware Diffusion Model (ContactDM) in the second stage to seamlessly interpolate sparse 3D HOI keyframes into densely temporally coherent 4D HOI sequences. To enhance the quality of generated 4D HOI sequences, we propose a novel Contact-Aware Encoder within ContactDM to extract human-object contact patterns and a novel Contact-Aware HOI Attention to effectively integrate the contact signals into diffusion models. Experimental results show that we achieve state-of-the-art results on the publicly available OMOMO and 3D-FUTURE datasets, demonstrating strong generalization abilities to unseen objects, while enabling high-fidelity 4D HOI generation.
<div id='section'>Paperid: <span id='pid'>548, <a href='https://arxiv.org/pdf/2503.15082.pdf' target='_blank'>https://arxiv.org/pdf/2503.15082.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Le Ma, Ziyu Meng, Tengyu Liu, Yuhan Li, Ran Song, Wei Zhang, Siyuan Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.15082">StyleLoco: Generative Adversarial Distillation for Natural Humanoid Robot Locomotion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humanoid robots are anticipated to acquire a wide range of locomotion capabilities while ensuring natural movement across varying speeds and terrains. Existing methods encounter a fundamental dilemma in learning humanoid locomotion: reinforcement learning with handcrafted rewards can achieve agile locomotion but produces unnatural gaits, while Generative Adversarial Imitation Learning (GAIL) with motion capture data yields natural movements but suffers from unstable training processes and restricted agility. Integrating these approaches proves challenging due to the inherent heterogeneity between expert policies and human motion datasets. To address this, we introduce StyleLoco, a novel two-stage framework that bridges this gap through a Generative Adversarial Distillation (GAD) process. Our framework begins by training a teacher policy using reinforcement learning to achieve agile and dynamic locomotion. It then employs a multi-discriminator architecture, where distinct discriminators concurrently extract skills from both the teacher policy and motion capture data. This approach effectively combines the agility of reinforcement learning with the natural fluidity of human-like movements while mitigating the instability issues commonly associated with adversarial training. Through extensive simulation and real-world experiments, we demonstrate that StyleLoco enables humanoid robots to perform diverse locomotion tasks with the precision of expertly trained policies and the natural aesthetics of human motion, successfully transferring styles across different movement types while maintaining stable locomotion across a broad spectrum of command inputs.
<div id='section'>Paperid: <span id='pid'>549, <a href='https://arxiv.org/pdf/2502.18180.pdf' target='_blank'>https://arxiv.org/pdf/2502.18180.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lei Li, Sen Jia, Jianhao Wang, Zhaochong An, Jiaang Li, Jenq-Neng Hwang, Serge Belongie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.18180">ChatMotion: A Multimodal Multi-Agent for Human Motion Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Advancements in Multimodal Large Language Models (MLLMs) have improved human motion understanding. However, these models remain constrained by their "instruct-only" nature, lacking interactivity and adaptability for diverse analytical perspectives. To address these challenges, we introduce ChatMotion, a multimodal multi-agent framework for human motion analysis. ChatMotion dynamically interprets user intent, decomposes complex tasks into meta-tasks, and activates specialized function modules for motion comprehension. It integrates multiple specialized modules, such as the MotionCore, to analyze human motion from various perspectives. Extensive experiments demonstrate ChatMotion's precision, adaptability, and user engagement for human motion understanding.
<div id='section'>Paperid: <span id='pid'>550, <a href='https://arxiv.org/pdf/2501.04595.pdf' target='_blank'>https://arxiv.org/pdf/2501.04595.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zifan Wang, Ziqing Chen, Junyu Chen, Jilong Wang, Yuxin Yang, Yunze Liu, Xueyi Liu, He Wang, Li Yi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.04595">MobileH2R: Learning Generalizable Human to Mobile Robot Handover Exclusively from Scalable and Diverse Synthetic Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces MobileH2R, a framework for learning generalizable vision-based human-to-mobile-robot (H2MR) handover skills. Unlike traditional fixed-base handovers, this task requires a mobile robot to reliably receive objects in a large workspace enabled by its mobility. Our key insight is that generalizable handover skills can be developed in simulators using high-quality synthetic data, without the need for real-world demonstrations. To achieve this, we propose a scalable pipeline for generating diverse synthetic full-body human motion data, an automated method for creating safe and imitation-friendly demonstrations, and an efficient 4D imitation learning method for distilling large-scale demonstrations into closed-loop policies with base-arm coordination. Experimental evaluations in both simulators and the real world show significant improvements (at least +15% success rate) over baseline methods in all cases. Experiments also validate that large-scale and diverse synthetic data greatly enhances robot learning, highlighting our scalable framework.
<div id='section'>Paperid: <span id='pid'>551, <a href='https://arxiv.org/pdf/2412.17730.pdf' target='_blank'>https://arxiv.org/pdf/2412.17730.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yun Liu, Bowen Yang, Licheng Zhong, He Wang, Li Yi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.17730">Mimicking-Bench: A Benchmark for Generalizable Humanoid-Scene Interaction Learning via Human Mimicking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning generic skills for humanoid robots interacting with 3D scenes by mimicking human data is a key research challenge with significant implications for robotics and real-world applications. However, existing methodologies and benchmarks are constrained by the use of small-scale, manually collected demonstrations, lacking the general dataset and benchmark support necessary to explore scene geometry generalization effectively. To address this gap, we introduce Mimicking-Bench, the first comprehensive benchmark designed for generalizable humanoid-scene interaction learning through mimicking large-scale human animation references. Mimicking-Bench includes six household full-body humanoid-scene interaction tasks, covering 11K diverse object shapes, along with 20K synthetic and 3K real-world human interaction skill references. We construct a complete humanoid skill learning pipeline and benchmark approaches for motion retargeting, motion tracking, imitation learning, and their various combinations. Extensive experiments highlight the value of human mimicking for skill learning, revealing key challenges and research directions.
<div id='section'>Paperid: <span id='pid'>552, <a href='https://arxiv.org/pdf/2405.04496.pdf' target='_blank'>https://arxiv.org/pdf/2405.04496.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi Zuo, Lingling Li, Licheng Jiao, Fang Liu, Xu Liu, Wenping Ma, Shuyuan Yang, Yuwei Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.04496">Edit-Your-Motion: Space-Time Diffusion Decoupling Learning for Video Motion Editing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing diffusion-based methods have achieved impressive results in human motion editing. However, these methods often exhibit significant ghosting and body distortion in unseen in-the-wild cases. In this paper, we introduce Edit-Your-Motion, a video motion editing method that tackles these challenges through one-shot fine-tuning on unseen cases. Specifically, firstly, we utilized DDIM inversion to initialize the noise, preserving the appearance of the source video and designed a lightweight motion attention adapter module to enhance motion fidelity. DDIM inversion aims to obtain the implicit representations by estimating the prediction noise from the source video, which serves as a starting point for the sampling process, ensuring the appearance consistency between the source and edited videos. The Motion Attention Module (MA) enhances the model's motion editing ability by resolving the conflict between the skeleton features and the appearance features. Secondly, to effectively decouple motion and appearance of source video, we design a spatio-temporal two-stage learning strategy (STL). In the first stage, we focus on learning temporal features of human motion and propose recurrent causal attention (RCA) to ensure consistency between video frames. In the second stage, we shift focus on learning the appearance features of the source video. With Edit-Your-Motion, users can edit the motion of humans in the source video, creating more engaging and diverse content. Extensive qualitative and quantitative experiments, along with user preference studies, show that Edit-Your-Motion outperforms other methods.
<div id='section'>Paperid: <span id='pid'>553, <a href='https://arxiv.org/pdf/2404.17031.pdf' target='_blank'>https://arxiv.org/pdf/2404.17031.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Wang, Jiayou Qin, Xiwen Chen, Ashish Bastola, John Suchanek, Zihao Gong, Abolfazl Razi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.17031">Motor Focus: Fast Ego-Motion Prediction for Assistive Visual Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Assistive visual navigation systems for visually impaired individuals have become increasingly popular thanks to the rise of mobile computing. Most of these devices work by translating visual information into voice commands. In complex scenarios where multiple objects are present, it is imperative to prioritize object detection and provide immediate notifications for key entities in specific directions. This brings the need for identifying the observer's motion direction (ego-motion) by merely processing visual information, which is the key contribution of this paper. Specifically, we introduce Motor Focus, a lightweight image-based framework that predicts the ego-motion - the humans (and humanoid machines) movement intentions based on their visual feeds, while filtering out camera motion without any camera calibration. To this end, we implement an optical flow-based pixel-wise temporal analysis method to compensate for the camera motion with a Gaussian aggregation to smooth out the movement prediction area. Subsequently, to evaluate the performance, we collect a dataset including 50 clips of pedestrian scenes in 5 different scenarios. We tested this framework with classical feature detectors such as SIFT and ORB to show the comparison. Our framework demonstrates its superiority in speed (> 40FPS), accuracy (MAE = 60pixels), and robustness (SNR = 23dB), confirming its potential to enhance the usability of vision-based assistive navigation tools in complex environments.
<div id='section'>Paperid: <span id='pid'>554, <a href='https://arxiv.org/pdf/2512.13903.pdf' target='_blank'>https://arxiv.org/pdf/2512.13903.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sibo Tian, Minghui Zheng, Xiao Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.13903">PrediFlow: A Flow-Based Prediction-Refinement Framework for Real-Time Human Motion Prediction in Human-Robot Collaboration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Stochastic human motion prediction is critical for safe and effective human-robot collaboration (HRC) in industrial remanufacturing, as it captures human motion uncertainties and multi-modal behaviors that deterministic methods cannot handle. While earlier works emphasize highly diverse predictions, they often generate unrealistic human motions. More recent methods focus on accuracy and real-time performance, yet there remains potential to improve prediction quality further without exceeding time budgets. Additionally, current research on stochastic human motion prediction in HRC typically considers human motion in isolation, neglecting the influence of robot motion on human behavior. To address these research gaps and enable real-time, realistic, and interaction-aware human motion prediction, we propose a novel prediction-refinement framework that integrates both human and robot observed motion to refine the initial predictions produced by a pretrained state-of-the-art predictor. The refinement module employs a Flow Matching structure to account for uncertainty. Experimental studies on the HRC desktop disassembly dataset demonstrate that our method significantly improves prediction accuracy while preserving the uncertainties and multi-modalities of human motion. Moreover, the total inference time of the proposed framework remains within the time budget, highlighting the effectiveness and practicality of our approach.
<div id='section'>Paperid: <span id='pid'>555, <a href='https://arxiv.org/pdf/2502.03449.pdf' target='_blank'>https://arxiv.org/pdf/2502.03449.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuan Li, Chang Yu, Wenxin Du, Ying Jiang, Tianyi Xie, Yunuo Chen, Yin Yang, Chenfanfu Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.03449">Dress-1-to-3: Single Image to Simulation-Ready 3D Outfit with Diffusion Prior and Differentiable Physics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in large models have significantly advanced image-to-3D reconstruction. However, the generated models are often fused into a single piece, limiting their applicability in downstream tasks. This paper focuses on 3D garment generation, a key area for applications like virtual try-on with dynamic garment animations, which require garments to be separable and simulation-ready. We introduce Dress-1-to-3, a novel pipeline that reconstructs physics-plausible, simulation-ready separated garments with sewing patterns and humans from an in-the-wild image. Starting with the image, our approach combines a pre-trained image-to-sewing pattern generation model for creating coarse sewing patterns with a pre-trained multi-view diffusion model to produce multi-view images. The sewing pattern is further refined using a differentiable garment simulator based on the generated multi-view images. Versatile experiments demonstrate that our optimization approach substantially enhances the geometric alignment of the reconstructed 3D garments and humans with the input image. Furthermore, by integrating a texture generation module and a human motion generation module, we produce customized physics-plausible and realistic dynamic garment demonstrations. Project page: https://dress-1-to-3.github.io/
<div id='section'>Paperid: <span id='pid'>556, <a href='https://arxiv.org/pdf/2501.13707.pdf' target='_blank'>https://arxiv.org/pdf/2501.13707.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pengteng Li, Yunfan Lu, Pinghao Song, Wuyang Li, Huizai Yao, Hui Xiong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.13707">EventVL: Understand Event Streams via Multimodal Large Language Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The event-based Vision-Language Model (VLM) recently has made good progress for practical vision tasks. However, most of these works just utilize CLIP for focusing on traditional perception tasks, which obstruct model understanding explicitly the sufficient semantics and context from event streams. To address the deficiency, we propose EventVL, the first generative event-based MLLM (Multimodal Large Language Model) framework for explicit semantic understanding. Specifically, to bridge the data gap for connecting different modalities semantics, we first annotate a large event-image/video-text dataset, containing almost 1.4 million high-quality pairs of data, which enables effective learning across various scenes, e.g., drive scene or human motion. After that, we design Event Spatiotemporal Representation to fully explore the comprehensive information by diversely aggregating and segmenting the event stream. To further promote a compact semantic space, Dynamic Semantic Alignment is introduced to improve and complete sparse semantic spaces of events. Extensive experiments show that our EventVL can significantly surpass existing MLLM baselines in event captioning and scene description generation tasks. We hope our research could contribute to the development of the event vision community.
<div id='section'>Paperid: <span id='pid'>557, <a href='https://arxiv.org/pdf/2411.16498.pdf' target='_blank'>https://arxiv.org/pdf/2411.16498.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>David Eduardo Moreno-VillamarÃ­n, Anna Hilsmann, Peter Eisert
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.16498">Multi-Resolution Generative Modeling of Human Motion from Limited Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a generative model that learns to synthesize human motion from limited training sequences. Our framework provides conditional generation and blending across multiple temporal resolutions. The model adeptly captures human motion patterns by integrating skeletal convolution layers and a multi-scale architecture. Our model contains a set of generative and adversarial networks, along with embedding modules, each tailored for generating motions at specific frame rates while exerting control over their content and details. Notably, our approach also extends to the synthesis of co-speech gestures, demonstrating its ability to generate synchronized gestures from speech inputs, even with limited paired data. Through direct synthesis of SMPL pose parameters, our approach avoids test-time adjustments to fit human body meshes. Experimental results showcase our model's ability to achieve extensive coverage of training examples, while generating diverse motions, as indicated by local and global diversity metrics.
<div id='section'>Paperid: <span id='pid'>558, <a href='https://arxiv.org/pdf/2409.12456.pdf' target='_blank'>https://arxiv.org/pdf/2409.12456.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sibo Tian, Minghui Zheng, Xiao Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.12456">Bayesian-Optimized One-Step Diffusion Model with Knowledge Distillation for Real-Time 3D Human Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion prediction is a cornerstone of human-robot collaboration (HRC), as robots need to infer the future movements of human workers based on past motion cues to proactively plan their motion, ensuring safety in close collaboration scenarios. The diffusion model has demonstrated remarkable performance in predicting high-quality motion samples with reasonable diversity, but suffers from a slow generative process which necessitates multiple model evaluations, hindering real-world applications. To enable real-time prediction, in this work, we propose training a one-step multi-layer perceptron-based (MLP-based) diffusion model for motion prediction using knowledge distillation and Bayesian optimization. Our method contains two steps. First, we distill a pretrained diffusion-based motion predictor, TransFusion, directly into a one-step diffusion model with the same denoiser architecture. Then, to further reduce the inference time, we remove the computationally expensive components from the original denoiser and use knowledge distillation once again to distill the obtained one-step diffusion model into an even smaller model based solely on MLPs. Bayesian optimization is used to tune the hyperparameters for training the smaller diffusion model. Extensive experimental studies are conducted on benchmark datasets, and our model can significantly improve the inference speed, achieving real-time prediction without noticeable degradation in performance.
<div id='section'>Paperid: <span id='pid'>559, <a href='https://arxiv.org/pdf/2408.00352.pdf' target='_blank'>https://arxiv.org/pdf/2408.00352.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Honglei Miao, Fan Ma, Ruijie Quan, Kun Zhan, Yi Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.00352">Autonomous LLM-Enhanced Adversarial Attack for Text-to-Motion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion generation driven by deep generative models has enabled compelling applications, but the ability of text-to-motion (T2M) models to produce realistic motions from text prompts raises security concerns if exploited maliciously. Despite growing interest in T2M, few methods focus on safeguarding these models against adversarial attacks, with existing work on text-to-image models proving insufficient for the unique motion domain. In the paper, we propose ALERT-Motion, an autonomous framework leveraging large language models (LLMs) to craft targeted adversarial attacks against black-box T2M models. Unlike prior methods modifying prompts through predefined rules, ALERT-Motion uses LLMs' knowledge of human motion to autonomously generate subtle yet powerful adversarial text descriptions. It comprises two key modules: an adaptive dispatching module that constructs an LLM-based agent to iteratively refine and search for adversarial prompts; and a multimodal information contrastive module that extracts semantically relevant motion information to guide the agent's search. Through this LLM-driven approach, ALERT-Motion crafts adversarial prompts querying victim models to produce outputs closely matching targeted motions, while avoiding obvious perturbations. Evaluations across popular T2M models demonstrate ALERT-Motion's superiority over previous methods, achieving higher attack success rates with stealthier adversarial prompts. This pioneering work on T2M adversarial attacks highlights the urgency of developing defensive measures as motion generation technology advances, urging further research into safe and responsible deployment.
<div id='section'>Paperid: <span id='pid'>560, <a href='https://arxiv.org/pdf/2405.09779.pdf' target='_blank'>https://arxiv.org/pdf/2405.09779.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wansong Liu, Kareem Eltouny, Sibo Tian, Xiao Liang, Minghui Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.09779">Integrating Uncertainty-Aware Human Motion Prediction into Graph-Based Manipulator Motion Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>There has been a growing utilization of industrial robots as complementary collaborators for human workers in re-manufacturing sites. Such a human-robot collaboration (HRC) aims to assist human workers in improving the flexibility and efficiency of labor-intensive tasks. In this paper, we propose a human-aware motion planning framework for HRC to effectively compute collision-free motions for manipulators when conducting collaborative tasks with humans. We employ a neural human motion prediction model to enable proactive planning for manipulators. Particularly, rather than blindly trusting and utilizing predicted human trajectories in the manipulator planning, we quantify uncertainties of the neural prediction model to further ensure human safety. Moreover, we integrate the uncertainty-aware prediction into a graph that captures key workspace elements and illustrates their interconnections. Then a graph neural network is leveraged to operate on the constructed graph. Consequently, robot motion planning considers both the dependencies among all the elements in the workspace and the potential influence of future movements of human workers. We experimentally validate the proposed planning framework using a 6-degree-of-freedom manipulator in a shared workspace where a human is performing disassembling tasks. The results demonstrate the benefits of our approach in terms of improving the smoothness and safety of HRC. A brief video introduction of this work is available as the supplemental materials.
<div id='section'>Paperid: <span id='pid'>561, <a href='https://arxiv.org/pdf/2402.13045.pdf' target='_blank'>https://arxiv.org/pdf/2402.13045.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wansong Liu, Sibo Tian, Boyi Hu, Xiao Liang, Minghui Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.13045">A Recurrent Neural Network Enhanced Unscented Kalman Filter for Human Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a deep learning enhanced adaptive unscented Kalman filter (UKF) for predicting human arm motion in the context of manufacturing. Unlike previous network-based methods that solely rely on captured human motion data, which is represented as bone vectors in this paper, we incorporate a human arm dynamic model into the motion prediction algorithm and use the UKF to iteratively forecast human arm motions. Specifically, a Lagrangian-mechanics-based physical model is employed to correlate arm motions with associated muscle forces. Then a Recurrent Neural Network (RNN) is integrated into the framework to predict future muscle forces, which are transferred back to future arm motions based on the dynamic model. Given the absence of measurement data for future human motions that can be input into the UKF to update the state, we integrate another RNN to directly predict human future motions and treat the prediction as surrogate measurement data fed into the UKF. A noteworthy aspect of this study involves the quantification of uncertainties associated with both the data-driven and physical models in one unified framework. These quantified uncertainties are used to dynamically adapt the measurement and process noises of the UKF over time. This adaption, driven by the uncertainties of the RNN models, addresses inaccuracies stemming from the data-driven model and mitigates discrepancies between the assumed and true physical models, ultimately enhancing the accuracy and robustness of our predictions. Compared to the traditional RNN-based prediction, our method demonstrates improved accuracy and robustness in extensive experimental validations of various types of human motions.
<div id='section'>Paperid: <span id='pid'>562, <a href='https://arxiv.org/pdf/2512.01582.pdf' target='_blank'>https://arxiv.org/pdf/2512.01582.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junran Peng, Yiheng Huang, Silei Shen, Zeji Wei, Jingwei Yang, Baojie Wang, Yonghao He, Chuanchen Luo, Man Zhang, Xucheng Yin, Wei Sui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.01582">RoleMotion: A Large-Scale Dataset towards Robust Scene-Specific Role-Playing Motion Synthesis with Fine-grained Descriptions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we introduce RoleMotion, a large-scale human motion dataset that encompasses a wealth of role-playing and functional motion data tailored to fit various specific scenes. Existing text datasets are mainly constructed decentrally as amalgamation of assorted subsets that their data are nonfunctional and isolated to work together to cover social activities in various scenes. Also, the quality of motion data is inconsistent, and textual annotation lacks fine-grained details in these datasets. In contrast, RoleMotion is meticulously designed and collected with a particular focus on scenes and roles. The dataset features 25 classic scenes, 110 functional roles, over 500 behaviors, and 10296 high-quality human motion sequences of body and hands, annotated with 27831 fine-grained text descriptions. We build an evaluator stronger than existing counterparts, prove its reliability, and evaluate various text-to-motion methods on our dataset. Finally, we explore the interplay of motion generation of body and hands. Experimental results demonstrate the high-quality and functionality of our dataset on text-driven whole-body generation.
<div id='section'>Paperid: <span id='pid'>563, <a href='https://arxiv.org/pdf/2507.04547.pdf' target='_blank'>https://arxiv.org/pdf/2507.04547.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xin You, Runze Yang, Chuyan Zhang, Zhongliang Jiang, Jie Yang, Nassir Navab
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.04547">FB-Diff: Fourier Basis-guided Diffusion for Temporal Interpolation of 4D Medical Imaging</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The temporal interpolation task for 4D medical imaging, plays a crucial role in clinical practice of respiratory motion modeling. Following the simplified linear-motion hypothesis, existing approaches adopt optical flow-based models to interpolate intermediate frames. However, realistic respiratory motions should be nonlinear and quasi-periodic with specific frequencies. Intuited by this property, we resolve the temporal interpolation task from the frequency perspective, and propose a Fourier basis-guided Diffusion model, termed FB-Diff. Specifically, due to the regular motion discipline of respiration, physiological motion priors are introduced to describe general characteristics of temporal data distributions. Then a Fourier motion operator is elaborately devised to extract Fourier bases by incorporating physiological motion priors and case-specific spectral information in the feature space of Variational Autoencoder. Well-learned Fourier bases can better simulate respiratory motions with motion patterns of specific frequencies. Conditioned on starting and ending frames, the diffusion model further leverages well-learned Fourier bases via the basis interaction operator, which promotes the temporal interpolation task in a generative manner. Extensive results demonstrate that FB-Diff achieves state-of-the-art (SOTA) perceptual performance with better temporal consistency while maintaining promising reconstruction metrics. Codes are available.
<div id='section'>Paperid: <span id='pid'>564, <a href='https://arxiv.org/pdf/2505.16524.pdf' target='_blank'>https://arxiv.org/pdf/2505.16524.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Huitong Yang, Zhuoxiao Chen, Fengyi Zhang, Zi Huang, Yadan Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.16524">CodeMerge: Codebook-Guided Model Merging for Robust Test-Time Adaptation in Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Maintaining robust 3D perception under dynamic and unpredictable test-time conditions remains a critical challenge for autonomous driving systems. Existing test-time adaptation (TTA) methods often fail in high-variance tasks like 3D object detection due to unstable optimization and sharp minima. While recent model merging strategies based on linear mode connectivity (LMC) offer improved stability by interpolating between fine-tuned checkpoints, they are computationally expensive, requiring repeated checkpoint access and multiple forward passes. In this paper, we introduce CodeMerge, a lightweight and scalable model merging framework that bypasses these limitations by operating in a compact latent space. Instead of loading full models, CodeMerge represents each checkpoint with a low-dimensional fingerprint derived from the source model's penultimate features and constructs a key-value codebook. We compute merging coefficients using ridge leverage scores on these fingerprints, enabling efficient model composition without compromising adaptation quality. Our method achieves strong performance across challenging benchmarks, improving end-to-end 3D detection 14.9% NDS on nuScenes-C and LiDAR-based detection by over 7.6% mAP on nuScenes-to-KITTI, while benefiting downstream tasks such as online mapping, motion prediction and planning even without training. Code and pretrained models are released in the supplementary material.
<div id='section'>Paperid: <span id='pid'>565, <a href='https://arxiv.org/pdf/2504.08449.pdf' target='_blank'>https://arxiv.org/pdf/2504.08449.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jian Wang, Rishabh Dabral, Diogo Luvizon, Zhe Cao, Lingjie Liu, Thabo Beeler, Christian Theobalt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.08449">Ego4o: Egocentric Human Motion Capture and Understanding from Multi-Modal Input</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work focuses on tracking and understanding human motion using consumer wearable devices, such as VR/AR headsets, smart glasses, cellphones, and smartwatches. These devices provide diverse, multi-modal sensor inputs, including egocentric images, and 1-3 sparse IMU sensors in varied combinations. Motion descriptions can also accompany these signals. The diverse input modalities and their intermittent availability pose challenges for consistent motion capture and understanding. In this work, we present Ego4o (o for omni), a new framework for simultaneous human motion capture and understanding from multi-modal egocentric inputs. This method maintains performance with partial inputs while achieving better results when multiple modalities are combined. First, the IMU sensor inputs, the optional egocentric image, and text description of human motion are encoded into the latent space of a motion VQ-VAE. Next, the latent vectors are sent to the VQ-VAE decoder and optimized to track human motion. When motion descriptions are unavailable, the latent vectors can be input into a multi-modal LLM to generate human motion descriptions, which can further enhance motion capture accuracy. Quantitative and qualitative evaluations demonstrate the effectiveness of our method in predicting accurate human motion and high-quality motion descriptions.
<div id='section'>Paperid: <span id='pid'>566, <a href='https://arxiv.org/pdf/2502.11149.pdf' target='_blank'>https://arxiv.org/pdf/2502.11149.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zongzhao Li, Jiacheng Cen, Bing Su, Wenbing Huang, Tingyang Xu, Yu Rong, Deli Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.11149">Large Language-Geometry Model: When LLM meets Equivariance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurately predicting 3D structures and dynamics of physical systems is crucial in scientific applications. Existing approaches that rely on geometric Graph Neural Networks (GNNs) effectively enforce $\mathrm{E}(3)$-equivariance, but they often fall in leveraging extensive broader information. While direct application of Large Language Models (LLMs) can incorporate external knowledge, they lack the capability for spatial reasoning with guaranteed equivariance. In this paper, we propose EquiLLM, a novel framework for representing 3D physical systems that seamlessly integrates E(3)-equivariance with LLM capabilities. Specifically, EquiLLM comprises four key components: geometry-aware prompting, an equivariant encoder, an LLM, and an equivariant adaptor. Essentially, the LLM guided by the instructive prompt serves as a sophisticated invariant feature processor, while 3D directional information is exclusively handled by the equivariant encoder and adaptor modules. Experimental results demonstrate that EquiLLM delivers significant improvements over previous methods across molecular dynamics simulation, human motion simulation, and antibody design, highlighting its promising generalizability.
<div id='section'>Paperid: <span id='pid'>567, <a href='https://arxiv.org/pdf/2404.15366.pdf' target='_blank'>https://arxiv.org/pdf/2404.15366.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiao-Yin Liu, Guotao Li, Xiao-Hu Zhou, Xu Liang, Zeng-Guang Hou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.15366">A Weight-aware-based Multi-source Unsupervised Domain Adaptation Method for Human Motion Intention Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate recognition of human motion intention (HMI) is beneficial for exoskeleton robots to improve the wearing comfort level and achieve natural human-robot interaction. A classifier trained on labeled source subjects (domains) performs poorly on unlabeled target subject since the difference in individual motor characteristics. The unsupervised domain adaptation (UDA) method has become an effective way to this problem. However, the labeled data are collected from multiple source subjects that might be different not only from the target subject but also from each other. The current UDA methods for HMI recognition ignore the difference between each source subject, which reduces the classification accuracy. Therefore, this paper considers the differences between source subjects and develops a novel theory and algorithm for UDA to recognize HMI, where the margin disparity discrepancy (MDD) is extended to multi-source UDA theory and a novel weight-aware-based multi-source UDA algorithm (WMDD) is proposed. The source domain weight, which can be adjusted adaptively by the MDD between each source subject and target subject, is incorporated into UDA to measure the differences between source subjects. The developed multi-source UDA theory is theoretical and the generalization error on target subject is guaranteed. The theory can be transformed into an optimization problem for UDA, successfully bridging the gap between theory and algorithm. Moreover, a lightweight network is employed to guarantee the real-time of classification and the adversarial learning between feature generator and ensemble classifiers is utilized to further improve the generalization ability. The extensive experiments verify theoretical analysis and show that WMDD outperforms previous UDA methods on HMI recognition tasks.
<div id='section'>Paperid: <span id='pid'>568, <a href='https://arxiv.org/pdf/2404.09967.pdf' target='_blank'>https://arxiv.org/pdf/2404.09967.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Han Lin, Jaemin Cho, Abhay Zala, Mohit Bansal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.09967">Ctrl-Adapter: An Efficient and Versatile Framework for Adapting Diverse Controls to Any Diffusion Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>ControlNets are widely used for adding spatial control to text-to-image diffusion models with different conditions, such as depth maps, scribbles/sketches, and human poses. However, when it comes to controllable video generation, ControlNets cannot be directly integrated into new backbones due to feature space mismatches, and training ControlNets for new backbones can be a significant burden for many users. Furthermore, applying ControlNets independently to different frames cannot effectively maintain object temporal consistency. To address these challenges, we introduce Ctrl-Adapter, an efficient and versatile framework that adds diverse controls to any image/video diffusion model through the adaptation of pretrained ControlNets. Ctrl-Adapter offers strong and diverse capabilities, including image and video control, sparse-frame video control, fine-grained patch-level multi-condition control (via an MoE router), zero-shot adaptation to unseen conditions, and supports a variety of downstream tasks beyond spatial control, including video editing, video style transfer, and text-guided motion control. With six diverse U-Net/DiT-based image/video diffusion models (SDXL, PixArt-$Î±$, I2VGen-XL, SVD, Latte, Hotshot-XL), Ctrl-Adapter matches the performance of pretrained ControlNets on COCO and achieves the state-of-the-art on DAVIS 2017 with significantly lower computation (< 10 GPU hours).
<div id='section'>Paperid: <span id='pid'>569, <a href='https://arxiv.org/pdf/2403.15100.pdf' target='_blank'>https://arxiv.org/pdf/2403.15100.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoyu Wang, Xiaoyu Tan, Xihe Qiu, Chao Qu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.15100">Subequivariant Reinforcement Learning Framework for Coordinated Motion Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Effective coordination is crucial for motion control with reinforcement learning, especially as the complexity of agents and their motions increases. However, many existing methods struggle to account for the intricate dependencies between joints. We introduce CoordiGraph, a novel architecture that leverages subequivariant principles from physics to enhance coordination of motion control with reinforcement learning. This method embeds the principles of equivariance as inherent patterns in the learning process under gravity influence, which aids in modeling the nuanced relationships between joints vital for motion control. Through extensive experimentation with sophisticated agents in diverse environments, we highlight the merits of our approach. Compared to current leading methods, CoordiGraph notably enhances generalization and sample efficiency.
<div id='section'>Paperid: <span id='pid'>570, <a href='https://arxiv.org/pdf/2401.02539.pdf' target='_blank'>https://arxiv.org/pdf/2401.02539.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dianye Huang, Chenguang Yang, Mingchuan Zhou, Angelos Karlas, Nassir Navab, Zhongliang Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.02539">Robot-Assisted Deep Venous Thrombosis Ultrasound Examination using Virtual Fixture</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep Venous Thrombosis (DVT) is a common vascular disease with blood clots inside deep veins, which may block blood flow or even cause a life-threatening pulmonary embolism. A typical exam for DVT using ultrasound (US) imaging is by pressing the target vein until its lumen is fully compressed. However, the compression exam is highly operator-dependent. To alleviate intra- and inter-variations, we present a robotic US system with a novel hybrid force motion control scheme ensuring position and force tracking accuracy, and soft landing of the probe onto the target surface. In addition, a path-based virtual fixture is proposed to realize easy human-robot interaction for repeat compression operation at the lesion location. To ensure the biometric measurements obtained in different examinations are comparable, the 6D scanning path is determined in a coarse-to-fine manner using both an external RGBD camera and US images. The RGBD camera is first used to extract a rough scanning path on the object. Then, the segmented vascular lumen from US images are used to optimize the scanning path to ensure the visibility of the target object. To generate a continuous scan path for developing virtual fixtures, an arc-length based path fitting model considering both position and orientation is proposed. Finally, the whole system is evaluated on a human-like arm phantom with an uneven surface.
<div id='section'>Paperid: <span id='pid'>571, <a href='https://arxiv.org/pdf/2511.17798.pdf' target='_blank'>https://arxiv.org/pdf/2511.17798.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Francesco D'Orazio, Sepehr Samavi, Xintong Du, Siqi Zhou, Giuseppe Oriolo, Angela P. Schoellig
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.17798">SM2ITH: Safe Mobile Manipulation with Interactive Human Prediction via Task-Hierarchical Bilevel Model Predictive Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Mobile manipulators are designed to perform complex sequences of navigation and manipulation tasks in human-centered environments. While recent optimization-based methods such as Hierarchical Task Model Predictive Control (HTMPC) enable efficient multitask execution with strict task priorities, they have so far been applied mainly to static or structured scenarios. Extending these approaches to dynamic human-centered environments requires predictive models that capture how humans react to the actions of the robot. This work introduces Safe Mobile Manipulation with Interactive Human Prediction via Task-Hierarchical Bilevel Model Predictive Control (SM$^2$ITH), a unified framework that combines HTMPC with interactive human motion prediction through bilevel optimization that jointly accounts for robot and human dynamics. The framework is validated on two different mobile manipulators, the Stretch 3 and the Ridgeback-UR10, across three experimental settings: (i) delivery tasks with different navigation and manipulation priorities, (ii) sequential pick-and-place tasks with different human motion prediction models, and (iii) interactions involving adversarial human behavior. Our results highlight how interactive prediction enables safe and efficient coordination, outperforming baselines that rely on weighted objectives or open-loop human models.
<div id='section'>Paperid: <span id='pid'>572, <a href='https://arxiv.org/pdf/2509.25704.pdf' target='_blank'>https://arxiv.org/pdf/2509.25704.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cheng Guo, Giuseppe L'Erario, Giulio Romualdi, Mattia Leonori, Marta Lorenzini, Arash Ajoudani, Daniele Pucci
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25704">Physics-Informed Learning for Human Whole-Body Kinematics Prediction via Sparse IMUs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate and physically feasible human motion prediction is crucial for safe and seamless human-robot collaboration. While recent advancements in human motion capture enable real-time pose estimation, the practical value of many existing approaches is limited by the lack of future predictions and consideration of physical constraints. Conventional motion prediction schemes rely heavily on past poses, which are not always available in real-world scenarios. To address these limitations, we present a physics-informed learning framework that integrates domain knowledge into both training and inference to predict human motion using inertial measurements from only 5 IMUs. We propose a network that accounts for the spatial characteristics of human movements. During training, we incorporate forward and differential kinematics functions as additional loss components to regularize the learned joint predictions. At the inference stage, we refine the prediction from the previous iteration to update a joint state buffer, which is used as extra inputs to the network. Experimental results demonstrate that our approach achieves high accuracy, smooth transitions between motions, and generalizes well to unseen subjects
<div id='section'>Paperid: <span id='pid'>573, <a href='https://arxiv.org/pdf/2509.05337.pdf' target='_blank'>https://arxiv.org/pdf/2509.05337.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Younggeol Cho, Gokhan Solak, Olivia Nocentini, Marta Lorenzini, Andrea Fortuna, Arash Ajoudani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.05337">Anticipatory Fall Detection in Humans with Hybrid Directed Graph Neural Networks and Long Short-Term Memory</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Detecting and preventing falls in humans is a critical component of assistive robotic systems. While significant progress has been made in detecting falls, the prediction of falls before they happen, and analysis of the transient state between stability and an impending fall remain unexplored. In this paper, we propose a anticipatory fall detection method that utilizes a hybrid model combining Dynamic Graph Neural Networks (DGNN) with Long Short-Term Memory (LSTM) networks that decoupled the motion prediction and gait classification tasks to anticipate falls with high accuracy. Our approach employs real-time skeletal features extracted from video sequences as input for the proposed model. The DGNN acts as a classifier, distinguishing between three gait states: stable, transient, and fall. The LSTM-based network then predicts human movement in subsequent time steps, enabling early detection of falls. The proposed model was trained and validated using the OUMVLP-Pose and URFD datasets, demonstrating superior performance in terms of prediction error and recognition accuracy compared to models relying solely on DGNN and models from literature. The results indicate that decoupling prediction and classification improves performance compared to addressing the unified problem using only the DGNN. Furthermore, our method allows for the monitoring of the transient state, offering valuable insights that could enhance the functionality of advanced assistance systems.
<div id='section'>Paperid: <span id='pid'>574, <a href='https://arxiv.org/pdf/2507.07633.pdf' target='_blank'>https://arxiv.org/pdf/2507.07633.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhitao Wang, Hengyu Man, Wenrui Li, Xingtao Wang, Xiaopeng Fan, Debin Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07633">T-GVC: Trajectory-Guided Generative Video Coding at Ultra-Low Bitrates</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in video generation techniques have given rise to an emerging paradigm of generative video coding for Ultra-Low Bitrate (ULB) scenarios by leveraging powerful generative priors. However, most existing methods are limited by domain specificity (e.g., facial or human videos) or excessive dependence on high-level text guidance, which tend to inadequately capture fine-grained motion details, leading to unrealistic or incoherent reconstructions. To address these challenges, we propose Trajectory-Guided Generative Video Coding (dubbed T-GVC), a novel framework that bridges low-level motion tracking with high-level semantic understanding. T-GVC features a semantic-aware sparse motion sampling pipeline that extracts pixel-wise motion as sparse trajectory points based on their semantic importance, significantly reducing the bitrate while preserving critical temporal semantic information. In addition, by integrating trajectory-aligned loss constraints into diffusion processes, we introduce a training-free guidance mechanism in latent space to ensure physically plausible motion patterns without sacrificing the inherent capabilities of generative models. Experimental results demonstrate that T-GVC outperforms both traditional and neural video codecs under ULB conditions. Furthermore, additional experiments confirm that our framework achieves more precise motion control than existing text-guided methods, paving the way for a novel direction of generative video coding guided by geometric motion modeling.
<div id='section'>Paperid: <span id='pid'>575, <a href='https://arxiv.org/pdf/2503.22249.pdf' target='_blank'>https://arxiv.org/pdf/2503.22249.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xianqi Zhang, Hongliang Wei, Wenrui Wang, Xingtao Wang, Xiaopeng Fan, Debin Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.22249">FLAM: Foundation Model-Based Body Stabilization for Humanoid Locomotion and Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humanoid robots have attracted significant attention in recent years. Reinforcement Learning (RL) is one of the main ways to control the whole body of humanoid robots. RL enables agents to complete tasks by learning from environment interactions, guided by task rewards. However, existing RL methods rarely explicitly consider the impact of body stability on humanoid locomotion and manipulation. Achieving high performance in whole-body control remains a challenge for RL methods that rely solely on task rewards. In this paper, we propose a Foundation model-based method for humanoid Locomotion And Manipulation (FLAM for short). FLAM integrates a stabilizing reward function with a basic policy. The stabilizing reward function is designed to encourage the robot to learn stable postures, thereby accelerating the learning process and facilitating task completion. Specifically, the robot pose is first mapped to the 3D virtual human model. Then, the human pose is stabilized and reconstructed through a human motion reconstruction model. Finally, the pose before and after reconstruction is used to compute the stabilizing reward. By combining this stabilizing reward with the task reward, FLAM effectively guides policy learning. Experimental results on a humanoid robot benchmark demonstrate that FLAM outperforms state-of-the-art RL methods, highlighting its effectiveness in improving stability and overall performance.
<div id='section'>Paperid: <span id='pid'>576, <a href='https://arxiv.org/pdf/2502.17322.pdf' target='_blank'>https://arxiv.org/pdf/2502.17322.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zifeng Zhuang, Diyuan Shi, Runze Suo, Xiao He, Hongyin Zhang, Ting Wang, Shangke Lyu, Donglin Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.17322">TDMPBC: Self-Imitative Reinforcement Learning for Humanoid Robot Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Complex high-dimensional spaces with high Degree-of-Freedom and complicated action spaces, such as humanoid robots equipped with dexterous hands, pose significant challenges for reinforcement learning (RL) algorithms, which need to wisely balance exploration and exploitation under limited sample budgets. In general, feasible regions for accomplishing tasks within complex high-dimensional spaces are exceedingly narrow. For instance, in the context of humanoid robot motion control, the vast majority of space corresponds to falling, while only a minuscule fraction corresponds to standing upright, which is conducive to the completion of downstream tasks. Once the robot explores into a potentially task-relevant region, it should place greater emphasis on the data within that region. Building on this insight, we propose the $\textbf{S}$elf-$\textbf{I}$mitative $\textbf{R}$einforcement $\textbf{L}$earning ($\textbf{SIRL}$) framework, where the RL algorithm also imitates potentially task-relevant trajectories. Specifically, trajectory return is utilized to determine its relevance to the task and an additional behavior cloning is adopted whose weight is dynamically adjusted based on the trajectory return. As a result, our proposed algorithm achieves 120% performance improvement on the challenging HumanoidBench with 5% extra computation overhead. With further visualization, we find the significant performance gain does lead to meaningful behavior improvement that several tasks are solved successfully.
<div id='section'>Paperid: <span id='pid'>577, <a href='https://arxiv.org/pdf/2510.03022.pdf' target='_blank'>https://arxiv.org/pdf/2510.03022.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rui Zhong, Yizhe Sun, Junjie Wen, Jinming Li, Chuang Cheng, Wei Dai, Zhiwen Zeng, Huimin Lu, Yichen Zhu, Yi Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03022">HumanoidExo: Scalable Whole-Body Humanoid Manipulation via Wearable Exoskeleton</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A significant bottleneck in humanoid policy learning is the acquisition of large-scale, diverse datasets, as collecting reliable real-world data remains both difficult and cost-prohibitive. To address this limitation, we introduce HumanoidExo, a novel system that transfers human motion to whole-body humanoid data. HumanoidExo offers a high-efficiency solution that minimizes the embodiment gap between the human demonstrator and the robot, thereby tackling the scarcity of whole-body humanoid data. By facilitating the collection of more voluminous and diverse datasets, our approach significantly enhances the performance of humanoid robots in dynamic, real-world scenarios. We evaluated our method across three challenging real-world tasks: table-top manipulation, manipulation integrated with stand-squat motions, and whole-body manipulation. Our results empirically demonstrate that HumanoidExo is a crucial addition to real-robot data, as it enables the humanoid policy to generalize to novel environments, learn complex whole-body control from only five real-robot demonstrations, and even acquire new skills (i.e., walking) solely from HumanoidExo data.
<div id='section'>Paperid: <span id='pid'>578, <a href='https://arxiv.org/pdf/2509.10426.pdf' target='_blank'>https://arxiv.org/pdf/2509.10426.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianxin Shi, Zengqi Peng, Xiaolong Chen, Tianyu Wo, Jun Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.10426">DECAMP: Towards Scene-Consistent Multi-Agent Motion Prediction with Disentangled Context-Aware Pre-Training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Trajectory prediction is a critical component of autonomous driving, essential for ensuring both safety and efficiency on the road. However, traditional approaches often struggle with the scarcity of labeled data and exhibit suboptimal performance in multi-agent prediction scenarios. To address these challenges, we introduce a disentangled context-aware pre-training framework for multi-agent motion prediction, named DECAMP. Unlike existing methods that entangle representation learning with pretext tasks, our framework decouples behavior pattern learning from latent feature reconstruction, prioritizing interpretable dynamics and thereby enhancing scene representation for downstream prediction. Additionally, our framework incorporates context-aware representation learning alongside collaborative spatial-motion pretext tasks, which enables joint optimization of structural and intentional reasoning while capturing the underlying dynamic intentions. Our experiments on the Argoverse 2 benchmark showcase the superior performance of our method, and the results attained underscore its effectiveness in multi-agent motion forecasting. To the best of our knowledge, this is the first context autoencoder framework for multi-agent motion forecasting in autonomous driving. The code and models will be made publicly available.
<div id='section'>Paperid: <span id='pid'>579, <a href='https://arxiv.org/pdf/2410.21229.pdf' target='_blank'>https://arxiv.org/pdf/2410.21229.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tairan He, Wenli Xiao, Toru Lin, Zhengyi Luo, Zhenjia Xu, Zhenyu Jiang, Jan Kautz, Changliu Liu, Guanya Shi, Xiaolong Wang, Linxi Fan, Yuke Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.21229">HOVER: Versatile Neural Whole-Body Controller for Humanoid Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humanoid whole-body control requires adapting to diverse tasks such as navigation, loco-manipulation, and tabletop manipulation, each demanding a different mode of control. For example, navigation relies on root velocity tracking, while tabletop manipulation prioritizes upper-body joint angle tracking. Existing approaches typically train individual policies tailored to a specific command space, limiting their transferability across modes. We present the key insight that full-body kinematic motion imitation can serve as a common abstraction for all these tasks and provide general-purpose motor skills for learning multiple modes of whole-body control. Building on this, we propose HOVER (Humanoid Versatile Controller), a multi-mode policy distillation framework that consolidates diverse control modes into a unified policy. HOVER enables seamless transitions between control modes while preserving the distinct advantages of each, offering a robust and scalable solution for humanoid control across a wide range of modes. By eliminating the need for policy retraining for each control mode, our approach improves efficiency and flexibility for future humanoid applications.
<div id='section'>Paperid: <span id='pid'>580, <a href='https://arxiv.org/pdf/2410.05015.pdf' target='_blank'>https://arxiv.org/pdf/2410.05015.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Simon Bultmann, Raphael Memmesheimer, Jan Nogga, Julian Hau, Sven Behnke
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.05015">Anticipating Human Behavior for Safe Navigation and Efficient Collaborative Manipulation with Mobile Service Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The anticipation of human behavior is a crucial capability for robots to interact with humans safely and efficiently. We employ a smart edge sensor network to provide global observations, future predictions, and goal information to integrate anticipatory behavior for the control of a mobile manipulation robot. We present approaches to anticipate human behavior in the context of safe navigation and collaborative mobile manipulation. First, we anticipate human motion by employing projections of predicted human trajectories from smart edge sensor observations into the planning map of a mobile robot. Second, we anticipate human intentions in a collaborative furniture-carrying task to achieve a given room layout. Our experiments indicate that anticipating human behavior allows for safer navigation and more efficient collaboration. Finally, we showcase an integrated robotic system that anticipates human behavior while collaborating with an operator to achieve a target room layout, including the placement of tables and chairs.
<div id='section'>Paperid: <span id='pid'>581, <a href='https://arxiv.org/pdf/2510.10254.pdf' target='_blank'>https://arxiv.org/pdf/2510.10254.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxiang Lai, Jike Zhong, Ming Li, Yuheng Li, Xiaofeng Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.10254">Are Video Models Emerging as Zero-Shot Learners and Reasoners in Medical Imaging?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in large generative models have shown that simple autoregressive formulations, when scaled appropriately, can exhibit strong zero-shot generalization across domains. Motivated by this trend, we investigate whether autoregressive video modeling principles can be directly applied to medical imaging tasks, despite the model never being trained on medical data. Specifically, we evaluate a large vision model (LVM) in a zero-shot setting across four representative tasks: organ segmentation, denoising, super-resolution, and motion prediction. Remarkably, even without domain-specific fine-tuning, the LVM can delineate anatomical structures in CT scans and achieve competitive performance on segmentation, denoising, and super-resolution. Most notably, in radiotherapy motion prediction, the model forecasts future 3D CT phases directly from prior phases of a 4D CT scan, producing anatomically consistent predictions that capture patient-specific respiratory dynamics with realistic temporal coherence. We evaluate the LVM on 4D CT data from 122 patients, totaling over 1,820 3D CT volumes. Despite no prior exposure to medical data, the model achieves strong performance across all tasks and surpasses specialized DVF-based and generative baselines in motion prediction, achieving state-of-the-art spatial accuracy. These findings reveal the emergence of zero-shot capabilities in medical video modeling and highlight the potential of general-purpose video models to serve as unified learners and reasoners laying the groundwork for future medical foundation models built on video models.
<div id='section'>Paperid: <span id='pid'>582, <a href='https://arxiv.org/pdf/2509.23635.pdf' target='_blank'>https://arxiv.org/pdf/2509.23635.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruibing Hou, Mingshuang Luo, Hongyu Pan, Hong Chang, Shiguang Shan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23635">MotionVerse: A Unified Multimodal Framework for Motion Comprehension, Generation and Editing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper proposes MotionVerse, a unified framework that harnesses the capabilities of Large Language Models (LLMs) to comprehend, generate, and edit human motion in both single-person and multi-person scenarios. To efficiently represent motion data, we employ a motion tokenizer with residual quantization, which converts continuous motion sequences into multi-stream discrete tokens. Furthermore, we introduce a \textit{Delay Parallel} Modeling strategy, which temporally staggers the encoding of residual token streams. This design enables LLMs to effectively capture inter-stream dependencies while maintaining computational efficiency comparable to single-stream modeling. Moreover, to alleviate modality interference between motion and language, we design a \textit{dual-tower architecture} with modality-specific parameters, ensuring stable integration of motion information for both comprehension and generation tasks. Comprehensive ablation studies demonstrate the effectiveness of each component in MotionVerse, and extensive experiments showcase its superior performance across a wide range of motion-relevant tasks.
<div id='section'>Paperid: <span id='pid'>583, <a href='https://arxiv.org/pdf/2507.02085.pdf' target='_blank'>https://arxiv.org/pdf/2507.02085.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wanjia Zhao, Jiaqi Han, Siyi Gu, Mingjian Jiang, James Zou, Stefano Ermon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02085">GeoAda: Efficiently Finetune Geometric Diffusion Models with Equivariant Adapters</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Geometric diffusion models have shown remarkable success in molecular dynamics and structure generation. However, efficiently fine-tuning them for downstream tasks with varying geometric controls remains underexplored. In this work, we propose an SE(3)-equivariant adapter framework ( GeoAda) that enables flexible and parameter-efficient fine-tuning for controlled generative tasks without modifying the original model architecture. GeoAda introduces a structured adapter design: control signals are first encoded through coupling operators, then processed by a trainable copy of selected pretrained model layers, and finally projected back via decoupling operators followed by an equivariant zero-initialized convolution. By fine-tuning only these lightweight adapter modules, GeoAda preserves the model's geometric consistency while mitigating overfitting and catastrophic forgetting. We theoretically prove that the proposed adapters maintain SE(3)-equivariance, ensuring that the geometric inductive biases of the pretrained diffusion model remain intact during adaptation. We demonstrate the wide applicability of GeoAda across diverse geometric control types, including frame control, global control, subgraph control, and a broad range of application domains such as particle dynamics, molecular dynamics, human motion prediction, and molecule generation. Empirical results show that GeoAda achieves state-of-the-art fine-tuning performance while preserving original task accuracy, whereas other baselines experience significant performance degradation due to overfitting and catastrophic forgetting.
<div id='section'>Paperid: <span id='pid'>584, <a href='https://arxiv.org/pdf/2506.05207.pdf' target='_blank'>https://arxiv.org/pdf/2506.05207.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yue Ma, Yulong Liu, Qiyuan Zhu, Ayden Yang, Kunyu Feng, Xinhua Zhang, Zhifeng Li, Sirui Han, Chenyang Qi, Qifeng Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.05207">Follow-Your-Motion: Video Motion Transfer via Efficient Spatial-Temporal Decoupled Finetuning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, breakthroughs in the video diffusion transformer have shown remarkable capabilities in diverse motion generations. As for the motion-transfer task, current methods mainly use two-stage Low-Rank Adaptations (LoRAs) finetuning to obtain better performance. However, existing adaptation-based motion transfer still suffers from motion inconsistency and tuning inefficiency when applied to large video diffusion transformers. Naive two-stage LoRA tuning struggles to maintain motion consistency between generated and input videos due to the inherent spatial-temporal coupling in the 3D attention operator. Additionally, they require time-consuming fine-tuning processes in both stages. To tackle these issues, we propose Follow-Your-Motion, an efficient two-stage video motion transfer framework that finetunes a powerful video diffusion transformer to synthesize complex motion. Specifically, we propose a spatial-temporal decoupled LoRA to decouple the attention architecture for spatial appearance and temporal motion processing. During the second training stage, we design the sparse motion sampling and adaptive RoPE to accelerate the tuning speed. To address the lack of a benchmark for this field, we introduce MotionBench, a comprehensive benchmark comprising diverse motion, including creative camera motion, single object motion, multiple object motion, and complex human motion. We show extensive evaluations on MotionBench to verify the superiority of Follow-Your-Motion.
<div id='section'>Paperid: <span id='pid'>585, <a href='https://arxiv.org/pdf/2505.10810.pdf' target='_blank'>https://arxiv.org/pdf/2505.10810.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gabriel Maldonado, Armin Danesh Pazho, Ghazal Alinezhad Noghre, Vinit Katariya, Hamed Tabkhi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.10810">MoCLIP: Motion-Aware Fine-Tuning and Distillation of CLIP for Human Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion generation is essential for fields such as animation, robotics, and virtual reality, requiring models that effectively capture motion dynamics from text descriptions. Existing approaches often rely on Contrastive Language-Image Pretraining (CLIP)-based text encoders, but their training on text-image pairs constrains their ability to understand temporal and kinematic structures inherent in motion and motion generation. This work introduces MoCLIP, a fine-tuned CLIP model with an additional motion encoding head, trained on motion sequences using contrastive learning and tethering loss. By explicitly incorporating motion-aware representations, MoCLIP enhances motion fidelity while remaining compatible with existing CLIP-based pipelines and seamlessly integrating into various CLIP-based methods. Experiments demonstrate that MoCLIP improves Top-1, Top-2, and Top-3 accuracy while maintaining competitive FID, leading to improved text-to-motion alignment results. These results highlight MoCLIP's versatility and effectiveness, establishing it as a robust framework for enhancing motion generation.
<div id='section'>Paperid: <span id='pid'>586, <a href='https://arxiv.org/pdf/2502.05432.pdf' target='_blank'>https://arxiv.org/pdf/2502.05432.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohammadreza Baharani, Ghazal Alinezhad Noghre, Armin Danesh Pazho, Gabriel Maldonado, Hamed Tabkhi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.05432">MoFM: A Large-Scale Human Motion Foundation Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Foundation Models (FM) have increasingly drawn the attention of researchers due to their scalability and generalization across diverse tasks. Inspired by the success of FMs and the principles that have driven advancements in Large Language Models (LLMs), we introduce MoFM as a novel Motion Foundation Model. MoFM is designed for the semantic understanding of complex human motions in both time and space. To facilitate large-scale training, MotionBook, a comprehensive human motion dictionary of discretized motions is designed and employed. MotionBook utilizes Thermal Cubes to capture spatio-temporal motion heatmaps, applying principles from discrete variational models to encode human movements into discrete units for a more efficient and scalable representation. MoFM, trained on a large corpus of motion data, provides a foundational backbone adaptable to diverse downstream tasks, supporting paradigms such as one-shot, unsupervised, and supervised tasks. This versatility makes MoFM well-suited for a wide range of motion-based applications.
<div id='section'>Paperid: <span id='pid'>587, <a href='https://arxiv.org/pdf/2412.14172.pdf' target='_blank'>https://arxiv.org/pdf/2412.14172.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiageng Mao, Siheng Zhao, Siqi Song, Tianheng Shi, Junjie Ye, Mingtong Zhang, Haoran Geng, Jitendra Malik, Vitor Guizilini, Yue Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.14172">Learning from Massive Human Videos for Universal Humanoid Pose Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scalable learning of humanoid robots is crucial for their deployment in real-world applications. While traditional approaches primarily rely on reinforcement learning or teleoperation to achieve whole-body control, they are often limited by the diversity of simulated environments and the high costs of demonstration collection. In contrast, human videos are ubiquitous and present an untapped source of semantic and motion information that could significantly enhance the generalization capabilities of humanoid robots. This paper introduces Humanoid-X, a large-scale dataset of over 20 million humanoid robot poses with corresponding text-based motion descriptions, designed to leverage this abundant data. Humanoid-X is curated through a comprehensive pipeline: data mining from the Internet, video caption generation, motion retargeting of humans to humanoid robots, and policy learning for real-world deployment. With Humanoid-X, we further train a large humanoid model, UH-1, which takes text instructions as input and outputs corresponding actions to control a humanoid robot. Extensive simulated and real-world experiments validate that our scalable training approach leads to superior generalization in text-based humanoid control, marking a significant step toward adaptable, real-world-ready humanoid robots.
<div id='section'>Paperid: <span id='pid'>588, <a href='https://arxiv.org/pdf/2411.14951.pdf' target='_blank'>https://arxiv.org/pdf/2411.14951.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhuo Li, Mingshuang Luo, Ruibing Hou, Xin Zhao, Hao Liu, Hong Chang, Zimo Liu, Chen Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.14951">Morph: A Motion-free Physics Optimization Framework for Human Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion generation has been widely studied due to its crucial role in areas such as digital humans and humanoid robot control. However, many current motion generation approaches disregard physics constraints, frequently resulting in physically implausible motions with pronounced artifacts such as floating and foot sliding. Meanwhile, training an effective motion physics optimizer with noisy motion data remains largely unexplored. In this paper, we propose \textbf{Morph}, a \textbf{Mo}tion-F\textbf{r}ee \textbf{ph}ysics optimization framework, consisting of a Motion Generator and a Motion Physics Refinement module, for enhancing physical plausibility without relying on expensive real-world motion data. Specifically, the motion generator is responsible for providing large-scale synthetic, noisy motion data, while the motion physics refinement module utilizes these synthetic data to learn a motion imitator within a physics simulator, enforcing physical constraints to project the noisy motions into a physically-plausible space. Additionally, we introduce a prior reward module to enhance the stability of the physics optimization process and generate smoother and more stable motions. These physically refined motions are then used to fine-tune the motion generator, further enhancing its capability. This collaborative training paradigm enables mutual enhancement between the motion generator and the motion physics refinement module, significantly improving practicality and robustness in real-world applications. Experiments on both text-to-motion and music-to-dance generation tasks demonstrate that our framework achieves state-of-the-art motion quality while improving physical plausibility drastically.
<div id='section'>Paperid: <span id='pid'>589, <a href='https://arxiv.org/pdf/2408.15185.pdf' target='_blank'>https://arxiv.org/pdf/2408.15185.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ghazal Alinezhad Noghre, Armin Danesh Pazho, Hamed Tabkhi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.15185">Human-Centric Video Anomaly Detection Through Spatio-Temporal Pose Tokenization and Transformer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video Anomaly Detection (VAD) presents a significant challenge in computer vision, particularly due to the unpredictable and infrequent nature of anomalous events, coupled with the diverse and dynamic environments in which they occur. Human-centric VAD, a specialized area within this domain, faces additional complexities, including variations in human behavior, potential biases in data, and substantial privacy concerns related to human subjects. These issues complicate the development of models that are both robust and generalizable. To address these challenges, recent advancements have focused on pose-based VAD, which leverages human pose as a high-level feature to mitigate privacy concerns, reduce appearance biases, and minimize background interference. In this paper, we introduce SPARTA, a novel transformer-based architecture designed specifically for human-centric pose-based VAD. SPARTA introduces an innovative Spatio-Temporal Pose and Relative Pose (ST-PRP) tokenization method that produces an enriched representation of human motion over time. This approach ensures that the transformer's attention mechanism captures both spatial and temporal patterns simultaneously, rather than focusing on only one aspect. The addition of the relative pose further emphasizes subtle deviations from normal human movements. The architecture's core, a novel Unified Encoder Twin Decoders (UETD) transformer, significantly improves the detection of anomalous behaviors in video data. Extensive evaluations across multiple benchmark datasets demonstrate that SPARTA consistently outperforms existing methods, establishing a new state-of-the-art in pose-based VAD.
<div id='section'>Paperid: <span id='pid'>590, <a href='https://arxiv.org/pdf/2403.05972.pdf' target='_blank'>https://arxiv.org/pdf/2403.05972.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianwen Li, Hyunsang Park, Wenjian Hao, Lei Xin, Jalil Chavez-Galaviz, Ajinkya Chaudhary, Meredith Bloss, Kyle Pattison, Christopher Vo, Devesh Upadhyay, Shreyas Sundaram, Shaoshuai Mou, Nina Mahmoudian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.05972">C3D: Cascade Control with Change Point Detection and Deep Koopman Learning for Autonomous Surface Vehicles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we discuss the development and deployment of a robust autonomous system capable of performing various tasks in the maritime domain under unknown dynamic conditions. We investigate a data-driven approach based on modular design for ease of transfer of autonomy across different maritime surface vessel platforms. The data-driven approach alleviates issues related to a priori identification of system models that may become deficient under evolving system behaviors or shifting, unanticipated, environmental influences. Our proposed learning-based platform comprises a deep Koopman system model and a change point detector that provides guidance on domain shifts prompting relearning under severe exogenous and endogenous perturbations. Motion control of the autonomous system is achieved via an optimal controller design. The Koopman linearized model naturally lends itself to a linear-quadratic regulator (LQR) control design. We propose the C3D control architecture Cascade Control with Change Point Detection and Deep Koopman Learning. The framework is verified in station keeping task on an ASV in both simulation and real experiments. The approach achieved at least 13.9 percent improvement in mean distance error in all test cases compared to the methods that do not consider system changes.
<div id='section'>Paperid: <span id='pid'>591, <a href='https://arxiv.org/pdf/2512.00345.pdf' target='_blank'>https://arxiv.org/pdf/2512.00345.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junqiao Fan, Haocong Rao, Jiarui Zhang, Jianfei Yang, Lihua Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.00345">mmPred: Radar-based Human Motion Prediction in the Dark</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing Human Motion Prediction (HMP) methods based on RGB-D cameras are sensitive to lighting conditions and raise privacy concerns, limiting their real-world applications such as firefighting and healthcare. Motivated by the robustness and privacy-preserving nature of millimeter-wave (mmWave) radar, this work introduces radar as a novel sensing modality for HMP, for the first time. Nevertheless, radar signals often suffer from specular reflections and multipath effects, resulting in noisy and temporally inconsistent measurements, such as body-part miss-detection. To address these radar-specific artifacts, we propose mmPred, the first diffusion-based framework tailored for radar-based HMP. mmPred introduces a dual-domain historical motion representation to guide the generation process, combining a Time-domain Pose Refinement (TPR) branch for learning fine-grained details and a Frequency-domain Dominant Motion (FDM) branch for capturing global motion trends and suppressing frame-level inconsistency. Furthermore, we design a Global Skeleton-relational Transformer (GST) as the diffusion backbone to model global inter-joint cooperation, enabling corrupted joints to dynamically aggregate information from others. Extensive experiments show that mmPred achieves state-of-the-art performance, outperforming existing methods by 8.6% on mmBody and 22% on mm-Fi.
<div id='section'>Paperid: <span id='pid'>592, <a href='https://arxiv.org/pdf/2511.15379.pdf' target='_blank'>https://arxiv.org/pdf/2511.15379.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunjiao Zhou, Xinyan Chen, Junlang Qian, Lihua Xie, Jianfei Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.15379">Zero-Shot Open-Vocabulary Human Motion Grounding with Test-Time Training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding complex human activities demands the ability to decompose motion into fine-grained, semantic-aligned sub-actions. This motion grounding process is crucial for behavior analysis, embodied AI and virtual reality. Yet, most existing methods rely on dense supervision with predefined action classes, which are infeasible in open-vocabulary, real-world settings. In this paper, we propose ZOMG, a zero-shot, open-vocabulary framework that segments motion sequences into semantically meaningful sub-actions without requiring any annotations or fine-tuning. Technically, ZOMG integrates (1) language semantic partition, which leverages large language models to decompose instructions into ordered sub-action units, and (2) soft masking optimization, which learns instance-specific temporal masks to focus on frames critical to sub-actions, while maintaining intra-segment continuity and enforcing inter-segment separation, all without altering the pretrained encoder. Experiments on three motion-language datasets demonstrate state-of-the-art effectiveness and efficiency of motion grounding performance, outperforming prior methods by +8.7\% mAP on HumanML3D benchmark. Meanwhile, significant improvements also exist in downstream retrieval, establishing a new paradigm for annotation-free motion understanding.
<div id='section'>Paperid: <span id='pid'>593, <a href='https://arxiv.org/pdf/2509.10678.pdf' target='_blank'>https://arxiv.org/pdf/2509.10678.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiahao Luo, Chaoyang Wang, Michael Vasilkovsky, Vladislav Shakhrai, Di Liu, Peiye Zhuang, Sergey Tulyakov, Peter Wonka, Hsin-Ying Lee, James Davis, Jian Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.10678">T2Bs: Text-to-Character Blendshapes via Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present T2Bs, a framework for generating high-quality, animatable character head morphable models from text by combining static text-to-3D generation with video diffusion. Text-to-3D models produce detailed static geometry but lack motion synthesis, while video diffusion models generate motion with temporal and multi-view geometric inconsistencies. T2Bs bridges this gap by leveraging deformable 3D Gaussian splatting to align static 3D assets with video outputs. By constraining motion with static geometry and employing a view-dependent deformation MLP, T2Bs (i) outperforms existing 4D generation methods in accuracy and expressiveness while reducing video artifacts and view inconsistencies, and (ii) reconstructs smooth, coherent, fully registered 3D geometries designed to scale for building morphable models with diverse, realistic facial motions. This enables synthesizing expressive, animatable character heads that surpass current 4D generation techniques.
<div id='section'>Paperid: <span id='pid'>594, <a href='https://arxiv.org/pdf/2508.12081.pdf' target='_blank'>https://arxiv.org/pdf/2508.12081.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haidong Xu, Guangwei Xu, Zhedong Zheng, Xiatian Zhu, Wei Ji, Xiangtai Li, Ruijie Guo, Meishan Zhang, Min zhang, Hao Fei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.12081">VimoRAG: Video-based Retrieval-augmented 3D Motion Generation for Motion Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces VimoRAG, a novel video-based retrieval-augmented motion generation framework for motion large language models (LLMs). As motion LLMs face severe out-of-domain/out-of-vocabulary issues due to limited annotated data, VimoRAG leverages large-scale in-the-wild video databases to enhance 3D motion generation by retrieving relevant 2D human motion signals. While video-based motion RAG is nontrivial, we address two key bottlenecks: (1) developing an effective motion-centered video retrieval model that distinguishes human poses and actions, and (2) mitigating the issue of error propagation caused by suboptimal retrieval results. We design the Gemini Motion Video Retriever mechanism and the Motion-centric Dual-alignment DPO Trainer, enabling effective retrieval and generation processes. Experimental results show that VimoRAG significantly boosts the performance of motion LLMs constrained to text-only input.
<div id='section'>Paperid: <span id='pid'>595, <a href='https://arxiv.org/pdf/2405.20340.pdf' target='_blank'>https://arxiv.org/pdf/2405.20340.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ling-Hao Chen, Shunlin Lu, Ailing Zeng, Hao Zhang, Benyou Wang, Ruimao Zhang, Lei Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.20340">MotionLLM: Understanding Human Behaviors from Human Motions and Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study delves into the realm of multi-modality (i.e., video and motion modalities) human behavior understanding by leveraging the powerful capabilities of Large Language Models (LLMs). Diverging from recent LLMs designed for video-only or motion-only understanding, we argue that understanding human behavior necessitates joint modeling from both videos and motion sequences (e.g., SMPL sequences) to capture nuanced body part dynamics and semantics effectively. In light of this, we present MotionLLM, a straightforward yet effective framework for human motion understanding, captioning, and reasoning. Specifically, MotionLLM adopts a unified video-motion training strategy that leverages the complementary advantages of existing coarse video-text data and fine-grained motion-text data to glean rich spatial-temporal insights. Furthermore, we collect a substantial dataset, MoVid, comprising diverse videos, motions, captions, and instructions. Additionally, we propose the MoVid-Bench, with carefully manual annotations, for better evaluation of human behavior understanding on video and motion. Extensive experiments show the superiority of MotionLLM in the caption, spatial-temporal comprehension, and reasoning ability.
<div id='section'>Paperid: <span id='pid'>596, <a href='https://arxiv.org/pdf/2403.13331.pdf' target='_blank'>https://arxiv.org/pdf/2403.13331.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaosong Jia, Shaoshuai Shi, Zijun Chen, Li Jiang, Wenlong Liao, Tao He, Junchi Yan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.13331">AMP: Autoregressive Motion Prediction Revisited with Next Token Prediction for Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As an essential task in autonomous driving (AD), motion prediction aims to predict the future states of surround objects for navigation. One natural solution is to estimate the position of other agents in a step-by-step manner where each predicted time-step is conditioned on both observed time-steps and previously predicted time-steps, i.e., autoregressive prediction. Pioneering works like SocialLSTM and MFP design their decoders based on this intuition. However, almost all state-of-the-art works assume that all predicted time-steps are independent conditioned on observed time-steps, where they use a single linear layer to generate positions of all time-steps simultaneously. They dominate most motion prediction leaderboards due to the simplicity of training MLPs compared to autoregressive networks.
  In this paper, we introduce the GPT style next token prediction into motion forecasting. In this way, the input and output could be represented in a unified space and thus the autoregressive prediction becomes more feasible. However, different from language data which is composed of homogeneous units -words, the elements in the driving scene could have complex spatial-temporal and semantic relations. To this end, we propose to adopt three factorized attention modules with different neighbors for information aggregation and different position encoding styles to capture their relations, e.g., encoding the transformation between coordinate systems for spatial relativity while adopting RoPE for temporal relativity. Empirically, by equipping with the aforementioned tailored designs, the proposed method achieves state-of-the-art performance in the Waymo Open Motion and Waymo Interaction datasets. Notably, AMP outperforms other recent autoregressive motion prediction methods: MotionLM and StateTransformer, which demonstrates the effectiveness of the proposed designs.
<div id='section'>Paperid: <span id='pid'>597, <a href='https://arxiv.org/pdf/2511.04235.pdf' target='_blank'>https://arxiv.org/pdf/2511.04235.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhengru Fang, Yu Guo, Jingjing Wang, Yuang Zhang, Haonan An, Yinhai Wang, Yuguang Fang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.04235">Shared Spatial Memory Through Predictive Coding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Sharing and reconstructing a consistent spatial memory is a critical challenge in multi-agent systems, where partial observability and limited bandwidth often lead to catastrophic failures in coordination. We introduce a multi-agent predictive coding framework that formulate coordination as the minimization of mutual uncertainty among agents. Instantiated as an information bottleneck objective, it prompts agents to learn not only who and what to communicate but also when. At the foundation of this framework lies a grid-cell-like metric as internal spatial coding for self-localization, emerging spontaneously from self-supervised motion prediction. Building upon this internal spatial code, agents gradually develop a bandwidth-efficient communication mechanism and specialized neural populations that encode partners' locations: an artificial analogue of hippocampal social place cells (SPCs). These social representations are further enacted by a hierarchical reinforcement learning policy that actively explores to reduce joint uncertainty. On the Memory-Maze benchmark, our approach shows exceptional resilience to bandwidth constraints: success degrades gracefully from 73.5% to 64.4% as bandwidth shrinks from 128 to 4 bits/step, whereas a full-broadcast baseline collapses from 67.6% to 28.6%. Our findings establish a theoretically principled and biologically plausible basis for how complex social representations emerge from a unified predictive drive, leading to social collective intelligence.
<div id='section'>Paperid: <span id='pid'>598, <a href='https://arxiv.org/pdf/2510.25241.pdf' target='_blank'>https://arxiv.org/pdf/2510.25241.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Huang, Geeta Chandra Raju Bethala, Shuaihang Yuan, Congcong Wen, Anthony Tzes, Yi Fang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.25241">One-shot Humanoid Whole-body Motion Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Whole-body humanoid motion represents a cornerstone challenge in robotics, integrating balance, coordination, and adaptability to enable human-like behaviors. However, existing methods typically require multiple training samples per motion category, rendering the collection of high-quality human motion datasets both labor-intensive and costly. To address this, we propose a novel approach that trains effective humanoid motion policies using only a single non-walking target motion sample alongside readily available walking motions. The core idea lies in leveraging order-preserving optimal transport to compute distances between walking and non-walking sequences, followed by interpolation along geodesics to generate new intermediate pose skeletons, which are then optimized for collision-free configurations and retargeted to the humanoid before integration into a simulated environment for policy training via reinforcement learning. Experimental evaluations on the CMU MoCap dataset demonstrate that our method consistently outperforms baselines, achieving superior performance across metrics. Code will be released upon acceptance.
<div id='section'>Paperid: <span id='pid'>599, <a href='https://arxiv.org/pdf/2506.05117.pdf' target='_blank'>https://arxiv.org/pdf/2506.05117.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zihan Xu, Mengxian Hu, Kaiyan Xiao, Qin Fang, Chengju Liu, Qijun Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.05117">Realizing Text-Driven Motion Generation on NAO Robot: A Reinforcement Learning-Optimized Control Pipeline</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion retargeting for humanoid robots, transferring human motion data to robots for imitation, presents significant challenges but offers considerable potential for real-world applications. Traditionally, this process relies on human demonstrations captured through pose estimation or motion capture systems. In this paper, we explore a text-driven approach to mapping human motion to humanoids. To address the inherent discrepancies between the generated motion representations and the kinematic constraints of humanoid robots, we propose an angle signal network based on norm-position and rotation loss (NPR Loss). It generates joint angles, which serve as inputs to a reinforcement learning-based whole-body joint motion control policy. The policy ensures tracking of the generated motions while maintaining the robot's stability during execution. Our experimental results demonstrate the efficacy of this approach, successfully transferring text-driven human motion to a real humanoid robot NAO.
<div id='section'>Paperid: <span id='pid'>600, <a href='https://arxiv.org/pdf/2501.16551.pdf' target='_blank'>https://arxiv.org/pdf/2501.16551.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhongyu Jiang, Wenhao Chai, Zhuoran Zhou, Cheng-Yen Yang, Hsiang-Wei Huang, Jenq-Neng Hwang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.16551">PackDiT: Joint Human Motion and Text Generation via Mutual Prompting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion generation has advanced markedly with the advent of diffusion models. Most recent studies have concentrated on generating motion sequences based on text prompts, commonly referred to as text-to-motion generation. However, the bidirectional generation of motion and text, enabling tasks such as motion-to-text alongside text-to-motion, has been largely unexplored. This capability is essential for aligning diverse modalities and supports unconditional generation. In this paper, we introduce PackDiT, the first diffusion-based generative model capable of performing various tasks simultaneously, including motion generation, motion prediction, text generation, text-to-motion, motion-to-text, and joint motion-text generation. Our core innovation leverages mutual blocks to integrate multiple diffusion transformers (DiTs) across different modalities seamlessly. We train PackDiT on the HumanML3D dataset, achieving state-of-the-art text-to-motion performance with an FID score of 0.106, along with superior results in motion prediction and in-between tasks. Our experiments further demonstrate that diffusion models are effective for motion-to-text generation, achieving performance comparable to that of autoregressive models.
<div id='section'>Paperid: <span id='pid'>601, <a href='https://arxiv.org/pdf/2501.04782.pdf' target='_blank'>https://arxiv.org/pdf/2501.04782.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andrew Bond, Jui-Hsien Wang, Long Mai, Erkut Erdem, Aykut Erdem
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.04782">GaussianVideo: Efficient Video Representation via Hierarchical Gaussian Splatting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Efficient neural representations for dynamic video scenes are critical for applications ranging from video compression to interactive simulations. Yet, existing methods often face challenges related to high memory usage, lengthy training times, and temporal consistency. To address these issues, we introduce a novel neural video representation that combines 3D Gaussian splatting with continuous camera motion modeling. By leveraging Neural ODEs, our approach learns smooth camera trajectories while maintaining an explicit 3D scene representation through Gaussians. Additionally, we introduce a spatiotemporal hierarchical learning strategy, progressively refining spatial and temporal features to enhance reconstruction quality and accelerate convergence. This memory-efficient approach achieves high-quality rendering at impressive speeds. Experimental results show that our hierarchical learning, combined with robust camera motion modeling, captures complex dynamic scenes with strong temporal consistency, achieving state-of-the-art performance across diverse video datasets in both high- and low-motion scenarios.
<div id='section'>Paperid: <span id='pid'>602, <a href='https://arxiv.org/pdf/2501.01770.pdf' target='_blank'>https://arxiv.org/pdf/2501.01770.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiajie Liu, Mengyuan Liu, Hong Liu, Wenhao Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.01770">TCPFormer: Learning Temporal Correlation with Implicit Pose Proxy for 3D Human Pose Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent multi-frame lifting methods have dominated the 3D human pose estimation. However, previous methods ignore the intricate dependence within the 2D pose sequence and learn single temporal correlation. To alleviate this limitation, we propose TCPFormer, which leverages an implicit pose proxy as an intermediate representation. Each proxy within the implicit pose proxy can build one temporal correlation therefore helping us learn more comprehensive temporal correlation of human motion. Specifically, our method consists of three key components: Proxy Update Module (PUM), Proxy Invocation Module (PIM), and Proxy Attention Module (PAM). PUM first uses pose features to update the implicit pose proxy, enabling it to store representative information from the pose sequence. PIM then invocates and integrates the pose proxy with the pose sequence to enhance the motion semantics of each pose. Finally, PAM leverages the above mapping between the pose sequence and pose proxy to enhance the temporal correlation of the whole pose sequence. Experiments on the Human3.6M and MPI-INF-3DHP datasets demonstrate that our proposed TCPFormer outperforms the previous state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>603, <a href='https://arxiv.org/pdf/2405.02791.pdf' target='_blank'>https://arxiv.org/pdf/2405.02791.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mengxian Hu, Minghao Zhu, Xun Zhou, Qingqing Yan, Shu Li, Chengju Liu, Qijun Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.02791">Efficient Text-driven Motion Generation via Latent Consistency Training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-driven human motion generation based on diffusion strategies establishes a reliable foundation for multimodal applications in human-computer interactions. However, existing advances face significant efficiency challenges due to the substantial computational overhead of iteratively solving for nonlinear reverse diffusion trajectories during the inference phase. To this end, we propose the motion latent consistency training framework (MLCT), which precomputes reverse diffusion trajectories from raw data in the training phase and enables few-step or single-step inference via self-consistency constraints in the inference phase. Specifically, a motion autoencoder with quantization constraints is first proposed for constructing concise and bounded solution distributions for motion diffusion processes. Subsequently, a classifier-free guidance format is constructed via an additional unconditional loss function to accomplish the precomputation of conditional diffusion trajectories in the training phase. Finally, a clustering guidance module based on the K-nearest-neighbor algorithm is developed for the chain-conduction optimization mechanism of self-consistency constraints, which provides additional references of solution distributions at a small query cost. By combining these enhancements, we achieve stable and consistency training in non-pixel modality and latent representation spaces. Benchmark experiments demonstrate that our method significantly outperforms traditional consistency distillation methods with reduced training cost and enhances the consistency model to perform comparably to state-of-the-art models with lower inference costs.
<div id='section'>Paperid: <span id='pid'>604, <a href='https://arxiv.org/pdf/2403.10826.pdf' target='_blank'>https://arxiv.org/pdf/2403.10826.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hsiang-Wei Huang, Cheng-Yen Yang, Wenhao Chai, Zhongyu Jiang, Jenq-Neng Hwang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.10826">MambaMOT: State-Space Model as Motion Predictor for Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the field of multi-object tracking (MOT), traditional methods often rely on the Kalman filter for motion prediction, leveraging its strengths in linear motion scenarios. However, the inherent limitations of these methods become evident when confronted with complex, nonlinear motions and occlusions prevalent in dynamic environments like sports and dance. This paper explores the possibilities of replacing the Kalman filter with a learning-based motion model that effectively enhances tracking accuracy and adaptability beyond the constraints of Kalman filter-based tracker. In this paper, our proposed method MambaMOT and MambaMOT+, demonstrate advanced performance on challenging MOT datasets such as DanceTrack and SportsMOT, showcasing their ability to handle intricate, non-linear motion patterns and frequent occlusions more effectively than traditional methods.
<div id='section'>Paperid: <span id='pid'>605, <a href='https://arxiv.org/pdf/2511.20887.pdf' target='_blank'>https://arxiv.org/pdf/2511.20887.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rui Yan, Jiajian Fu, Shiqi Yang, Lars Paulsen, Xuxin Cheng, Xiaolong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.20887">ACE-F: A Cross Embodiment Foldable System with Force Feedback for Dexterous Teleoperation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Teleoperation systems are essential for efficiently collecting diverse and high-quality robot demonstration data, especially for complex, contact-rich tasks. However, current teleoperation platforms typically lack integrated force feedback, cross-embodiment generalization, and portable, user-friendly designs, limiting their practical deployment. To address these limitations, we introduce ACE-F, a cross embodiment foldable teleoperation system with integrated force feedback. Our approach leverages inverse kinematics (IK) combined with a carefully designed human-robot interface (HRI), enabling users to capture precise and high-quality demonstrations effortlessly. We further propose a generalized soft-controller pipeline integrating PD control and inverse dynamics to ensure robot safety and precise motion control across diverse robotic embodiments. Critically, to achieve cross-embodiment generalization of force feedback without additional sensors, we innovatively interpret end-effector positional deviations as virtual force signals, which enhance data collection and enable applications in imitation learning. Extensive teleoperation experiments confirm that ACE-F significantly simplifies the control of various robot embodiments, making dexterous manipulation tasks as intuitive as operating a computer mouse. The system is open-sourced at: https://acefoldable.github.io/
<div id='section'>Paperid: <span id='pid'>606, <a href='https://arxiv.org/pdf/2510.14827.pdf' target='_blank'>https://arxiv.org/pdf/2510.14827.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yufei Zhu, Shih-Min Yang, Andrey Rudenko, Tomasz P. Kucner, Achim J. Lilienthal, Martin Magnusson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.14827">Neural Implicit Flow Fields for Spatio-Temporal Motion Mapping</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Safe and efficient robot operation in complex human environments can benefit from good models of site-specific motion patterns. Maps of Dynamics (MoDs) provide such models by encoding statistical motion patterns in a map, but existing representations use discrete spatial sampling and typically require costly offline construction. We propose a continuous spatio-temporal MoD representation based on implicit neural functions that directly map coordinates to the parameters of a Semi-Wrapped Gaussian Mixture Model. This removes the need for discretization and imputation for unevenly sampled regions, enabling smooth generalization across both space and time. Evaluated on a large public dataset with long-term real-world people tracking data, our method achieves better accuracy of motion representation and smoother velocity distributions in sparse regions while still being computationally efficient, compared to available baselines. The proposed approach demonstrates a powerful and efficient way of modeling complex human motion patterns.
<div id='section'>Paperid: <span id='pid'>607, <a href='https://arxiv.org/pdf/2510.03776.pdf' target='_blank'>https://arxiv.org/pdf/2510.03776.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tiago Rodrigues de Almeida, Yufei Zhu, Andrey Rudenko, Tomasz P. Kucner, Johannes A. Stork, Martin Magnusson, Achim J. Lilienthal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03776">Trajectory prediction for heterogeneous agents: A performance analysis on small and imbalanced datasets</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robots and other intelligent systems navigating in complex dynamic environments should predict future actions and intentions of surrounding agents to reach their goals efficiently and avoid collisions. The dynamics of those agents strongly depends on their tasks, roles, or observable labels. Class-conditioned motion prediction is thus an appealing way to reduce forecast uncertainty and get more accurate predictions for heterogeneous agents. However, this is hardly explored in the prior art, especially for mobile robots and in limited data applications. In this paper, we analyse different class-conditioned trajectory prediction methods on two datasets. We propose a set of conditional pattern-based and efficient deep learning-based baselines, and evaluate their performance on robotics and outdoors datasets (THÖR-MAGNI and Stanford Drone Dataset). Our experiments show that all methods improve accuracy in most of the settings when considering class labels. More importantly, we observe that there are significant differences when learning from imbalanced datasets, or in new environments where sufficient data is not available. In particular, we find that deep learning methods perform better on balanced datasets, but in applications with limited data, e.g., cold start of a robot in a new environment, or imbalanced classes, pattern-based methods may be preferable.
<div id='section'>Paperid: <span id='pid'>608, <a href='https://arxiv.org/pdf/2509.13534.pdf' target='_blank'>https://arxiv.org/pdf/2509.13534.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chunxin Zheng, Kai Chen, Zhihai Bi, Yulin Li, Liang Pan, Jinni Zhou, Haoang Li, Jun Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.13534">Embracing Bulky Objects with Humanoid Robots: Whole-Body Manipulation with Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Whole-body manipulation (WBM) for humanoid robots presents a promising approach for executing embracing tasks involving bulky objects, where traditional grasping relying on end-effectors only remains limited in such scenarios due to inherent stability and payload constraints. This paper introduces a reinforcement learning framework that integrates a pre-trained human motion prior with a neural signed distance field (NSDF) representation to achieve robust whole-body embracing. Our method leverages a teacher-student architecture to distill large-scale human motion data, generating kinematically natural and physically feasible whole-body motion patterns. This facilitates coordinated control across the arms and torso, enabling stable multi-contact interactions that enhance the robustness in manipulation and also the load capacity. The embedded NSDF further provides accurate and continuous geometric perception, improving contact awareness throughout long-horizon tasks. We thoroughly evaluate the approach through comprehensive simulations and real-world experiments. The results demonstrate improved adaptability to diverse shapes and sizes of objects and also successful sim-to-real transfer. These indicate that the proposed framework offers an effective and practical solution for multi-contact and long-horizon WBM tasks of humanoid robots.
<div id='section'>Paperid: <span id='pid'>609, <a href='https://arxiv.org/pdf/2507.20170.pdf' target='_blank'>https://arxiv.org/pdf/2507.20170.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Clinton Ansun Mo, Kun Hu, Chengjiang Long, Dong Yuan, Wan-Chi Siu, Zhiyong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.20170">PUMPS: Skeleton-Agnostic Point-based Universal Motion Pre-Training for Synthesis in Human Motion Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion skeletons drive 3D character animation by transforming bone hierarchies, but differences in proportions or structure make motion data hard to transfer across skeletons, posing challenges for data-driven motion synthesis. Temporal Point Clouds (TPCs) offer an unstructured, cross-compatible motion representation. Though reversible with skeletons, TPCs mainly serve for compatibility, not for direct motion task learning. Doing so would require data synthesis capabilities for the TPC format, which presents unexplored challenges regarding its unique temporal consistency and point identifiability. Therefore, we propose PUMPS, the primordial autoencoder architecture for TPC data. PUMPS independently reduces frame-wise point clouds into sampleable feature vectors, from which a decoder extracts distinct temporal points using latent Gaussian noise vectors as sampling identifiers. We introduce linear assignment-based point pairing to optimise the TPC reconstruction process, and negate the use of expensive point-wise attention mechanisms in the architecture. Using these latent features, we pre-train a motion synthesis model capable of performing motion prediction, transition generation, and keyframe interpolation. For these pre-training tasks, PUMPS performs remarkably well even without native dataset supervision, matching state-of-the-art performance. When fine-tuned for motion denoising or estimation, PUMPS outperforms many respective methods without deviating from its generalist architecture.
<div id='section'>Paperid: <span id='pid'>610, <a href='https://arxiv.org/pdf/2505.11832.pdf' target='_blank'>https://arxiv.org/pdf/2505.11832.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxiang Lai, Jike Zhong, Vanessa Su, Xiaofeng Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.11832">Patient-Specific Autoregressive Models for Organ Motion Prediction in Radiotherapy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Radiotherapy often involves a prolonged treatment period. During this time, patients may experience organ motion due to breathing and other physiological factors. Predicting and modeling this motion before treatment is crucial for ensuring precise radiation delivery. However, existing pre-treatment organ motion prediction methods primarily rely on deformation analysis using principal component analysis (PCA), which is highly dependent on registration quality and struggles to capture periodic temporal dynamics for motion modeling.In this paper, we observe that organ motion prediction closely resembles an autoregressive process, a technique widely used in natural language processing (NLP). Autoregressive models predict the next token based on previous inputs, naturally aligning with our objective of predicting future organ motion phases. Building on this insight, we reformulate organ motion prediction as an autoregressive process to better capture patient-specific motion patterns. Specifically, we acquire 4D CT scans for each patient before treatment, with each sequence comprising multiple 3D CT phases. These phases are fed into the autoregressive model to predict future phases based on prior phase motion patterns. We evaluate our method on a real-world test set of 4D CT scans from 50 patients who underwent radiotherapy at our institution and a public dataset containing 4D CT scans from 20 patients (some with multiple scans), totaling over 1,300 3D CT phases. The performance in predicting the motion of the lung and heart surpasses existing benchmarks, demonstrating its effectiveness in capturing motion dynamics from CT images. These results highlight the potential of our method to improve pre-treatment planning in radiotherapy, enabling more precise and adaptive radiation delivery.
<div id='section'>Paperid: <span id='pid'>611, <a href='https://arxiv.org/pdf/2505.03738.pdf' target='_blank'>https://arxiv.org/pdf/2505.03738.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jialong Li, Xuxin Cheng, Tianshu Huang, Shiqi Yang, Ri-Zhao Qiu, Xiaolong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.03738">AMO: Adaptive Motion Optimization for Hyper-Dexterous Humanoid Whole-Body Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humanoid robots derive much of their dexterity from hyper-dexterous whole-body movements, enabling tasks that require a large operational workspace: such as picking objects off the ground. However, achieving these capabilities on real humanoids remains challenging due to their high degrees of freedom (DoF) and nonlinear dynamics. We propose Adaptive Motion Optimization (AMO), a framework that integrates sim-to-real reinforcement learning (RL) with trajectory optimization for real-time, adaptive whole-body control. To mitigate distribution bias in motion imitation RL, we construct a hybrid AMO dataset and train a network capable of robust, on-demand adaptation to potentially O.O.D. commands. We validate AMO in simulation and on a 29-DoF Unitree G1 humanoid robot, demonstrating superior stability and an expanded workspace compared to strong baselines. Finally, we show that AMO's consistent performance supports autonomous task execution via imitation learning, underscoring the system's versatility and robustness.
<div id='section'>Paperid: <span id='pid'>612, <a href='https://arxiv.org/pdf/2502.01143.pdf' target='_blank'>https://arxiv.org/pdf/2502.01143.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tairan He, Jiawei Gao, Wenli Xiao, Yuanhang Zhang, Zi Wang, Jiashun Wang, Zhengyi Luo, Guanqi He, Nikhil Sobanbab, Chaoyi Pan, Zeji Yi, Guannan Qu, Kris Kitani, Jessica Hodgins, Linxi "Jim" Fan, Yuke Zhu, Changliu Liu, Guanya Shi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.01143">ASAP: Aligning Simulation and Real-World Physics for Learning Agile Humanoid Whole-Body Skills</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humanoid robots hold the potential for unparalleled versatility in performing human-like, whole-body skills. However, achieving agile and coordinated whole-body motions remains a significant challenge due to the dynamics mismatch between simulation and the real world. Existing approaches, such as system identification (SysID) and domain randomization (DR) methods, often rely on labor-intensive parameter tuning or result in overly conservative policies that sacrifice agility. In this paper, we present ASAP (Aligning Simulation and Real-World Physics), a two-stage framework designed to tackle the dynamics mismatch and enable agile humanoid whole-body skills. In the first stage, we pre-train motion tracking policies in simulation using retargeted human motion data. In the second stage, we deploy the policies in the real world and collect real-world data to train a delta (residual) action model that compensates for the dynamics mismatch. Then, ASAP fine-tunes pre-trained policies with the delta action model integrated into the simulator to align effectively with real-world dynamics. We evaluate ASAP across three transfer scenarios: IsaacGym to IsaacSim, IsaacGym to Genesis, and IsaacGym to the real-world Unitree G1 humanoid robot. Our approach significantly improves agility and whole-body coordination across various dynamic motions, reducing tracking error compared to SysID, DR, and delta dynamics learning baselines. ASAP enables highly agile motions that were previously difficult to achieve, demonstrating the potential of delta action learning in bridging simulation and real-world dynamics. These results suggest a promising sim-to-real direction for developing more expressive and agile humanoids.
<div id='section'>Paperid: <span id='pid'>613, <a href='https://arxiv.org/pdf/2412.13196.pdf' target='_blank'>https://arxiv.org/pdf/2412.13196.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mazeyu Ji, Xuanbin Peng, Fangchen Liu, Jialong Li, Ge Yang, Xuxin Cheng, Xiaolong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.13196">ExBody2: Advanced Expressive Humanoid Whole-Body Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper tackles the challenge of enabling real-world humanoid robots to perform expressive and dynamic whole-body motions while maintaining overall stability and robustness. We propose Advanced Expressive Whole-Body Control (Exbody2), a method for producing whole-body tracking controllers that are trained on both human motion capture and simulated data and then transferred to the real world. We introduce a technique for decoupling the velocity tracking of the entire body from tracking body landmarks. We use a teacher policy to produce intermediate data that better conforms to the robot's kinematics and to automatically filter away infeasible whole-body motions. This two-step approach enabled us to produce a student policy that can be deployed on the robot that can walk, crouch, and dance. We also provide insight into the trade-off between versatility and the tracking performance on specific motions. We observed significant improvement of tracking performance after fine-tuning on a small amount of data, at the expense of the others.
<div id='section'>Paperid: <span id='pid'>614, <a href='https://arxiv.org/pdf/2412.08948.pdf' target='_blank'>https://arxiv.org/pdf/2412.08948.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuehai He, Shuohang Wang, Jianwei Yang, Xiaoxia Wu, Yiping Wang, Kuan Wang, Zheng Zhan, Olatunji Ruwase, Yelong Shen, Xin Eric Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.08948">Mojito: Motion Trajectory and Intensity Control for Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in diffusion models have shown great promise in producing high-quality video content. However, efficiently training video diffusion models capable of integrating directional guidance and controllable motion intensity remains a challenging and under-explored area. To tackle these challenges, this paper introduces Mojito, a diffusion model that incorporates both motion trajectory and intensity control for text-to-video generation. Specifically, Mojito features a Directional Motion Control (DMC) module that leverages cross-attention to efficiently direct the generated object's motion without training, alongside a Motion Intensity Modulator (MIM) that uses optical flow maps generated from videos to guide varying levels of motion intensity. Extensive experiments demonstrate Mojito's effectiveness in achieving precise trajectory and intensity control with high computational efficiency, generating motion patterns that closely match specified directions and intensities, providing realistic dynamics that align well with natural motion in real-world scenarios.
<div id='section'>Paperid: <span id='pid'>615, <a href='https://arxiv.org/pdf/2412.07773.pdf' target='_blank'>https://arxiv.org/pdf/2412.07773.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenhao Lu, Xuxin Cheng, Jialong Li, Shiqi Yang, Mazeyu Ji, Chengjing Yuan, Ge Yang, Sha Yi, Xiaolong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.07773">Mobile-TeleVision: Predictive Motion Priors for Humanoid Whole-Body Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humanoid robots require both robust lower-body locomotion and precise upper-body manipulation. While recent Reinforcement Learning (RL) approaches provide whole-body loco-manipulation policies, they lack precise manipulation with high DoF arms. In this paper, we propose decoupling upper-body control from locomotion, using inverse kinematics (IK) and motion retargeting for precise manipulation, while RL focuses on robust lower-body locomotion. We introduce PMP (Predictive Motion Priors), trained with Conditional Variational Autoencoder (CVAE) to effectively represent upper-body motions. The locomotion policy is trained conditioned on this upper-body motion representation, ensuring that the system remains robust with both manipulation and locomotion. We show that CVAE features are crucial for stability and robustness, and significantly outperforms RL-based whole-body control in precise manipulation. With precise upper-body motion and robust lower-body locomotion control, operators can remotely control the humanoid to walk around and explore different environments, while performing diverse manipulation tasks.
<div id='section'>Paperid: <span id='pid'>616, <a href='https://arxiv.org/pdf/2411.11913.pdf' target='_blank'>https://arxiv.org/pdf/2411.11913.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Can Cui, Zichong Yang, Yupeng Zhou, Juntong Peng, Sung-Yeon Park, Cong Zhang, Yunsheng Ma, Xu Cao, Wenqian Ye, Yiheng Feng, Jitesh Panchal, Lingxi Li, Yaobin Chen, Ziran Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.11913">On-Board Vision-Language Models for Personalized Autonomous Vehicle Motion Control: System Design and Real-World Validation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Personalized driving refers to an autonomous vehicle's ability to adapt its driving behavior or control strategies to match individual users' preferences and driving styles while maintaining safety and comfort standards. However, existing works either fail to capture every individual preference precisely or become computationally inefficient as the user base expands. Vision-Language Models (VLMs) offer promising solutions to this front through their natural language understanding and scene reasoning capabilities. In this work, we propose a lightweight yet effective on-board VLM framework that provides low-latency personalized driving performance while maintaining strong reasoning capabilities. Our solution incorporates a Retrieval-Augmented Generation (RAG)-based memory module that enables continuous learning of individual driving preferences through human feedback. Through comprehensive real-world vehicle deployment and experiments, our system has demonstrated the ability to provide safe, comfortable, and personalized driving experiences across various scenarios and significantly reduce takeover rates by up to 76.9%. To the best of our knowledge, this work represents the first end-to-end VLM-based motion control system in real-world autonomous vehicles.
<div id='section'>Paperid: <span id='pid'>617, <a href='https://arxiv.org/pdf/2410.12237.pdf' target='_blank'>https://arxiv.org/pdf/2410.12237.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yufei Zhu, Andrey Rudenko, Luigi Palmieri, Lukas Heuer, Achim J. Lilienthal, Martin Magnusson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.12237">Fast Online Learning of CLiFF-maps in Changing Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Maps of dynamics are effective representations of motion patterns learned from prior observations, with recent research demonstrating their ability to enhance various downstream tasks such as human-aware robot navigation, long-term human motion prediction, and robot localization. Current advancements have primarily concentrated on methods for learning maps of human flow in environments where the flow is static, i.e., not assumed to change over time. In this paper we propose an online update method of the CLiFF-map (an advanced map of dynamics type that models motion patterns as velocity and orientation mixtures) to actively detect and adapt to human flow changes. As new observations are collected, our goal is to update a CLiFF-map to effectively and accurately integrate them, while retaining relevant historic motion patterns. The proposed online update method maintains a probabilistic representation in each observed location, updating parameters by continuously tracking sufficient statistics. In experiments using both synthetic and real-world datasets, we show that our method is able to maintain accurate representations of human motion dynamics, contributing to high performance flow-compliant planning downstream tasks, while being orders of magnitude faster than the comparable baselines.
<div id='section'>Paperid: <span id='pid'>618, <a href='https://arxiv.org/pdf/2408.16659.pdf' target='_blank'>https://arxiv.org/pdf/2408.16659.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xingjian Han, Yu Jiang, Weiming Wang, Guoxin Fang, Simeon Gill, Zhiqiang Zhang, Shengfa Wang, Jun Saito, Deepak Kumar, Zhongxuan Luo, Emily Whiting, Charlie C. L. Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.16659">Motion-Driven Neural Optimizer for Prophylactic Braces Made by Distributed Microstructures</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Joint injuries, and their long-term consequences, present a substantial global health burden. Wearable prophylactic braces are an attractive potential solution to reduce the incidence of joint injuries by limiting joint movements that are related to injury risk. Given human motion and ground reaction forces, we present a computational framework that enables the design of personalized braces by optimizing the distribution of microstructures and elasticity. As varied brace designs yield different reaction forces that influence kinematics and kinetics analysis outcomes, the optimization process is formulated as a differentiable end-to-end pipeline in which the design domain of microstructure distribution is parameterized onto a neural network. The optimized distribution of microstructures is obtained via a self-learning process to determine the network coefficients according to a carefully designed set of losses and the integrated biomechanical and physical analyses. Since knees and ankles are the most commonly injured joints, we demonstrate the effectiveness of our pipeline by designing, fabricating, and testing prophylactic braces for the knee and ankle to prevent potentially harmful joint movements.
<div id='section'>Paperid: <span id='pid'>619, <a href='https://arxiv.org/pdf/2407.09475.pdf' target='_blank'>https://arxiv.org/pdf/2407.09475.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinning Li, Jiachen Li, Sangjae Bae, David Isele
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.09475">Adaptive Prediction Ensemble: Improving Out-of-Distribution Generalization of Motion Forecasting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning-based trajectory prediction models for autonomous driving often struggle with generalization to out-of-distribution (OOD) scenarios, sometimes performing worse than simple rule-based models. To address this limitation, we propose a novel framework, Adaptive Prediction Ensemble (APE), which integrates deep learning and rule-based prediction experts. A learned routing function, trained concurrently with the deep learning model, dynamically selects the most reliable prediction based on the input scenario. Our experiments on large-scale datasets, including Waymo Open Motion Dataset (WOMD) and Argoverse, demonstrate improvement in zero-shot generalization across datasets. We show that our method outperforms individual prediction models and other variants, particularly in long-horizon prediction and scenarios with a high proportion of OOD data. This work highlights the potential of hybrid approaches for robust and generalizable motion prediction in autonomous driving. More details can be found on the project page: https://sites.google.com/view/ape-generalization.
<div id='section'>Paperid: <span id='pid'>620, <a href='https://arxiv.org/pdf/2407.02104.pdf' target='_blank'>https://arxiv.org/pdf/2407.02104.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nicola Messina, Jan Sedmidubsky, Fabrizio Falchi, TomÃ¡Å¡ Rebok
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.02104">Joint-Dataset Learning and Cross-Consistent Regularization for Text-to-Motion Retrieval</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pose-estimation methods enable extracting human motion from common videos in the structured form of 3D skeleton sequences. Despite great application opportunities, effective content-based access to such spatio-temporal motion data is a challenging problem. In this paper, we focus on the recently introduced text-motion retrieval tasks, which aim to search for database motions that are the most relevant to a specified natural-language textual description (text-to-motion) and vice-versa (motion-to-text). Despite recent efforts to explore these promising avenues, a primary challenge remains the insufficient data available to train robust text-motion models effectively. To address this issue, we propose to investigate joint-dataset learning - where we train on multiple text-motion datasets simultaneously - together with the introduction of a Cross-Consistent Contrastive Loss function (CCCL), which regularizes the learned text-motion common space by imposing uni-modal constraints that augment the representation ability of the trained network. To learn a proper motion representation, we also introduce a transformer-based motion encoder, called MoT++, which employs spatio-temporal attention to process sequences of skeleton data. We demonstrate the benefits of the proposed approaches on the widely-used KIT Motion-Language and HumanML3D datasets. We perform detailed experimentation on joint-dataset learning and cross-dataset scenarios, showing the effectiveness of each introduced module in a carefully conducted ablation study and, in turn, pointing out the limitations of state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>621, <a href='https://arxiv.org/pdf/2406.17256.pdf' target='_blank'>https://arxiv.org/pdf/2406.17256.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jaihyun Lew, Jooyoung Choi, Chaehun Shin, Dahuin Jung, Sungroh Yoon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.17256">Disentangled Motion Modeling for Video Frame Interpolation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video Frame Interpolation (VFI) aims to synthesize intermediate frames between existing frames to enhance visual smoothness and quality. Beyond the conventional methods based on the reconstruction loss, recent works have employed generative models for improved perceptual quality. However, they require complex training and large computational costs for pixel space modeling. In this paper, we introduce disentangled Motion Modeling (MoMo), a diffusion-based approach for VFI that enhances visual quality by focusing on intermediate motion modeling. We propose a disentangled two-stage training process. In the initial stage, frame synthesis and flow models are trained to generate accurate frames and flows optimal for synthesis. In the subsequent stage, we introduce a motion diffusion model, which incorporates our novel U-Net architecture specifically designed for optical flow, to generate bi-directional flows between frames. By learning the simpler low-frequency representation of motions, MoMo achieves superior perceptual quality with reduced computational demands compared to the generative modeling methods on the pixel space. MoMo surpasses state-of-the-art methods in perceptual metrics across various benchmarks, demonstrating its efficacy and efficiency in VFI.
<div id='section'>Paperid: <span id='pid'>622, <a href='https://arxiv.org/pdf/2406.08858.pdf' target='_blank'>https://arxiv.org/pdf/2406.08858.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tairan He, Zhengyi Luo, Xialin He, Wenli Xiao, Chong Zhang, Weinan Zhang, Kris Kitani, Changliu Liu, Guanya Shi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.08858">OmniH2O: Universal and Dexterous Human-to-Humanoid Whole-Body Teleoperation and Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present OmniH2O (Omni Human-to-Humanoid), a learning-based system for whole-body humanoid teleoperation and autonomy. Using kinematic pose as a universal control interface, OmniH2O enables various ways for a human to control a full-sized humanoid with dexterous hands, including using real-time teleoperation through VR headset, verbal instruction, and RGB camera. OmniH2O also enables full autonomy by learning from teleoperated demonstrations or integrating with frontier models such as GPT-4. OmniH2O demonstrates versatility and dexterity in various real-world whole-body tasks through teleoperation or autonomy, such as playing multiple sports, moving and manipulating objects, and interacting with humans. We develop an RL-based sim-to-real pipeline, which involves large-scale retargeting and augmentation of human motion datasets, learning a real-world deployable policy with sparse sensor input by imitating a privileged teacher policy, and reward designs to enhance robustness and stability. We release the first humanoid whole-body control dataset, OmniH2O-6, containing six everyday tasks, and demonstrate humanoid whole-body skill learning from teleoperated datasets.
<div id='section'>Paperid: <span id='pid'>623, <a href='https://arxiv.org/pdf/2406.06300.pdf' target='_blank'>https://arxiv.org/pdf/2406.06300.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tim Schreiter, Andrey Rudenko, Martin Magnusson, Achim J. Lilienthal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.06300">Human Gaze and Head Rotation during Navigation, Exploration and Object Manipulation in Shared Environments with Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The human gaze is an important cue to signal intention, attention, distraction, and the regions of interest in the immediate surroundings. Gaze tracking can transform how robots perceive, understand, and react to people, enabling new modes of robot control, interaction, and collaboration. In this paper, we use gaze tracking data from a rich dataset of human motion (THÃR-MAGNI) to investigate the coordination between gaze direction and head rotation of humans engaged in various indoor activities involving navigation, interaction with objects, and collaboration with a mobile robot. In particular, we study the spread and central bias of fixations in diverse activities and examine the correlation between gaze direction and head rotation. We introduce various human motion metrics to enhance the understanding of gaze behavior in dynamic interactions. Finally, we apply semantic object labeling to decompose the gaze distribution into activity-relevant regions.
<div id='section'>Paperid: <span id='pid'>624, <a href='https://arxiv.org/pdf/2405.17817.pdf' target='_blank'>https://arxiv.org/pdf/2405.17817.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vida Adeli, Soroush Mehraban, Irene Ballester, Yasamin Zarghami, Andrea Sabo, Andrea Iaboni, Babak Taati
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.17817">Benchmarking Skeleton-based Motion Encoder Models for Clinical Applications: Estimating Parkinson's Disease Severity in Walking Sequences</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study investigates the application of general human motion encoders trained on large-scale human motion datasets for analyzing gait patterns in PD patients. Although these models have learned a wealth of human biomechanical knowledge, their effectiveness in analyzing pathological movements, such as parkinsonian gait, has yet to be fully validated. We propose a comparative framework and evaluate six pre-trained state-of-the-art human motion encoder models on their ability to predict the Movement Disorder Society - Unified Parkinson's Disease Rating Scale (MDS-UPDRS-III) gait scores from motion capture data. We compare these against a traditional gait feature-based predictive model in a recently released large public PD dataset, including PD patients on and off medication. The feature-based model currently shows higher weighted average accuracy, precision, recall, and F1-score. Motion encoder models with closely comparable results demonstrate promise for scalability and efficiency in clinical settings. This potential is underscored by the enhanced performance of the encoder model upon fine-tuning on PD training set. Four of the six human motion models examined provided prediction scores that were significantly different between on- and off-medication states. This finding reveals the sensitivity of motion encoder models to nuanced clinical changes. It also underscores the necessity for continued customization of these models to better capture disease-specific features, thereby reducing the reliance on labor-intensive feature engineering. Lastly, we establish a benchmark for the analysis of skeleton-based motion encoder models in clinical settings. To the best of our knowledge, this is the first study to provide a benchmark that enables state-of-the-art models to be tested and compete in a clinical context. Codes and benchmark leaderboard are available at code.
<div id='section'>Paperid: <span id='pid'>625, <a href='https://arxiv.org/pdf/2405.17405.pdf' target='_blank'>https://arxiv.org/pdf/2405.17405.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruizhi Shao, Youxin Pang, Zerong Zheng, Jingxiang Sun, Yebin Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.17405">Human4DiT: 360-degree Human Video Generation with 4D Diffusion Transformer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a novel approach for generating 360-degree high-quality, spatio-temporally coherent human videos from a single image. Our framework combines the strengths of diffusion transformers for capturing global correlations across viewpoints and time, and CNNs for accurate condition injection. The core is a hierarchical 4D transformer architecture that factorizes self-attention across views, time steps, and spatial dimensions, enabling efficient modeling of the 4D space. Precise conditioning is achieved by injecting human identity, camera parameters, and temporal signals into the respective transformers. To train this model, we collect a multi-dimensional dataset spanning images, videos, multi-view data, and limited 4D footage, along with a tailored multi-dimensional training strategy. Our approach overcomes the limitations of previous methods based on generative adversarial networks or vanilla diffusion models, which struggle with complex motions, viewpoint changes, and generalization. Through extensive experiments, we demonstrate our method's ability to synthesize 360-degree realistic, coherent human motion videos, paving the way for advanced multimedia applications in areas such as virtual reality and animation.
<div id='section'>Paperid: <span id='pid'>626, <a href='https://arxiv.org/pdf/2403.13640.pdf' target='_blank'>https://arxiv.org/pdf/2403.13640.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yufei Zhu, Han Fan, Andrey Rudenko, Martin Magnusson, Erik Schaffernicht, Achim J. Lilienthal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.13640">LaCE-LHMP: Airflow Modelling-Inspired Long-Term Human Motion Prediction By Enhancing Laminar Characteristics in Human Flow</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Long-term human motion prediction (LHMP) is essential for safely operating autonomous robots and vehicles in populated environments. It is fundamental for various applications, including motion planning, tracking, human-robot interaction and safety monitoring. However, accurate prediction of human trajectories is challenging due to complex factors, including, for example, social norms and environmental conditions. The influence of such factors can be captured through Maps of Dynamics (MoDs), which encode spatial motion patterns learned from (possibly scattered and partial) past observations of motion in the environment and which can be used for data-efficient, interpretable motion prediction (MoD-LHMP). To address the limitations of prior work, especially regarding accuracy and sensitivity to anomalies in long-term prediction, we propose the Laminar Component Enhanced LHMP approach (LaCE-LHMP). Our approach is inspired by data-driven airflow modelling, which estimates laminar and turbulent flow components and uses predominantly the laminar components to make flow predictions. Based on the hypothesis that human trajectory patterns also manifest laminar flow (that represents predictable motion) and turbulent flow components (that reflect more unpredictable and arbitrary motion), LaCE-LHMP extracts the laminar patterns in human dynamics and uses them for human motion prediction. We demonstrate the superior prediction performance of LaCE-LHMP through benchmark comparisons with state-of-the-art LHMP methods, offering an unconventional perspective and a more intuitive understanding of human movement patterns.
<div id='section'>Paperid: <span id='pid'>627, <a href='https://arxiv.org/pdf/2403.09285.pdf' target='_blank'>https://arxiv.org/pdf/2403.09285.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tim Schreiter, Tiago Rodrigues de Almeida, Yufei Zhu, Eduardo Gutierrez Maestro, Lucas Morillo-Mendez, Andrey Rudenko, Luigi Palmieri, Tomasz P. Kucner, Martin Magnusson, Achim J. Lilienthal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.09285">THÃR-MAGNI: A Large-scale Indoor Motion Capture Recording of Human Movement and Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a new large dataset of indoor human and robot navigation and interaction, called THÃR-MAGNI, that is designed to facilitate research on social navigation: e.g., modelling and predicting human motion, analyzing goal-oriented interactions between humans and robots, and investigating visual attention in a social interaction context. THÃR-MAGNI was created to fill a gap in available datasets for human motion analysis and HRI. This gap is characterized by a lack of comprehensive inclusion of exogenous factors and essential target agent cues, which hinders the development of robust models capable of capturing the relationship between contextual cues and human behavior in different scenarios. Unlike existing datasets, THÃR-MAGNI includes a broader set of contextual features and offers multiple scenario variations to facilitate factor isolation. The dataset includes many social human-human and human-robot interaction scenarios, rich context annotations, and multi-modal data, such as walking trajectories, gaze tracking data, and lidar and camera streams recorded from a mobile robot. We also provide a set of tools for visualization and processing of the recorded data. THÃR-MAGNI is, to the best of our knowledge, unique in the amount and diversity of sensor data collected in a contextualized and socially dynamic environment, capturing natural human-robot interactions.
<div id='section'>Paperid: <span id='pid'>628, <a href='https://arxiv.org/pdf/2402.16796.pdf' target='_blank'>https://arxiv.org/pdf/2402.16796.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuxin Cheng, Yandong Ji, Junming Chen, Ruihan Yang, Ge Yang, Xiaolong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.16796">Expressive Whole-Body Control for Humanoid Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Can we enable humanoid robots to generate rich, diverse, and expressive motions in the real world? We propose to learn a whole-body control policy on a human-sized robot to mimic human motions as realistic as possible. To train such a policy, we leverage the large-scale human motion capture data from the graphics community in a Reinforcement Learning framework. However, directly performing imitation learning with the motion capture dataset would not work on the real humanoid robot, given the large gap in degrees of freedom and physical capabilities. Our method Expressive Whole-Body Control (Exbody) tackles this problem by encouraging the upper humanoid body to imitate a reference motion, while relaxing the imitation constraint on its two legs and only requiring them to follow a given velocity robustly. With training in simulation and Sim2Real transfer, our policy can control a humanoid robot to walk in different styles, shake hands with humans, and even dance with a human in the real world. We conduct extensive studies and comparisons on diverse motions in both simulation and the real world to show the effectiveness of our approach.
<div id='section'>Paperid: <span id='pid'>629, <a href='https://arxiv.org/pdf/2510.17525.pdf' target='_blank'>https://arxiv.org/pdf/2510.17525.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Simon Schaefer, Helen Oleynikova, Sandra Hirche, Stefan Leutenegger
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.17525">HumanMPC - Safe and Efficient MAV Navigation among Humans</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Safe and efficient robotic navigation among humans is essential for integrating robots into everyday environments. Most existing approaches focus on simplified 2D crowd navigation and fail to account for the full complexity of human body dynamics beyond root motion. We present HumanMPC, a Model Predictive Control (MPC) framework for 3D Micro Air Vehicle (MAV) navigation among humans that combines theoretical safety guarantees with data-driven models for realistic human motion forecasting. Our approach introduces a novel twist to reachability-based safety formulation that constrains only the initial control input for safety while modeling its effects over the entire planning horizon, enabling safe yet efficient navigation. We validate HumanMPC in both simulated experiments using real human trajectories and in the real-world, demonstrating its effectiveness across tasks ranging from goal-directed navigation to visual servoing for human tracking. While we apply our method to MAVs in this work, it is generic and can be adapted by other platforms. Our results show that the method ensures safety without excessive conservatism and outperforms baseline approaches in both efficiency and reliability.
<div id='section'>Paperid: <span id='pid'>630, <a href='https://arxiv.org/pdf/2509.09283.pdf' target='_blank'>https://arxiv.org/pdf/2509.09283.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yueqi Zhang, Quancheng Qian, Taixian Hou, Peng Zhai, Xiaoyi Wei, Kangmai Hu, Jiafu Yi, Lihua Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09283">RENet: Fault-Tolerant Motion Control for Quadruped Robots via Redundant Estimator Networks under Visual Collapse</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-based locomotion in outdoor environments presents significant challenges for quadruped robots. Accurate environmental prediction and effective handling of depth sensor noise during real-world deployment remain difficult, severely restricting the outdoor applications of such algorithms. To address these deployment challenges in vision-based motion control, this letter proposes the Redundant Estimator Network (RENet) framework. The framework employs a dual-estimator architecture that ensures robust motion performance while maintaining deployment stability during onboard vision failures. Through an online estimator adaptation, our method enables seamless transitions between estimation modules when handling visual perception uncertainties. Experimental validation on a real-world robot demonstrates the framework's effectiveness in complex outdoor environments, showing particular advantages in scenarios with degraded visual perception. This framework demonstrates its potential as a practical solution for reliable robotic deployment in challenging field conditions. Project website: https://RENet-Loco.github.io/
<div id='section'>Paperid: <span id='pid'>631, <a href='https://arxiv.org/pdf/2505.20857.pdf' target='_blank'>https://arxiv.org/pdf/2505.20857.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhefeng Cao, Ben Liu, Sen Li, Wei Zhang, Hua Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.20857">G-DReaM: Graph-conditioned Diffusion Retargeting across Multiple Embodiments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion retargeting for specific robot from existing motion datasets is one critical step in transferring motion patterns from human behaviors to and across various robots. However, inconsistencies in topological structure, geometrical parameters as well as joint correspondence make it difficult to handle diverse embodiments with a unified retargeting architecture. In this work, we propose a novel unified graph-conditioned diffusion-based motion generation framework for retargeting reference motions across diverse embodiments. The intrinsic characteristics of heterogeneous embodiments are represented with graph structure that effectively captures topological and geometrical features of different robots. Such a graph-based encoding further allows for knowledge exploitation at the joint level with a customized attention mechanisms developed in this work. For lacking ground truth motions of the desired embodiment, we utilize an energy-based guidance formulated as retargeting losses to train the diffusion model. As one of the first cross-embodiment motion retargeting methods in robotics, our experiments validate that the proposed model can retarget motions across heterogeneous embodiments in a unified manner. Moreover, it demonstrates a certain degree of generalization to both diverse skeletal structures and similar motion patterns.
<div id='section'>Paperid: <span id='pid'>632, <a href='https://arxiv.org/pdf/2505.05851.pdf' target='_blank'>https://arxiv.org/pdf/2505.05851.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Janik Kaden, Maximilian Hilger, Tim Schreiter, Marius Schaab, Thomas Graichen, Andrey Rudenko, Ulrich Heinkel, Achim J. Lilienthal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.05851">Collecting Human Motion Data in Large and Occlusion-Prone Environments using Ultra-Wideband Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With robots increasingly integrating into human environments, understanding and predicting human motion is essential for safe and efficient interactions. Modern human motion and activity prediction approaches require high quality and quantity of data for training and evaluation, usually collected from motion capture systems, onboard or stationary sensors. Setting up these systems is challenging due to the intricate setup of hardware components, extensive calibration procedures, occlusions, and substantial costs. These constraints make deploying such systems in new and large environments difficult and limit their usability for in-the-wild measurements. In this paper we investigate the possibility to apply the novel Ultra-Wideband (UWB) localization technology as a scalable alternative for human motion capture in crowded and occlusion-prone environments. We include additional sensing modalities such as eye-tracking, onboard robot LiDAR and radar sensors, and record motion capture data as ground truth for evaluation and comparison. The environment imitates a museum setup, with up to four active participants navigating toward random goals in a natural way, and offers more than 130 minutes of multi-modal data. Our investigation provides a step toward scalable and accurate motion data collection beyond vision-based systems, laying a foundation for evaluating sensing modalities like UWB in larger and complex environments like warehouses, airports, or convention centers.
<div id='section'>Paperid: <span id='pid'>633, <a href='https://arxiv.org/pdf/2411.18654.pdf' target='_blank'>https://arxiv.org/pdf/2411.18654.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haonan Han, Xiangzuo Wu, Huan Liao, Zunnan Xu, Zhongyuan Hu, Ronghui Li, Yachao Zhang, Xiu Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.18654">AToM: Aligning Text-to-Motion Model at Event-Level with GPT-4Vision Reward</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, text-to-motion models have opened new possibilities for creating realistic human motion with greater efficiency and flexibility. However, aligning motion generation with event-level textual descriptions presents unique challenges due to the complex relationship between textual prompts and desired motion outcomes. To address this, we introduce AToM, a framework that enhances the alignment between generated motion and text prompts by leveraging reward from GPT-4Vision. AToM comprises three main stages: Firstly, we construct a dataset MotionPrefer that pairs three types of event-level textual prompts with generated motions, which cover the integrity, temporal relationship and frequency of motion. Secondly, we design a paradigm that utilizes GPT-4Vision for detailed motion annotation, including visual data formatting, task-specific instructions and scoring rules for each sub-task. Finally, we fine-tune an existing text-to-motion model using reinforcement learning guided by this paradigm. Experimental results demonstrate that AToM significantly improves the event-level alignment quality of text-to-motion generation.
<div id='section'>Paperid: <span id='pid'>634, <a href='https://arxiv.org/pdf/2512.18181.pdf' target='_blank'>https://arxiv.org/pdf/2512.18181.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaixing Yang, Jiashu Zhu, Xulong Tang, Ziqiao Peng, Xiangyue Zhang, Puwei Wang, Jiahong Wu, Xiangxiang Chu, Hongyan Liu, Jun He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.18181">MACE-Dance: Motion-Appearance Cascaded Experts for Music-Driven Dance Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rise of online dance-video platforms and rapid advances in AI-generated content (AIGC), music-driven dance generation has emerged as a compelling research direction. Despite substantial progress in related domains such as music-driven 3D dance generation, pose-driven image animation, and audio-driven talking-head synthesis, existing methods cannot be directly adapted to this task. Moreover, the limited studies in this area still struggle to jointly achieve high-quality visual appearance and realistic human motion. Accordingly, we present MACE-Dance, a music-driven dance video generation framework with cascaded Mixture-of-Experts (MoE). The Motion Expert performs music-to-3D motion generation while enforcing kinematic plausibility and artistic expressiveness, whereas the Appearance Expert carries out motion- and reference-conditioned video synthesis, preserving visual identity with spatiotemporal coherence. Specifically, the Motion Expert adopts a diffusion model with a BiMamba-Transformer hybrid architecture and a Guidance-Free Training (GFT) strategy, achieving state-of-the-art (SOTA) performance in 3D dance generation. The Appearance Expert employs a decoupled kinematic-aesthetic fine-tuning strategy, achieving state-of-the-art (SOTA) performance in pose-driven image animation. To better benchmark this task, we curate a large-scale and diverse dataset and design a motion-appearance evaluation protocol. Based on this protocol, MACE-Dance also achieves state-of-the-art performance. Project page: https://macedance.github.io/
<div id='section'>Paperid: <span id='pid'>635, <a href='https://arxiv.org/pdf/2511.21029.pdf' target='_blank'>https://arxiv.org/pdf/2511.21029.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaixing Yang, Xulong Tang, Ziqiao Peng, Xiangyue Zhang, Puwei Wang, Jun He, Hongyan Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.21029">FlowerDance: MeanFlow for Efficient and Refined 3D Dance Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Music-to-dance generation aims to translate auditory signals into expressive human motion, with broad applications in virtual reality, choreography, and digital entertainment. Despite promising progress, the limited generation efficiency of existing methods leaves insufficient computational headroom for high-fidelity 3D rendering, thereby constraining the expressiveness of 3D characters during real-world applications. Thus, we propose FlowerDance, which not only generates refined motion with physical plausibility and artistic expressiveness, but also achieves significant generation efficiency on inference speed and memory utilization . Specifically, FlowerDance combines MeanFlow with Physical Consistency Constraints, which enables high-quality motion generation with only a few sampling steps. Moreover, FlowerDance leverages a simple but efficient model architecture with BiMamba-based backbone and Channel-Level Cross-Modal Fusion, which generates dance with efficient non-autoregressive manner. Meanwhile, FlowerDance supports motion editing, enabling users to interactively refine dance sequences. Extensive experiments on AIST++ and FineDance show that FlowerDance achieves state-of-the-art results in both motion quality and generation efficiency. Code will be released upon acceptance.
<div id='section'>Paperid: <span id='pid'>636, <a href='https://arxiv.org/pdf/2504.12667.pdf' target='_blank'>https://arxiv.org/pdf/2504.12667.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lin Liu, Caiyan Jia, Ziying Song, Hongyu Pan, Bencheng Liao, Wenchao Sun, Yongchang Zhang, Lei Yang, Yandan Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.12667">Fully Unified Motion Planning for End-to-End Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current end-to-end autonomous driving methods typically learn only from expert planning data collected from a single ego vehicle, severely limiting the diversity of learnable driving policies and scenarios. However, a critical yet overlooked fact is that in any driving scenario, multiple high-quality trajectories from other vehicles coexist with a specific ego vehicle's trajectory. Existing methods fail to fully exploit this valuable resource, missing important opportunities to improve the models' performance (including long-tail scenarios) through learning from other experts. Intuitively, Jointly learning from both ego and other vehicles' expert data is beneficial for planning tasks. However, this joint learning faces two critical challenges. (1) Different scene observation perspectives across vehicles hinder inter-vehicle alignment of scene feature representations; (2) The absence of partial modality in other vehicles' data (e.g., vehicle states) compared to ego-vehicle data introduces learning bias. To address these challenges, we propose FUMP (Fully Unified Motion Planning), a novel two-stage trajectory generation framework. Building upon probabilistic decomposition, we model the planning task as a specialized subtask of motion prediction. Specifically, our approach decouples trajectory planning into two stages. In Stage 1, a shared decoder jointly generates initial trajectories for both tasks. In Stage 2, the model performs planning-specific refinement conditioned on an ego-vehicle's state. The transition between the two stages is bridged by a state predictor trained exclusively on ego-vehicle data. To address the cross-vehicle discrepancy in observational perspectives, we propose an Equivariant Context-Sharing Adapter (ECSA) before Stage 1 for improving cross-vehicle generalization of scene representations.
<div id='section'>Paperid: <span id='pid'>637, <a href='https://arxiv.org/pdf/2501.05098.pdf' target='_blank'>https://arxiv.org/pdf/2501.05098.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuhong Zhang, Jing Lin, Ailing Zeng, Guanlin Wu, Shunlin Lu, Yurong Fu, Yuanhao Cai, Ruimao Zhang, Haoqian Wang, Lei Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.05098">Motion-X++: A Large-Scale Multimodal 3D Whole-body Human Motion Dataset</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we introduce Motion-X++, a large-scale multimodal 3D expressive whole-body human motion dataset. Existing motion datasets predominantly capture body-only poses, lacking facial expressions, hand gestures, and fine-grained pose descriptions, and are typically limited to lab settings with manually labeled text descriptions, thereby restricting their scalability. To address this issue, we develop a scalable annotation pipeline that can automatically capture 3D whole-body human motion and comprehensive textural labels from RGB videos and build the Motion-X dataset comprising 81.1K text-motion pairs. Furthermore, we extend Motion-X into Motion-X++ by improving the annotation pipeline, introducing more data modalities, and scaling up the data quantities. Motion-X++ provides 19.5M 3D whole-body pose annotations covering 120.5K motion sequences from massive scenes, 80.8K RGB videos, 45.3K audios, 19.5M frame-level whole-body pose descriptions, and 120.5K sequence-level semantic labels. Comprehensive experiments validate the accuracy of our annotation pipeline and highlight Motion-X++'s significant benefits for generating expressive, precise, and natural motion with paired multimodal labels supporting several downstream tasks, including text-driven whole-body motion generation,audio-driven motion generation, 3D whole-body human mesh recovery, and 2D whole-body keypoints estimation, etc.
<div id='section'>Paperid: <span id='pid'>638, <a href='https://arxiv.org/pdf/2410.18977.pdf' target='_blank'>https://arxiv.org/pdf/2410.18977.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ling-Hao Chen, Shunlin Lu, Wenxun Dai, Zhiyang Dou, Xuan Ju, Jingbo Wang, Taku Komura, Lei Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.18977">Pay Attention and Move Better: Harnessing Attention for Interactive Motion Generation and Training-free Editing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This research delves into the problem of interactive editing of human motion generation. Previous motion diffusion models lack explicit modeling of the word-level text-motion correspondence and good explainability, hence restricting their fine-grained editing ability. To address this issue, we propose an attention-based motion diffusion model, namely MotionCLR, with CLeaR modeling of attention mechanisms. Technically, MotionCLR models the in-modality and cross-modality interactions with self-attention and cross-attention, respectively. More specifically, the self-attention mechanism aims to measure the sequential similarity between frames and impacts the order of motion features. By contrast, the cross-attention mechanism works to find the fine-grained word-sequence correspondence and activate the corresponding timesteps in the motion sequence. Based on these key properties, we develop a versatile set of simple yet effective motion editing methods via manipulating attention maps, such as motion (de-)emphasizing, in-place motion replacement, and example-based motion generation, etc. For further verification of the explainability of the attention mechanism, we additionally explore the potential of action-counting and grounded motion generation ability via attention maps. Our experimental results show that our method enjoys good generation and editing ability with good explainability.
<div id='section'>Paperid: <span id='pid'>639, <a href='https://arxiv.org/pdf/2410.15281.pdf' target='_blank'>https://arxiv.org/pdf/2410.15281.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Can Cui, Yunsheng Ma, Sung-Yeon Park, Zichong Yang, Yupeng Zhou, Juanwu Lu, Juntong Peng, Jiaru Zhang, Ruqi Zhang, Lingxi Li, Yaobin Chen, Jitesh H. Panchal, Amr Abdelraouf, Rohit Gupta, Kyungtae Han, Ziran Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.15281">LLM4AD: Large Language Models for Autonomous Driving -- Concept, Review, Benchmark, Experiments, and Future Trends</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the broader adoption and highly successful development of Large Language Models (LLMs), there has been growing interest and demand for applying LLMs to autonomous driving technology. Driven by their natural language understanding and reasoning capabilities, LLMs have the potential to enhance various aspects of autonomous driving systems, from perception and scene understanding to interactive decision-making. In this paper, we first introduce the novel concept of designing Large Language Models for Autonomous Driving (LLM4AD), followed by a review of existing LLM4AD studies. Then, we propose a comprehensive benchmark for evaluating the instruction-following and reasoning abilities of LLM4AD systems, which includes LaMPilot-Bench, CARLA Leaderboard 1.0 Benchmark in simulation and NuPlanQA for multi-view visual question answering. Furthermore, we conduct extensive real-world experiments on autonomous vehicle platforms, examining both on-cloud and on-edge LLM deployment for personalized decision-making and motion control. Next, we explore the future trends of integrating language diffusion models into autonomous driving, exemplified by the proposed ViLaD (Vision-Language Diffusion) framework. Finally, we discuss the main challenges of LLM4AD, including latency, deployment, security and privacy, safety, trust and transparency, and personalization.
<div id='section'>Paperid: <span id='pid'>640, <a href='https://arxiv.org/pdf/2410.09681.pdf' target='_blank'>https://arxiv.org/pdf/2410.09681.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Christopher Diehl, Peter Karkus, Sushant Veer, Marco Pavone, Torsten Bertram
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.09681">LoRD: Adapting Differentiable Driving Policies to Distribution Shifts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Distribution shifts between operational domains can severely affect the performance of learned models in self-driving vehicles (SDVs). While this is a well-established problem, prior work has mostly explored naive solutions such as fine-tuning, focusing on the motion prediction task. In this work, we explore novel adaptation strategies for differentiable autonomy stacks consisting of prediction, planning, and control, perform evaluation in closed-loop, and investigate the often-overlooked issue of catastrophic forgetting. Specifically, we introduce two simple yet effective techniques: a low-rank residual decoder (LoRD) and multi-task fine-tuning. Through experiments across three models conducted on two real-world autonomous driving datasets (nuPlan, exiD), we demonstrate the effectiveness of our methods and highlight a significant performance gap between open-loop and closed-loop evaluation in prior approaches. Our approach improves forgetting by up to 23.33% and the closed-loop OOD driving score by 9.93% in comparison to standard fine-tuning.
<div id='section'>Paperid: <span id='pid'>641, <a href='https://arxiv.org/pdf/2410.07849.pdf' target='_blank'>https://arxiv.org/pdf/2410.07849.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Giulio Romualdi, Paolo Maria Viceconte, Lorenzo Moretti, Ines Sorrentino, Stefano Dafarra, Silvio Traversaro, Daniele Pucci
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.07849">Online DNN-driven Nonlinear MPC for Stylistic Humanoid Robot Walking with Step Adjustment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a three-layered architecture that enables stylistic locomotion with online contact location adjustment. Our method combines an autoregressive Deep Neural Network (DNN) acting as a trajectory generation layer with a model-based trajectory adjustment and trajectory control layers. The DNN produces centroidal and postural references serving as an initial guess and regularizer for the other layers. Being the DNN trained on human motion capture data, the resulting robot motion exhibits locomotion patterns, resembling a human walking style. The trajectory adjustment layer utilizes non-linear optimization to ensure dynamically feasible center of mass (CoM) motion while addressing step adjustments. We compare two implementations of the trajectory adjustment layer: one as a receding horizon planner (RHP) and the other as a model predictive controller (MPC). To enhance MPC performance, we introduce a Kalman filter to reduce measurement noise. The filter parameters are automatically tuned with a Genetic Algorithm. Experimental results on the ergoCub humanoid robot demonstrate the system's ability to prevent falls, replicate human walking styles, and withstand disturbances up to 68 Newton.
  Website: https://sites.google.com/view/dnn-mpc-walking
  Youtube video: https://www.youtube.com/watch?v=x3tzEfxO-xQ
<div id='section'>Paperid: <span id='pid'>642, <a href='https://arxiv.org/pdf/2409.00736.pdf' target='_blank'>https://arxiv.org/pdf/2409.00736.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziqiang Dang, Tianxing Fan, Boming Zhao, Xujie Shen, Lei Wang, Guofeng Zhang, Zhaopeng Cui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.00736">MoManifold: Learning to Measure 3D Human Motion via Decoupled Joint Acceleration Manifolds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Incorporating temporal information effectively is important for accurate 3D human motion estimation and generation which have wide applications from human-computer interaction to AR/VR. In this paper, we present MoManifold, a novel human motion prior, which models plausible human motion in continuous high-dimensional motion space. Different from existing mathematical or VAE-based methods, our representation is designed based on the neural distance field, which makes human dynamics explicitly quantified to a score and thus can measure human motion plausibility. Specifically, we propose novel decoupled joint acceleration manifolds to model human dynamics from existing limited motion data. Moreover, we introduce a novel optimization method using the manifold distance as guidance, which facilitates a variety of motion-related tasks. Extensive experiments demonstrate that MoManifold outperforms existing SOTAs as a prior in several downstream tasks such as denoising real-world human mocap data, recovering human motion from partial 3D observations, mitigating jitters for SMPL-based pose estimators, and refining the results of motion in-betweening.
<div id='section'>Paperid: <span id='pid'>643, <a href='https://arxiv.org/pdf/2406.17333.pdf' target='_blank'>https://arxiv.org/pdf/2406.17333.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mike Allenspach, Michael Pantic, Rik Girod, Lionel Ott, Roland Siegwart
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.17333">Task Adaptation in Industrial Human-Robot Interaction: Leveraging Riemannian Motion Policies</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In real-world industrial environments, modern robots often rely on human operators for crucial decision-making and mission synthesis from individual tasks. Effective and safe collaboration between humans and robots requires systems that can adjust their motion based on human intentions, enabling dynamic task planning and adaptation. Addressing the needs of industrial applications, we propose a motion control framework that (i) removes the need for manual control of the robot's movement; (ii) facilitates the formulation and combination of complex tasks; and (iii) allows the seamless integration of human intent recognition and robot motion planning. For this purpose, we leverage a modular and purely reactive approach for task parametrization and motion generation, embodied by Riemannian Motion Policies. The effectiveness of our method is demonstrated, evaluated, and compared to \remove{state-of-the-art approaches}\add{a representative state-of-the-art approach} in experimental scenarios inspired by realistic industrial Human-Robot Interaction settings.
<div id='section'>Paperid: <span id='pid'>644, <a href='https://arxiv.org/pdf/2406.16601.pdf' target='_blank'>https://arxiv.org/pdf/2406.16601.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sifan Wu, Zhenguang Liu, Beibei Zhang, Roger Zimmermann, Zhongjie Ba, Xiaosong Zhang, Kui Ren
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.16601">Do As I Do: Pose Guided Human Motion Copy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion copy is an intriguing yet challenging task in artificial intelligence and computer vision, which strives to generate a fake video of a target person performing the motion of a source person. The problem is inherently challenging due to the subtle human-body texture details to be generated and the temporal consistency to be considered. Existing approaches typically adopt a conventional GAN with an L1 or L2 loss to produce the target fake video, which intrinsically necessitates a large number of training samples that are challenging to acquire. Meanwhile, current methods still have difficulties in attaining realistic image details and temporal consistency, which unfortunately can be easily perceived by human observers. Motivated by this, we try to tackle the issues from three aspects: (1) We constrain pose-to-appearance generation with a perceptual loss and a theoretically motivated Gromov-Wasserstein loss to bridge the gap between pose and appearance. (2) We present an episodic memory module in the pose-to-appearance generation to propel continuous learning that helps the model learn from its past poor generations. We also utilize geometrical cues of the face to optimize facial details and refine each key body part with a dedicated local GAN. (3) We advocate generating the foreground in a sequence-to-sequence manner rather than a single-frame manner, explicitly enforcing temporal inconsistency. Empirical results on five datasets, iPER, ComplexMotion, SoloDance, Fish, and Mouse datasets, demonstrate that our method is capable of generating realistic target videos while precisely copying motion from a source video. Our method significantly outperforms state-of-the-art approaches and gains 7.2% and 12.4% improvements in PSNR and FID respectively.
<div id='section'>Paperid: <span id='pid'>645, <a href='https://arxiv.org/pdf/2511.11436.pdf' target='_blank'>https://arxiv.org/pdf/2511.11436.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuanyu Tian, Lixuan Chen, Qing Wu, Xiao Wang, Jie Feng, Yuyao Zhang, Hongjiang Wei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.11436">Unsupervised Motion-Compensated Decomposition for Cardiac MRI Reconstruction via Neural Representation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cardiac magnetic resonance (CMR) imaging is widely used to characterize cardiac morphology and function. To accelerate CMR imaging, various methods have been proposed to recover high-quality spatiotemporal CMR images from highly undersampled k-t space data. However, current CMR reconstruction techniques either fail to achieve satisfactory image quality or are restricted by the scarcity of ground truth data, leading to limited applicability in clinical scenarios. In this work, we proposed MoCo-INR, a new unsupervised method that integrates implicit neural representations (INR) with the conventional motion-compensated (MoCo) framework. Using explicit motion modeling and the continuous prior of INRs, MoCo-INR can produce accurate cardiac motion decomposition and high-quality CMR reconstruction. Furthermore, we introduce a new INR network architecture tailored to the CMR problem, which significantly stabilizes model optimization. Experiments on retrospective (simulated) datasets demonstrate the superiority of MoCo-INR over state-of-the-art methods, achieving fast convergence and fine-detailed reconstructions at ultra-high acceleration factors (e.g., 20x in VISTA sampling). Additionally, evaluations on prospective (real-acquired) free-breathing CMR scans highlight the clinical practicality of MoCo-INR for real-time imaging. Several ablation studies further confirm the effectiveness of the critical components of MoCo-INR.
<div id='section'>Paperid: <span id='pid'>646, <a href='https://arxiv.org/pdf/2510.02566.pdf' target='_blank'>https://arxiv.org/pdf/2510.02566.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiao Feng, Yiming Huang, Yufu Wang, Jiatao Gu, Lingjie Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.02566">PhysHMR: Learning Humanoid Control Policies from Vision for Physically Plausible Human Motion Reconstruction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reconstructing physically plausible human motion from monocular videos remains a challenging problem in computer vision and graphics. Existing methods primarily focus on kinematics-based pose estimation, often leading to unrealistic results due to the lack of physical constraints. To address such artifacts, prior methods have typically relied on physics-based post-processing following the initial kinematics-based motion estimation. However, this two-stage design introduces error accumulation, ultimately limiting the overall reconstruction quality. In this paper, we present PhysHMR, a unified framework that directly learns a visual-to-action policy for humanoid control in a physics-based simulator, enabling motion reconstruction that is both physically grounded and visually aligned with the input video. A key component of our approach is the pixel-as-ray strategy, which lifts 2D keypoints into 3D spatial rays and transforms them into global space. These rays are incorporated as policy inputs, providing robust global pose guidance without depending on noisy 3D root predictions. This soft global grounding, combined with local visual features from a pretrained encoder, allows the policy to reason over both detailed pose and global positioning. To overcome the sample inefficiency of reinforcement learning, we further introduce a distillation scheme that transfers motion knowledge from a mocap-trained expert to the vision-conditioned policy, which is then refined using physically motivated reinforcement learning rewards. Extensive experiments demonstrate that PhysHMR produces high-fidelity, physically plausible motion across diverse scenarios, outperforming prior approaches in both visual accuracy and physical realism.
<div id='section'>Paperid: <span id='pid'>647, <a href='https://arxiv.org/pdf/2508.07162.pdf' target='_blank'>https://arxiv.org/pdf/2508.07162.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaotong Lin, Tianming Liang, Jian-Fang Hu, Kun-Yu Lin, Yulei Kang, Chunwei Tian, Jianhuang Lai, Wei-Shi Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.07162">CoopDiff: Anticipating 3D Human-object Interactions via Contact-consistent Decoupled Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D human-object interaction (HOI) anticipation aims to predict the future motion of humans and their manipulated objects, conditioned on the historical context. Generally, the articulated humans and rigid objects exhibit different motion patterns, due to their distinct intrinsic physical properties. However, this distinction is ignored by most of the existing works, which intend to capture the dynamics of both humans and objects within a single prediction model. In this work, we propose a novel contact-consistent decoupled diffusion framework CoopDiff, which employs two distinct branches to decouple human and object motion modeling, with the human-object contact points as shared anchors to bridge the motion generation across branches. The human dynamics branch is aimed to predict highly structured human motion, while the object dynamics branch focuses on the object motion with rigid translations and rotations. These two branches are bridged by a series of shared contact points with consistency constraint for coherent human-object motion prediction. To further enhance human-object consistency and prediction reliability, we propose a human-driven interaction module to guide object motion modeling. Extensive experiments on the BEHAVE and Human-object Interaction datasets demonstrate that our CoopDiff outperforms state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>648, <a href='https://arxiv.org/pdf/2508.04224.pdf' target='_blank'>https://arxiv.org/pdf/2508.04224.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiahui Li, Shengeng Tang, Jingxuan He, Gang Huang, Zhangye Wang, Yantao Pan, Lechao Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.04224">SplitGaussian: Reconstructing Dynamic Scenes via Visual Geometry Decomposition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reconstructing dynamic 3D scenes from monocular video remains fundamentally challenging due to the need to jointly infer motion, structure, and appearance from limited observations. Existing dynamic scene reconstruction methods based on Gaussian Splatting often entangle static and dynamic elements in a shared representation, leading to motion leakage, geometric distortions, and temporal flickering. We identify that the root cause lies in the coupled modeling of geometry and appearance across time, which hampers both stability and interpretability. To address this, we propose \textbf{SplitGaussian}, a novel framework that explicitly decomposes scene representations into static and dynamic components. By decoupling motion modeling from background geometry and allowing only the dynamic branch to deform over time, our method prevents motion artifacts in static regions while supporting view- and time-dependent appearance refinement. This disentangled design not only enhances temporal consistency and reconstruction fidelity but also accelerates convergence. Extensive experiments demonstrate that SplitGaussian outperforms prior state-of-the-art methods in rendering quality, geometric stability, and motion separation.
<div id='section'>Paperid: <span id='pid'>649, <a href='https://arxiv.org/pdf/2507.16121.pdf' target='_blank'>https://arxiv.org/pdf/2507.16121.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shanshan Zhang, Qi Zhang, Siyue Wang, Tianshui Wen, Ziheng Zhou, Lingxiang Zheng, Yu Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.16121">DWSFormer: A Lightweight Inertial Odometry Network for Complex Motion Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Inertial odometry (IO) directly estimates the position of a carrier from inertial sensor measurements and serves as a core technology for the widespread deployment of consumer grade localization systems. While existing IO methods can accurately reconstruct simple and near linear motion trajectories, they often fail to account for drift errors caused by complex motion patterns such as turning. This limitation significantly degrades localization accuracy and restricts the applicability of IO systems in real world scenarios. To address these challenges, we propose a lightweight IO framework. Specifically, inertial data is projected into a high dimensional implicit nonlinear feature space using the Star Operation method, enabling the extraction of complex motion features that are typically overlooked. We further introduce a collaborative attention mechanism that jointly models global motion dynamics across both channel and temporal dimensions. In addition, we design Multi Scale Gated Convolution Units to capture fine grained dynamic variations throughout the motion process, thereby enhancing the model's ability to learn rich and expressive motion representations. Extensive experiments demonstrate that our proposed method consistently outperforms SOTA baselines across six widely used inertial datasets. Compared to baseline models on the RoNIN dataset, it achieves reductions in ATE ranging from 2.26% to 65.78%, thereby establishing a new benchmark in the field.
<div id='section'>Paperid: <span id='pid'>650, <a href='https://arxiv.org/pdf/2507.12463.pdf' target='_blank'>https://arxiv.org/pdf/2507.12463.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Renjie Li, Ruijie Ye, Mingyang Wu, Hao Frank Yang, Zhiwen Fan, Hezhen Hu, Zhengzhong Tu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.12463">MMHU: A Massive-Scale Multimodal Benchmark for Human Behavior Understanding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humans are integral components of the transportation ecosystem, and understanding their behaviors is crucial to facilitating the development of safe driving systems. Although recent progress has explored various aspects of human behavior$\unicode{x2014}$such as motion, trajectories, and intention$\unicode{x2014}$a comprehensive benchmark for evaluating human behavior understanding in autonomous driving remains unavailable. In this work, we propose $\textbf{MMHU}$, a large-scale benchmark for human behavior analysis featuring rich annotations, such as human motion and trajectories, text description for human motions, human intention, and critical behavior labels relevant to driving safety. Our dataset encompasses 57k human motion clips and 1.73M frames gathered from diverse sources, including established driving datasets such as Waymo, in-the-wild videos from YouTube, and self-collected data. A human-in-the-loop annotation pipeline is developed to generate rich behavior captions. We provide a thorough dataset analysis and benchmark multiple tasks$\unicode{x2014}$ranging from motion prediction to motion generation and human behavior question answering$\unicode{x2014}$thereby offering a broad evaluation suite. Project page : https://MMHU-Benchmark.github.io.
<div id='section'>Paperid: <span id='pid'>651, <a href='https://arxiv.org/pdf/2506.17912.pdf' target='_blank'>https://arxiv.org/pdf/2506.17912.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chuhao Jin, Haosen Li, Bingzi Zhang, Che Liu, Xiting Wang, Ruihua Song, Wenbing Huang, Ying Qin, Fuzheng Zhang, Di Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.17912">PlanMoGPT: Flow-Enhanced Progressive Planning for Text to Motion Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in large language models (LLMs) have enabled breakthroughs in many multimodal generation tasks, but a significant performance gap still exists in text-to-motion generation, where LLM-based methods lag far behind non-LLM methods. We identify the granularity of motion tokenization as a critical bottleneck: fine-grained tokenization induces local dependency issues, where LLMs overemphasize short-term coherence at the expense of global semantic alignment, while coarse-grained tokenization sacrifices motion details. To resolve this issue, we propose PlanMoGPT, an LLM-based framework integrating progressive planning and flow-enhanced fine-grained motion tokenization. First, our progressive planning mechanism leverages LLMs' autoregressive capabilities to hierarchically generate motion tokens by starting from sparse global plans and iteratively refining them into full sequences. Second, our flow-enhanced tokenizer doubles the downsampling resolution and expands the codebook size by eight times, minimizing detail loss during discretization, while a flow-enhanced decoder recovers motion nuances. Extensive experiments on text-to-motion benchmarks demonstrate that it achieves state-of-the-art performance, improving FID scores by 63.8% (from 0.380 to 0.141) on long-sequence generation while enhancing motion diversity by 49.9% compared to existing methods. The proposed framework successfully resolves the diversity-quality trade-off that plagues current non-LLM approaches, establishing new standards for text-to-motion generation.
<div id='section'>Paperid: <span id='pid'>652, <a href='https://arxiv.org/pdf/2504.08366.pdf' target='_blank'>https://arxiv.org/pdf/2504.08366.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sauradip Nag, Daniel Cohen-Or, Hao Zhang, Ali Mahdavi-Amiri
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.08366">In-2-4D: Inbetweening from Two Single-View Images to 4D Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We pose a new problem, In-2-4D, for generative 4D (i.e., 3D + motion) inbetweening to interpolate two single-view images. In contrast to video/4D generation from only text or a single image, our interpolative task can leverage more precise motion control to better constrain the generation. Given two monocular RGB images representing the start and end states of an object in motion, our goal is to generate and reconstruct the motion in 4D, without making assumptions on the object category, motion type, length, or complexity. To handle such arbitrary and diverse motions, we utilize a foundational video interpolation model for motion prediction. However, large frame-to-frame motion gaps can lead to ambiguous interpretations. To this end, we employ a hierarchical approach to identify keyframes that are visually close to the input states while exhibiting significant motions, then generate smooth fragments between them. For each fragment, we construct a 3D representation of the keyframe using Gaussian Splatting (3DGS). The temporal frames within the fragment guide the motion, enabling their transformation into dynamic 3DGS through a deformation field. To improve temporal consistency and refine the 3D motion, we expand the self-attention of multi-view diffusion across timesteps and apply rigid transformation regularization. Finally, we merge the independently generated 3D motion segments by interpolating boundary deformation fields and optimizing them to align with the guiding video, ensuring smooth and flicker-free transitions. Through extensive qualitative and quantitive experiments as well as a user study, we demonstrate the effectiveness of our method and design choices.
<div id='section'>Paperid: <span id='pid'>653, <a href='https://arxiv.org/pdf/2502.14140.pdf' target='_blank'>https://arxiv.org/pdf/2502.14140.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiming Huang, Zhiyang Dou, Lingjie Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.14140">ModSkill: Physical Character Skill Modularization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion is highly diverse and dynamic, posing challenges for imitation learning algorithms that aim to generalize motor skills for controlling simulated characters. Previous methods typically rely on a universal full-body controller for tracking reference motion (tracking-based model) or a unified full-body skill embedding space (skill embedding). However, these approaches often struggle to generalize and scale to larger motion datasets. In this work, we introduce a novel skill learning framework, ModSkill, that decouples complex full-body skills into compositional, modular skills for independent body parts. Our framework features a skill modularization attention layer that processes policy observations into modular skill embeddings that guide low-level controllers for each body part. We also propose an Active Skill Learning approach with Generative Adaptive Sampling, using large motion generation models to adaptively enhance policy learning in challenging tracking scenarios. Our results show that this modularized skill learning framework, enhanced by generative sampling, outperforms existing methods in precise full-body motion tracking and enables reusable skill embeddings for diverse goal-driven tasks.
<div id='section'>Paperid: <span id='pid'>654, <a href='https://arxiv.org/pdf/2501.18543.pdf' target='_blank'>https://arxiv.org/pdf/2501.18543.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Placido Falqueto, Alberto Sanfeliu, Luigi Palopoli, Daniele Fontanelli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.18543">Learning Priors of Human Motion With Vision Transformers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A clear understanding of where humans move in a scenario, their usual paths and speeds, and where they stop, is very important for different applications, such as mobility studies in urban areas or robot navigation tasks within human-populated environments. We propose in this article, a neural architecture based on Vision Transformers (ViTs) to provide this information. This solution can arguably capture spatial correlations more effectively than Convolutional Neural Networks (CNNs). In the paper, we describe the methodology and proposed neural architecture and show the experiments' results with a standard dataset. We show that the proposed ViT architecture improves the metrics compared to a method based on a CNN.
<div id='section'>Paperid: <span id='pid'>655, <a href='https://arxiv.org/pdf/2406.09982.pdf' target='_blank'>https://arxiv.org/pdf/2406.09982.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jacinto Colan, Ana Davila, Yasuhisa Hasegawa
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.09982">Constrained Motion Planning for a Robotic Endoscope Holder based on Hierarchical Quadratic Programming</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Minimally Invasive Surgeries (MIS) are challenging for surgeons due to the limited field of view and constrained range of motion imposed by narrow access ports. These challenges can be addressed by robot-assisted endoscope systems which provide precise and stabilized positioning, as well as constrained and smooth motion control of the endoscope. In this work, we propose an online hierarchical optimization framework for visual servoing control of the endoscope in MIS. The framework prioritizes maintaining a remote-center-of-motion (RCM) constraint to prevent tissue damage, while a visual tracking task is defined as a secondary task to enable autonomous tracking of visual features of interest. We validated our approach using a 6-DOF Denso VS050 manipulator and achieved optimization solving times under 0.4 ms and maximum RCM deviation of approximately 0.4 mm. Our results demonstrate the effectiveness of the proposed approach in addressing the constrained motion planning challenges of MIS, enabling precise and autonomous endoscope positioning and visual tracking.
<div id='section'>Paperid: <span id='pid'>656, <a href='https://arxiv.org/pdf/2404.01700.pdf' target='_blank'>https://arxiv.org/pdf/2404.01700.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Biao Jiang, Xin Chen, Chi Zhang, Fukun Yin, Zhuoyuan Li, Gang YU, Jiayuan Fan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.01700">MotionChain: Conversational Motion Controllers via Multimodal Prompts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in language models have demonstrated their adeptness in conducting multi-turn dialogues and retaining conversational context. However, this proficiency remains largely unexplored in other multimodal generative models, particularly in human motion models. By integrating multi-turn conversations in controlling continuous virtual human movements, generative human motion models can achieve an intuitive and step-by-step process of human task execution for humanoid robotics, game agents, or other embodied systems. In this work, we present MotionChain, a conversational human motion controller to generate continuous and long-term human motion through multimodal prompts. Specifically, MotionChain consists of multi-modal tokenizers that transform various data types such as text, image, and motion, into discrete tokens, coupled with a Vision-Motion-aware Language model. By leveraging large-scale language, vision-language, and vision-motion data to assist motion-related generation tasks, MotionChain thus comprehends each instruction in multi-turn conversation and generates human motions followed by these prompts. Extensive experiments validate the efficacy of MotionChain, demonstrating state-of-the-art performance in conversational motion generation, as well as more intuitive manners of controlling and interacting with virtual humans.
<div id='section'>Paperid: <span id='pid'>657, <a href='https://arxiv.org/pdf/2403.19501.pdf' target='_blank'>https://arxiv.org/pdf/2403.19501.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ming Yan, Yan Zhang, Shuqiang Cai, Shuqi Fan, Xincheng Lin, Yudi Dai, Siqi Shen, Chenglu Wen, Lan Xu, Yuexin Ma, Cheng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.19501">RELI11D: A Comprehensive Multimodal Human Motion Dataset and Method</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Comprehensive capturing of human motions requires both accurate captures of complex poses and precise localization of the human within scenes. Most of the HPE datasets and methods primarily rely on RGB, LiDAR, or IMU data. However, solely using these modalities or a combination of them may not be adequate for HPE, particularly for complex and fast movements. For holistic human motion understanding, we present RELI11D, a high-quality multimodal human motion dataset involves LiDAR, IMU system, RGB camera, and Event camera. It records the motions of 10 actors performing 5 sports in 7 scenes, including 3.32 hours of synchronized LiDAR point clouds, IMU measurement data, RGB videos and Event steams. Through extensive experiments, we demonstrate that the RELI11D presents considerable challenges and opportunities as it contains many rapid and complex motions that require precise location. To address the challenge of integrating different modalities, we propose LEIR, a multimodal baseline that effectively utilizes LiDAR Point Cloud, Event stream, and RGB through our cross-attention fusion strategy. We show that LEIR exhibits promising results for rapid motions and daily motions and that utilizing the characteristics of multiple modalities can indeed improve HPE performance. Both the dataset and source code will be released publicly to the research community, fostering collaboration and enabling further exploration in this field.
<div id='section'>Paperid: <span id='pid'>658, <a href='https://arxiv.org/pdf/2403.13900.pdf' target='_blank'>https://arxiv.org/pdf/2403.13900.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiming Huang, Weilin Wan, Yue Yang, Chris Callison-Burch, Mark Yatskar, Lingjie Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.13900">CoMo: Controllable Motion Generation through Language Guided Pose Code Editing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-to-motion models excel at efficient human motion generation, but existing approaches lack fine-grained controllability over the generation process. Consequently, modifying subtle postures within a motion or inserting new actions at specific moments remains a challenge, limiting the applicability of these methods in diverse scenarios. In light of these challenges, we introduce CoMo, a Controllable Motion generation model, adept at accurately generating and editing motions by leveraging the knowledge priors of large language models (LLMs). Specifically, CoMo decomposes motions into discrete and semantically meaningful pose codes, with each code encapsulating the semantics of a body part, representing elementary information such as "left knee slightly bent". Given textual inputs, CoMo autoregressively generates sequences of pose codes, which are then decoded into 3D motions. Leveraging pose codes as interpretable representations, an LLM can directly intervene in motion editing by adjusting the pose codes according to editing instructions. Experiments demonstrate that CoMo achieves competitive performance in motion generation compared to state-of-the-art models while, in human studies, CoMo substantially surpasses previous work in motion editing abilities.
<div id='section'>Paperid: <span id='pid'>659, <a href='https://arxiv.org/pdf/2403.07788.pdf' target='_blank'>https://arxiv.org/pdf/2403.07788.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chen Wang, Haochen Shi, Weizhuo Wang, Ruohan Zhang, Li Fei-Fei, C. Karen Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.07788">DexCap: Scalable and Portable Mocap Data Collection System for Dexterous Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Imitation learning from human hand motion data presents a promising avenue for imbuing robots with human-like dexterity in real-world manipulation tasks. Despite this potential, substantial challenges persist, particularly with the portability of existing hand motion capture (mocap) systems and the complexity of translating mocap data into effective robotic policies. To tackle these issues, we introduce DexCap, a portable hand motion capture system, alongside DexIL, a novel imitation algorithm for training dexterous robot skills directly from human hand mocap data. DexCap offers precise, occlusion-resistant tracking of wrist and finger motions based on SLAM and electromagnetic field together with 3D observations of the environment. Utilizing this rich dataset, DexIL employs inverse kinematics and point cloud-based imitation learning to seamlessly replicate human actions with robot hands. Beyond direct learning from human motion, DexCap also offers an optional human-in-the-loop correction mechanism during policy rollouts to refine and further improve task performance. Through extensive evaluation across six challenging dexterous manipulation tasks, our approach not only demonstrates superior performance but also showcases the system's capability to effectively learn from in-the-wild mocap data, paving the way for future data collection methods in the pursuit of human-level robot dexterity. More details can be found at https://dex-cap.github.io
<div id='section'>Paperid: <span id='pid'>660, <a href='https://arxiv.org/pdf/2512.01996.pdf' target='_blank'>https://arxiv.org/pdf/2512.01996.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Younggyo Seo, Carmelo Sferrazza, Juyue Chen, Guanya Shi, Rocky Duan, Pieter Abbeel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.01996">Learning Sim-to-Real Humanoid Locomotion in 15 Minutes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Massively parallel simulation has reduced reinforcement learning (RL) training time for robots from days to minutes. However, achieving fast and reliable sim-to-real RL for humanoid control remains difficult due to the challenges introduced by factors such as high dimensionality and domain randomization. In this work, we introduce a simple and practical recipe based on off-policy RL algorithms, i.e., FastSAC and FastTD3, that enables rapid training of humanoid locomotion policies in just 15 minutes with a single RTX 4090 GPU. Our simple recipe stabilizes off-policy RL algorithms at massive scale with thousands of parallel environments through carefully tuned design choices and minimalist reward functions. We demonstrate rapid end-to-end learning of humanoid locomotion controllers on Unitree G1 and Booster T1 robots under strong domain randomization, e.g., randomized dynamics, rough terrain, and push perturbations, as well as fast training of whole-body human-motion tracking policies. We provide videos and open-source implementation at: https://younggyo.me/fastsac-humanoid.
<div id='section'>Paperid: <span id='pid'>661, <a href='https://arxiv.org/pdf/2511.22181.pdf' target='_blank'>https://arxiv.org/pdf/2511.22181.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maitrayee Keskar, Mohan Trivedi, Ross Greer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.22181">MTR-VP: Towards End-to-End Trajectory Planning through Context-Driven Image Encoding and Multiple Trajectory Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a method for trajectory planning for autonomous driving, learning image-based context embeddings that align with motion prediction frameworks and planning-based intention input. Within our method, a ViT encoder takes raw images and past kinematic state as input and is trained to produce context embeddings, drawing inspiration from those generated by the recent MTR (Motion Transformer) encoder, effectively substituting map-based features with learned visual representations. MTR provides a strong foundation for multimodal trajectory prediction by localizing agent intent and refining motion iteratively via motion query pairs; we name our approach MTR-VP (Motion Transformer for Vision-based Planning), and instead of the learnable intention queries used in the MTR decoder, we use cross attention on the intent and the context embeddings, which reflect a combination of information encoded from the driving scene and past vehicle states. We evaluate our methods on the Waymo End-to-End Driving Dataset, which requires predicting the agent's future 5-second trajectory in bird's-eye-view coordinates using prior camera images, agent pose history, and routing goals. We analyze our architecture using ablation studies, removing input images and multiple trajectory output. Our results suggest that transformer-based methods that are used to combine the visual features along with the kinetic features such as the past trajectory features are not effective at combining both modes to produce useful scene context embeddings, even when intention embeddings are augmented with foundation-model representations of scene context from CLIP and DINOv2, but that predicting a distribution over multiple futures instead of a single future trajectory boosts planning performance.
<div id='section'>Paperid: <span id='pid'>662, <a href='https://arxiv.org/pdf/2511.17925.pdf' target='_blank'>https://arxiv.org/pdf/2511.17925.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jeonghwan Kim, Wontaek Kim, Yidan Lu, Jin Cheng, Fatemeh Zargarbashi, Zicheng Zeng, Zekun Qi, Zhiyang Dou, Nitish Sontakke, Donghoon Baek, Sehoon Ha, Tianyu Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.17925">Switch-JustDance: Benchmarking Whole Body Motion Tracking Policies Using a Commercial Console Game</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in whole-body robot control have enabled humanoid and legged robots to perform increasingly agile and coordinated motions. However, standardized benchmarks for evaluating these capabilities in real-world settings, and in direct comparison to humans, remain scarce. Existing evaluations often rely on pre-collected human motion datasets or simulation-based experiments, which limit reproducibility, overlook hardware factors, and hinder fair human-robot comparisons. We present Switch-JustDance, a low-cost and reproducible benchmarking pipeline that leverages motion-sensing console games, Just Dance on the Nintendo Switch, to evaluate robot whole-body control. Using Just Dance on the Nintendo Switch as a representative platform, Switch-JustDance converts in-game choreography into robot-executable motions through streaming, motion reconstruction, and motion retargeting modules and enables users to evaluate controller performance through the game's built-in scoring system. We first validate the evaluation properties of Just Dance, analyzing its reliability, validity, sensitivity, and potential sources of bias. Our results show that the platform provides consistent and interpretable performance measures, making it a suitable tool for benchmarking embodied AI. Building on this foundation, we benchmark three state-of-the-art humanoid whole-body controllers on hardware and provide insights into their relative strengths and limitations.
<div id='section'>Paperid: <span id='pid'>663, <a href='https://arxiv.org/pdf/2509.25600.pdf' target='_blank'>https://arxiv.org/pdf/2509.25600.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wontaek Kim, Tianyu Li, Sehoon Ha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25600">MoReFlow: Motion Retargeting Learning through Unsupervised Flow Matching</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion retargeting holds a premise of offering a larger set of motion data for characters and robots with different morphologies. Many prior works have approached this problem via either handcrafted constraints or paired motion datasets, limiting their applicability to humanoid characters or narrow behaviors such as locomotion. Moreover, they often assume a fixed notion of retargeting, overlooking domain-specific objectives like style preservation in animation or task-space alignment in robotics. In this work, we propose MoReFlow, Motion Retargeting via Flow Matching, an unsupervised framework that learns correspondences between characters' motion embedding spaces. Our method consists of two stages. First, we train tokenized motion embeddings for each character using a VQ-VAE, yielding compact latent representations. Then, we employ flow matching with conditional coupling to align the latent spaces across characters, which simultaneously learns conditioned and unconditioned matching to achieve robust but flexible retargeting. Once trained, MoReFlow enables flexible and reversible retargeting without requiring paired data. Experiments demonstrate that MoReFlow produces high-quality motions across diverse characters and tasks, offering improved controllability, generalization, and motion realism compared to the baselines.
<div id='section'>Paperid: <span id='pid'>664, <a href='https://arxiv.org/pdf/2508.21043.pdf' target='_blank'>https://arxiv.org/pdf/2508.21043.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhi Su, Bike Zhang, Nima Rahmanian, Yuman Gao, Qiayuan Liao, Caitlin Regan, Koushil Sreenath, S. Shankar Sastry
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.21043">HITTER: A HumanoId Table TEnnis Robot via Hierarchical Planning and Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humanoid robots have recently achieved impressive progress in locomotion and whole-body control, yet they remain constrained in tasks that demand rapid interaction with dynamic environments through manipulation. Table tennis exemplifies such a challenge: with ball speeds exceeding 5 m/s, players must perceive, predict, and act within sub-second reaction times, requiring both agility and precision. To address this, we present a hierarchical framework for humanoid table tennis that integrates a model-based planner for ball trajectory prediction and racket target planning with a reinforcement learning-based whole-body controller. The planner determines striking position, velocity and timing, while the controller generates coordinated arm and leg motions that mimic human strikes and maintain stability and agility across consecutive rallies. Moreover, to encourage natural movements, human motion references are incorporated during training. We validate our system on a general-purpose humanoid robot, achieving up to 106 consecutive shots with a human opponent and sustained exchanges against another humanoid. These results demonstrate real-world humanoid table tennis with sub-second reactive control, marking a step toward agile and interactive humanoid behaviors.
<div id='section'>Paperid: <span id='pid'>665, <a href='https://arxiv.org/pdf/2507.14915.pdf' target='_blank'>https://arxiv.org/pdf/2507.14915.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaojie Li, Ronghui Li, Shukai Fang, Shuzhao Xie, Xiaoyang Guo, Jiaqing Zhou, Junkun Peng, Zhi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.14915">Music-Aligned Holistic 3D Dance Generation via Hierarchical Motion Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Well-coordinated, music-aligned holistic dance enhances emotional expressiveness and audience engagement. However, generating such dances remains challenging due to the scarcity of holistic 3D dance datasets, the difficulty of achieving cross-modal alignment between music and dance, and the complexity of modeling interdependent motion across the body, hands, and face. To address these challenges, we introduce SoulDance, a high-precision music-dance paired dataset captured via professional motion capture systems, featuring meticulously annotated holistic dance movements. Building on this dataset, we propose SoulNet, a framework designed to generate music-aligned, kinematically coordinated holistic dance sequences. SoulNet consists of three principal components: (1) Hierarchical Residual Vector Quantization, which models complex, fine-grained motion dependencies across the body, hands, and face; (2) Music-Aligned Generative Model, which composes these hierarchical motion units into expressive and coordinated holistic dance; (3) Music-Motion Retrieval Module, a pre-trained cross-modal model that functions as a music-dance alignment prior, ensuring temporal synchronization and semantic coherence between generated dance and input music throughout the generation process. Extensive experiments demonstrate that SoulNet significantly surpasses existing approaches in generating high-quality, music-coordinated, and well-aligned holistic 3D dance sequences.
<div id='section'>Paperid: <span id='pid'>666, <a href='https://arxiv.org/pdf/2506.24121.pdf' target='_blank'>https://arxiv.org/pdf/2506.24121.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sisi Dai, Xinxin Su, Boyan Wan, Ruizhen Hu, Kai Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.24121">TextMesh4D: High-Quality Text-to-4D Mesh Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in diffusion generative models significantly advanced image, video, and 3D content creation from user-provided text prompts. However, the challenging problem of dynamic 3D content generation (text-to-4D) with diffusion guidance remains largely unexplored. In this paper, we introduce TextMesh4D, a novel framework for high-quality text-to-4D generation. Our approach leverages per-face Jacobians as a differentiable mesh representation and decomposes 4D generation into two stages: static object creation and dynamic motion synthesis. We further propose a flexibility-rigidity regularization term to stabilize Jacobian optimization under video diffusion priors, ensuring robust geometric performance. Experiments demonstrate that TextMesh4D achieves state-of-the-art results in terms of temporal consistency, structural fidelity, and visual realism. Moreover, TextMesh4D operates with a low GPU memory overhead-requiring only a single 24GB GPU-offering a cost-effective yet high-quality solution for text-driven 4D mesh generation. The code will be released to facilitate future research in text-to-4D generation.
<div id='section'>Paperid: <span id='pid'>667, <a href='https://arxiv.org/pdf/2506.14305.pdf' target='_blank'>https://arxiv.org/pdf/2506.14305.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhirui Sun, Xingrong Diao, Yao Wang, Bi-Ke Zhu, Jiankun Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.14305">Socially Aware Robot Crowd Navigation via Online Uncertainty-Driven Risk Adaptation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Navigation in human-robot shared crowded environments remains challenging, as robots are expected to move efficiently while respecting human motion conventions. However, many existing approaches emphasize safety or efficiency while overlooking social awareness. This article proposes Learning-Risk Model Predictive Control (LR-MPC), a data-driven navigation algorithm that balances efficiency, safety, and social awareness. LR-MPC consists of two phases: an offline risk learning phase, where a Probabilistic Ensemble Neural Network (PENN) is trained using risk data from a heuristic MPC-based baseline (HR-MPC), and an online adaptive inference phase, where local waypoints are sampled and globally guided by a Multi-RRT planner. Each candidate waypoint is evaluated for risk by PENN, and predictions are filtered using epistemic and aleatoric uncertainty to ensure robust decision-making. The safest waypoint is selected as the MPC input for real-time navigation. Extensive experiments demonstrate that LR-MPC outperforms baseline methods in success rate and social awareness, enabling robots to navigate complex crowds with high adaptability and low disruption. A website about this work is available at https://sites.google.com/view/lr-mpc.
<div id='section'>Paperid: <span id='pid'>668, <a href='https://arxiv.org/pdf/2505.02833.pdf' target='_blank'>https://arxiv.org/pdf/2505.02833.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanjie Ze, Zixuan Chen, JoÃ£o Pedro AraÃºjo, Zi-ang Cao, Xue Bin Peng, Jiajun Wu, C. Karen Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.02833">TWIST: Teleoperated Whole-Body Imitation System</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Teleoperating humanoid robots in a whole-body manner marks a fundamental step toward developing general-purpose robotic intelligence, with human motion providing an ideal interface for controlling all degrees of freedom. Yet, most current humanoid teleoperation systems fall short of enabling coordinated whole-body behavior, typically limiting themselves to isolated locomotion or manipulation tasks. We present the Teleoperated Whole-Body Imitation System (TWIST), a system for humanoid teleoperation through whole-body motion imitation. We first generate reference motion clips by retargeting human motion capture data to the humanoid robot. We then develop a robust, adaptive, and responsive whole-body controller using a combination of reinforcement learning and behavior cloning (RL+BC). Through systematic analysis, we demonstrate how incorporating privileged future motion frames and real-world motion capture (MoCap) data improves tracking accuracy. TWIST enables real-world humanoid robots to achieve unprecedented, versatile, and coordinated whole-body motor skills--spanning whole-body manipulation, legged manipulation, locomotion, and expressive movement--using a single unified neural network controller. Our project website: https://humanoid-teleop.github.io
<div id='section'>Paperid: <span id='pid'>669, <a href='https://arxiv.org/pdf/2504.01275.pdf' target='_blank'>https://arxiv.org/pdf/2504.01275.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Subhradip Chakraborty, Shay Snyder, Md Abdullah-Al Kaiser, Maryam Parsa, Gregory Schwartz, Akhilesh R. Jaiswal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.01275">A Retina-Inspired Pathway to Real-Time Motion Prediction inside Image Sensors for Extreme-Edge Intelligence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The ability to predict motion in real time is fundamental to many maneuvering activities in animals, particularly those critical for survival, such as attack and escape responses. Given its significance, it is no surprise that motion prediction in animals begins in the retina. Similarly, autonomous systems utilizing computer vision could greatly benefit from the capability to predict motion in real time. Therefore, for computer vision applications, motion prediction should be integrated directly at the camera pixel level. Towards that end, we present a retina-inspired neuromorphic framework capable of performing real-time, energy-efficient MP directly within camera pixels. Our hardware-algorithm framework, implemented using GlobalFoundries 22nm FDSOI technology, integrates key retinal MP compute blocks, including a biphasic filter, spike adder, nonlinear circuit, and a 2D array for multi-directional motion prediction. Additionally, integrating the sensor and MP compute die using a 3D Cu-Cu hybrid bonding approach improves design compactness by minimizing area usage and simplifying routing complexity. Validated on real-world object stimuli, the model delivers efficient, low-latency MP for decision-making scenarios reliant on predictive visual computation, while consuming only 18.56 pJ/MP in our mixed-signal hardware implementation.
<div id='section'>Paperid: <span id='pid'>670, <a href='https://arxiv.org/pdf/2412.17377.pdf' target='_blank'>https://arxiv.org/pdf/2412.17377.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Youliang Zhang, Ronghui Li, Yachao Zhang, Liang Pan, Jingbo Wang, Yebin Liu, Xiu Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.17377">A Plug-and-Play Physical Motion Restoration Approach for In-the-Wild High-Difficulty Motions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Extracting physically plausible 3D human motion from videos is a critical task. Although existing simulation-based motion imitation methods can enhance the physical quality of daily motions estimated from monocular video capture, extending this capability to high-difficulty motions remains an open challenge. This can be attributed to some flawed motion clips in video-based motion capture results and the inherent complexity in modeling high-difficulty motions. Therefore, sensing the advantage of segmentation in localizing human body, we introduce a mask-based motion correction module (MCM) that leverages motion context and video mask to repair flawed motions, producing imitation-friendly motions; and propose a physics-based motion transfer module (PTM), which employs a pretrain and adapt approach for motion imitation, improving physical plausibility with the ability to handle in-the-wild and challenging motions. Our approach is designed as a plug-and-play module to physically refine the video motion capture results, including high-difficulty in-the-wild motions. Finally, to validate our approach, we collected a challenging in-the-wild test set to establish a benchmark, and our method has demonstrated effectiveness on both the new benchmark and existing public datasets.https://physicalmotionrestoration.github.io
<div id='section'>Paperid: <span id='pid'>671, <a href='https://arxiv.org/pdf/2412.16982.pdf' target='_blank'>https://arxiv.org/pdf/2412.16982.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ronghui Li, Youliang Zhang, Yachao Zhang, Yuxiang Zhang, Mingyang Su, Jie Guo, Ziwei Liu, Yebin Liu, Xiu Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.16982">InterDance:Reactive 3D Dance Generation with Realistic Duet Interactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humans perform a variety of interactive motions, among which duet dance is one of the most challenging interactions. However, in terms of human motion generative models, existing works are still unable to generate high-quality interactive motions, especially in the field of duet dance. On the one hand, it is due to the lack of large-scale high-quality datasets. On the other hand, it arises from the incomplete representation of interactive motion and the lack of fine-grained optimization of interactions. To address these challenges, we propose, InterDance, a large-scale duet dance dataset that significantly enhances motion quality, data scale, and the variety of dance genres. Built upon this dataset, we propose a new motion representation that can accurately and comprehensively describe interactive motion. We further introduce a diffusion-based framework with an interaction refinement guidance strategy to optimize the realism of interactions progressively. Extensive experiments demonstrate the effectiveness of our dataset and algorithm.
<div id='section'>Paperid: <span id='pid'>672, <a href='https://arxiv.org/pdf/2412.14484.pdf' target='_blank'>https://arxiv.org/pdf/2412.14484.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kunpeng Song, Tingbo Hou, Zecheng He, Haoyu Ma, Jialiang Wang, Animesh Sinha, Sam Tsai, Yaqiao Luo, Xiaoliang Dai, Li Chen, Xide Xia, Peizhao Zhang, Peter Vajda, Ahmed Elgammal, Felix Juefei-Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.14484">Llama Learns to Direct: DirectorLLM for Human-Centric Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we introduce DirectorLLM, a novel video generation model that employs a large language model (LLM) to orchestrate human poses within videos. As foundational text-to-video models rapidly evolve, the demand for high-quality human motion and interaction grows. To address this need and enhance the authenticity of human motions, we extend the LLM from a text generator to a video director and human motion simulator. Utilizing open-source resources from Llama 3, we train the DirectorLLM to generate detailed instructional signals, such as human poses, to guide video generation. This approach offloads the simulation of human motion from the video generator to the LLM, effectively creating informative outlines for human-centric scenes. These signals are used as conditions by the video renderer, facilitating more realistic and prompt-following video generation. As an independent LLM module, it can be applied to different video renderers, including UNet and DiT, with minimal effort. Experiments on automatic evaluation benchmarks and human evaluations show that our model outperforms existing ones in generating videos with higher human motion fidelity, improved prompt faithfulness, and enhanced rendered subject naturalness.
<div id='section'>Paperid: <span id='pid'>673, <a href='https://arxiv.org/pdf/2406.04858.pdf' target='_blank'>https://arxiv.org/pdf/2406.04858.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bingheng Wang, Rui Huang, Lin Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.04858">Auto-Multilift: Distributed Learning and Control for Cooperative Load Transportation With Quadrotors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Designing motion control and planning algorithms for multilift systems remains challenging due to the complexities of dynamics, collision avoidance, actuator limits, and scalability. Existing methods that use optimization and distributed techniques effectively address these constraints and scalability issues. However, they often require substantial manual tuning, leading to suboptimal performance. This paper proposes Auto-Multilift, a novel framework that automates the tuning of model predictive controllers (MPCs) for multilift systems. We model the MPC cost functions with deep neural networks (DNNs), enabling fast online adaptation to various scenarios. We develop a distributed policy gradient algorithm to train these DNNs efficiently in a closed-loop manner. Central to our algorithm is distributed sensitivity propagation, which is built on fully exploiting the unique dynamic couplings within the multilift system. It parallelizes gradient computation across quadrotors and focuses on actual system state sensitivities relative to key MPC parameters. Extensive simulations demonstrate favorable scalability to a large number of quadrotors. Our method outperforms a state-of-the-art open-loop MPC tuning approach by effectively learning adaptive MPCs from trajectory tracking errors. It also excels in learning an adaptive reference for reconfiguring the system when traversing multiple narrow slots.
<div id='section'>Paperid: <span id='pid'>674, <a href='https://arxiv.org/pdf/2401.06146.pdf' target='_blank'>https://arxiv.org/pdf/2401.06146.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianyu Li, Calvin Qiao, Guanqiao Ren, KangKang Yin, Sehoon Ha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.06146">AAMDM: Accelerated Auto-regressive Motion Diffusion Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Interactive motion synthesis is essential in creating immersive experiences in entertainment applications, such as video games and virtual reality. However, generating animations that are both high-quality and contextually responsive remains a challenge. Traditional techniques in the game industry can produce high-fidelity animations but suffer from high computational costs and poor scalability. Trained neural network models alleviate the memory and speed issues, yet fall short on generating diverse motions. Diffusion models offer diverse motion synthesis with low memory usage, but require expensive reverse diffusion processes. This paper introduces the Accelerated Auto-regressive Motion Diffusion Model (AAMDM), a novel motion synthesis framework designed to achieve quality, diversity, and efficiency all together. AAMDM integrates Denoising Diffusion GANs as a fast Generation Module, and an Auto-regressive Diffusion Model as a Polishing Module. Furthermore, AAMDM operates in a lower-dimensional embedded space rather than the full-dimensional pose space, which reduces the training complexity as well as further improves the performance. We show that AAMDM outperforms existing methods in motion quality, diversity, and runtime efficiency, through comprehensive quantitative analyses and visual comparisons. We also demonstrate the effectiveness of each algorithmic component through ablation studies.
<div id='section'>Paperid: <span id='pid'>675, <a href='https://arxiv.org/pdf/2511.19217.pdf' target='_blank'>https://arxiv.org/pdf/2511.19217.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wanjiang Weng, Xiaofeng Tan, Junbo Wang, Guo-Sen Xie, Pan Zhou, Hongsong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.19217">ReAlign: Text-to-Motion Generation via Step-Aware Reward-Guided Alignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-to-motion generation, which synthesizes 3D human motions from text inputs, holds immense potential for applications in gaming, film, and robotics. Recently, diffusion-based methods have been shown to generate more diversity and realistic motion. However, there exists a misalignment between text and motion distributions in diffusion models, which leads to semantically inconsistent or low-quality motions. To address this limitation, we propose Reward-guided sampling Alignment (ReAlign), comprising a step-aware reward model to assess alignment quality during the denoising sampling and a reward-guided strategy that directs the diffusion process toward an optimally aligned distribution. This reward model integrates step-aware tokens and combines a text-aligned module for semantic consistency and a motion-aligned module for realism, refining noisy motions at each timestep to balance probability density and alignment. Extensive experiments of both motion generation and retrieval tasks demonstrate that our approach significantly improves text-motion alignment and motion quality compared to existing state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>676, <a href='https://arxiv.org/pdf/2511.10411.pdf' target='_blank'>https://arxiv.org/pdf/2511.10411.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Benjamin Stoler, Jonathan Francis, Jean Oh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.10411">LongComp: Long-Tail Compositional Zero-Shot Generalization for Robust Trajectory Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Methods for trajectory prediction in Autonomous Driving must contend with rare, safety-critical scenarios that make reliance on real-world data collection alone infeasible. To assess robustness under such conditions, we propose new long-tail evaluation settings that repartition datasets to create challenging out-of-distribution (OOD) test sets. We first introduce a safety-informed scenario factorization framework, which disentangles scenarios into discrete ego and social contexts. Building on analogies to compositional zero-shot image-labeling in Computer Vision, we then hold out novel context combinations to construct challenging closed-world and open-world settings. This process induces OOD performance gaps in future motion prediction of 5.0% and 14.7% in closed-world and open-world settings, respectively, relative to in-distribution performance for a state-of-the-art baseline. To improve generalization, we extend task-modular gating networks to operate within trajectory prediction models, and develop an auxiliary, difficulty-prediction head to refine internal representations. Our strategies jointly reduce the OOD performance gaps to 2.8% and 11.5% in the two settings, respectively, while still improving in-distribution performance.
<div id='section'>Paperid: <span id='pid'>677, <a href='https://arxiv.org/pdf/2510.02284.pdf' target='_blank'>https://arxiv.org/pdf/2510.02284.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>David Romero, Ariana Bermudez, Hao Li, Fabio Pizzati, Ivan Laptev
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.02284">Learning to Generate Object Interactions with Physics-Guided Video Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent models for video generation have achieved remarkable progress and are now deployed in film, social media production, and advertising. Beyond their creative potential, such models also hold promise as world simulators for robotics and embodied decision making. Despite strong advances, however, current approaches still struggle to generate physically plausible object interactions and lack physics-grounded control mechanisms. To address this limitation, we introduce KineMask, an approach for physics-guided video generation that enables realistic rigid body control, interactions, and effects. Given a single image and a specified object velocity, our method generates videos with inferred motions and future object interactions. We propose a two-stage training strategy that gradually removes future motion supervision via object masks. Using this strategy we train video diffusion models (VDMs) on synthetic scenes of simple interactions and demonstrate significant improvements of object interactions in real scenes. Furthermore, KineMask integrates low-level motion control with high-level textual conditioning via predictive scene descriptions, leading to effective support for synthesis of complex dynamical phenomena. Extensive experiments show that KineMask achieves strong improvements over recent models of comparable size. Ablation studies further highlight the complementary roles of low- and high-level conditioning in VDMs. Our code, model, and data will be made publicly available.
<div id='section'>Paperid: <span id='pid'>678, <a href='https://arxiv.org/pdf/2506.12103.pdf' target='_blank'>https://arxiv.org/pdf/2506.12103.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Amazon AGI, Aaron Langford, Aayush Shah, Abhanshu Gupta, Abhimanyu Bhatter, Abhinav Goyal, Abhinav Mathur, Abhinav Mohanty, Abhishek Kumar, Abhishek Sethi, Abi Komma, Abner Pena, Achin Jain, Adam Kunysz, Adam Opyrchal, Adarsh Singh, Aditya Rawal, Adok Achar Budihal Prasad, AdriÃ  de Gispert, Agnika Kumar, Aishwarya Aryamane, Ajay Nair, Akilan M, Akshaya Iyengar, Akshaya Vishnu Kudlu Shanbhogue, Alan He, Alessandra Cervone, Alex Loeb, Alex Zhang, Alexander Fu, Alexander Lisnichenko, Alexander Zhipa, Alexandros Potamianos, Ali Kebarighotbi, Aliakbar Daronkolaei, Alok Parmesh, Amanjot Kaur Samra, Ameen Khan, Amer Rez, Amir Saffari, Amit Agarwalla, Amit Jhindal, Amith Mamidala, Ammar Asmro, Amulya Ballakur, Anand Mishra, Anand Sridharan, Anastasiia Dubinina, Andre Lenz, Andreas Doerr, Andrew Keating, Andrew Leaver, Andrew Smith, Andrew Wirth, Andy Davey, Andy Rosenbaum, Andy Sohn, Angela Chan, Aniket Chakrabarti, Anil Ramakrishna, Anirban Roy, Anita Iyer, Anjali Narayan-Chen, Ankith Yennu, Anna Dabrowska, Anna Gawlowska, Anna Rumshisky, Anna Turek, Anoop Deoras, Anton Bezruchkin, Anup Prasad, Anupam Dewan, Anwith Kiran, Apoorv Gupta, Aram Galstyan, Aravind Manoharan, Arijit Biswas, Arindam Mandal, Arpit Gupta, Arsamkhan Pathan, Arun Nagarajan, Arushan Rajasekaram, Arvind Sundararajan, Ashwin Ganesan, Ashwin Swaminathan, Athanasios Mouchtaris, Audrey Champeau, Avik Ray, Ayush Jaiswal, Ayush Sharma, Bailey Keefer, Balamurugan Muthiah, Beatriz Leon-Millan, Ben Koopman, Ben Li, Benjamin Biggs, Benjamin Ott, Bhanu Vinzamuri, Bharath Venkatesh, Bhavana Ganesh, Bhoomit Vasani, Bill Byrne, Bill Hsu, Bincheng Wang, Blake King, Blazej Gorny, Bo Feng, Bo Zheng, Bodhisattwa Paul, Bofan Sun, Bofeng Luo, Bowen Chen, Bowen Xie, Boya Yu, Brendan Jugan, Brett Panosh, Brian Collins, Brian Thompson, Can Karakus, Can Liu, Carl Lambrecht, Carly Lin, Carolyn Wang, Carrie Yuan, Casey Loyda, Cezary Walczak, Chalapathi Choppa, Chandana Satya Prakash, Chankrisna Richy Meas, Charith Peris, Charles Recaido, Charlie Xu, Charul Sharma, Chase Kernan, Chayut Thanapirom, Chengwei Su, Chenhao Xu, Chenhao Yin, Chentao Ye, Chenyang Tao, Chethan Parameshwara, Ching-Yun Chang, Chong Li, Chris Hench, Chris Tran, Christophe Dupuy, Christopher Davis, Christopher DiPersio, Christos Christodoulopoulos, Christy Li, Chun Chen, Claudio Delli Bovi, Clement Chung, Cole Hawkins, Connor Harris, Corey Ropell, Cynthia He, DK Joo, Dae Yon Hwang, Dan Rosen, Daniel Elkind, Daniel Pressel, Daniel Zhang, Danielle Kimball, Daniil Sorokin, Dave Goodell, Davide Modolo, Dawei Zhu, Deepikaa Suresh, Deepti Ragha, Denis Filimonov, Denis Foo Kune, Denis Romasanta Rodriguez, Devamanyu Hazarika, Dhananjay Ram, Dhawal Parkar, Dhawal Patel, Dhwanil Desai, Dinesh Singh Rajput, Disha Sule, Diwakar Singh, Dmitriy Genzel, Dolly Goldenberg, Dongyi He, Dumitru Hanciu, Dushan Tharmal, Dzmitry Siankovich, Edi Cikovic, Edwin Abraham, Ekraam Sabir, Elliott Olson, Emmett Steven, Emre Barut, Eric Jackson, Ethan Wu, Evelyn Chen, Ezhilan Mahalingam, Fabian Triefenbach, Fan Yang, Fangyu Liu, Fanzi Wu, Faraz Tavakoli, Farhad Khozeimeh, Feiyang Niu, Felix Hieber, Feng Li, Firat Elbey, Florian Krebs, Florian Saupe, Florian SprÃ¼nken, Frank Fan, Furqan Khan, Gabriela De Vincenzo, Gagandeep Kang, George Ding, George He, George Yeung, Ghada Qaddoumi, Giannis Karamanolakis, Goeric Huybrechts, Gokul Maddali, Gonzalo Iglesias, Gordon McShane, Gozde Sahin, Guangtai Huang, Gukyeong Kwon, Gunnar A. Sigurdsson, Gurpreet Chadha, Gururaj Kosuru, Hagen Fuerstenau, Hah Hah, Haja Maideen, Hajime Hosokawa, Han Liu, Han-Kai Hsu, Hann Wang, Hao Li, Hao Yang, Haofeng Zhu, Haozheng Fan, Harman Singh, Harshavardhan Kaluvala, Hashim Saeed, He Xie, Helian Feng, Hendrix Luo, Hengzhi Pei, Henrik Nielsen, Hesam Ilati, Himanshu Patel, Hongshan Li, Hongzhou Lin, Hussain Raza, Ian Cullinan, Imre Kiss, Inbarasan Thangamani, Indrayani Fadnavis, Ionut Teodor Sorodoc, Irem Ertuerk, Iryna Yemialyanava, Ishan Soni, Ismail Jelal, Ivan Tse, Jack FitzGerald, Jack Zhao, Jackson Rothgeb, Jacky Lee, Jake Jung, Jakub Debski, Jakub Tomczak, James Jeun, James Sanders, Jason Crowley, Jay Lee, Jayakrishna Anvesh Paidy, Jayant Tiwari, Jean Farmer, Jeff Solinsky, Jenna Lau, Jeremy Savareese, Jerzy Zagorski, Ji Dai, Jiacheng, Gu, Jiahui Li, Jian, Zheng, Jianhua Lu, Jianhua Wang, Jiawei Dai, Jiawei Mo, Jiaxi Xu, Jie Liang, Jie Yang, Jim Logan, Jimit Majmudar, Jing Liu, Jinghong Miao, Jingru Yi, Jingyang Jin, Jiun-Yu Kao, Jixuan Wang, Jiyang Wang, Joe Pemberton, Joel Carlson, Joey Blundell, John Chin-Jew, John He, Jonathan Ho, Jonathan Hueser, Jonathan Lunt, Jooyoung Lee, Joshua Tan, Joyjit Chatterjee, Judith Gaspers, Jue Wang, Jun Fang, Jun Tang, Jun Wan, Jun Wu, Junlei Wang, Junyi Shi, Justin Chiu, Justin Satriano, Justin Yee, Jwala Dhamala, Jyoti Bansal, Kai Zhen, Kai-Wei Chang, Kaixiang Lin, Kalyan Raman, Kanthashree Mysore Sathyendra, Karabo Moroe, Karan Bhandarkar, Karan Kothari, Karolina Owczarzak, Karthick Gopalswamy, Karthick Ravi, Karthik Ramakrishnan, Karthika Arumugam, Kartik Mehta, Katarzyna Konczalska, Kavya Ravikumar, Ke Tran, Kechen Qin, Kelin Li, Kelvin Li, Ketan Kulkarni, Kevin Angelo Rodrigues, Keyur Patel, Khadige Abboud, Kiana Hajebi, Klaus Reiter, Kris Schultz, Krishna Anisetty, Krishna Kotnana, Kristen Li, Kruthi Channamallikarjuna, Krzysztof Jakubczyk, Kuba Pierewoj, Kunal Pal, Kunwar Srivastav, Kyle Bannerman, Lahari Poddar, Lakshmi Prasad, Larry Tseng, Laxmikant Naik, Leena Chennuru Vankadara, Lenon Minorics, Leo Liu, Leonard Lausen, Leonardo F. R. Ribeiro, Li Zhang, Lili Gehorsam, Ling Qi, Lisa Bauer, Lori Knapp, Lu Zeng, Lucas Tong, Lulu Wong, Luoxin Chen, Maciej Rudnicki, Mahdi Namazifar, Mahesh Jaliminche, Maira Ladeira Tanke, Manasi Gupta, Mandeep Ahlawat, Mani Khanuja, Mani Sundaram, Marcin Leyk, Mariusz Momotko, Markus Boese, Markus Dreyer, Markus Mueller, Mason Fu, Mateusz GÃ³rski, Mateusz Mastalerczyk, Matias Mora, Matt Johnson, Matt Scott, Matthew Wen, Max Barysau, Maya Boumerdassi, Maya Krishnan, Mayank Gupta, Mayank Hirani, Mayank Kulkarni, Meganathan Narayanasamy, Melanie Bradford, Melanie Gens, Melissa Burke, Meng Jin, Miao Chen, Michael Denkowski, Michael Heymel, Michael Krestyaninov, Michal Obirek, Michalina Wichorowska, MichaÅ Miotk, Milosz Watroba, Mingyi Hong, Mingzhi Yu, Miranda Liu, Mohamed Gouda, Mohammad El-Shabani, Mohammad Ghavamzadeh, Mohit Bansal, Morteza Ziyadi, Nan Xia, Nathan Susanj, Nav Bhasin, Neha Goswami, Nehal Belgamwar, Nicolas Anastassacos, Nicolas Bergeron, Nidhi Jain, Nihal Jain, Niharika Chopparapu, Nik Xu, Nikko Strom, Nikolaos Malandrakis, Nimisha Mishra, Ninad Parkhi, Ninareh Mehrabi, Nishita Sant, Nishtha Gupta, Nitesh Sekhar, Nithin Rajeev, Nithish Raja Chidambaram, Nitish Dhar, Noor Bhagwagar, Noy Konforty, Omar Babu, Omid Razavi, Orchid Majumder, Osama Dar, Oscar Hsu, Pablo Kvitca, Pallavi Pandey, Parker Seegmiller, Patrick Lange, Paul Ferraro, Payal Motwani, Pegah Kharazmi, Pei Wang, Pengfei Liu, Peter Bradtke, Peter GÃ¶tz, Peter Zhou, Pichao Wang, Piotr Poskart, Pooja Sonawane, Pradeep Natarajan, Pradyun Ramadorai, Pralam Shah, Prasad Nirantar, Prasanthi Chavali, Prashan Wanigasekara, Prashant Saraf, Prashun Dey, Pratyush Pant, Prerak Pradhan, Preyaa Patel, Priyanka Dadlani, Prudhvee Narasimha Sadha, Qi Dong, Qian Hu, Qiaozi, Gao, Qing Liu, Quinn Lam, Quynh Do, R. Manmatha, Rachel Willis, Rafael Liu, Rafal Ellert, Rafal Kalinski, Rafi Al Attrach, Ragha Prasad, Ragini Prasad, Raguvir Kunani, Rahul Gupta, Rahul Sharma, Rahul Tewari, Rajaganesh Baskaran, Rajan Singh, Rajiv Gupta, Rajiv Reddy, Rajshekhar Das, Rakesh Chada, Rakesh Vaideeswaran Mahesh, Ram Chandrasekaran, Ramesh Nallapati, Ran Xue, Rashmi Gangadharaiah, Ravi Rachakonda, Renxian Zhang, Rexhina Blloshmi, Rishabh Agrawal, Robert Enyedi, Robert Lowe, Robik Shrestha, Robinson Piramuthu, Rohail Asad, Rohan Khanna, Rohan Mukherjee, Rohit Mittal, Rohit Prasad, Rohith Mysore Vijaya Kumar, Ron Diamant, Ruchita Gupta, Ruiwen Li, Ruoying Li, Rushabh Fegade, Ruxu Zhang, Ryan Arbow, Ryan Chen, Ryan Gabbard, Ryan Hoium, Ryan King, Sabarishkumar Iyer, Sachal Malick, Sahar Movaghati, Sai Balakavi, Sai Jakka, Sai Kashyap Paruvelli, Sai Muralidhar Jayanthi, Saicharan Shriram Mujumdar, Sainyam Kapoor, Sajjad Beygi, Saket Dingliwal, Saleh Soltan, Sam Ricklin, Sam Tucker, Sameer Sinha, Samridhi Choudhary, Samson Tan, Samuel Broscheit, Samuel Schulter, Sanchit Agarwal, Sandeep Atluri, Sander Valstar, Sanjana Shankar, Sanyukta Sanyukta, Sarthak Khanna, Sarvpriye Khetrapal, Satish Janakiraman, Saumil Shah, Saurabh Akolkar, Saurabh Giri, Saurabh Khandelwal, Saurabh Pawar, Saurabh Sahu, Sean Huang, Sejun Ra, Senthilkumar Gopal, Sergei Dobroshinsky, Shadi Saba, Shamik Roy, Shamit Lal, Shankar Ananthakrishnan, Sharon Li, Shashwat Srijan, Shekhar Bhide, Sheng Long Tang, Sheng Zha, Shereen Oraby, Sherif Mostafa, Shiqi Li, Shishir Bharathi, Shivam Prakash, Shiyuan Huang, Shreya Yembarwar, Shreyas Pansare, Shreyas Subramanian, Shrijeet Joshi, Shuai Liu, Shuai Tang, Shubham Chandak, Shubham Garg, Shubham Katiyar, Shubham Mehta, Shubham Srivastav, Shuo Yang, Siddalingesha D S, Siddharth Choudhary, Siddharth Singh Senger, Simon Babb, Sina Moeini, Siqi Deng, Siva Loganathan, Slawomir Domagala, Sneha Narkar, Sneha Wadhwa, Songyang Zhang, Songyao Jiang, Sony Trenous, Soumajyoti Sarkar, Soumya Saha, Sourabh Reddy, Sourav Dokania, Spurthideepika Sandiri, Spyros Matsoukas, Sravan Bodapati, Sri Harsha Reddy Wdaru, Sridevi Yagati Venkateshdatta, Srikanth Ronanki, Srinivasan R Veeravanallur, Sriram Venkatapathy, Sriramprabhu Sankaraguru, Sruthi Gorantla, Sruthi Karuturi, Stefan Schroedl, Subendhu Rongali, Subhasis Kundu, Suhaila Shakiah, Sukriti Tiwari, Sumit Bharti, Sumita Sami, Sumith Mathew, Sunny Yu, Sunwoo Kim, Suraj Bajirao Malode, Susana Cumplido Riel, Swapnil Palod, Swastik Roy, Syed Furqhan, Tagyoung Chung, Takuma Yoshitani, Taojiannan Yang, Tejaswi Chillakura, Tejwant Bajwa, Temi Lajumoke, Thanh Tran, Thomas Gueudre, Thomas Jung, Tianhui Li, Tim Seemman, Timothy Leffel, Tingting Xiang, Tirth Patel, Tobias Domhan, Tobias Falke, Toby Guo, Tom Li, Tomasz Horszczaruk, Tomasz Jedynak, Tushar Kulkarni, Tyst Marin, Tytus Metrycki, Tzu-Yen Wang, Umang Jain, Upendra Singh, Utkarsh Chirimar, Vaibhav Gupta, Vanshil Shah, Varad Deshpande, Varad Gunjal, Varsha Srikeshava, Varsha Vivek, Varun Bharadwaj, Varun Gangal, Varun Kumar, Venkatesh Elango, Vicente Ordonez, Victor Soto, Vignesh Radhakrishnan, Vihang Patel, Vikram Singh, Vinay Varma Kolanuvada, Vinayshekhar Bannihatti Kumar, Vincent Auvray, Vincent Cartillier, Vincent Ponzo, Violet Peng, Vishal Khandelwal, Vishal Naik, Vishvesh Sahasrabudhe, Vitaliy Korolev, Vivek Gokuladas, Vivek Madan, Vivek Subramanian, Volkan Cevher, Vrinda Gupta, Wael Hamza, Wei Zhang, Weitong Ruan, Weiwei Cheng, Wen Zhang, Wenbo Zhao, Wenyan Yao, Wenzhuo Ouyang, Wesley Dashner, William Campbell, William Lin, Willian Martin, Wyatt Pearson, Xiang Jiang, Xiangxing Lu, Xiangyang Shi, Xianwen Peng, Xiaofeng Gao, Xiaoge Jiang, Xiaohan Fei, Xiaohui Wang, Xiaozhou Joey Zhou, Xin Feng, Xinyan Zhao, Xinyao Wang, Xinyu Li, Xu Zhang, Xuan Wang, Xuandi Fu, Xueling Yuan, Xuning Wang, Yadunandana Rao, Yair Tavizon, Yan Rossiytsev, Yanbei Chen, Yang Liu, Yang Zou, Yangsook Park, Yannick Versley, Yanyan Zhang, Yash Patel, Yen-Cheng Lu, Yi Pan, Yi-Hsiang, Lai, Yichen Hu, Yida Wang, Yiheng Zhou, Yilin Xiang, Ying Shi, Ying Wang, Yishai Galatzer, Yongxin Wang, Yorick Shen, Yuchen Sun, Yudi Purwatama, Yue, Wu, Yue Gu, Yuechun Wang, Yujun Zeng, Yuncong Chen, Yunke Zhou, Yusheng Xie, Yvon Guy, Zbigniew Ambrozinski, Zhaowei Cai, Zhen Zhang, Zheng Wang, Zhenghui Jin, Zhewei Zhao, Zhiheng Li, Zhiheng Luo, Zhikang Zhang, Zhilin Fang, Zhiqi Bu, Zhiyuan Wang, Zhizhong Li, Zijian Wang, Zimeng, Qiu, Zishi Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.12103">The Amazon Nova Family of Models: Technical Report and Model Card</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present Amazon Nova, a new generation of state-of-the-art foundation models that deliver frontier intelligence and industry-leading price performance. Amazon Nova Pro is a highly-capable multimodal model with the best combination of accuracy, speed, and cost for a wide range of tasks. Amazon Nova Lite is a low-cost multimodal model that is lightning fast for processing images, video, documents and text. Amazon Nova Micro is a text-only model that delivers our lowest-latency responses at very low cost. Amazon Nova Canvas is an image generation model that creates professional grade images with rich customization controls. Amazon Nova Reel is a video generation model offering high-quality outputs, customization, and motion control. Our models were built responsibly and with a commitment to customer trust, security, and reliability. We report benchmarking results for core capabilities, agentic performance, long context, functional adaptation, runtime performance, and human evaluation.
<div id='section'>Paperid: <span id='pid'>679, <a href='https://arxiv.org/pdf/2503.17626.pdf' target='_blank'>https://arxiv.org/pdf/2503.17626.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziang Zheng, Guojian Zhan, Bin Shuai, Shengtao Qin, Jiangtao Li, Tao Zhang, Shengbo Eben Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.17626">Transferable Latent-to-Latent Locomotion Policy for Efficient and Versatile Motion Control of Diverse Legged Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reinforcement learning (RL) has demonstrated remarkable capability in acquiring robot skills, but learning each new skill still requires substantial data collection for training. The pretrain-and-finetune paradigm offers a promising approach for efficiently adapting to new robot entities and tasks. Inspired by the idea that acquired knowledge can accelerate learning new tasks with the same robot and help a new robot master a trained task, we propose a latent training framework where a transferable latent-to-latent locomotion policy is pretrained alongside diverse task-specific observation encoders and action decoders. This policy in latent space processes encoded latent observations to generate latent actions to be decoded, with the potential to learn general abstract motion skills. To retain essential information for decision-making and control, we introduce a diffusion recovery module that minimizes information reconstruction loss during pretrain stage. During fine-tune stage, the pretrained latent-to-latent locomotion policy remains fixed, while only the lightweight task-specific encoder and decoder are optimized for efficient adaptation. Our method allows a robot to leverage its own prior experience across different tasks as well as the experience of other morphologically diverse robots to accelerate adaptation. We validate our approach through extensive simulations and real-world experiments, demonstrating that the pretrained latent-to-latent locomotion policy effectively generalizes to new robot entities and tasks with improved efficiency.
<div id='section'>Paperid: <span id='pid'>680, <a href='https://arxiv.org/pdf/2503.14229.pdf' target='_blank'>https://arxiv.org/pdf/2503.14229.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifei Dong, Fengyi Wu, Qi He, Heng Li, Minghan Li, Zebang Cheng, Yuxuan Zhou, Jingdong Sun, Qi Dai, Zhi-Qi Cheng, Alexander G Hauptmann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.14229">HA-VLN: A Benchmark for Human-Aware Navigation in Discrete-Continuous Environments with Dynamic Multi-Human Interactions, Real-World Validation, and an Open Leaderboard</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-and-Language Navigation (VLN) systems often focus on either discrete (panoramic) or continuous (free-motion) paradigms alone, overlooking the complexities of human-populated, dynamic environments. We introduce a unified Human-Aware VLN (HA-VLN) benchmark that merges these paradigms under explicit social-awareness constraints. Our contributions include: 1. A standardized task definition that balances discrete-continuous navigation with personal-space requirements; 2. An enhanced human motion dataset (HAPS 2.0) and upgraded simulators capturing realistic multi-human interactions, outdoor contexts, and refined motion-language alignment; 3. Extensive benchmarking on 16,844 human-centric instructions, revealing how multi-human dynamics and partial observability pose substantial challenges for leading VLN agents; 4. Real-world robot tests validating sim-to-real transfer in crowded indoor spaces; and 5. A public leaderboard supporting transparent comparisons across discrete and continuous tasks. Empirical results show improved navigation success and fewer collisions when social context is integrated, underscoring the need for human-centric design. By releasing all datasets, simulators, agent code, and evaluation tools, we aim to advance safer, more capable, and socially responsible VLN research.
<div id='section'>Paperid: <span id='pid'>681, <a href='https://arxiv.org/pdf/2412.02700.pdf' target='_blank'>https://arxiv.org/pdf/2412.02700.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Daniel Geng, Charles Herrmann, Junhwa Hur, Forrester Cole, Serena Zhang, Tobias Pfaff, Tatiana Lopez-Guevara, Carl Doersch, Yusuf Aytar, Michael Rubinstein, Chen Sun, Oliver Wang, Andrew Owens, Deqing Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.02700">Motion Prompting: Controlling Video Generation with Motion Trajectories</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion control is crucial for generating expressive and compelling video content; however, most existing video generation models rely mainly on text prompts for control, which struggle to capture the nuances of dynamic actions and temporal compositions. To this end, we train a video generation model conditioned on spatio-temporally sparse or dense motion trajectories. In contrast to prior motion conditioning work, this flexible representation can encode any number of trajectories, object-specific or global scene motion, and temporally sparse motion; due to its flexibility we refer to this conditioning as motion prompts. While users may directly specify sparse trajectories, we also show how to translate high-level user requests into detailed, semi-dense motion prompts, a process we term motion prompt expansion. We demonstrate the versatility of our approach through various applications, including camera and object motion control, "interacting" with an image, motion transfer, and image editing. Our results showcase emergent behaviors, such as realistic physics, suggesting the potential of motion prompts for probing video models and interacting with future generative world models. Finally, we evaluate quantitatively, conduct a human study, and demonstrate strong performance. Video results are available on our webpage: https://motion-prompting.github.io/
<div id='section'>Paperid: <span id='pid'>682, <a href='https://arxiv.org/pdf/2411.18521.pdf' target='_blank'>https://arxiv.org/pdf/2411.18521.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Demir Arikan, Peiyao Zhang, Michael Sommersperger, Shervin Dehghani, Mojtaba Esfandiari, Russel H. Taylor, M. Ali Nasseri, Peter Gehlbach, Nassir Navab, Iulian Iordachita
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.18521">Towards Motion Compensation in Autonomous Robotic Subretinal Injections</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Exudative (wet) age-related macular degeneration (AMD) is a leading cause of vision loss in older adults, typically treated with intravitreal injections. Emerging therapies, such as subretinal injections of stem cells, gene therapy, small molecules and RPE cells require precise delivery to avoid damaging delicate retinal structures. Robotic systems can potentially offer the necessary precision for these procedures. This paper presents a novel approach for motion compensation in robotic subretinal injections, utilizing real time Optical Coherence Tomography (OCT). The proposed method leverages B$^5$-scans, a rapid acquisition of small-volume OCT data, for dynamic tracking of retinal motion along the Z-axis, compensating for physiological movements such as breathing and heartbeat. Validation experiments on ex vivo porcine eyes revealed challenges in maintaining a consistent tool-to-retina distance, with deviations of up to 200 $Î¼m$ for 100 $Î¼m$ amplitude motions and over 80 $Î¼m$ for 25 $Î¼m$ amplitude motions over one minute. Subretinal injections faced additional difficulties, with phase shifts causing the needle to move off-target and inject into the vitreous. These results highlight the need for improved motion prediction and horizontal stability to enhance the accuracy and safety of robotic subretinal procedures.
<div id='section'>Paperid: <span id='pid'>683, <a href='https://arxiv.org/pdf/2409.00904.pdf' target='_blank'>https://arxiv.org/pdf/2409.00904.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhanwen Liu, Chao Li, Yang Wang, Nan Yang, Xing Fan, Jiaqi Ma, Xiangmo Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.00904">Multi-scale Temporal Fusion Transformer for Incomplete Vehicle Trajectory Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion prediction plays an essential role in autonomous driving systems, enabling autonomous vehicles to achieve more accurate local-path planning and driving decisions based on predictions of the surrounding vehicles. However, existing methods neglect the potential missing values caused by object occlusion, perception failures, etc., which inevitably degrades the trajectory prediction performance in real traffic scenarios. To address this limitation, we propose a novel end-to-end framework for incomplete vehicle trajectory prediction, named Multi-scale Temporal Fusion Transformer (MTFT), which consists of the Multi-scale Attention Head (MAH) and the Continuity Representation-guided Multi-scale Fusion (CRMF) module. Specifically, the MAH leverages the multi-head attention mechanism to parallelly capture multi-scale motion representation of trajectory from different temporal granularities, thus mitigating the adverse effect of missing values on prediction. Furthermore, the multi-scale motion representation is input into the CRMF module for multi-scale fusion to obtain the robust temporal feature of the vehicle. During the fusion process, the continuity representation of vehicle motion is first extracted across time steps to guide the fusion, ensuring that the resulting temporal feature incorporates both detailed information and the overall trend of vehicle motion, which facilitates the accurate decoding of future trajectory that is consistent with the vehicle's motion trend. We evaluate the proposed model on four datasets derived from highway and urban traffic scenarios. The experimental results demonstrate its superior performance in the incomplete vehicle trajectory prediction task compared with state-of-the-art models, e.g., a comprehensive performance improvement of more than 39% on the HighD dataset.
<div id='section'>Paperid: <span id='pid'>684, <a href='https://arxiv.org/pdf/2406.06211.pdf' target='_blank'>https://arxiv.org/pdf/2406.06211.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Abdulwahab Felemban, Eslam Mohamed Bakr, Xiaoqian Shen, Jian Ding, Abduallah Mohamed, Mohamed Elhoseiny
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.06211">iMotion-LLM: Motion Prediction Instruction Tuning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce iMotion-LLM: a Multimodal Large Language Models (LLMs) with trajectory prediction, tailored to guide interactive multi-agent scenarios. Different from conventional motion prediction approaches, iMotion-LLM capitalizes on textual instructions as key inputs for generating contextually relevant trajectories. By enriching the real-world driving scenarios in the Waymo Open Dataset with textual motion instructions, we created InstructWaymo. Leveraging this dataset, iMotion-LLM integrates a pretrained LLM, fine-tuned with LoRA, to translate scene features into the LLM input space. iMotion-LLM offers significant advantages over conventional motion prediction models. First, it can generate trajectories that align with the provided instructions if it is a feasible direction. Second, when given an infeasible direction, it can reject the instruction, thereby enhancing safety. These findings act as milestones in empowering autonomous navigation systems to interpret and predict the dynamics of multi-agent environments, laying the groundwork for future advancements in this field.
<div id='section'>Paperid: <span id='pid'>685, <a href='https://arxiv.org/pdf/2404.06860.pdf' target='_blank'>https://arxiv.org/pdf/2404.06860.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fulong Ma, Weiqing Qi, Guoyang Zhao, Linwei Zheng, Sheng Wang, Yuxuan Liu, Ming Liu, Jun Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.06860">Monocular 3D lane detection for Autonomous Driving: Recent Achievements, Challenges, and Outlooks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D lane detection is essential in autonomous driving as it extracts structural and traffic information from the road in three-dimensional space, aiding self-driving cars in logical, safe, and comfortable path planning and motion control. Given the cost of sensors and the advantages of visual data in color information, 3D lane detection based on monocular vision is an important research direction in the realm of autonomous driving, increasingly gaining attention in both industry and academia. Regrettably, recent advancements in visual perception seem inadequate for the development of fully reliable 3D lane detection algorithms, which also hampers the progress of vision-based fully autonomous vehicles. We believe that there is still considerable room for improvement in 3D lane detection algorithms for autonomous vehicles using visual sensors, and significant enhancements are needed. This review looks back and analyzes the current state of achievements in the field of 3D lane detection research. It covers all current monocular-based 3D lane detection processes, discusses the performance of these cutting-edge algorithms, analyzes the time complexity of various algorithms, and highlights the main achievements and limitations of ongoing research efforts. The survey also includes a comprehensive discussion of available 3D lane detection datasets and the challenges that researchers face but have not yet resolved. Finally, our work outlines future research directions and invites researchers and practitioners to join this exciting field.
<div id='section'>Paperid: <span id='pid'>686, <a href='https://arxiv.org/pdf/2512.19679.pdf' target='_blank'>https://arxiv.org/pdf/2512.19679.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Farzam Tajdari, Georgios Papaioannou, Riender Happee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.19679">Optimal-coupling-observer AV motion control securing comfort in the presence of cyber attacks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The security of Automated Vehicles (AVs) is an important emerging area of research in traffic safety. Methods have been published and evaluated in experimental vehicles to secure safe AV control in the presence of attacks, but human motion comfort is rarely investigated in such studies. In this paper, we present an innovative optimal-coupling-observer-based framework that rejects the impact of bounded sensor attacks in a network of connected and automated vehicles from safety and comfort point of view. We demonstrate its performance in car following with cooperative adaptive cruise control for platoons with redundant distance and velocity sensors. The error dynamics are formulated as a Linear Time Variant (LTV) system, resulting in complex stability conditions that are investigated using a Linear Matrix Inequality (LMI) approach guaranteeing global asymptotic stability. We prove the capability of the framework to secure occupants' safety and comfort in the presence of bounded attacks. In the onset of attack, the framework rapidly detects attacked sensors and switches to the most reliable observer eliminating attacked sensors, even with modest attack magnitudes. Without our proposed method, severe (but bounded) attacks result in collisions and major discomfort. With our method, attacks had negligible effects on motion comfort evaluated using ISO-2631 Ride Comfort and Motion Sickness indexes. The results pave the path to bring comfort to the forefront of AVs security.
<div id='section'>Paperid: <span id='pid'>687, <a href='https://arxiv.org/pdf/2511.12921.pdf' target='_blank'>https://arxiv.org/pdf/2511.12921.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Huiqiang Sun, Liao Shen, Zhan Peng, Kun Wang, Size Wu, Yuhang Zang, Tianqi Liu, Zihao Huang, Xingyu Zeng, Zhiguo Cao, Wei Li, Chen Change Loy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.12921">Generative Photographic Control for Scene-Consistent Video Cinematic Editing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cinematic storytelling is profoundly shaped by the artful manipulation of photographic elements such as depth of field and exposure. These effects are crucial in conveying mood and creating aesthetic appeal. However, controlling these effects in generative video models remains highly challenging, as most existing methods are restricted to camera motion control. In this paper, we propose CineCtrl, the first video cinematic editing framework that provides fine control over professional camera parameters (e.g., bokeh, shutter speed). We introduce a decoupled cross-attention mechanism to disentangle camera motion from photographic inputs, allowing fine-grained, independent control without compromising scene consistency. To overcome the shortage of training data, we develop a comprehensive data generation strategy that leverages simulated photographic effects with a dedicated real-world collection pipeline, enabling the construction of a large-scale dataset for robust model training. Extensive experiments demonstrate that our model generates high-fidelity videos with precisely controlled, user-specified photographic camera effects.
<div id='section'>Paperid: <span id='pid'>688, <a href='https://arxiv.org/pdf/2511.12251.pdf' target='_blank'>https://arxiv.org/pdf/2511.12251.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaohui Li, Xiaolong Liu, Zhongchen Shi, Wei Chen, Liang Xie, Meng Gai, Jun Cao, Suxia Zhang, Erwei Yin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.12251">Locomotion in CAVE: Enhancing Immersion through Full-Body Motion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cave Automatic Virtual Environment (CAVE) is one of the virtual reality (VR) immersive devices currently used to present virtual environments. However, the locomotion methods in the CAVE are limited by unnatural interaction methods, severely hindering the user experience and immersion in the CAVE. We proposed a locomotion framework for CAVE environments aimed at enhancing the immersive locomotion experience through optimized human motion recognition technology. Firstly, we construct a four-sided display CAVE system, then through the dynamic method based on Perspective-n-Point to calibrate the camera, using the obtained camera intrinsics and extrinsic parameters, and an action recognition architecture to get the action category. At last, transform the action category to a graphical workstation that renders display effects on the screen. We designed a user study to validate the effectiveness of our method. Compared to the traditional methods, our method has significant improvements in realness and self-presence in the virtual environment, effectively reducing motion sickness.
<div id='section'>Paperid: <span id='pid'>689, <a href='https://arxiv.org/pdf/2510.18002.pdf' target='_blank'>https://arxiv.org/pdf/2510.18002.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junli Ren, Junfeng Long, Tao Huang, Huayi Wang, Zirui Wang, Feiyu Jia, Wentao Zhang, Jingbo Wang, Ping Luo, Jiangmiao Pang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.18002">Humanoid Goalkeeper: Learning from Position Conditioned Task-Motion Constraints</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a reinforcement learning framework for autonomous goalkeeping with humanoid robots in real-world scenarios. While prior work has demonstrated similar capabilities on quadrupedal platforms, humanoid goalkeeping introduces two critical challenges: (1) generating natural, human-like whole-body motions, and (2) covering a wider guarding range with an equivalent response time. Unlike existing approaches that rely on separate teleoperation or fixed motion tracking for whole-body control, our method learns a single end-to-end RL policy, enabling fully autonomous, highly dynamic, and human-like robot-object interactions. To achieve this, we integrate multiple human motion priors conditioned on perceptual inputs into the RL training via an adversarial scheme. We demonstrate the effectiveness of our method through real-world experiments, where the humanoid robot successfully performs agile, autonomous, and naturalistic interceptions of fast-moving balls. In addition to goalkeeping, we demonstrate the generalization of our approach through tasks such as ball escaping and grabbing. Our work presents a practical and scalable solution for enabling highly dynamic interactions between robots and moving objects, advancing the field toward more adaptive and lifelike robotic behaviors.
<div id='section'>Paperid: <span id='pid'>690, <a href='https://arxiv.org/pdf/2510.17274.pdf' target='_blank'>https://arxiv.org/pdf/2510.17274.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Katie Luo, Jingwei Ji, Tong He, Runsheng Xu, Yichen Xie, Dragomir Anguelov, Mingxing Tan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.17274">Enhanced Motion Forecasting with Plug-and-Play Multimodal Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current autonomous driving systems rely on specialized models for perceiving and predicting motion, which demonstrate reliable performance in standard conditions. However, generalizing cost-effectively to diverse real-world scenarios remains a significant challenge. To address this, we propose Plug-and-Forecast (PnF), a plug-and-play approach that augments existing motion forecasting models with multimodal large language models (MLLMs). PnF builds on the insight that natural language provides a more effective way to describe and handle complex scenarios, enabling quick adaptation to targeted behaviors. We design prompts to extract structured scene understanding from MLLMs and distill this information into learnable embeddings to augment existing behavior prediction models. Our method leverages the zero-shot reasoning capabilities of MLLMs to achieve significant improvements in motion prediction performance, while requiring no fine-tuning -- making it practical to adopt. We validate our approach on two state-of-the-art motion forecasting models using the Waymo Open Motion Dataset and the nuScenes Dataset, demonstrating consistent performance improvements across both benchmarks.
<div id='section'>Paperid: <span id='pid'>691, <a href='https://arxiv.org/pdf/2510.08260.pdf' target='_blank'>https://arxiv.org/pdf/2510.08260.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mu Li, Yin Wang, Zhiying Leng, Jiapeng Liu, Frederick W. B. Li, Xiaohui Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.08260">Fine-grained text-driven dual-human motion generation via dynamic hierarchical interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human interaction is inherently dynamic and hierarchical, where the dynamic refers to the motion changes with distance, and the hierarchy is from individual to inter-individual and ultimately to overall motion. Exploiting these properties is vital for dual-human motion generation, while existing methods almost model human interaction temporally invariantly, ignoring distance and hierarchy. To address it, we propose a fine-grained dual-human motion generation method, namely FineDual, a tri-stage method to model the dynamic hierarchical interaction from individual to inter-individual. The first stage, Self-Learning Stage, divides the dual-human overall text into individual texts through a Large Language Model, aligning text features and motion features at the individual level. The second stage, Adaptive Adjustment Stage, predicts interaction distance by an interaction distance predictor, modeling human interactions dynamically at the inter-individual level by an interaction-aware graph network. The last stage, Teacher-Guided Refinement Stage, utilizes overall text features as guidance to refine motion features at the overall level, generating fine-grained and high-quality dual-human motion. Extensive quantitative and qualitative evaluations on dual-human motion datasets demonstrate that our proposed FineDual outperforms existing approaches, effectively modeling dynamic hierarchical human interaction.
<div id='section'>Paperid: <span id='pid'>692, <a href='https://arxiv.org/pdf/2509.12880.pdf' target='_blank'>https://arxiv.org/pdf/2509.12880.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anna Deichler, Siyang Wang, Simon Alexanderson, Jonas Beskow
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.12880">Towards Context-Aware Human-like Pointing Gestures with RL Motion Imitation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pointing is a key mode of interaction with robots, yet most prior work has focused on recognition rather than generation. We present a motion capture dataset of human pointing gestures covering diverse styles, handedness, and spatial targets. Using reinforcement learning with motion imitation, we train policies that reproduce human-like pointing while maximizing precision. Results show our approach enables context-aware pointing behaviors in simulation, balancing task performance with natural dynamics.
<div id='section'>Paperid: <span id='pid'>693, <a href='https://arxiv.org/pdf/2508.09383.pdf' target='_blank'>https://arxiv.org/pdf/2508.09383.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guoxian Song, Hongyi Xu, Xiaochen Zhao, You Xie, Tianpei Gu, Zenan Li, Chenxu Zhang, Linjie Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09383">X-UniMotion: Animating Human Images with Expressive, Unified and Identity-Agnostic Motion Latents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present X-UniMotion, a unified and expressive implicit latent representation for whole-body human motion, encompassing facial expressions, body poses, and hand gestures. Unlike prior motion transfer methods that rely on explicit skeletal poses and heuristic cross-identity adjustments, our approach encodes multi-granular motion directly from a single image into a compact set of four disentangled latent tokens -- one for facial expression, one for body pose, and one for each hand. These motion latents are both highly expressive and identity-agnostic, enabling high-fidelity, detailed cross-identity motion transfer across subjects with diverse identities, poses, and spatial configurations. To achieve this, we introduce a self-supervised, end-to-end framework that jointly learns the motion encoder and latent representation alongside a DiT-based video generative model, trained on large-scale, diverse human motion datasets. Motion-identity disentanglement is enforced via 2D spatial and color augmentations, as well as synthetic 3D renderings of cross-identity subject pairs under shared poses. Furthermore, we guide motion token learning with auxiliary decoders that promote fine-grained, semantically aligned, and depth-aware motion embeddings. Extensive experiments show that X-UniMotion outperforms state-of-the-art methods, producing highly expressive animations with superior motion fidelity and identity preservation.
<div id='section'>Paperid: <span id='pid'>694, <a href='https://arxiv.org/pdf/2507.14694.pdf' target='_blank'>https://arxiv.org/pdf/2507.14694.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yue Ma, Kanglei Zhou, Fuyang Yu, Frederick W. B. Li, Xiaohui Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.14694">Uncertainty-aware Probabilistic 3D Human Motion Forecasting via Invertible Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D human motion forecasting aims to enable autonomous applications. Estimating uncertainty for each prediction (i.e., confidence based on probability density or quantile) is essential for safety-critical contexts like human-robot collaboration to minimize risks. However, existing diverse motion forecasting approaches struggle with uncertainty quantification due to implicit probabilistic representations hindering uncertainty modeling. We propose ProbHMI, which introduces invertible networks to parameterize poses in a disentangled latent space, enabling probabilistic dynamics modeling. A forecasting module then explicitly predicts future latent distributions, allowing effective uncertainty quantification. Evaluated on benchmarks, ProbHMI achieves strong performance for both deterministic and diverse prediction while validating uncertainty calibration, critical for risk-aware decision making.
<div id='section'>Paperid: <span id='pid'>695, <a href='https://arxiv.org/pdf/2507.07356.pdf' target='_blank'>https://arxiv.org/pdf/2507.07356.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kangning Yin, Weishuai Zeng, Ke Fan, Minyue Dai, Zirui Wang, Qiang Zhang, Zheng Tian, Jingbo Wang, Jiangmiao Pang, Weinan Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07356">UniTracker: Learning Universal Whole-Body Motion Tracker for Humanoid Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Achieving expressive and generalizable whole-body motion control is essential for deploying humanoid robots in real-world environments. In this work, we propose UniTracker, a three-stage training framework that enables robust and scalable motion tracking across a wide range of human behaviors. In the first stage, we train a teacher policy with privileged observations to generate high-quality actions. In the second stage, we introduce a Conditional Variational Autoencoder (CVAE) to model a universal student policy that can be deployed directly on real hardware. The CVAE structure allows the policy to learn a global latent representation of motion, enhancing generalization to unseen behaviors and addressing the limitations of standard MLP-based policies under partial observations. Unlike pure MLPs that suffer from drift in global attributes like orientation, our CVAE-student policy incorporates global intent during training by aligning a partial-observation prior to the full-observation encoder. In the third stage, we introduce a fast adaptation module that fine-tunes the universal policy on harder motion sequences that are difficult to track directly. This adaptation can be performed both for single sequences and in batch mode, further showcasing the flexibility and scalability of our approach. We evaluate UniTracker in both simulation and real-world settings using a Unitree G1 humanoid, demonstrating strong performance in motion diversity, tracking accuracy, and deployment robustness.
<div id='section'>Paperid: <span id='pid'>696, <a href='https://arxiv.org/pdf/2507.06590.pdf' target='_blank'>https://arxiv.org/pdf/2507.06590.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yin Wang, Mu li, Zhiying Leng, Frederick W. B. Li, Xiaohui Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06590">MOST: Motion Diffusion Model for Rare Text via Temporal Clip Banzhaf Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce MOST, a novel motion diffusion model via temporal clip Banzhaf interaction, aimed at addressing the persistent challenge of generating human motion from rare language prompts. While previous approaches struggle with coarse-grained matching and overlook important semantic cues due to motion redundancy, our key insight lies in leveraging fine-grained clip relationships to mitigate these issues. MOST's retrieval stage presents the first formulation of its kind - temporal clip Banzhaf interaction - which precisely quantifies textual-motion coherence at the clip level. This facilitates direct, fine-grained text-to-motion clip matching and eliminates prevalent redundancy. In the generation stage, a motion prompt module effectively utilizes retrieved motion clips to produce semantically consistent movements. Extensive evaluations confirm that MOST achieves state-of-the-art text-to-motion retrieval and generation performance by comprehensively addressing previous challenges, as demonstrated through quantitative and qualitative results highlighting its effectiveness, especially for rare prompts.
<div id='section'>Paperid: <span id='pid'>697, <a href='https://arxiv.org/pdf/2507.04522.pdf' target='_blank'>https://arxiv.org/pdf/2507.04522.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anna Deichler, Jim O'Regan, Teo Guichoux, David Johansson, Jonas Beskow
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.04522">Grounded Gesture Generation: Language, Motion, and Space</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion generation has advanced rapidly in recent years, yet the critical problem of creating spatially grounded, context-aware gestures has been largely overlooked. Existing models typically specialize either in descriptive motion generation, such as locomotion and object interaction, or in isolated co-speech gesture synthesis aligned with utterance semantics. However, both lines of work often treat motion and environmental grounding separately, limiting advances toward embodied, communicative agents. To address this gap, our work introduces a multimodal dataset and framework for grounded gesture generation, combining two key resources: (1) a synthetic dataset of spatially grounded referential gestures, and (2) MM-Conv, a VR-based dataset capturing two-party dialogues. Together, they provide over 7.7 hours of synchronized motion, speech, and 3D scene information, standardized in the HumanML3D format. Our framework further connects to a physics-based simulator, enabling synthetic data generation and situated evaluation. By bridging gesture modeling and spatial grounding, our contribution establishes a foundation for advancing research in situated gesture generation and grounded multimodal interaction.
  Project page: https://groundedgestures.github.io/
<div id='section'>Paperid: <span id='pid'>698, <a href='https://arxiv.org/pdf/2506.24086.pdf' target='_blank'>https://arxiv.org/pdf/2506.24086.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bingfan Zhu, Biao Jiang, Sunyi Wang, Shixiang Tang, Tao Chen, Linjie Luo, Youyi Zheng, Xin Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.24086">MotionGPT3: Human Motion as a Second Modality</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Though recent advances in multimodal models have demonstrated strong capabilities and opportunities in unified understanding and generation, the development of unified motion-language models remains underexplored. To enable such models with high-fidelity human motion, two core challenges must be addressed. The first is the reconstruction gap between the continuous motion modality and discrete representation in an autoregressive manner, and the second is the degradation of language intelligence during unified training. Inspired by the mixture of experts, we propose MotionGPT3, a bimodal motion-language model that treats human motion as a second modality, decoupling motion modeling via separate model parameters and enabling both effective cross-modal interaction and efficient multimodal scaling training. To preserve language intelligence, the text branch retains the original structure and parameters of the pretrained language model, while a new motion branch is integrated via a shared attention mechanism, enabling bidirectional information flow between two modalities. We first employ a motion Variational Autoencoder (VAE) to encode raw human motion into latent representations. Based on this continuous latent space, the motion branch predicts motion latents directly from intermediate hidden states using a diffusion head, bypassing discrete tokenization. Extensive experiments show that our approach achieves competitive performance on both motion understanding and generation tasks while preserving strong language capabilities, establishing a unified bimodal motion diffusion framework within an autoregressive manner.
<div id='section'>Paperid: <span id='pid'>699, <a href='https://arxiv.org/pdf/2506.07713.pdf' target='_blank'>https://arxiv.org/pdf/2506.07713.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ge Wang, Songlin Fan, Hangxu Liu, Quanjian Song, Hewei Wang, Jinfeng Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.07713">Consistent Video Editing as Flow-Driven Image-to-Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the prosper of video diffusion models, down-stream applications like video editing have been significantly promoted without consuming much computational cost. One particular challenge in this task lies at the motion transfer process from the source video to the edited one, where it requires the consideration of the shape deformation in between, meanwhile maintaining the temporal consistency in the generated video sequence. However, existing methods fail to model complicated motion patterns for video editing, and are fundamentally limited to object replacement, where tasks with non-rigid object motions like multi-object and portrait editing are largely neglected. In this paper, we observe that optical flows offer a promising alternative in complex motion modeling, and present FlowV2V to re-investigate video editing as a task of flow-driven Image-to-Video (I2V) generation. Specifically, FlowV2V decomposes the entire pipeline into first-frame editing and conditional I2V generation, and simulates pseudo flow sequence that aligns with the deformed shape, thus ensuring the consistency during editing. Experimental results on DAVIS-EDIT with improvements of 13.67% and 50.66% on DOVER and warping error illustrate the superior temporal consistency and sample quality of FlowV2V compared to existing state-of-the-art ones. Furthermore, we conduct comprehensive ablation studies to analyze the internal functionalities of the first-frame paradigm and flow alignment in the proposed method.
<div id='section'>Paperid: <span id='pid'>700, <a href='https://arxiv.org/pdf/2506.02661.pdf' target='_blank'>https://arxiv.org/pdf/2506.02661.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingyang Huang, Peng Zhang, Bang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.02661">MotionRAG-Diff: A Retrieval-Augmented Diffusion Framework for Long-Term Music-to-Dance Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating long-term, coherent, and realistic music-conditioned dance sequences remains a challenging task in human motion synthesis. Existing approaches exhibit critical limitations: motion graph methods rely on fixed template libraries, restricting creative generation; diffusion models, while capable of producing novel motions, often lack temporal coherence and musical alignment. To address these challenges, we propose $\textbf{MotionRAG-Diff}$, a hybrid framework that integrates Retrieval-Augmented Generation (RAG) with diffusion-based refinement to enable high-quality, musically coherent dance generation for arbitrary long-term music inputs. Our method introduces three core innovations: (1) A cross-modal contrastive learning architecture that aligns heterogeneous music and dance representations in a shared latent space, establishing unsupervised semantic correspondence without paired data; (2) An optimized motion graph system for efficient retrieval and seamless concatenation of motion segments, ensuring realism and temporal coherence across long sequences; (3) A multi-condition diffusion model that jointly conditions on raw music signals and contrastive features to enhance motion quality and global synchronization. Extensive experiments demonstrate that MotionRAG-Diff achieves state-of-the-art performance in motion quality, diversity, and music-motion synchronization accuracy. This work establishes a new paradigm for music-driven dance generation by synergizing retrieval-based template fidelity with diffusion-based creative enhancement.
<div id='section'>Paperid: <span id='pid'>701, <a href='https://arxiv.org/pdf/2504.03939.pdf' target='_blank'>https://arxiv.org/pdf/2504.03939.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianle Wu, Mojtaba Esfandiari, Peiyao Zhang, Russell H. Taylor, Peter Gehlbach, Iulian Iordachita
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.03939">Deep Learning-Enhanced Robotic Subretinal Injection with Real-Time Retinal Motion Compensation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Subretinal injection is a critical procedure for delivering therapeutic agents to treat retinal diseases such as age-related macular degeneration (AMD). However, retinal motion caused by physiological factors such as respiration and heartbeat significantly impacts precise needle positioning, increasing the risk of retinal pigment epithelium (RPE) damage. This paper presents a fully autonomous robotic subretinal injection system that integrates intraoperative optical coherence tomography (iOCT) imaging and deep learning-based motion prediction to synchronize needle motion with retinal displacement. A Long Short-Term Memory (LSTM) neural network is used to predict internal limiting membrane (ILM) motion, outperforming a Fast Fourier Transform (FFT)-based baseline model. Additionally, a real-time registration framework aligns the needle tip position with the robot's coordinate frame. Then, a dynamic proportional speed control strategy ensures smooth and adaptive needle insertion. Experimental validation in both simulation and ex vivo open-sky porcine eyes demonstrates precise motion synchronization and successful subretinal injections. The experiment achieves a mean tracking error below 16.4 Î¼m in pre-insertion phases. These results show the potential of AI-driven robotic assistance to improve the safety and accuracy of retinal microsurgery.
<div id='section'>Paperid: <span id='pid'>702, <a href='https://arxiv.org/pdf/2503.23300.pdf' target='_blank'>https://arxiv.org/pdf/2503.23300.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenqi Jia, Bolin Lai, Miao Liu, Danfei Xu, James M. Rehg
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.23300">Learning Predictive Visuomotor Coordination</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding and predicting human visuomotor coordination is crucial for applications in robotics, human-computer interaction, and assistive technologies. This work introduces a forecasting-based task for visuomotor modeling, where the goal is to predict head pose, gaze, and upper-body motion from egocentric visual and kinematic observations. We propose a \textit{Visuomotor Coordination Representation} (VCR) that learns structured temporal dependencies across these multimodal signals. We extend a diffusion-based motion modeling framework that integrates egocentric vision and kinematic sequences, enabling temporally coherent and accurate visuomotor predictions. Our approach is evaluated on the large-scale EgoExo4D dataset, demonstrating strong generalization across diverse real-world activities. Our results highlight the importance of multimodal integration in understanding visuomotor coordination, contributing to research in visuomotor learning and human behavior modeling.
<div id='section'>Paperid: <span id='pid'>703, <a href='https://arxiv.org/pdf/2503.13120.pdf' target='_blank'>https://arxiv.org/pdf/2503.13120.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Siyuan Fan, Wenke Huang, Xiantao Cai, Bo Du
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.13120">3D Human Interaction Generation: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D human interaction generation has emerged as a key research area, focusing on producing dynamic and contextually relevant interactions between humans and various interactive entities. Recent rapid advancements in 3D model representation methods, motion capture technologies, and generative models have laid a solid foundation for the growing interest in this domain. Existing research in this field can be broadly categorized into three areas: human-scene interaction, human-object interaction, and human-human interaction. Despite the rapid advancements in this area, challenges remain due to the need for naturalness in human motion generation and the accurate interaction between humans and interactive entities. In this survey, we present a comprehensive literature review of human interaction generation, which, to the best of our knowledge, is the first of its kind. We begin by introducing the foundational technologies, including model representations, motion capture methods, and generative models. Subsequently, we introduce the approaches proposed for the three sub-tasks, along with their corresponding datasets and evaluation metrics. Finally, we discuss potential future research directions in this area and conclude the survey. Through this survey, we aim to offer a comprehensive overview of the current advancements in the field, highlight key challenges, and inspire future research works.
<div id='section'>Paperid: <span id='pid'>704, <a href='https://arxiv.org/pdf/2502.17414.pdf' target='_blank'>https://arxiv.org/pdf/2502.17414.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zeyuan Chen, Hongyi Xu, Guoxian Song, You Xie, Chenxu Zhang, Xin Chen, Chao Wang, Di Chang, Linjie Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.17414">X-Dancer: Expressive Music to Human Dance Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present X-Dancer, a novel zero-shot music-driven image animation pipeline that creates diverse and long-range lifelike human dance videos from a single static image. As its core, we introduce a unified transformer-diffusion framework, featuring an autoregressive transformer model that synthesize extended and music-synchronized token sequences for 2D body, head and hands poses, which then guide a diffusion model to produce coherent and realistic dance video frames. Unlike traditional methods that primarily generate human motion in 3D, X-Dancer addresses data limitations and enhances scalability by modeling a wide spectrum of 2D dance motions, capturing their nuanced alignment with musical beats through readily available monocular videos. To achieve this, we first build a spatially compositional token representation from 2D human pose labels associated with keypoint confidences, encoding both large articulated body movements (e.g., upper and lower body) and fine-grained motions (e.g., head and hands). We then design a music-to-motion transformer model that autoregressively generates music-aligned dance pose token sequences, incorporating global attention to both musical style and prior motion context. Finally we leverage a diffusion backbone to animate the reference image with these synthesized pose tokens through AdaIN, forming a fully differentiable end-to-end framework. Experimental results demonstrate that X-Dancer is able to produce both diverse and characterized dance videos, substantially outperforming state-of-the-art methods in term of diversity, expressiveness and realism. Code and model will be available for research purposes.
<div id='section'>Paperid: <span id='pid'>705, <a href='https://arxiv.org/pdf/2502.05534.pdf' target='_blank'>https://arxiv.org/pdf/2502.05534.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yin Wang, Mu Li, Jiapeng Liu, Zhiying Leng, Frederick W. B. Li, Ziyao Zhang, Xiaohui Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.05534">Fg-T2M++: LLMs-Augmented Fine-Grained Text Driven Human Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We address the challenging problem of fine-grained text-driven human motion generation. Existing works generate imprecise motions that fail to accurately capture relationships specified in text due to: (1) lack of effective text parsing for detailed semantic cues regarding body parts, (2) not fully modeling linguistic structures between words to comprehend text comprehensively. To tackle these limitations, we propose a novel fine-grained framework Fg-T2M++ that consists of: (1) an LLMs semantic parsing module to extract body part descriptions and semantics from text, (2) a hyperbolic text representation module to encode relational information between text units by embedding the syntactic dependency graph into hyperbolic space, and (3) a multi-modal fusion module to hierarchically fuse text and motion features. Extensive experiments on HumanML3D and KIT-ML datasets demonstrate that Fg-T2M++ outperforms SOTA methods, validating its ability to accurately generate motions adhering to comprehensive text semantics.
<div id='section'>Paperid: <span id='pid'>706, <a href='https://arxiv.org/pdf/2412.11552.pdf' target='_blank'>https://arxiv.org/pdf/2412.11552.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mario Rosenfelder, Hendrik Carius, Markus Herrmann-Wicklmayr, Peter Eberhard, Kathrin FlaÃkamp, Henrik Ebel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.11552">Efficient Avoidance of Ellipsoidal Obstacles with Model Predictive Control for Mobile Robots and Vehicles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In real-world applications of mobile robots, collision avoidance is of critical importance. Typically, global motion planning in constrained environments is addressed through high-level control schemes. However, additionally integrating local collision avoidance into robot motion control offers significant advantages. For instance, it reduces the reliance on heuristics and conservatism that can arise from a two-stage approach separating local collision avoidance and control. Moreover, using model predictive control (MPC), a robot's full potential can be harnessed by considering jointly local collision avoidance, the robot's dynamics, and actuation constraints. In this context, the present paper focuses on obstacle avoidance for wheeled mobile robots, where both the robot's and obstacles' occupied volumes are modeled as ellipsoids. To this end, a computationally efficient overlap test, that works for arbitrary ellipsoids, is conducted and novelly integrated into the MPC framework. We propose a particularly efficient implementation tailored to robots moving in the plane. The functionality of the proposed obstacle-avoiding MPC is demonstrated for two exemplary types of kinematics by means of simulations. A hardware experiment using a real-world wheeled mobile robot shows transferability to reality and real-time applicability. The general computational approach to ellipsoidal obstacle avoidance can also be applied to other robotic systems and vehicles as well as three-dimensional scenarios.
<div id='section'>Paperid: <span id='pid'>707, <a href='https://arxiv.org/pdf/2411.19258.pdf' target='_blank'>https://arxiv.org/pdf/2411.19258.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Amon Lahr, Joshua NÃ¤f, Kim P. Wabersich, Jonathan Frey, Pascal Siehl, Andrea Carron, Moritz Diehl, Melanie N. Zeilinger
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.19258">L4acados: Learning-based models for acados, applied to Gaussian process-based predictive control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Incorporating learning-based models, such as artificial neural networks or Gaussian processes, into model predictive control (MPC) strategies can significantly improve control performance and online adaptation capabilities for real-world applications. Still, enabling state-of-the-art implementations of learning-based models for MPC is complicated by the challenge of interfacing machine learning frameworks with real-time optimal control software. This work aims at filling this gap by incorporating external sensitivities in sequential quadratic programming solvers for nonlinear optimal control. To this end, we provide L4acados, a general framework for incorporating Python-based residual models in the real-time optimal control software acados. By computing external sensitivities via a user-defined Python module, L4acados enables the implementation of MPC controllers with learning-based residual models in acados, while supporting parallelization of sensitivity computations when preparing the quadratic subproblems. We demonstrate significant speed-ups and superior scaling properties of L4acados compared to available software using a neural-network-based control example. Last, we provide an efficient and modular real-time implementation of Gaussian process-based MPC using L4acados, which is applied to two hardware examples: autonomous miniature racing, as well as motion control of a full-scale autonomous vehicle for an ISO lane change maneuver.
<div id='section'>Paperid: <span id='pid'>708, <a href='https://arxiv.org/pdf/2409.09777.pdf' target='_blank'>https://arxiv.org/pdf/2409.09777.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haisheng Su, Wei Wu, Junchi Yan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.09777">DiFSD: Ego-Centric Fully Sparse Paradigm with Uncertainty Denoising and Iterative Refinement for Efficient End-to-End Self-Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current end-to-end autonomous driving methods resort to unifying modular designs for various tasks (e.g. perception, prediction and planning). Although optimized in a planning-oriented spirit with a fully differentiable framework, existing end-to-end driving systems without ego-centric designs still suffer from unsatisfactory performance and inferior efficiency, owing to the rasterized scene representation learning and redundant information transmission. In this paper, we revisit the human driving behavior and propose an ego-centric fully sparse paradigm, named DiFSD, for end-to-end self-driving. Specifically, DiFSD mainly consists of sparse perception, hierarchical interaction and iterative motion planner. The sparse perception module performs detection, tracking and online mapping based on sparse representation of the driving scene. The hierarchical interaction module aims to select the Closest In-Path Vehicle / Stationary (CIPV / CIPS) from coarse to fine, benefiting from an additional geometric prior. As for the iterative motion planner, both selected interactive agents and ego-vehicle are considered for joint motion prediction, where the output multi-modal ego-trajectories are optimized in an iterative fashion. Besides, both position-level motion diffusion and trajectory-level planning denoising are introduced for uncertainty modeling, thus facilitating the training stability and convergence of the whole framework. Extensive experiments conducted on nuScenes and Bench2Drive datasets demonstrate the superior planning performance and great efficiency of DiFSD.
<div id='section'>Paperid: <span id='pid'>709, <a href='https://arxiv.org/pdf/2403.15931.pdf' target='_blank'>https://arxiv.org/pdf/2403.15931.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>You Xie, Hongyi Xu, Guoxian Song, Chao Wang, Yichun Shi, Linjie Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.15931">X-Portrait: Expressive Portrait Animation with Hierarchical Motion Attention</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose X-Portrait, an innovative conditional diffusion model tailored for generating expressive and temporally coherent portrait animation. Specifically, given a single portrait as appearance reference, we aim to animate it with motion derived from a driving video, capturing both highly dynamic and subtle facial expressions along with wide-range head movements. As its core, we leverage the generative prior of a pre-trained diffusion model as the rendering backbone, while achieve fine-grained head pose and expression control with novel controlling signals within the framework of ControlNet. In contrast to conventional coarse explicit controls such as facial landmarks, our motion control module is learned to interpret the dynamics directly from the original driving RGB inputs. The motion accuracy is further enhanced with a patch-based local control module that effectively enhance the motion attention to small-scale nuances like eyeball positions. Notably, to mitigate the identity leakage from the driving signals, we train our motion control modules with scaling-augmented cross-identity images, ensuring maximized disentanglement from the appearance reference modules. Experimental results demonstrate the universal effectiveness of X-Portrait across a diverse range of facial portraits and expressive driving sequences, and showcase its proficiency in generating captivating portrait animations with consistently maintained identity characteristics.
<div id='section'>Paperid: <span id='pid'>710, <a href='https://arxiv.org/pdf/2403.11469.pdf' target='_blank'>https://arxiv.org/pdf/2403.11469.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaxu Zhang, Xin Chen, Gang Yu, Zhigang Tu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.11469">Generative Motion Stylization of Cross-structure Characters within Canonical Motion Space</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Stylized motion breathes life into characters. However, the fixed skeleton structure and style representation hinder existing data-driven motion synthesis methods from generating stylized motion for various characters. In this work, we propose a generative motion stylization pipeline, named MotionS, for synthesizing diverse and stylized motion on cross-structure characters using cross-modality style prompts. Our key insight is to embed motion style into a cross-modality latent space and perceive the cross-structure skeleton topologies, allowing for motion stylization within a canonical motion space. Specifically, the large-scale Contrastive-Language-Image-Pre-training (CLIP) model is leveraged to construct the cross-modality latent space, enabling flexible style representation within it. Additionally, two topology-encoded tokens are learned to capture the canonical and specific skeleton topologies, facilitating cross-structure topology shifting. Subsequently, the topology-shifted stylization diffusion is designed to generate motion content for the particular skeleton and stylize it in the shifted canonical motion space using multi-modality style descriptions. Through an extensive set of examples, we demonstrate the flexibility and generalizability of our pipeline across various characters and style descriptions. Qualitative and quantitative comparisons show the superiority of our pipeline over state-of-the-arts, consistently delivering high-quality stylized motion across a broad spectrum of skeletal structures.
<div id='section'>Paperid: <span id='pid'>711, <a href='https://arxiv.org/pdf/2512.07673.pdf' target='_blank'>https://arxiv.org/pdf/2512.07673.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Matthias Heyrman, Chenhao Li, Victor Klemm, Dongho Kang, Stelian Coros, Marco Hutter
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.07673">Multi-Domain Motion Embedding: Expressive Real-Time Mimicry for Legged Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Effective motion representation is crucial for enabling robots to imitate expressive behaviors in real time, yet existing motion controllers often ignore inherent patterns in motion. Previous efforts in representation learning do not attempt to jointly capture structured periodic patterns and irregular variations in human and animal movement. To address this, we present Multi-Domain Motion Embedding (MDME), a motion representation that unifies the embedding of structured and unstructured features using a wavelet-based encoder and a probabilistic embedding in parallel. This produces a rich representation of reference motions from a minimal input set, enabling improved generalization across diverse motion styles and morphologies. We evaluate MDME on retargeting-free real-time motion imitation by conditioning robot control policies on the learned embeddings, demonstrating accurate reproduction of complex trajectories on both humanoid and quadruped platforms. Our comparative studies confirm that MDME outperforms prior approaches in reconstruction fidelity and generalizability to unseen motions. Furthermore, we demonstrate that MDME can reproduce novel motion styles in real-time through zero-shot deployment, eliminating the need for task-specific tuning or online retargeting. These results position MDME as a generalizable and structure-aware foundation for scalable real-time robot imitation.
<div id='section'>Paperid: <span id='pid'>712, <a href='https://arxiv.org/pdf/2511.09241.pdf' target='_blank'>https://arxiv.org/pdf/2511.09241.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxi Wei, Zirui Wang, Kangning Yin, Yue Hu, Jingbo Wang, Siheng Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.09241">Unveiling the Impact of Data and Model Scaling on High-Level Control for Humanoid Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Data scaling has long remained a critical bottleneck in robot learning. For humanoid robots, human videos and motion data are abundant and widely available, offering a free and large-scale data source. Besides, the semantics related to the motions enable modality alignment and high-level robot control learning. However, how to effectively mine raw video, extract robot-learnable representations, and leverage them for scalable learning remains an open problem. To address this, we introduce Humanoid-Union, a large-scale dataset generated through an autonomous pipeline, comprising over 260 hours of diverse, high-quality humanoid robot motion data with semantic annotations derived from human motion videos. The dataset can be further expanded via the same pipeline. Building on this data resource, we propose SCHUR, a scalable learning framework designed to explore the impact of large-scale data on high-level control in humanoid robots. Experimental results demonstrate that SCHUR achieves high robot motion generation quality and strong text-motion alignment under data and model scaling, with 37\% reconstruction improvement under MPJPE and 25\% alignment improvement under FID comparing with previous methods. Its effectiveness is further validated through deployment in real-world humanoid robot.
<div id='section'>Paperid: <span id='pid'>713, <a href='https://arxiv.org/pdf/2510.12363.pdf' target='_blank'>https://arxiv.org/pdf/2510.12363.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiale Fan, Andrei Cramariuc, Tifanny Portela, Marco Hutter
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.12363">Pretraining in Actor-Critic Reinforcement Learning for Robot Motion Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The pretraining-finetuning paradigm has facilitated numerous transformative advancements in artificial intelligence research in recent years. However, in the domain of reinforcement learning (RL) for robot motion control, individual skills are often learned from scratch despite the high likelihood that some generalizable knowledge is shared across all task-specific policies belonging to a single robot embodiment. This work aims to define a paradigm for pretraining neural network models that encapsulate such knowledge and can subsequently serve as a basis for warm-starting the RL process in classic actor-critic algorithms, such as Proximal Policy Optimization (PPO). We begin with a task-agnostic exploration-based data collection algorithm to gather diverse, dynamic transition data, which is then used to train a Proprioceptive Inverse Dynamics Model (PIDM) through supervised learning. The pretrained weights are loaded into both the actor and critic networks to warm-start the policy optimization of actual tasks. We systematically validated our proposed method on seven distinct robot motion control tasks, showing significant benefits to this initialization strategy. Our proposed approach on average improves sample efficiency by 40.1% and task performance by 7.5%, compared to random initialization. We further present key ablation studies and empirical analyses that shed light on the mechanisms behind the effectiveness of our method.
<div id='section'>Paperid: <span id='pid'>714, <a href='https://arxiv.org/pdf/2510.05070.pdf' target='_blank'>https://arxiv.org/pdf/2510.05070.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Siheng Zhao, Yanjie Ze, Yue Wang, C. Karen Liu, Pieter Abbeel, Guanya Shi, Rocky Duan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.05070">ResMimic: From General Motion Tracking to Humanoid Whole-body Loco-Manipulation via Residual Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humanoid whole-body loco-manipulation promises transformative capabilities for daily service and warehouse tasks. While recent advances in general motion tracking (GMT) have enabled humanoids to reproduce diverse human motions, these policies lack the precision and object awareness required for loco-manipulation. To this end, we introduce ResMimic, a two-stage residual learning framework for precise and expressive humanoid control from human motion data. First, a GMT policy, trained on large-scale human-only motion, serves as a task-agnostic base for generating human-like whole-body movements. An efficient but precise residual policy is then learned to refine the GMT outputs to improve locomotion and incorporate object interaction. To further facilitate efficient training, we design (i) a point-cloud-based object tracking reward for smoother optimization, (ii) a contact reward that encourages accurate humanoid body-object interactions, and (iii) a curriculum-based virtual object controller to stabilize early training. We evaluate ResMimic in both simulation and on a real Unitree G1 humanoid. Results show substantial gains in task success, training efficiency, and robustness over strong baselines. Videos are available at https://resmimic.github.io/ .
<div id='section'>Paperid: <span id='pid'>715, <a href='https://arxiv.org/pdf/2509.00339.pdf' target='_blank'>https://arxiv.org/pdf/2509.00339.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Md. Taherul Islam Shawon, Yuan Li, Yincai Cai, Junjie Niu, Ting Peng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.00339">Autonomous Aggregate Sorting in Construction and Mining via Computer Vision-Aided Robotic Arm Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Traditional aggregate sorting methods, whether manual or mechanical, often suffer from low precision, limited flexibility, and poor adaptability to diverse material properties such as size, shape, and lithology. To address these limitations, this study presents a computer vision-aided robotic arm system designed for autonomous aggregate sorting in construction and mining applications. The system integrates a six-degree-of-freedom robotic arm, a binocular stereo camera for 3D perception, and a ROS-based control framework. Core techniques include an attention-augmented YOLOv8 model for aggregate detection, stereo matching for 3D localization, Denavit-Hartenberg kinematic modeling for arm motion control, minimum enclosing rectangle analysis for size estimation, and hand-eye calibration for precise coordinate alignment. Experimental validation with four aggregate types achieved an average grasping and sorting success rate of 97.5%, with comparable classification accuracy. Remaining challenges include the reliable handling of small aggregates and texture-based misclassification. Overall, the proposed system demonstrates significant potential to enhance productivity, reduce operational costs, and improve safety in aggregate handling, while providing a scalable framework for advancing smart automation in construction, mining, and recycling industries.
<div id='section'>Paperid: <span id='pid'>716, <a href='https://arxiv.org/pdf/2508.21556.pdf' target='_blank'>https://arxiv.org/pdf/2508.21556.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ilya A. Petrov, Vladimir Guzov, Riccardo Marin, Emre Aksan, Xu Chen, Daniel Cremers, Thabo Beeler, Gerard Pons-Moll
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.21556">ECHO: Ego-Centric modeling of Human-Object interactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modeling human-object interactions (HOI) from an egocentric perspective is a largely unexplored yet important problem due to the increasing adoption of wearable devices, such as smart glasses and watches. We investigate how much information about interaction can be recovered from only head and wrists tracking. Our answer is ECHO (Ego-Centric modeling of Human-Object interactions), which, for the first time, proposes a unified framework to recover three modalities: human pose, object motion, and contact from such minimal observation. ECHO employs a Diffusion Transformer architecture and a unique three-variate diffusion process, which jointly models human motion, object trajectory, and contact sequence, allowing for flexible input configurations. Our method operates in a head-centric canonical space, enhancing robustness to global orientation. We propose a conveyor-based inference, which progressively increases the diffusion timestamp with the frame position, allowing us to process sequences of any length. Through extensive evaluation, we demonstrate that ECHO outperforms existing methods that do not offer the same flexibility, setting a state-of-the-art in egocentric HOI reconstruction.
<div id='section'>Paperid: <span id='pid'>717, <a href='https://arxiv.org/pdf/2508.01590.pdf' target='_blank'>https://arxiv.org/pdf/2508.01590.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hua Yu, Jiao Liu, Xu Gui, Melvin Wong, Yaqing Hou, Yew-Soon Ong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01590">A Plug-and-Play Multi-Criteria Guidance for Diverse In-Betweening Human Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In-betweening human motion generation aims to synthesize intermediate motions that transition between user-specified keyframes. In addition to maintaining smooth transitions, a crucial requirement of this task is to generate diverse motion sequences. It is still challenging to maintain diversity, particularly when it is necessary for the motions within a generated batch sampling to differ meaningfully from one another due to complex motion dynamics. In this paper, we propose a novel method, termed the Multi-Criteria Guidance with In-Betweening Motion Model (MCG-IMM), for in-betweening human motion generation. A key strength of MCG-IMM lies in its plug-and-play nature: it enhances the diversity of motions generated by pretrained models without introducing additional parameters This is achieved by providing a sampling process of pretrained generative models with multi-criteria guidance. Specifically, MCG-IMM reformulates the sampling process of pretrained generative model as a multi-criteria optimization problem, and introduces an optimization process to explore motion sequences that satisfy multiple criteria, e.g., diversity and smoothness. Moreover, our proposed plug-and-play multi-criteria guidance is compatible with different families of generative models, including denoised diffusion probabilistic models, variational autoencoders, and generative adversarial networks. Experiments on four popular human motion datasets demonstrate that MCG-IMM consistently state-of-the-art methods in in-betweening motion generation task.
<div id='section'>Paperid: <span id='pid'>718, <a href='https://arxiv.org/pdf/2507.16841.pdf' target='_blank'>https://arxiv.org/pdf/2507.16841.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Waseem Akram, Muhayy Ud Din, Abdelhaleem Saad, Irfan Hussain
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.16841">AquaChat: An LLM-Guided ROV Framework for Adaptive Inspection of Aquaculture Net Pens</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Inspection of aquaculture net pens is essential for maintaining the structural integrity, biosecurity, and operational efficiency of fish farming systems. Traditional inspection approaches rely on pre-programmed missions or manual control, offering limited adaptability to dynamic underwater conditions and user-specific demands. In this study, we propose AquaChat, a novel Remotely Operated Vehicle (ROV) framework that integrates Large Language Models (LLMs) for intelligent and adaptive net pen inspection. The system features a multi-layered architecture: (1) a high-level planning layer that interprets natural language user commands using an LLM to generate symbolic task plans; (2) a mid-level task manager that translates plans into ROV control sequences; and (3) a low-level motion control layer that executes navigation and inspection tasks with precision. Real-time feedback and event-triggered replanning enhance robustness in challenging aquaculture environments. The framework is validated through experiments in both simulated and controlled aquatic environments representative of aquaculture net pens. Results demonstrate improved task flexibility, inspection accuracy, and operational efficiency. AquaChat illustrates the potential of integrating language-based AI with marine robotics to enable intelligent, user-interactive inspection systems for sustainable aquaculture operations.
<div id='section'>Paperid: <span id='pid'>719, <a href='https://arxiv.org/pdf/2507.00677.pdf' target='_blank'>https://arxiv.org/pdf/2507.00677.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongho Kang, Jin Cheng, Fatemeh Zargarbashi, Taerim Yoon, Sungjoon Choi, Stelian Coros
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.00677">Learning Steerable Imitation Controllers from Unstructured Animal Motions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a control framework for legged robots that leverages unstructured real-world animal motion data to generate animal-like and user-steerable behaviors. Our framework learns to follow velocity commands while reproducing the diverse gait patterns in the original dataset. To begin with, animal motion data is transformed into a robot-compatible database using constrained inverse kinematics and model predictive control, bridging the morphological and physical gap between the animal and the robot. Subsequently, a variational autoencoder-based motion synthesis module captures the diverse locomotion patterns in the motion database and generates smooth transitions between them in response to velocity commands. The resulting kinematic motions serve as references for a reinforcement learning-based feedback controller deployed on physical robots. We show that this approach enables a quadruped robot to adaptively switch gaits and accurately track user velocity commands while maintaining the stylistic coherence of the motion data. Additionally, we provide component-wise evaluations to analyze the system's behavior in depth and demonstrate the efficacy of our method for more accurate and reliable motion imitation.
<div id='section'>Paperid: <span id='pid'>720, <a href='https://arxiv.org/pdf/2505.22974.pdf' target='_blank'>https://arxiv.org/pdf/2505.22974.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuntao Ma, Andrei Cramariuc, Farbod Farshidian, Marco Hutter
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.22974">Learning coordinated badminton skills for legged manipulators</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Coordinating the motion between lower and upper limbs and aligning limb control with perception are substantial challenges in robotics, particularly in dynamic environments. To this end, we introduce an approach for enabling legged mobile manipulators to play badminton, a task that requires precise coordination of perception, locomotion, and arm swinging. We propose a unified reinforcement learning-based control policy for whole-body visuomotor skills involving all degrees of freedom to achieve effective shuttlecock tracking and striking. This policy is informed by a perception noise model that utilizes real-world camera data, allowing for consistent perception error levels between simulation and deployment and encouraging learned active perception behaviors. Our method includes a shuttlecock prediction model, constrained reinforcement learning for robust motion control, and integrated system identification techniques to enhance deployment readiness. Extensive experimental results in a variety of environments validate the robot's capability to predict shuttlecock trajectories, navigate the service area effectively, and execute precise strikes against human players, demonstrating the feasibility of using legged mobile manipulators in complex and dynamic sports scenarios.
<div id='section'>Paperid: <span id='pid'>721, <a href='https://arxiv.org/pdf/2505.10239.pdf' target='_blank'>https://arxiv.org/pdf/2505.10239.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gokhan Solak, Gustavo J. G. Lahr, Idil Ozdamar, Arash Ajoudani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.10239">Context-aware collaborative pushing of heavy objects using skeleton-based intention prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In physical human-robot interaction, force feedback has been the most common sensing modality to convey the human intention to the robot. It is widely used in admittance control to allow the human to direct the robot. However, it cannot be used in scenarios where direct force feedback is not available since manipulated objects are not always equipped with a force sensor. In this work, we study one such scenario: the collaborative pushing and pulling of heavy objects on frictional surfaces, a prevalent task in industrial settings. When humans do it, they communicate through verbal and non-verbal cues, where body poses, and movements often convey more than words. We propose a novel context-aware approach using Directed Graph Neural Networks to analyze spatio-temporal human posture data to predict human motion intention for non-verbal collaborative physical manipulation. Our experiments demonstrate that robot assistance significantly reduces human effort and improves task efficiency. The results indicate that incorporating posture-based context recognition, either together with or as an alternative to force sensing, enhances robot decision-making and control efficiency.
<div id='section'>Paperid: <span id='pid'>722, <a href='https://arxiv.org/pdf/2503.02587.pdf' target='_blank'>https://arxiv.org/pdf/2503.02587.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Piotr Koczy, Michael C. Welle, Danica Kragic
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.02587">Learning Dexterous In-Hand Manipulation with Multifingered Hands via Visuomotor Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a framework for learning dexterous in-hand manipulation with multifingered hands using visuomotor diffusion policies. Our system enables complex in-hand manipulation tasks, such as unscrewing a bottle lid with one hand, by leveraging a fast and responsive teleoperation setup for the four-fingered Allegro Hand. We collect high-quality expert demonstrations using an augmented reality (AR) interface that tracks hand movements and applies inverse kinematics and motion retargeting for precise control. The AR headset provides real-time visualization, while gesture controls streamline teleoperation. To enhance policy learning, we introduce a novel demonstration outlier removal approach based on HDBSCAN clustering and the Global-Local Outlier Score from Hierarchies (GLOSH) algorithm, effectively filtering out low-quality demonstrations that could degrade performance. We evaluate our approach extensively in real-world settings and provide all experimental videos on the project website: https://dex-manip.github.io/
<div id='section'>Paperid: <span id='pid'>723, <a href='https://arxiv.org/pdf/2503.02353.pdf' target='_blank'>https://arxiv.org/pdf/2503.02353.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Luobin Wang, Hongzhan Yu, Chenning Yu, Sicun Gao, Henrik Christensen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.02353">Controllable Motion Generation via Diffusion Modal Coupling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion models have recently gained significant attention in robotics due to their ability to generate multi-modal distributions of system states and behaviors. However, a key challenge remains: ensuring precise control over the generated outcomes without compromising realism. This is crucial for applications such as motion planning or trajectory forecasting, where adherence to physical constraints and task-specific objectives is essential. We propose a novel framework that enhances controllability in diffusion models by leveraging multi-modal prior distributions and enforcing strong modal coupling. This allows us to initiate the denoising process directly from distinct prior modes that correspond to different possible system behaviors, ensuring sampling to align with the training distribution. We evaluate our approach on motion prediction using the Waymo dataset and multi-task control in Maze2D environments. Experimental results show that our framework outperforms both guidance-based techniques and conditioned models with unimodal priors, achieving superior fidelity, diversity, and controllability, even in the absence of explicit conditioning. Overall, our approach provides a more reliable and scalable solution for controllable motion generation in robotics.
<div id='section'>Paperid: <span id='pid'>724, <a href='https://arxiv.org/pdf/2503.01178.pdf' target='_blank'>https://arxiv.org/pdf/2503.01178.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoyuan Zhang, Xinyan Cai, Bo Liu, Weidong Huang, Song-Chun Zhu, Siyuan Qi, Yaodong Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.01178">Differentiable Information Enhanced Model-Based Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Differentiable environments have heralded new possibilities for learning control policies by offering rich differentiable information that facilitates gradient-based methods. In comparison to prevailing model-free reinforcement learning approaches, model-based reinforcement learning (MBRL) methods exhibit the potential to effectively harness the power of differentiable information for recovering the underlying physical dynamics. However, this presents two primary challenges: effectively utilizing differentiable information to 1) construct models with more accurate dynamic prediction and 2) enhance the stability of policy training. In this paper, we propose a Differentiable Information Enhanced MBRL method, MB-MIX, to address both challenges. Firstly, we adopt a Sobolev model training approach that penalizes incorrect model gradient outputs, enhancing prediction accuracy and yielding more precise models that faithfully capture system dynamics. Secondly, we introduce mixing lengths of truncated learning windows to reduce the variance in policy gradient estimation, resulting in improved stability during policy learning. To validate the effectiveness of our approach in differentiable environments, we provide theoretical analysis and empirical results. Notably, our approach outperforms previous model-based and model-free methods, in multiple challenging tasks involving controllable rigid robots such as humanoid robots' motion control and deformable object manipulation.
<div id='section'>Paperid: <span id='pid'>725, <a href='https://arxiv.org/pdf/2502.19049.pdf' target='_blank'>https://arxiv.org/pdf/2502.19049.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Patrick Seifner, Kostadin Cvejoski, David Berghaus, Cesar Ojeda, Ramses J. Sanchez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.19049">Foundation Inference Models for Stochastic Differential Equations: A Transformer-based Approach for Zero-shot Function Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Stochastic differential equations (SDEs) describe dynamical systems where deterministic flows, governed by a drift function, are superimposed with random fluctuations dictated by a diffusion function. The accurate estimation (or discovery) of these functions from data is a central problem in machine learning, with wide application across natural and social sciences alike. Yet current solutions are brittle, and typically rely on symbolic regression or Bayesian non-parametrics. In this work, we introduce FIM-SDE (Foundation Inference Model for SDEs), a transformer-based recognition model capable of performing accurate zero-shot estimation of the drift and diffusion functions of SDEs, from noisy and sparse observations on empirical processes of different dimensionalities. Leveraging concepts from amortized inference and neural operators, we train FIM-SDE in a supervised fashion, to map a large set of noisy and discretely observed SDE paths to their corresponding drift and diffusion functions. We demonstrate that one and the same (pretrained) FIM-SDE achieves robust zero-shot function estimation (i.e. without any parameter fine-tuning) across a wide range of synthetic and real-world processes, from canonical SDE systems (e.g. double-well dynamics or weakly perturbed Hopf bifurcations) to human motion recordings and oil price and wind speed fluctuations.
<div id='section'>Paperid: <span id='pid'>726, <a href='https://arxiv.org/pdf/2412.10235.pdf' target='_blank'>https://arxiv.org/pdf/2412.10235.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Songpengcheng Xia, Yu Zhang, Zhuo Su, Xiaozheng Zheng, Zheng Lv, Guidong Wang, Yongjie Zhang, Qi Wu, Lei Chu, Ling Pei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.10235">EnvPoser: Environment-aware Realistic Human Motion Estimation from Sparse Observations with Uncertainty Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Estimating full-body motion using the tracking signals of head and hands from VR devices holds great potential for various applications. However, the sparsity and unique distribution of observations present a significant challenge, resulting in an ill-posed problem with multiple feasible solutions (i.e., hypotheses). This amplifies uncertainty and ambiguity in full-body motion estimation, especially for the lower-body joints. Therefore, we propose a new method, EnvPoser, that employs a two-stage framework to perform full-body motion estimation using sparse tracking signals and pre-scanned environment from VR devices. EnvPoser models the multi-hypothesis nature of human motion through an uncertainty-aware estimation module in the first stage. In the second stage, we refine these multi-hypothesis estimates by integrating semantic and geometric environmental constraints, ensuring that the final motion estimation aligns realistically with both the environmental context and physical interactions. Qualitative and quantitative experiments on two public datasets demonstrate that our method achieves state-of-the-art performance, highlighting significant improvements in human motion estimation within motion-environment interaction scenarios.
<div id='section'>Paperid: <span id='pid'>727, <a href='https://arxiv.org/pdf/2412.07493.pdf' target='_blank'>https://arxiv.org/pdf/2412.07493.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Muhayy Ud Din, Jan Rosell, Waseem Akram, Isiah Zaplana, Maximo A Roa, Irfan Hussain
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.07493">Onto-LLM-TAMP: Knowledge-oriented Task and Motion Planning using Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Performing complex manipulation tasks in dynamic environments requires efficient Task and Motion Planning (TAMP) approaches that combine high-level symbolic plans with low-level motion control. Advances in Large Language Models (LLMs), such as GPT-4, are transforming task planning by offering natural language as an intuitive and flexible way to describe tasks, generate symbolic plans, and reason. However, the effectiveness of LLM-based TAMP approaches is limited due to static and template-based prompting, which limits adaptability to dynamic environments and complex task contexts. To address these limitations, this work proposes a novel Onto-LLM-TAMP framework that employs knowledge-based reasoning to refine and expand user prompts with task-contextual reasoning and knowledge-based environment state descriptions. Integrating domain-specific knowledge into the prompt ensures semantically accurate and context-aware task plans. The proposed framework demonstrates its effectiveness by resolving semantic errors in symbolic plan generation, such as maintaining logical temporal goal ordering in scenarios involving hierarchical object placement. The proposed framework is validated through both simulation and real-world scenarios, demonstrating significant improvements over the baseline approach in terms of adaptability to dynamic environments and the generation of semantically correct task plans.
<div id='section'>Paperid: <span id='pid'>728, <a href='https://arxiv.org/pdf/2411.17532.pdf' target='_blank'>https://arxiv.org/pdf/2411.17532.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chengjian Li, Xiangbo Shu, Qiongjie Cui, Yazhou Yao, Jinhui Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.17532">FTMoMamba: Motion Generation with Frequency and Text State Space Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion models achieve impressive performance in human motion generation. However, current approaches typically ignore the significance of frequency-domain information in capturing fine-grained motions within the latent space (e.g., low frequencies correlate with static poses, and high frequencies align with fine-grained motions). Additionally, there is a semantic discrepancy between text and motion, leading to inconsistency between the generated motions and the text descriptions. In this work, we propose a novel diffusion-based FTMoMamba framework equipped with a Frequency State Space Model (FreqSSM) and a Text State Space Model (TextSSM). Specifically, to learn fine-grained representation, FreqSSM decomposes sequences into low-frequency and high-frequency components, guiding the generation of static pose (e.g., sits, lay) and fine-grained motions (e.g., transition, stumble), respectively. To ensure the consistency between text and motion, TextSSM encodes text features at the sentence level, aligning textual semantics with sequential features. Extensive experiments show that FTMoMamba achieves superior performance on the text-to-motion generation task, especially gaining the lowest FID of 0.181 (rather lower than 0.421 of MLD) on the HumanML3D dataset.
<div id='section'>Paperid: <span id='pid'>729, <a href='https://arxiv.org/pdf/2411.16273.pdf' target='_blank'>https://arxiv.org/pdf/2411.16273.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Silas Ruhrberg EstÃ©vez, JosÃ©e Mallah, Dominika Kazieczko, Chenyu Tang, Luigi G. Occhipinti
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.16273">Deep Learning for Motion Classification in Ankle Exoskeletons Using Surface EMG and IMU Signals</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Ankle exoskeletons have garnered considerable interest for their potential to enhance mobility and reduce fall risks, particularly among the aging population. The efficacy of these devices relies on accurate real-time prediction of the user's intended movements through sensor-based inputs. This paper presents a novel motion prediction framework that integrates three Inertial Measurement Units (IMUs) and eight surface Electromyography (sEMG) sensors to capture both kinematic and muscular activity data. A comprehensive set of activities, representative of everyday movements in barrier-free environments, was recorded for the purpose. Our findings reveal that Convolutional Neural Networks (CNNs) slightly outperform Long Short-Term Memory (LSTM) networks on a dataset of five motion tasks, achieving classification accuracies of $96.5 \pm 0.8 \%$ and $87.5 \pm 2.9 \%$, respectively. Furthermore, we demonstrate the system's proficiency in transfer learning, enabling accurate motion classification for new subjects using just ten samples per class for finetuning. The robustness of the model is demonstrated by its resilience to sensor failures resulting in absent signals, maintaining reliable performance in real-world scenarios. These results underscore the potential of deep learning algorithms to enhance the functionality and safety of ankle exoskeletons, ultimately improving their usability in daily life.
<div id='section'>Paperid: <span id='pid'>730, <a href='https://arxiv.org/pdf/2410.20907.pdf' target='_blank'>https://arxiv.org/pdf/2410.20907.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Seyed Adel Alizadeh Kolagar, Mehdi Heydari Shahna, Jouni Mattila
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.20907">Combining Deep Reinforcement Learning with a Jerk-Bounded Trajectory Generator for Kinematically Constrained Motion Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep reinforcement learning (DRL) is emerging as a promising method for adaptive robotic motion and complex task automation, effectively addressing the limitations of traditional control methods. However, ensuring safety throughout both the learning process and policy deployment remains a key challenge due to the risky exploration inherent in DRL, as well as the discrete nature of actions taken at intervals. These discontinuities, despite being part of a continuous action space, can lead to abrupt changes between successive actions, causing instability and unsafe intermediate states. To address these challenges, this paper proposes an integrated framework that combines DRL with a jerk-bounded trajectory generator (JBTG) and a robust low-level control strategy, significantly enhancing the safety, stability, and reliability of robotic manipulators. The low-level controller ensures the precise execution of DRL-generated commands, while the JBTG refines these motions to produce smooth, continuous trajectories that prevent abrupt or unsafe actions. The framework also includes pre-calculated safe velocity zones for smooth braking, preventing joint limit violations and ensuring compliance with kinematic constraints. This approach not only guarantees the robustness and safety of the robotic system but also optimizes motion control, making it suitable for practical applications. The effectiveness of the proposed framework is demonstrated through its application to a highly complex heavy-duty manipulator.
<div id='section'>Paperid: <span id='pid'>731, <a href='https://arxiv.org/pdf/2410.14508.pdf' target='_blank'>https://arxiv.org/pdf/2410.14508.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nefeli Andreou, Xi Wang, Victoria FernÃ¡ndez Abrevaya, Marie-Paule Cani, Yiorgos Chrysanthou, Vicky Kalogeiton
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.14508">LEAD: Latent Realignment for Human Motion Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Our goal is to generate realistic human motion from natural language. Modern methods often face a trade-off between model expressiveness and text-to-motion alignment. Some align text and motion latent spaces but sacrifice expressiveness; others rely on diffusion models producing impressive motions, but lacking semantic meaning in their latent space. This may compromise realism, diversity, and applicability. Here, we address this by combining latent diffusion with a realignment mechanism, producing a novel, semantically structured space that encodes the semantics of language. Leveraging this capability, we introduce the task of textual motion inversion to capture novel motion concepts from a few examples. For motion synthesis, we evaluate LEAD on HumanML3D and KIT-ML and show comparable performance to the state-of-the-art in terms of realism, diversity, and text-motion consistency. Our qualitative analysis and user study reveal that our synthesized motions are sharper, more human-like and comply better with the text compared to modern methods. For motion textual inversion, our method demonstrates improved capacity in capturing out-of-distribution characteristics in comparison to traditional VAEs.
<div id='section'>Paperid: <span id='pid'>732, <a href='https://arxiv.org/pdf/2409.18896.pdf' target='_blank'>https://arxiv.org/pdf/2409.18896.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Denys Iliash, Hanxiao Jiang, Yiming Zhang, Manolis Savva, Angel X. Chang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.18896">S2O: Static to Openable Enhancement for Articulated 3D Objects</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite much progress in large 3D datasets there are currently few interactive 3D object datasets, and their scale is limited due to the manual effort required in their construction. We introduce the static to openable (S2O) task which creates interactive articulated 3D objects from static counterparts through openable part detection, motion prediction, and interior geometry completion. We formulate a unified framework to tackle this task, and curate a challenging dataset of openable 3D objects that serves as a test bed for systematic evaluation. Our experiments benchmark methods from prior work, extended and improved methods, and simple yet effective heuristics for the S2O task. We find that turning static 3D objects into interactively openable counterparts is possible but that all methods struggle to generalize to realistic settings of the task, and we highlight promising future work directions. Our work enables efficient creation of interactive 3D objects for robotic manipulation and embodied AI tasks.
<div id='section'>Paperid: <span id='pid'>733, <a href='https://arxiv.org/pdf/2409.02324.pdf' target='_blank'>https://arxiv.org/pdf/2409.02324.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lina MarÃ­a Amaya-MejÃ­a, Mohamed Ghita, Jan Dentler, Miguel Olivares-Mendez, Carol Martinez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.02324">Visual Servoing for Robotic On-Orbit Servicing: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>On-orbit servicing (OOS) activities will power the next big step for sustainable exploration and commercialization of space. Developing robotic capabilities for autonomous OOS operations is a priority for the space industry. Visual Servoing (VS) enables robots to achieve the precise manoeuvres needed for critical OOS missions by utilizing visual information for motion control. This article presents an overview of existing VS approaches for autonomous OOS operations with space manipulator systems (SMS). We divide the approaches according to their contribution to the typical phases of a robotic OOS mission: a) Recognition, b) Approach, and c) Contact. We also present a discussion on the reviewed VS approaches, identifying current trends. Finally, we highlight the challenges and areas for future research on VS techniques for robotic OOS.
<div id='section'>Paperid: <span id='pid'>734, <a href='https://arxiv.org/pdf/2407.17502.pdf' target='_blank'>https://arxiv.org/pdf/2407.17502.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fatemeh Zargarbashi, Fabrizio Di Giuro, Jin Cheng, Dongho Kang, Bhavya Sukhija, Stelian Coros
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.17502">MetaLoco: Universal Quadrupedal Locomotion with Meta-Reinforcement Learning and Motion Imitation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work presents a meta-reinforcement learning approach to develop a universal locomotion control policy capable of zero-shot generalization across diverse quadrupedal platforms. The proposed method trains an RL agent equipped with a memory unit to imitate reference motions using a small set of procedurally generated quadruped robots. Through comprehensive simulation and real-world hardware experiments, we demonstrate the efficacy of our approach in achieving locomotion across various robots without requiring robot-specific fine-tuning. Furthermore, we highlight the critical role of the memory unit in enabling generalization, facilitating rapid adaptation to changes in the robot properties, and improving sample efficiency.
<div id='section'>Paperid: <span id='pid'>735, <a href='https://arxiv.org/pdf/2407.11532.pdf' target='_blank'>https://arxiv.org/pdf/2407.11532.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alessio Sampieri, Alessio Palma, Indro Spinelli, Fabio Galasso
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.11532">Length-Aware Motion Synthesis via Latent Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The target duration of a synthesized human motion is a critical attribute that requires modeling control over the motion dynamics and style. Speeding up an action performance is not merely fast-forwarding it. However, state-of-the-art techniques for human behavior synthesis have limited control over the target sequence length.
  We introduce the problem of generating length-aware 3D human motion sequences from textual descriptors, and we propose a novel model to synthesize motions of variable target lengths, which we dub "Length-Aware Latent Diffusion" (LADiff). LADiff consists of two new modules: 1) a length-aware variational auto-encoder to learn motion representations with length-dependent latent codes; 2) a length-conforming latent diffusion model to generate motions with a richness of details that increases with the required target sequence length. LADiff significantly improves over the state-of-the-art across most of the existing motion synthesis metrics on the two established benchmarks of HumanML3D and KIT-ML.
<div id='section'>Paperid: <span id='pid'>736, <a href='https://arxiv.org/pdf/2404.11557.pdf' target='_blank'>https://arxiv.org/pdf/2404.11557.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Taerim Yoon, Dongho Kang, Seungmin Kim, Jin Cheng, Minsung Ahn, Stelian Coros, Sungjoon Choi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.11557">Spatio-Temporal Motion Retargeting for Quadruped Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work presents a motion retargeting approach for legged robots, aimed at transferring the dynamic and agile movements to robots from source motions. In particular, we guide the imitation learning procedures by transferring motions from source to target, effectively bridging the morphological disparities while ensuring the physical feasibility of the target system. In the first stage, we focus on motion retargeting at the kinematic level by generating kinematically feasible whole-body motions from keypoint trajectories. Following this, we refine the motion at the dynamic level by adjusting it in the temporal domain while adhering to physical constraints. This process facilitates policy training via reinforcement learning, enabling precise and robust motion tracking. We demonstrate that our approach successfully transforms noisy motion sources, such as hand-held camera videos, into robot-specific motions that align with the morphology and physical properties of the target robots. Moreover, we demonstrate terrain-aware motion retargeting to perform BackFlip on top of a box. We successfully deployed these skills to four robots with different dimensions and physical properties in the real world through hardware experiments.
<div id='section'>Paperid: <span id='pid'>737, <a href='https://arxiv.org/pdf/2401.13441.pdf' target='_blank'>https://arxiv.org/pdf/2401.13441.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maximilian StÃ¶lzle, Sonal Santosh Baberwal, Daniela Rus, Shirley Coyle, Cosimo Della Santina
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.13441">Guiding Soft Robots with Motor-Imagery Brain Signals and Impedance Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Integrating Brain-Machine Interfaces into non-clinical applications like robot motion control remains difficult - despite remarkable advancements in clinical settings. Specifically, EEG-based motor imagery systems are still error-prone, posing safety risks when rigid robots operate near humans. This work presents an alternative pathway towards safe and effective operation by combining wearable EEG with physically embodied safety in soft robots. We introduce and test a pipeline that allows a user to move a soft robot's end effector in real time via brain waves that are measured by as few as three EEG channels. A robust motor imagery algorithm interprets the user's intentions to move the position of a virtual attractor to which the end effector is attracted, thanks to a new Cartesian impedance controller. We specifically focus here on planar soft robot-based architected metamaterials, which require the development of a novel control architecture to deal with the peculiar nonlinearities - e.g., non-affinity in control. We preliminarily but quantitatively evaluate the approach on the task of setpoint regulation. We observe that the user reaches the proximity of the setpoint in 66% of steps and that for successful steps, the average response time is 21.5s. We also demonstrate the execution of simple real-world tasks involving interaction with the environment, which would be extremely hard to perform if it were not for the robot's softness.
<div id='section'>Paperid: <span id='pid'>738, <a href='https://arxiv.org/pdf/2511.14394.pdf' target='_blank'>https://arxiv.org/pdf/2511.14394.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Joachim Tesch, Giorgio Becherini, Prerana Achar, Anastasios Yiannakidis, Muhammed Kocabas, Priyanka Patel, Michael J. Black
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.14394">BEDLAM2.0: Synthetic Humans and Cameras in Motion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Inferring 3D human motion from video remains a challenging problem with many applications. While traditional methods estimate the human in image coordinates, many applications require human motion to be estimated in world coordinates. This is particularly challenging when there is both human and camera motion. Progress on this topic has been limited by the lack of rich video data with ground truth human and camera movement. We address this with BEDLAM2.0, a new dataset that goes beyond the popular BEDLAM dataset in important ways. In addition to introducing more diverse and realistic cameras and camera motions, BEDLAM2.0 increases diversity and realism of body shape, motions, clothing, hair, and 3D environments. Additionally, it adds shoes, which were missing in BEDLAM. BEDLAM has become a key resource for training 3D human pose and motion regressors today and we show that BEDLAM2.0 is significantly better, particularly for training methods that estimate humans in world coordinates. We compare state-of-the art methods trained on BEDLAM and BEDLAM2.0, and find that BEDLAM2.0 significantly improves accuracy over BEDLAM. For research purposes, we provide the rendered videos, ground truth body parameters, and camera motions. We also provide the 3D assets to which we have rights and links to those from third parties.
<div id='section'>Paperid: <span id='pid'>739, <a href='https://arxiv.org/pdf/2510.19789.pdf' target='_blank'>https://arxiv.org/pdf/2510.19789.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guowei Xu, Yuxuan Bian, Ailing Zeng, Mingyi Shi, Shaoli Huang, Wen Li, Lixin Duan, Qiang Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.19789">OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces OmniMotion-X, a versatile multimodal framework for whole-body human motion generation, leveraging an autoregressive diffusion transformer in a unified sequence-to-sequence manner. OmniMotion-X efficiently supports diverse multimodal tasks, including text-to-motion, music-to-dance, speech-to-gesture, and global spatial-temporal control scenarios (e.g., motion prediction, in-betweening, completion, and joint/trajectory-guided synthesis), as well as flexible combinations of these tasks. Specifically, we propose the use of reference motion as a novel conditioning signal, substantially enhancing the consistency of generated content, style, and temporal dynamics crucial for realistic animations. To handle multimodal conflicts, we introduce a progressive weak-to-strong mixed-condition training strategy. To enable high-quality multimodal training, we construct OmniMoCap-X, the largest unified multimodal motion dataset to date, integrating 28 publicly available MoCap sources across 10 distinct tasks, standardized to the SMPL-X format at 30 fps. To ensure detailed and consistent annotations, we render sequences into videos and use GPT-4o to automatically generate structured and hierarchical captions, capturing both low-level actions and high-level semantics. Extensive experimental evaluations confirm that OmniMotion-X significantly surpasses existing methods, demonstrating state-of-the-art performance across multiple multimodal tasks and enabling the interactive generation of realistic, coherent, and controllable long-duration motions.
<div id='section'>Paperid: <span id='pid'>740, <a href='https://arxiv.org/pdf/2510.04753.pdf' target='_blank'>https://arxiv.org/pdf/2510.04753.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Masoumeh Chapariniya, Teodora Vukovic, Sarah Ebling, Volker Dellwo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04753">Beyond Appearance: Transformer-based Person Identification from Conversational Dynamics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper investigates the performance of transformer-based architectures for person identification in natural, face-to-face conversation scenario. We implement and evaluate a two-stream framework that separately models spatial configurations and temporal motion patterns of 133 COCO WholeBody keypoints, extracted from a subset of the CANDOR conversational corpus. Our experiments compare pre-trained and from-scratch training, investigate the use of velocity features, and introduce a multi-scale temporal transformer for hierarchical motion modeling. Results demonstrate that domain-specific training significantly outperforms transfer learning, and that spatial configurations carry more discriminative information than temporal dynamics. The spatial transformer achieves 95.74% accuracy, while the multi-scale temporal transformer achieves 93.90%. Feature-level fusion pushes performance to 98.03%, confirming that postural and dynamic information are complementary. These findings highlight the potential of transformer architectures for person identification in natural interactions and provide insights for future multimodal and cross-cultural studies.
<div id='section'>Paperid: <span id='pid'>741, <a href='https://arxiv.org/pdf/2509.19259.pdf' target='_blank'>https://arxiv.org/pdf/2509.19259.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Markos Diomataris, Berat Mert Albaba, Giorgio Becherini, Partha Ghosh, Omid Taheri, Michael J. Black
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.19259">Moving by Looking: Towards Vision-Driven Avatar Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The way we perceive the world fundamentally shapes how we move, whether it is how we navigate in a room or how we interact with other humans. Current human motion generation methods, neglect this interdependency and use task-specific ``perception'' that differs radically from that of humans. We argue that the generation of human-like avatar behavior requires human-like perception. Consequently, in this work we present CLOPS, the first human avatar that solely uses egocentric vision to perceive its surroundings and navigate. Using vision as the primary driver of motion however, gives rise to a significant challenge for training avatars: existing datasets have either isolated human motion, without the context of a scene, or lack scale. We overcome this challenge by decoupling the learning of low-level motion skills from learning of high-level control that maps visual input to motion. First, we train a motion prior model on a large motion capture dataset. Then, a policy is trained using Q-learning to map egocentric visual inputs to high-level control commands for the motion prior. Our experiments empirically demonstrate that egocentric vision can give rise to human-like motion characteristics in our avatars. For example, the avatars walk such that they avoid obstacles present in their visual field. These findings suggest that equipping avatars with human-like sensors, particularly egocentric vision, holds promise for training avatars that behave like humans.
<div id='section'>Paperid: <span id='pid'>742, <a href='https://arxiv.org/pdf/2509.18046.pdf' target='_blank'>https://arxiv.org/pdf/2509.18046.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yinuo Wang, Yuanyang Qi, Jinzhao Zhou, Gavin Tao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.18046">HuMam: Humanoid Motion Control via End-to-End Deep Reinforcement Learning with Mamba</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>End-to-end reinforcement learning (RL) for humanoid locomotion is appealing for its compact perception-action mapping, yet practical policies often suffer from training instability, inefficient feature fusion, and high actuation cost. We present HuMam, a state-centric end-to-end RL framework that employs a single-layer Mamba encoder to fuse robot-centric states with oriented footstep targets and a continuous phase clock. The policy outputs joint position targets tracked by a low-level PD loop and is optimized with PPO. A concise six-term reward balances contact quality, swing smoothness, foot placement, posture, and body stability while implicitly promoting energy saving. On the JVRC-1 humanoid in mc-mujoco, HuMam consistently improves learning efficiency, training stability, and overall task performance over a strong feedforward baseline, while reducing power consumption and torque peaks. To our knowledge, this is the first end-to-end humanoid RL controller that adopts Mamba as the fusion backbone, demonstrating tangible gains in efficiency, stability, and control economy.
<div id='section'>Paperid: <span id='pid'>743, <a href='https://arxiv.org/pdf/2509.16919.pdf' target='_blank'>https://arxiv.org/pdf/2509.16919.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Huong Hoang, Keito Suzuki, Truong Nguyen, Pamela Cosman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.16919">Bi-modal Prediction and Transformation Coding for Compressing Complex Human Dynamics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>For dynamic human motion sequences, the original KeyNode-Driven codec often struggles to retain compression efficiency when confronted with rapid movements or strong non-rigid deformations. This paper proposes a novel Bi-modal coding framework that enhances the flexibility of motion representation by integrating semantic segmentation and region-specific transformation modeling. The rigid transformation model (rotation & translation) is extended with a hybrid scheme that selectively applies affine transformations-rotation, translation, scaling, and shearing-only to deformation-rich regions (e.g., the torso, where loose clothing induces high variability), while retaining rigid models elsewhere. The affine model is decomposed into minimal parameter sets for efficient coding and combined through a component selection strategy guided by a Lagrangian Rate-Distortion optimization. The results show that the Bi-modal method achieves more accurate mesh deformation, especially in sequences involving complex non-rigid motion, without compromising compression efficiency in simpler regions, with an average bit-rate saving of 33.81% compared to the baseline.
<div id='section'>Paperid: <span id='pid'>744, <a href='https://arxiv.org/pdf/2509.07593.pdf' target='_blank'>https://arxiv.org/pdf/2509.07593.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gavin Tao, Yinuo Wang, Jinzhao Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.07593">Can SSD-Mamba2 Unlock Reinforcement Learning for End-to-End Motion Control?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>End-to-end reinforcement learning for motion control promises unified perception-action policies that scale across embodiments and tasks, yet most deployed controllers are either blind (proprioception-only) or rely on fusion backbones with unfavorable compute-memory trade-offs. Recurrent controllers struggle with long-horizon credit assignment, and Transformer-based fusion incurs quadratic cost in token length, limiting temporal and spatial context. We present a vision-driven cross-modal RL framework built on SSD-Mamba2, a selective state-space backbone that applies state-space duality (SSD) to enable both recurrent and convolutional scanning with hardware-aware streaming and near-linear scaling. Proprioceptive states and exteroceptive observations (e.g., depth tokens) are encoded into compact tokens and fused by stacked SSD-Mamba2 layers. The selective state-space updates retain long-range dependencies with markedly lower latency and memory use than quadratic self-attention, enabling longer look-ahead, higher token resolution, and stable training under limited compute. Policies are trained end-to-end under curricula that randomize terrain and appearance and progressively increase scene complexity. A compact, state-centric reward balances task progress, energy efficiency, and safety. Across diverse motion-control scenarios, our approach consistently surpasses strong state-of-the-art baselines in return, safety (collisions and falls), and sample efficiency, while converging faster at the same compute budget. These results suggest that SSD-Mamba2 provides a practical fusion backbone for scalable, foresightful, and efficient end-to-end motion control.
<div id='section'>Paperid: <span id='pid'>745, <a href='https://arxiv.org/pdf/2508.19153.pdf' target='_blank'>https://arxiv.org/pdf/2508.19153.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yinuo Wang, Gavin Tao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19153">QuadKAN: KAN-Enhanced Quadruped Motion Control via End-to-End Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We address vision-guided quadruped motion control with reinforcement learning (RL) and highlight the necessity of combining proprioception with vision for robust control. We propose QuadKAN, a spline-parameterized cross-modal policy instantiated with Kolmogorov-Arnold Networks (KANs). The framework incorporates a spline encoder for proprioception and a spline fusion head for proprioception-vision inputs. This structured function class aligns the state-to-action mapping with the piecewise-smooth nature of gait, improving sample efficiency, reducing action jitter and energy consumption, and providing interpretable posture-action sensitivities. We adopt Multi-Modal Delay Randomization (MMDR) and perform end-to-end training with Proximal Policy Optimization (PPO). Evaluations across diverse terrains, including both even and uneven surfaces and scenarios with static or dynamic obstacles, demonstrate that QuadKAN achieves consistently higher returns, greater distances, and fewer collisions than state-of-the-art (SOTA) baselines. These results show that spline-parameterized policies offer a simple, effective, and interpretable alternative for robust vision-guided locomotion. A repository will be made available upon acceptance.
<div id='section'>Paperid: <span id='pid'>746, <a href='https://arxiv.org/pdf/2507.11892.pdf' target='_blank'>https://arxiv.org/pdf/2507.11892.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Liu, Leyuan Qu, Hanlei Shi, Di Gao, Yuhua Zheng, Taihao Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.11892">From Coarse to Nuanced: Cross-Modal Alignment of Fine-Grained Linguistic Cues and Visual Salient Regions for Dynamic Emotion Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dynamic Facial Expression Recognition (DFER) aims to identify human emotions from temporally evolving facial movements and plays a critical role in affective computing. While recent vision-language approaches have introduced semantic textual descriptions to guide expression recognition, existing methods still face two key limitations: they often underutilize the subtle emotional cues embedded in generated text, and they have yet to incorporate sufficiently effective mechanisms for filtering out facial dynamics that are irrelevant to emotional expression. To address these gaps, We propose GRACE, Granular Representation Alignment for Cross-modal Emotion recognition that integrates dynamic motion modeling, semantic text refinement, and token-level cross-modal alignment to facilitate the precise localization of emotionally salient spatiotemporal features. Our method constructs emotion-aware textual descriptions via a Coarse-to-fine Affective Text Enhancement (CATE) module and highlights expression-relevant facial motion through a motion-difference weighting mechanism. These refined semantic and visual signals are aligned at the token level using entropy-regularized optimal transport. Experiments on three benchmark datasets demonstrate that our method significantly improves recognition performance, particularly in challenging settings with ambiguous or imbalanced emotion classes, establishing new state-of-the-art (SOTA) results in terms of both UAR and WAR.
<div id='section'>Paperid: <span id='pid'>747, <a href='https://arxiv.org/pdf/2507.05906.pdf' target='_blank'>https://arxiv.org/pdf/2507.05906.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenhao Li, Marco Hutter, Andreas Krause
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.05906">Feature-Based vs. GAN-Based Learning from Demonstrations: When and Why</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This survey provides a comparative analysis of feature-based and GAN-based approaches to learning from demonstrations, with a focus on the structure of reward functions and their implications for policy learning. Feature-based methods offer dense, interpretable rewards that excel at high-fidelity motion imitation, yet often require sophisticated representations of references and struggle with generalization in unstructured settings. GAN-based methods, in contrast, use implicit, distributional supervision that enables scalability and adaptation flexibility, but are prone to training instability and coarse reward signals. Recent advancements in both paradigms converge on the importance of structured motion representations, which enable smoother transitions, controllable synthesis, and improved task integration. We argue that the dichotomy between feature-based and GAN-based methods is increasingly nuanced: rather than one paradigm dominating the other, the choice should be guided by task-specific priorities such as fidelity, diversity, interpretability, and adaptability. This work outlines the algorithmic trade-offs and design considerations that underlie method selection, offering a framework for principled decision-making in learning from demonstrations.
<div id='section'>Paperid: <span id='pid'>748, <a href='https://arxiv.org/pdf/2506.23552.pdf' target='_blank'>https://arxiv.org/pdf/2506.23552.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingi Kwon, Joonghyuk Shin, Jaeseok Jung, Jaesik Park, Youngjung Uh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.23552">JAM-Flow: Joint Audio-Motion Synthesis with Flow Matching</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The intrinsic link between facial motion and speech is often overlooked in generative modeling, where talking head synthesis and text-to-speech (TTS) are typically addressed as separate tasks. This paper introduces JAM-Flow, a unified framework to simultaneously synthesize and condition on both facial motion and speech. Our approach leverages flow matching and a novel Multi-Modal Diffusion Transformer (MM-DiT) architecture, integrating specialized Motion-DiT and Audio-DiT modules. These are coupled via selective joint attention layers and incorporate key architectural choices, such as temporally aligned positional embeddings and localized joint attention masking, to enable effective cross-modal interaction while preserving modality-specific strengths. Trained with an inpainting-style objective, JAM-Flow supports a wide array of conditioning inputs-including text, reference audio, and reference motion-facilitating tasks such as synchronized talking head generation from text, audio-driven animation, and much more, within a single, coherent model. JAM-Flow significantly advances multi-modal generative modeling by providing a practical solution for holistic audio-visual synthesis. project page: https://joonghyuk.com/jamflow-web
<div id='section'>Paperid: <span id='pid'>749, <a href='https://arxiv.org/pdf/2506.13040.pdf' target='_blank'>https://arxiv.org/pdf/2506.13040.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hanz Cuevas-Velasquez, Anastasios Yiannakidis, Soyong Shin, Giorgio Becherini, Markus HÃ¶schle, Joachim Tesch, Taylor Obersat, Tsvetelina Alexiadis, Michael J. Black
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.13040">MAMMA: Markerless & Automatic Multi-Person Motion Action Capture</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present MAMMA, a markerless motion-capture pipeline that accurately recovers SMPL-X parameters from multi-view video of two-person interaction sequences. Traditional motion-capture systems rely on physical markers. Although they offer high accuracy, their requirements of specialized hardware, manual marker placement, and extensive post-processing make them costly and time-consuming. Recent learning-based methods attempt to overcome these limitations, but most are designed for single-person capture, rely on sparse keypoints, or struggle with occlusions and physical interactions. In this work, we introduce a method that predicts dense 2D surface landmarks conditioned on segmentation masks, enabling person-specific correspondence estimation even under heavy occlusion. We employ a novel architecture that exploits learnable queries for each landmark. We demonstrate that our approach can handle complex person--person interaction and offers greater accuracy than existing methods. To train our network, we construct a large, synthetic multi-view dataset combining human motions from diverse sources, including extreme poses, hand motions, and close interactions. Our dataset yields high-variability synthetic sequences with rich body contact and occlusion, and includes SMPL-X ground-truth annotations with dense 2D landmarks. The result is a system capable of capturing human motion without the need for markers. Our approach offers competitive reconstruction quality compared to commercial marker-based motion-capture solutions, without the extensive manual cleanup. Finally, we address the absence of common benchmarks for dense-landmark prediction and markerless motion capture by introducing two evaluation settings built from real multi-view sequences. We will release our dataset, benchmark, method, training code, and pre-trained model weights for research purposes.
<div id='section'>Paperid: <span id='pid'>750, <a href='https://arxiv.org/pdf/2506.12769.pdf' target='_blank'>https://arxiv.org/pdf/2506.12769.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junpeng Yue, Zepeng Wang, Yuxuan Wang, Weishuai Zeng, Jiangxing Wang, Xinrun Xu, Yu Zhang, Sipeng Zheng, Ziluo Ding, Zongqing Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.12769">RL from Physical Feedback: Aligning Large Motion Models with Humanoid Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper focuses on a critical challenge in robotics: translating text-driven human motions into executable actions for humanoid robots, enabling efficient and cost-effective learning of new behaviors. While existing text-to-motion generation methods achieve semantic alignment between language and motion, they often produce kinematically or physically infeasible motions unsuitable for real-world deployment. To bridge this sim-to-real gap, we propose Reinforcement Learning from Physical Feedback (RLPF), a novel framework that integrates physics-aware motion evaluation with text-conditioned motion generation. RLPF employs a motion tracking policy to assess feasibility in a physics simulator, generating rewards for fine-tuning the motion generator. Furthermore, RLPF introduces an alignment verification module to preserve semantic fidelity to text instructions. This joint optimization ensures both physical plausibility and instruction alignment. Extensive experiments show that RLPF greatly outperforms baseline methods in generating physically feasible motions while maintaining semantic correspondence with text instruction, enabling successful deployment on real humanoid robots.
<div id='section'>Paperid: <span id='pid'>751, <a href='https://arxiv.org/pdf/2505.19377.pdf' target='_blank'>https://arxiv.org/pdf/2505.19377.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zichong Meng, Zeyu Han, Xiaogang Peng, Yiming Xie, Huaizu Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.19377">Absolute Coordinates Make Motion Generation Easy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>State-of-the-art text-to-motion generation models rely on the kinematic-aware, local-relative motion representation popularized by HumanML3D, which encodes motion relative to the pelvis and to the previous frame with built-in redundancy. While this design simplifies training for earlier generation models, it introduces critical limitations for diffusion models and hinders applicability to downstream tasks. In this work, we revisit the motion representation and propose a radically simplified and long-abandoned alternative for text-to-motion generation: absolute joint coordinates in global space. Through systematic analysis of design choices, we show that this formulation achieves significantly higher motion fidelity, improved text alignment, and strong scalability, even with a simple Transformer backbone and no auxiliary kinematic-aware losses. Moreover, our formulation naturally supports downstream tasks such as text-driven motion control and temporal/spatial editing without additional task-specific reengineering and costly classifier guidance generation from control signals. Finally, we demonstrate promising generalization to directly generate SMPL-H mesh vertices in motion from text, laying a strong foundation for future research and motion-related applications.
<div id='section'>Paperid: <span id='pid'>752, <a href='https://arxiv.org/pdf/2505.16084.pdf' target='_blank'>https://arxiv.org/pdf/2505.16084.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zewei Zhang, Chenhao Li, Takahiro Miki, Marco Hutter
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.16084">Motion Priors Reimagined: Adapting Flat-Terrain Skills for Complex Quadruped Mobility</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reinforcement learning (RL)-based motion imitation methods trained on demonstration data can effectively learn natural and expressive motions with minimal reward engineering but often struggle to generalize to novel environments. We address this by proposing a hierarchical RL framework in which a low-level policy is first pre-trained to imitate animal motions on flat ground, thereby establishing motion priors. A subsequent high-level, goal-conditioned policy then builds on these priors, learning residual corrections that enable perceptive locomotion, local obstacle avoidance, and goal-directed navigation across diverse and rugged terrains. Simulation experiments illustrate the effectiveness of learned residuals in adapting to progressively challenging uneven terrains while still preserving the locomotion characteristics provided by the motion priors. Furthermore, our results demonstrate improvements in motion regularization over baseline models trained without motion priors under similar reward setups. Real-world experiments with an ANYmal-D quadruped robot confirm our policy's capability to generalize animal-like locomotion skills to complex terrains, demonstrating smooth and efficient locomotion and local navigation performance amidst challenging terrains with obstacles.
<div id='section'>Paperid: <span id='pid'>753, <a href='https://arxiv.org/pdf/2505.03728.pdf' target='_blank'>https://arxiv.org/pdf/2505.03728.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chung Min Kim, Brent Yi, Hongsuk Choi, Yi Ma, Ken Goldberg, Angjoo Kanazawa
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.03728">PyRoki: A Modular Toolkit for Robot Kinematic Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robot motion can have many goals. Depending on the task, we might optimize for pose error, speed, collision, or similarity to a human demonstration. Motivated by this, we present PyRoki: a modular, extensible, and cross-platform toolkit for solving kinematic optimization problems. PyRoki couples an interface for specifying kinematic variables and costs with an efficient nonlinear least squares optimizer. Unlike existing tools, it is also cross-platform: optimization runs natively on CPU, GPU, and TPU. In this paper, we present (i) the design and implementation of PyRoki, (ii) motion retargeting and planning case studies that highlight the advantages of PyRoki's modularity, and (iii) optimization benchmarking, where PyRoki can be 1.4-1.7x faster and converges to lower errors than cuRobo, an existing GPU-accelerated inverse kinematics library.
<div id='section'>Paperid: <span id='pid'>754, <a href='https://arxiv.org/pdf/2504.17371.pdf' target='_blank'>https://arxiv.org/pdf/2504.17371.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Oussema Dhaouadi, Johannes Meier, Luca Wahl, Jacques Kaiser, Luca Scalerandi, Nick Wandelburg, Zhuolun Zhou, Nijanthan Berinpanathan, Holger Banzhaf, Daniel Cremers
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.17371">Highly Accurate and Diverse Traffic Data: The DeepScenario Open 3D Dataset</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate 3D trajectory data is crucial for advancing autonomous driving. Yet, traditional datasets are usually captured by fixed sensors mounted on a car and are susceptible to occlusion. Additionally, such an approach can precisely reconstruct the dynamic environment in the close vicinity of the measurement vehicle only, while neglecting objects that are further away. In this paper, we introduce the DeepScenario Open 3D Dataset (DSC3D), a high-quality, occlusion-free dataset of 6 degrees of freedom bounding box trajectories acquired through a novel monocular camera drone tracking pipeline. Our dataset includes more than 175,000 trajectories of 14 types of traffic participants and significantly exceeds existing datasets in terms of diversity and scale, containing many unprecedented scenarios such as complex vehicle-pedestrian interaction on highly populated urban streets and comprehensive parking maneuvers from entry to exit. DSC3D dataset was captured in five various locations in Europe and the United States and include: a parking lot, a crowded inner-city, a steep urban intersection, a federal highway, and a suburban intersection. Our 3D trajectory dataset aims to enhance autonomous driving systems by providing detailed environmental 3D representations, which could lead to improved obstacle interactions and safety. We demonstrate its utility across multiple applications including motion prediction, motion planning, scenario mining, and generative reactive traffic agents. Our interactive online visualization platform and the complete dataset are publicly available at https://app.deepscenario.com, facilitating research in motion prediction, behavior modeling, and safety validation.
<div id='section'>Paperid: <span id='pid'>755, <a href='https://arxiv.org/pdf/2504.01204.pdf' target='_blank'>https://arxiv.org/pdf/2504.01204.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuan Li, Qianli Ma, Tsung-Yi Lin, Yongxin Chen, Chenfanfu Jiang, Ming-Yu Liu, Donglai Xiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.01204">Articulated Kinematics Distillation from Video Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present Articulated Kinematics Distillation (AKD), a framework for generating high-fidelity character animations by merging the strengths of skeleton-based animation and modern generative models. AKD uses a skeleton-based representation for rigged 3D assets, drastically reducing the Degrees of Freedom (DoFs) by focusing on joint-level control, which allows for efficient, consistent motion synthesis. Through Score Distillation Sampling (SDS) with pre-trained video diffusion models, AKD distills complex, articulated motions while maintaining structural integrity, overcoming challenges faced by 4D neural deformation fields in preserving shape consistency. This approach is naturally compatible with physics-based simulation, ensuring physically plausible interactions. Experiments show that AKD achieves superior 3D consistency and motion quality compared with existing works on text-to-4D generation. Project page: https://research.nvidia.com/labs/dir/akd/
<div id='section'>Paperid: <span id='pid'>756, <a href='https://arxiv.org/pdf/2502.11370.pdf' target='_blank'>https://arxiv.org/pdf/2502.11370.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pengming Zhu, Zongtan Zhou, Weijia Yao, Wei Dai, Zhiwen Zeng, Huimin Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.11370">HI-GVF: Shared Control based on Human-Influenced Guiding Vector Fields for Human-multi-robot Cooperation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human-multi-robot shared control leverages human decision-making and robotic autonomy to enhance human-robot collaboration. While widely studied, existing systems often adopt a leader-follower model, limiting robot autonomy to some extent. Besides, a human is required to directly participate in the motion control of robots through teleoperation, which significantly burdens the operator. To alleviate these two issues, we propose a layered shared control computing framework using human-influenced guiding vector fields (HI-GVF) for human-robot collaboration. HI-GVF guides the multi-robot system along a desired path specified by the human. Then, an intention field is designed to merge the human and robot intentions, accelerating the propagation of the human intention within the multi-robot system. Moreover, we give the stability analysis of the proposed model and use collision avoidance based on safety barrier certificates to fine-tune the velocity. Eventually, considering the firefighting task as an example scenario, we conduct simulations and experiments using multiple human-robot interfaces (brain-computer interface, myoelectric wristband, eye-tracking), and the results demonstrate that our proposed approach boosts the effectiveness and performance of the task.
<div id='section'>Paperid: <span id='pid'>757, <a href='https://arxiv.org/pdf/2502.09533.pdf' target='_blank'>https://arxiv.org/pdf/2502.09533.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fei Shen, Cong Wang, Junyao Gao, Qin Guo, Jisheng Dang, Jinhui Tang, Tat-Seng Chua
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.09533">Long-Term TalkingFace Generation via Motion-Prior Conditional Diffusion Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in conditional diffusion models have shown promise for generating realistic TalkingFace videos, yet challenges persist in achieving consistent head movement, synchronized facial expressions, and accurate lip synchronization over extended generations. To address these, we introduce the \textbf{M}otion-priors \textbf{C}onditional \textbf{D}iffusion \textbf{M}odel (\textbf{MCDM}), which utilizes both archived and current clip motion priors to enhance motion prediction and ensure temporal consistency. The model consists of three key elements: (1) an archived-clip motion-prior that incorporates historical frames and a reference frame to preserve identity and context; (2) a present-clip motion-prior diffusion model that captures multimodal causality for accurate predictions of head movements, lip sync, and expressions; and (3) a memory-efficient temporal attention mechanism that mitigates error accumulation by dynamically storing and updating motion features. We also release the \textbf{TalkingFace-Wild} dataset, a multilingual collection of over 200 hours of footage across 10 languages. Experimental results demonstrate the effectiveness of MCDM in maintaining identity and motion continuity for long-term TalkingFace generation. Code, models, and datasets will be publicly available.
<div id='section'>Paperid: <span id='pid'>758, <a href='https://arxiv.org/pdf/2411.16575.pdf' target='_blank'>https://arxiv.org/pdf/2411.16575.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zichong Meng, Yiming Xie, Xiaogang Peng, Zeyu Han, Huaizu Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.16575">Rethinking Diffusion for Text-Driven Human Motion Generation: Redundant Representations, Evaluation, and Masked Autoregression</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Since 2023, Vector Quantization (VQ)-based discrete generation methods have rapidly dominated human motion generation, primarily surpassing diffusion-based continuous generation methods in standard performance metrics. However, VQ-based methods have inherent limitations. Representing continuous motion data as limited discrete tokens leads to inevitable information loss, reduces the diversity of generated motions, and restricts their ability to function effectively as motion priors or generation guidance. In contrast, the continuous space generation nature of diffusion-based methods makes them well-suited to address these limitations and with even potential for model scalability. In this work, we systematically investigate why current VQ-based methods perform well and explore the limitations of existing diffusion-based methods from the perspective of motion data representation and distribution. Drawing on these insights, we preserve the inherent strengths of a diffusion-based human motion generation model and gradually optimize it with inspiration from VQ-based approaches. Our approach introduces a human motion diffusion model enabled to perform masked autoregression, optimized with a reformed data representation and distribution. Additionally, we propose a more robust evaluation method to assess different approaches. Extensive experiments on various datasets demonstrate our method outperforms previous methods and achieves state-of-the-art performances.
<div id='section'>Paperid: <span id='pid'>759, <a href='https://arxiv.org/pdf/2409.16154.pdf' target='_blank'>https://arxiv.org/pdf/2409.16154.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alexander Prutsch, Horst Bischof, Horst Possegger
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.16154">Efficient Motion Prediction: A Lightweight & Accurate Trajectory Prediction Model With Fast Training and Inference Speed</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>For efficient and safe autonomous driving, it is essential that autonomous vehicles can predict the motion of other traffic agents. While highly accurate, current motion prediction models often impose significant challenges in terms of training resource requirements and deployment on embedded hardware. We propose a new efficient motion prediction model, which achieves highly competitive benchmark results while training only a few hours on a single GPU. Due to our lightweight architectural choices and the focus on reducing the required training resources, our model can easily be applied to custom datasets. Furthermore, its low inference latency makes it particularly suitable for deployment in autonomous applications with limited computing resources.
<div id='section'>Paperid: <span id='pid'>760, <a href='https://arxiv.org/pdf/2409.15179.pdf' target='_blank'>https://arxiv.org/pdf/2409.15179.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yue Han, Junwei Zhu, Yuxiang Feng, Xiaozhong Ji, Keke He, Xiangtai Li, zhucun xue, Yong Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.15179">MIMAFace: Face Animation via Motion-Identity Modulated Appearance Feature Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current diffusion-based face animation methods generally adopt a ReferenceNet (a copy of U-Net) and a large amount of curated self-acquired data to learn appearance features, as robust appearance features are vital for ensuring temporal stability. However, when trained on public datasets, the results often exhibit a noticeable performance gap in image quality and temporal consistency. To address this issue, we meticulously examine the essential appearance features in the facial animation tasks, which include motion-agnostic (e.g., clothing, background) and motion-related (e.g., facial details) texture components, along with high-level discriminative identity features. Drawing from this analysis, we introduce a Motion-Identity Modulated Appearance Learning Module (MIA) that modulates CLIP features at both motion and identity levels. Additionally, to tackle the semantic/ color discontinuities between clips, we design an Inter-clip Affinity Learning Module (ICA) to model temporal relationships across clips. Our method achieves precise facial motion control (i.e., expressions and gaze), faithful identity preservation, and generates animation videos that maintain both intra/inter-clip temporal consistency. Moreover, it easily adapts to various modalities of driving sources. Extensive experiments demonstrate the superiority of our method.
<div id='section'>Paperid: <span id='pid'>761, <a href='https://arxiv.org/pdf/2409.11696.pdf' target='_blank'>https://arxiv.org/pdf/2409.11696.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiawei Sun, Jiahui Li, Tingchen Liu, Chengran Yuan, Shuo Sun, Zefan Huang, Anthony Wong, Keng Peng Tee, Marcelo H. Ang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.11696">RMP-YOLO: A Robust Motion Predictor for Partially Observable Scenarios even if You Only Look Once</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce RMP-YOLO, a unified framework designed to provide robust motion predictions even with incomplete input data. Our key insight stems from the observation that complete and reliable historical trajectory data plays a pivotal role in ensuring accurate motion prediction. Therefore, we propose a new paradigm that prioritizes the reconstruction of intact historical trajectories before feeding them into the prediction modules. Our approach introduces a novel scene tokenization module to enhance the extraction and fusion of spatial and temporal features. Following this, our proposed recovery module reconstructs agents' incomplete historical trajectories by leveraging local map topology and interactions with nearby agents. The reconstructed, clean historical data is then integrated into the downstream prediction modules. Our framework is able to effectively handle missing data of varying lengths and remains robust against observation noise, while maintaining high prediction accuracy. Furthermore, our recovery module is compatible with existing prediction models, ensuring seamless integration. Extensive experiments validate the effectiveness of our approach, and deployment in real-world autonomous vehicles confirms its practical utility. In the 2024 Waymo Motion Prediction Competition, our method, RMP-YOLO, achieves state-of-the-art performance, securing third place.
<div id='section'>Paperid: <span id='pid'>762, <a href='https://arxiv.org/pdf/2405.01461.pdf' target='_blank'>https://arxiv.org/pdf/2405.01461.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenshuo Chen, Hongru Xiao, Erhang Zhang, Lijie Hu, Lei Wang, Mengyuan Liu, Chen Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.01461">SATO: Stable Text-to-Motion Framework</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Is the Text to Motion model robust? Recent advancements in Text to Motion models primarily stem from more accurate predictions of specific actions. However, the text modality typically relies solely on pre-trained Contrastive Language-Image Pretraining (CLIP) models. Our research has uncovered a significant issue with the text-to-motion model: its predictions often exhibit inconsistent outputs, resulting in vastly different or even incorrect poses when presented with semantically similar or identical text inputs. In this paper, we undertake an analysis to elucidate the underlying causes of this instability, establishing a clear link between the unpredictability of model outputs and the erratic attention patterns of the text encoder module. Consequently, we introduce a formal framework aimed at addressing this issue, which we term the Stable Text-to-Motion Framework (SATO). SATO consists of three modules, each dedicated to stable attention, stable prediction, and maintaining a balance between accuracy and robustness trade-off. We present a methodology for constructing an SATO that satisfies the stability of attention and prediction. To verify the stability of the model, we introduced a new textual synonym perturbation dataset based on HumanML3D and KIT-ML. Results show that SATO is significantly more stable against synonyms and other slight perturbations while keeping its high accuracy performance.
<div id='section'>Paperid: <span id='pid'>763, <a href='https://arxiv.org/pdf/2405.00797.pdf' target='_blank'>https://arxiv.org/pdf/2405.00797.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiahui Li, Tianle Shen, Zekai Gu, Jiawei Sun, Chengran Yuan, Yuhang Han, Shuo Sun, Marcelo H. Ang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.00797">ADM: Accelerated Diffusion Model via Estimated Priors for Robust Motion Prediction under Uncertainties</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion prediction is a challenging problem in autonomous driving as it demands the system to comprehend stochastic dynamics and the multi-modal nature of real-world agent interactions. Diffusion models have recently risen to prominence, and have proven particularly effective in pedestrian motion prediction tasks. However, the significant time consumption and sensitivity to noise have limited the real-time predictive capability of diffusion models. In response to these impediments, we propose a novel diffusion-based, acceleratable framework that adeptly predicts future trajectories of agents with enhanced resistance to noise. The core idea of our model is to learn a coarse-grained prior distribution of trajectory, which can skip a large number of denoise steps. This advancement not only boosts sampling efficiency but also maintains the fidelity of prediction accuracy. Our method meets the rigorous real-time operational standards essential for autonomous vehicles, enabling prompt trajectory generation that is vital for secure and efficient navigation. Through extensive experiments, our method speeds up the inference time to 136ms compared to standard diffusion model, and achieves significant improvement in multi-agent motion prediction on the Argoverse 1 motion forecasting dataset.
<div id='section'>Paperid: <span id='pid'>764, <a href='https://arxiv.org/pdf/2404.18630.pdf' target='_blank'>https://arxiv.org/pdf/2404.18630.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenbo Wang, Hsuan-I Ho, Chen Guo, Boxiang Rong, Artur Grigorev, Jie Song, Juan Jose Zarate, Otmar Hilliges
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.18630">4D-DRESS: A 4D Dataset of Real-world Human Clothing with Semantic Annotations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The studies of human clothing for digital avatars have predominantly relied on synthetic datasets. While easy to collect, synthetic data often fall short in realism and fail to capture authentic clothing dynamics. Addressing this gap, we introduce 4D-DRESS, the first real-world 4D dataset advancing human clothing research with its high-quality 4D textured scans and garment meshes. 4D-DRESS captures 64 outfits in 520 human motion sequences, amounting to 78k textured scans. Creating a real-world clothing dataset is challenging, particularly in annotating and segmenting the extensive and complex 4D human scans. To address this, we develop a semi-automatic 4D human parsing pipeline. We efficiently combine a human-in-the-loop process with automation to accurately label 4D scans in diverse garments and body movements. Leveraging precise annotations and high-quality garment meshes, we establish several benchmarks for clothing simulation and reconstruction. 4D-DRESS offers realistic and challenging data that complements synthetic sources, paving the way for advancements in research of lifelike human clothing. Website: https://ait.ethz.ch/4d-dress.
<div id='section'>Paperid: <span id='pid'>765, <a href='https://arxiv.org/pdf/2404.10295.pdf' target='_blank'>https://arxiv.org/pdf/2404.10295.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiawei Sun, Chengran Yuan, Shuo Sun, Shanze Wang, Yuhang Han, Shuailei Ma, Zefan Huang, Anthony Wong, Keng Peng Tee, Marcelo H. Ang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.10295">ControlMTR: Control-Guided Motion Transformer with Scene-Compliant Intention Points for Feasible Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The ability to accurately predict feasible multimodal future trajectories of surrounding traffic participants is crucial for behavior planning in autonomous vehicles. The Motion Transformer (MTR), a state-of-the-art motion prediction method, alleviated mode collapse and instability during training and enhanced overall prediction performance by replacing conventional dense future endpoints with a small set of fixed prior motion intention points. However, the fixed prior intention points make the MTR multi-modal prediction distribution over-scattered and infeasible in many scenarios. In this paper, we propose the ControlMTR framework to tackle the aforementioned issues by generating scene-compliant intention points and additionally predicting driving control commands, which are then converted into trajectories by a simple kinematic model with soft constraints. These control-generated trajectories will guide the directly predicted trajectories by an auxiliary loss function. Together with our proposed scene-compliant intention points, they can effectively restrict the prediction distribution within the road boundaries and suppress infeasible off-road predictions while enhancing prediction performance. Remarkably, without resorting to additional model ensemble techniques, our method surpasses the baseline MTR model across all performance metrics, achieving notable improvements of 5.22% in SoftmAP and a 4.15% reduction in MissRate. Our approach notably results in a 41.85% reduction in the cross-boundary rate of the MTR, effectively ensuring that the prediction distribution is confined within the drivable area.
<div id='section'>Paperid: <span id='pid'>766, <a href='https://arxiv.org/pdf/2511.17401.pdf' target='_blank'>https://arxiv.org/pdf/2511.17401.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoshan Zhou, Carol C. Menassa, Vineet R. Kamat
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.17401">Feasibility of Embodied Dynamics Based Bayesian Learning for Continuous Pursuit Motion Control of Assistive Mobile Robots in the Built Environment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Non-invasive electroencephalography (EEG)-based brain-computer interfaces (BCIs) offer an intuitive means for individuals with severe motor impairments to independently operate assistive robotic wheelchairs and navigate built environments. Despite considerable progress in BCI research, most current motion control systems are limited to discrete commands, rather than supporting continuous pursuit, where users can freely adjust speed and direction in real time. Such natural mobility control is, however, essential for wheelchair users to navigate complex public spaces, such as transit stations, airports, hospitals, and indoor corridors, to interact socially with the dynamic populations with agility, and to move flexibly and comfortably as autonomous driving is refined to allow movement at will. In this study, we address the gap of continuous pursuit motion control in BCIs by proposing and validating a brain-inspired Bayesian inference framework, where embodied dynamics in acceleration-based motor representations are decoded. This approach contrasts with conventional kinematics-level decoding and deep learning-based methods. Using a public dataset with sixteen hours of EEG from four subjects performing motor imagery-based target-following, we demonstrate that our method, utilizing Automatic Relevance Determination for feature selection and continual online learning, reduces the normalized mean squared error between predicted and true velocities by 72% compared to autoregressive and EEGNet-based methods in a session-accumulative transfer learning setting. Theoretically, these findings empirically support embodied cognition theory and reveal the brain's intrinsic motor control dynamics in an embodied and predictive nature. Practically, grounding EEG decoding in the same dynamical principles that govern biological motion offers a promising path toward more stable and intuitive BCI control.
<div id='section'>Paperid: <span id='pid'>767, <a href='https://arxiv.org/pdf/2511.09827.pdf' target='_blank'>https://arxiv.org/pdf/2511.09827.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aymen Mir, Jian Wang, Riza Alp Guler, Chuan Guo, Gerard Pons-Moll, Bing Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.09827">AHA! Animating Human Avatars in Diverse Scenes with Gaussian Splatting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a novel framework for animating humans in 3D scenes using 3D Gaussian Splatting (3DGS), a neural scene representation that has recently achieved state-of-the-art photorealistic results for novel-view synthesis but remains under-explored for human-scene animation and interaction. Unlike existing animation pipelines that use meshes or point clouds as the underlying 3D representation, our approach introduces the use of 3DGS as the 3D representation to the problem of animating humans in scenes. By representing humans and scenes as Gaussians, our approach allows for geometry-consistent free-viewpoint rendering of humans interacting with 3D scenes. Our key insight is that the rendering can be decoupled from the motion synthesis and each sub-problem can be addressed independently, without the need for paired human-scene data. Central to our method is a Gaussian-aligned motion module that synthesizes motion without explicit scene geometry, using opacity-based cues and projected Gaussian structures to guide human placement and pose alignment. To ensure natural interactions, we further propose a human-scene Gaussian refinement optimization that enforces realistic contact and navigation. We evaluate our approach on scenes from Scannet++ and the SuperSplat library, and on avatars reconstructed from sparse and dense multi-view human capture. Finally, we demonstrate that our framework allows for novel applications such as geometry-consistent free-viewpoint rendering of edited monocular RGB videos with new animated humans, showcasing the unique advantage of 3DGS for monocular video-based human animation.
<div id='section'>Paperid: <span id='pid'>768, <a href='https://arxiv.org/pdf/2509.22058.pdf' target='_blank'>https://arxiv.org/pdf/2509.22058.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qifeng Wang, Weigang Li, Lei Nie, Xin Xu, Wenping Liu, Zhe Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22058">An Adaptive ICP LiDAR Odometry Based on Reliable Initial Pose</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As a key technology for autonomous navigation and positioning in mobile robots, light detection and ranging (LiDAR) odometry is widely used in autonomous driving applications. The Iterative Closest Point (ICP)-based methods have become the core technique in LiDAR odometry due to their efficient and accurate point cloud registration capability. However, some existing ICP-based methods do not consider the reliability of the initial pose, which may cause the method to converge to a local optimum. Furthermore, the absence of an adaptive mechanism hinders the effective handling of complex dynamic environments, resulting in a significant degradation of registration accuracy. To address these issues, this paper proposes an adaptive ICP-based LiDAR odometry method that relies on a reliable initial pose. First, distributed coarse registration based on density filtering is employed to obtain the initial pose estimation. The reliable initial pose is then selected by comparing it with the motion prediction pose, reducing the initial error between the source and target point clouds. Subsequently, by combining the current and historical errors, the adaptive threshold is dynamically adjusted to accommodate the real-time changes in the dynamic environment. Finally, based on the reliable initial pose and the adaptive threshold, point-to-plane adaptive ICP registration is performed from the current frame to the local map, achieving high-precision alignment of the source and target point clouds. Extensive experiments on the public KITTI dataset demonstrate that the proposed method outperforms existing approaches and significantly enhances the accuracy of LiDAR odometry.
<div id='section'>Paperid: <span id='pid'>769, <a href='https://arxiv.org/pdf/2508.15185.pdf' target='_blank'>https://arxiv.org/pdf/2508.15185.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dingzhu Wen, Sijing Xie, Xiaowen Cao, Yuanhao Cui, Jie Xu, Yuanming Shi, Shuguang Cui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15185">Integrated Sensing, Communication, and Computation for Over-the-Air Federated Edge Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper studies an over-the-air federated edge learning (Air-FEEL) system with integrated sensing, communication, and computation (ISCC), in which one edge server coordinates multiple edge devices to wirelessly sense the objects and use the sensing data to collaboratively train a machine learning model for recognition tasks. In this system, over-the-air computation (AirComp) is employed to enable one-shot model aggregation from edge devices. Under this setup, we analyze the convergence behavior of the ISCC-enabled Air-FEEL in terms of the loss function degradation, by particularly taking into account the wireless sensing noise during the training data acquisition and the AirComp distortions during the over-the-air model aggregation. The result theoretically shows that sensing, communication, and computation compete for network resources to jointly decide the convergence rate. Based on the analysis, we design the ISCC parameters under the target of maximizing the loss function degradation while ensuring the latency and energy budgets in each round. The challenge lies on the tightly coupled processes of sensing, communication, and computation among different devices. To tackle the challenge, we derive a low-complexity ISCC algorithm by alternately optimizing the batch size control and the network resource allocation. It is found that for each device, less sensing power should be consumed if a larger batch of data samples is obtained and vice versa. Besides, with a given batch size, the optimal computation speed of one device is the minimum one that satisfies the latency constraint. Numerical results based on a human motion recognition task verify the theoretical convergence analysis and show that the proposed ISCC algorithm well coordinates the batch size control and resource allocation among sensing, communication, and computation to enhance the learning performance.
<div id='section'>Paperid: <span id='pid'>770, <a href='https://arxiv.org/pdf/2505.19239.pdf' target='_blank'>https://arxiv.org/pdf/2505.19239.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chen Shi, Shaoshuai Shi, Kehua Sheng, Bo Zhang, Li Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.19239">DriveX: Omni Scene Modeling for Learning Generalizable World Knowledge in Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Data-driven learning has advanced autonomous driving, yet task-specific models struggle with out-of-distribution scenarios due to their narrow optimization objectives and reliance on costly annotated data. We present DriveX, a self-supervised world model that learns generalizable scene dynamics and holistic representations (geometric, semantic, and motion) from large-scale driving videos. DriveX introduces Omni Scene Modeling (OSM), a module that unifies multimodal supervision-3D point cloud forecasting, 2D semantic representation, and image generation-to capture comprehensive scene evolution. To simplify learning complex dynamics, we propose a decoupled latent world modeling strategy that separates world representation learning from future state decoding, augmented by dynamic-aware ray sampling to enhance motion modeling. For downstream adaptation, we design Future Spatial Attention (FSA), a unified paradigm that dynamically aggregates spatiotemporal features from DriveX's predictions to enhance task-specific inference. Extensive experiments demonstrate DriveX's effectiveness: it achieves significant improvements in 3D future point cloud prediction over prior work, while attaining state-of-the-art results on diverse tasks including occupancy prediction, flow estimation, and end-to-end driving. These results validate DriveX's capability as a general-purpose world model, paving the way for robust and unified autonomous driving frameworks.
<div id='section'>Paperid: <span id='pid'>771, <a href='https://arxiv.org/pdf/2412.01234.pdf' target='_blank'>https://arxiv.org/pdf/2412.01234.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenru Liu, Yongkang Song, Chengzhen Meng, Zhiyu Huang, Haochen Liu, Chen Lv, Jun Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.01234">Integrating Decision-Making Into Differentiable Optimization Guided Learning for End-to-End Planning of Autonomous Vehicles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We address the decision-making capability within an end-to-end planning framework that focuses on motion prediction, decision-making, and trajectory planning. Specifically, we formulate decision-making and trajectory planning as a differentiable nonlinear optimization problem, which ensures compatibility with learning-based modules to establish an end-to-end trainable architecture. This optimization introduces explicit objectives related to safety, traveling efficiency, and riding comfort, guiding the learning process in our proposed pipeline. Intrinsic constraints resulting from the decision-making task are integrated into the optimization formulation and preserved throughout the learning process. By integrating the differentiable optimizer with a neural network predictor, the proposed framework is end-to-end trainable, aligning various driving tasks with ultimate performance goals defined by the optimization objectives. The proposed framework is trained and validated using the Waymo Open Motion dataset. The open-loop testing reveals that while the planning outcomes using our method do not always resemble the expert trajectory, they consistently outperform baseline approaches with improved safety, traveling efficiency, and riding comfort. The closed-loop testing further demonstrates the effectiveness of optimizing decisions and improving driving performance. Ablation studies demonstrate that the initialization provided by the learning-based prediction module is essential for the convergence of the optimizer as well as the overall driving performance.
<div id='section'>Paperid: <span id='pid'>772, <a href='https://arxiv.org/pdf/2411.19324.pdf' target='_blank'>https://arxiv.org/pdf/2411.19324.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zeqi Xiao, Wenqi Ouyang, Yifan Zhou, Shuai Yang, Lei Yang, Jianlou Si, Xingang Pan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.19324">Trajectory Attention for Fine-grained Video Motion Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in video generation have been greatly driven by video diffusion models, with camera motion control emerging as a crucial challenge in creating view-customized visual content. This paper introduces trajectory attention, a novel approach that performs attention along available pixel trajectories for fine-grained camera motion control. Unlike existing methods that often yield imprecise outputs or neglect temporal correlations, our approach possesses a stronger inductive bias that seamlessly injects trajectory information into the video generation process. Importantly, our approach models trajectory attention as an auxiliary branch alongside traditional temporal attention. This design enables the original temporal attention and the trajectory attention to work in synergy, ensuring both precise motion control and new content generation capability, which is critical when the trajectory is only partially available. Experiments on camera motion control for images and videos demonstrate significant improvements in precision and long-range consistency while maintaining high-quality generation. Furthermore, we show that our approach can be extended to other video motion control tasks, such as first-frame-guided video editing, where it excels in maintaining content consistency over large spatial and temporal ranges.
<div id='section'>Paperid: <span id='pid'>773, <a href='https://arxiv.org/pdf/2411.01442.pdf' target='_blank'>https://arxiv.org/pdf/2411.01442.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Beomseok Kang, Priyabrata Saha, Sudarshan Sharma, Biswadeep Chakraborty, Saibal Mukhopadhyay
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.01442">Online Relational Inference for Evolving Multi-agent Interacting Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a novel framework, Online Relational Inference (ORI), designed to efficiently identify hidden interaction graphs in evolving multi-agent interacting systems using streaming data. Unlike traditional offline methods that rely on a fixed training set, ORI employs online backpropagation, updating the model with each new data point, thereby allowing it to adapt to changing environments in real-time. A key innovation is the use of an adjacency matrix as a trainable parameter, optimized through a new adaptive learning rate technique called AdaRelation, which adjusts based on the historical sensitivity of the decoder to changes in the interaction graph. Additionally, a data augmentation method named Trajectory Mirror (TM) is introduced to improve generalization by exposing the model to varied trajectory patterns. Experimental results on both synthetic datasets and real-world data (CMU MoCap for human motion) demonstrate that ORI significantly improves the accuracy and adaptability of relational inference in dynamic settings compared to existing methods. This approach is model-agnostic, enabling seamless integration with various neural relational inference (NRI) architectures, and offers a robust solution for real-time applications in complex, evolving systems.
<div id='section'>Paperid: <span id='pid'>774, <a href='https://arxiv.org/pdf/2410.05186.pdf' target='_blank'>https://arxiv.org/pdf/2410.05186.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Filip NovÃ¡k, TomÃ¡Å¡ BÃ¡Äa, OndÅej ProchÃ¡zka, Martin Saska
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.05186">State estimation of marine vessels affected by waves by unmanned aerial vehicles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A novel approach for robust state estimation of marine vessels in rough water is proposed in this paper to enable tight collaboration between Unmanned Aerial Vehicles (UAVs) and a marine vessel, such as cooperative landing or object manipulation, regardless of weather conditions. Our study of marine vessel (in our case Unmanned Surface Vehicle (USV)) dynamics influenced by strong wave motion has resulted in a novel nonlinear mathematical USV model with 6 degrees of freedom (DOFs), which is required for precise USV state estimation and motion prediction. The proposed state estimation and prediction approach fuses data from multiple sensors onboard the UAV and the USV to enable redundancy and robustness under varying weather conditions of real-world applications. The proposed approach provides estimated states of the USV with 6 DOFs and predicts its future states to enable tight control of both vehicles on a receding control horizon. The proposed approach was extensively tested in the realistic Gazebo simulator and successfully experimentally validated in many real-world experiments representing different application scenarios, including agile landing on an oscillating and moving USV. A comparative study indicates that the proposed approach significantly surpassed the current state-of-the-art.
<div id='section'>Paperid: <span id='pid'>775, <a href='https://arxiv.org/pdf/2408.15250.pdf' target='_blank'>https://arxiv.org/pdf/2408.15250.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kleio Fragkedaki, Frank J. Jiang, Karl H. Johansson, Jonas MÃ¥rtensson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.15250">Pedestrian Motion Prediction Using Transformer-based Behavior Clustering and Data-Driven Reachability Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we present a transformer-based framework for predicting future pedestrian states based on clustered historical trajectory data. In previous studies, researchers propose enhancing pedestrian trajectory predictions by using manually crafted labels to categorize pedestrian behaviors and intentions. However, these approaches often only capture a limited range of pedestrian behaviors and introduce human bias into the predictions. To alleviate the dependency on manually crafted labels, we utilize a transformer encoder coupled with hierarchical density-based clustering to automatically identify diverse behavior patterns, and use these clusters in data-driven reachability analysis. By using a transformer-based approach, we seek to enhance the representation of pedestrian trajectories and uncover characteristics or features that are subsequently used to group trajectories into different "behavior" clusters. We show that these behavior clusters can be used with data-driven reachability analysis, yielding an end-to-end data-driven approach to predicting the future motion of pedestrians. We train and evaluate our approach on a real pedestrian dataset, showcasing its effectiveness in forecasting pedestrian movements.
<div id='section'>Paperid: <span id='pid'>776, <a href='https://arxiv.org/pdf/2405.14017.pdf' target='_blank'>https://arxiv.org/pdf/2405.14017.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Zhang, Di Chang, Fang Li, Mohammad Soleymani, Narendra Ahuja
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.14017">MagicPose4D: Crafting Articulated Models with Appearance and Motion Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the success of 2D and 3D visual generative models, there is growing interest in generating 4D content. Existing methods primarily rely on text prompts to produce 4D content, but they often fall short of accurately defining complex or rare motions. To address this limitation, we propose MagicPose4D, a novel framework for refined control over both appearance and motion in 4D generation. Unlike current 4D generation methods, MagicPose4D accepts monocular videos or mesh sequences as motion prompts, enabling precise and customizable motion control. MagicPose4D comprises two key modules: (i) Dual-Phase 4D Reconstruction Module, which operates in two phases. The first phase focuses on capturing the model's shape using accurate 2D supervision and less accurate but geometrically informative 3D pseudo-supervision without imposing skeleton constraints. The second phase extracts the 3D motion (skeleton poses) using more accurate pseudo-3D supervision, obtained in the first phase and introduces kinematic chain-based skeleton constraints to ensure physical plausibility. Additionally, we propose a Global-local Chamfer loss that aligns the overall distribution of predicted mesh vertices with the supervision while maintaining part-level alignment without extra annotations. (ii) Cross-category Motion Transfer Module, which leverages the extracted motion from the 4D reconstruction module and uses a kinematic-chain-based skeleton to achieve cross-category motion transfer. It ensures smooth transitions between frames through dynamic rigidity, facilitating robust generalization without additional training. Through extensive experiments, we demonstrate that MagicPose4D significantly improves the accuracy and consistency of 4D content generation, outperforming existing methods in various benchmarks.
<div id='section'>Paperid: <span id='pid'>777, <a href='https://arxiv.org/pdf/2405.13955.pdf' target='_blank'>https://arxiv.org/pdf/2405.13955.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoshan Zhou, Carol C. Menassa, Vineet R. Kamat
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.13955">Biologically Inspired Predictive Coding TCN-Transformer for Anticipatory Human-Robot Interaction in Shared Physical Spaces</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As mobile robots increasingly operate in environments shared with humans, proactively anticipating human motion rather than responding reactively is critical for preempting collisions during close-proximity navigation, while maintaining mobility efficiency and avoiding unnecessary yields. A timely and motivating engineering application is how autonomous vehicles interpret ambiguous right-of-way such as unsignalized pedestrian crossings. To address this challenge, this study explores the feasibility of decoding preparatory neural activity from wearable electroencephalography (EEG) to predict human motion intention before it is behaviorally expressed. Drawing inspiration from biological predictive coding mechanisms between the sensorimotor cortex and insula-frontoparietal network, we implement this principle in a Temporal Convolutional Network-Transformer architecture to decode fast-evolving EEG signals underlying perception-action transitions. In experiments involving twelve participants simulating road-crossing decisions under varying traffic volume, marked crosswalks, and traffic signals, neurophysiological analyses reveal hemispheric asymmetries in functional specialization and identify high-beta oscillations (16-25 Hz) in the right fronto-central region (F4) as robust neural markers of motor readiness and decision commitment. Using sliding-window feature extraction, we benchmarked sixteen classification models across traditional, recurrent, and convolutional deep learning architectures, and found that our approach achieved the highest Area Under the Curve (AUC) of 0.727 with an approximate 1-second look-ahead. This work demonstrates how biologically grounded temporal architectures can enhance anticipatory intelligence in autonomous systems and represents the first step toward proactive and adaptive human-robot interaction in the built environment.
<div id='section'>Paperid: <span id='pid'>778, <a href='https://arxiv.org/pdf/2405.04963.pdf' target='_blank'>https://arxiv.org/pdf/2405.04963.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yitong Jin, Zhiping Qiu, Yi Shi, Shuangpeng Sun, Chongwu Wang, Donghao Pan, Jiachen Zhao, Zhenghao Liang, Yuan Wang, Xiaobing Li, Feng Yu, Tao Yu, Qionghai Dai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.04963">Audio Matters Too! Enhancing Markerless Motion Capture with Audio Signals for String Performance Capture</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we touch on the problem of markerless multi-modal human motion capture especially for string performance capture which involves inherently subtle hand-string contacts and intricate movements. To fulfill this goal, we first collect a dataset, named String Performance Dataset (SPD), featuring cello and violin performances. The dataset includes videos captured from up to 23 different views, audio signals, and detailed 3D motion annotations of the body, hands, instrument, and bow. Moreover, to acquire the detailed motion annotations, we propose an audio-guided multi-modal motion capture framework that explicitly incorporates hand-string contacts detected from the audio signals for solving detailed hand poses. This framework serves as a baseline for string performance capture in a completely markerless manner without imposing any external devices on performers, eliminating the potential of introducing distortion in such delicate movements. We argue that the movements of performers, particularly the sound-producing gestures, contain subtle information often elusive to visual methods but can be inferred and retrieved from audio cues. Consequently, we refine the vision-based motion capture results through our innovative audio-guided approach, simultaneously clarifying the contact relationship between the performer and the instrument, as deduced from the audio. We validate the proposed framework and conduct ablation studies to demonstrate its efficacy. Our results outperform current state-of-the-art vision-based algorithms, underscoring the feasibility of augmenting visual motion capture with audio modality. To the best of our knowledge, SPD is the first dataset for musical instrument performance, covering fine-grained hand motion details in a multi-modal, large-scale collection.
<div id='section'>Paperid: <span id='pid'>779, <a href='https://arxiv.org/pdf/2403.19417.pdf' target='_blank'>https://arxiv.org/pdf/2403.19417.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinyu Zhan, Lixin Yang, Yifei Zhao, Kangrui Mao, Hanlin Xu, Zenan Lin, Kailin Li, Cewu Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.19417">OAKINK2: A Dataset of Bimanual Hands-Object Manipulation in Complex Task Completion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present OAKINK2, a dataset of bimanual object manipulation tasks for complex daily activities. In pursuit of constructing the complex tasks into a structured representation, OAKINK2 introduces three level of abstraction to organize the manipulation tasks: Affordance, Primitive Task, and Complex Task. OAKINK2 features on an object-centric perspective for decoding the complex tasks, treating them as a sequence of object affordance fulfillment. The first level, Affordance, outlines the functionalities that objects in the scene can afford, the second level, Primitive Task, describes the minimal interaction units that humans interact with the object to achieve its affordance, and the third level, Complex Task, illustrates how Primitive Tasks are composed and interdependent. OAKINK2 dataset provides multi-view image streams and precise pose annotations for the human body, hands and various interacting objects. This extensive collection supports applications such as interaction reconstruction and motion synthesis. Based on the 3-level abstraction of OAKINK2, we explore a task-oriented framework for Complex Task Completion (CTC). CTC aims to generate a sequence of bimanual manipulation to achieve task objectives. Within the CTC framework, we employ Large Language Models (LLMs) to decompose the complex task objectives into sequences of Primitive Tasks and have developed a Motion Fulfillment Model that generates bimanual hand motion for each Primitive Task. OAKINK2 datasets and models are available at https://oakink.net/v2.
<div id='section'>Paperid: <span id='pid'>780, <a href='https://arxiv.org/pdf/2512.06133.pdf' target='_blank'>https://arxiv.org/pdf/2512.06133.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Melone Nyoba Tchonkeu, Soulaimane Berkane, Tarek Hamel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.06133">A Nonlinear Observer for Air-Velocity and Attitude Estimation Using Pitot and Barometric Measurements</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper addresses the problem of estimating air velocity and full attitude for unmanned aerial vehicles (UAVs) in GNSS-denied environments using minimal onboard sensing-an interesting and practically relevant challenge for UAV navigation. The contribution of the paper is twofold: (i) an observability analysis establishing the conditions for uniform observability, which are useful for trajectory planning and motion control of the UAV; and (ii) the design of a nonlinear observer on SO3R3R that incorporates pitot-tube, barometric altitude, and magnetometer measurements as outputs, with IMU data used as inputs, within a unified framework. Simulation results are presented to confirm the convergence and robustness of the proposed design, including under minimally excited trajectories.
<div id='section'>Paperid: <span id='pid'>781, <a href='https://arxiv.org/pdf/2511.01266.pdf' target='_blank'>https://arxiv.org/pdf/2511.01266.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Joonghyuk Shin, Zhengqi Li, Richard Zhang, Jun-Yan Zhu, Jaesik Park, Eli Schechtman, Xun Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.01266">MotionStream: Real-Time Video Generation with Interactive Motion Controls</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current motion-conditioned video generation methods suffer from prohibitive latency (minutes per video) and non-causal processing that prevents real-time interaction. We present MotionStream, enabling sub-second latency with up to 29 FPS streaming generation on a single GPU. Our approach begins by augmenting a text-to-video model with motion control, which generates high-quality videos that adhere to the global text prompt and local motion guidance, but does not perform inference on the fly. As such, we distill this bidirectional teacher into a causal student through Self Forcing with Distribution Matching Distillation, enabling real-time streaming inference. Several key challenges arise when generating videos of long, potentially infinite time-horizons: (1) bridging the domain gap from training on finite length and extrapolating to infinite horizons, (2) sustaining high quality by preventing error accumulation, and (3) maintaining fast inference, without incurring growth in computational cost due to increasing context windows. A key to our approach is introducing carefully designed sliding-window causal attention, combined with attention sinks. By incorporating self-rollout with attention sinks and KV cache rolling during training, we properly simulate inference-time extrapolations with a fixed context window, enabling constant-speed generation of arbitrarily long videos. Our models achieve state-of-the-art results in motion following and video quality while being two orders of magnitude faster, uniquely enabling infinite-length streaming. With MotionStream, users can paint trajectories, control cameras, or transfer motion, and see results unfold in real-time, delivering a truly interactive experience.
<div id='section'>Paperid: <span id='pid'>782, <a href='https://arxiv.org/pdf/2510.07345.pdf' target='_blank'>https://arxiv.org/pdf/2510.07345.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Danush Kumar Venkatesh, Adam Schmidt, Muhammad Abdullah Jamal, Omid Mohareri
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07345">Mitigating Surgical Data Imbalance with Dual-Prediction Video Diffusion Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Surgical video datasets are essential for scene understanding, enabling procedural modeling and intra-operative support. However, these datasets are often heavily imbalanced, with rare actions and tools under-represented, which limits the robustness of downstream models. We address this challenge with $SurgiFlowVid$, a sparse and controllable video diffusion framework for generating surgical videos of under-represented classes. Our approach introduces a dual-prediction diffusion module that jointly denoises RGB frames and optical flow, providing temporal inductive biases to improve motion modeling from limited samples. In addition, a sparse visual encoder conditions the generation process on lightweight signals (e.g., sparse segmentation masks or RGB frames), enabling controllability without dense annotations. We validate our approach on three surgical datasets across tasks including action recognition, tool presence detection, and laparoscope motion prediction. Synthetic data generated by our method yields consistent gains of 10-20% over competitive baselines, establishing $SurgiFlowVid$ as a promising strategy to mitigate data imbalance and advance surgical video understanding methods.
<div id='section'>Paperid: <span id='pid'>783, <a href='https://arxiv.org/pdf/2509.24469.pdf' target='_blank'>https://arxiv.org/pdf/2509.24469.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Heechang Kim, Gwanghyun Kim, Se Young Chun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24469">LaMoGen: Laban Movement-Guided Diffusion for Text-to-Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diverse human motion generation is an increasingly important task, having various applications in computer vision, human-computer interaction and animation. While text-to-motion synthesis using diffusion models has shown success in generating high-quality motions, achieving fine-grained expressive motion control remains a significant challenge. This is due to the lack of motion style diversity in datasets and the difficulty of expressing quantitative characteristics in natural language. Laban movement analysis has been widely used by dance experts to express the details of motion including motion quality as consistent as possible. Inspired by that, this work aims for interpretable and expressive control of human motion generation by seamlessly integrating the quantification methods of Laban Effort and Shape components into the text-guided motion generation models. Our proposed zero-shot, inference-time optimization method guides the motion generation model to have desired Laban Effort and Shape components without any additional motion data by updating the text embedding of pretrained diffusion models during the sampling step. We demonstrate that our approach yields diverse expressive motion qualities while preserving motion identity by successfully manipulating motion attributes according to target Laban tags.
<div id='section'>Paperid: <span id='pid'>784, <a href='https://arxiv.org/pdf/2509.23279.pdf' target='_blank'>https://arxiv.org/pdf/2509.23279.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rohit Chowdhury, Aniruddha Bala, Rohan Jaiswal, Siddharth Roheda
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23279">Vid-Freeze: Protecting Images from Malicious Image-to-Video Generation via Temporal Freezing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid progress of image-to-video (I2V) generation models has introduced significant risks, enabling video synthesis from static images and facilitating deceptive or malicious content creation. While prior defenses such as I2VGuard attempt to immunize images, effective and principled protection to block motion remains underexplored. In this work, we introduce Vid-Freeze - a novel attention-suppressing adversarial attack that adds carefully crafted adversarial perturbations to images. Our method explicitly targets the attention mechanism of I2V models, completely disrupting motion synthesis while preserving semantic fidelity of the input image. The resulting immunized images generate stand-still or near-static videos, effectively blocking malicious content creation. Our experiments demonstrate the impressive protection provided by the proposed approach, highlighting the importance of attention attacks as a promising direction for robust and proactive defenses against misuse of I2V generation models.
<div id='section'>Paperid: <span id='pid'>785, <a href='https://arxiv.org/pdf/2509.20696.pdf' target='_blank'>https://arxiv.org/pdf/2509.20696.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qingpeng Li, Chengrui Zhu, Yanming Wu, Xin Yuan, Zhen Zhang, Jian Yang, Yong Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20696">RuN: Residual Policy for Natural Humanoid Locomotion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Enabling humanoid robots to achieve natural and dynamic locomotion across a wide range of speeds, including smooth transitions from walking to running, presents a significant challenge. Existing deep reinforcement learning methods typically require the policy to directly track a reference motion, forcing a single policy to simultaneously learn motion imitation, velocity tracking, and stability maintenance. To address this, we introduce RuN, a novel decoupled residual learning framework. RuN decomposes the control task by pairing a pre-trained Conditional Motion Generator, which provides a kinematically natural motion prior, with a reinforcement learning policy that learns a lightweight residual correction to handle dynamical interactions. Experiments in simulation and reality on the Unitree G1 humanoid robot demonstrate that RuN achieves stable, natural gaits and smooth walk-run transitions across a broad velocity range (0-2.5 m/s), outperforming state-of-the-art methods in both training efficiency and final performance.
<div id='section'>Paperid: <span id='pid'>786, <a href='https://arxiv.org/pdf/2508.10269.pdf' target='_blank'>https://arxiv.org/pdf/2508.10269.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kejun Li, Jeeseop Kim, Maxime Brunet, Marine PÃ©triaux, Yisong Yue, Aaron D. Ames
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10269">Hybrid Data-Driven Predictive Control for Robust and Reactive Exoskeleton Locomotion Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robust bipedal locomotion in exoskeletons requires the ability to dynamically react to changes in the environment in real time. This paper introduces the hybrid data-driven predictive control (HDDPC) framework, an extension of the data-enabled predictive control, that addresses these challenges by simultaneously planning foot contact schedules and continuous domain trajectories. The proposed framework utilizes a Hankel matrix-based representation to model system dynamics, incorporating step-to-step (S2S) transitions to enhance adaptability in dynamic environments. By integrating contact scheduling with trajectory planning, the framework offers an efficient, unified solution for locomotion motion synthesis that enables robust and reactive walking through online replanning. We validate the approach on the Atalante exoskeleton, demonstrating improved robustness and adaptability.
<div id='section'>Paperid: <span id='pid'>787, <a href='https://arxiv.org/pdf/2502.07358.pdf' target='_blank'>https://arxiv.org/pdf/2502.07358.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoran Chen, Yiteng Xu, Yiming Ren, Yaoqin Ye, Xinran Li, Ning Ding, Yuxuan Wu, Yaoze Liu, Peishan Cong, Ziyi Wang, Bushi Liu, Yuhan Chen, Zhiyang Dou, Xiaokun Leng, Manyi Li, Yuexin Ma, Changhe Tu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.07358">SymBridge: A Human-in-the-Loop Cyber-Physical Interactive System for Adaptive Human-Robot Symbiosis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The development of intelligent robots seeks to seamlessly integrate them into the human world, providing assistance and companionship in daily life and work, with the ultimate goal of achieving human-robot symbiosis. This requires robots with intelligent interaction abilities to work naturally and effectively with humans. However, current robotic simulators fail to support real human participation, limiting their ability to provide authentic interaction experiences and gather valuable human feedback essential for enhancing robotic capabilities. In this paper, we introduce SymBridge, the first human-in-the-loop cyber-physical interactive system designed to enable the safe and efficient development, evaluation, and optimization of human-robot interaction methods. Specifically, we employ augmented reality technology to enable real humans to interact with virtual robots in physical environments, creating an authentic interactive experience. Building on this, we propose a novel robotic interaction model that generates responsive, precise robot actions in real time through continuous human behavior observation. The model incorporates multi-resolution human motion features and environmental affordances, ensuring contextually adaptive robotic responses. Additionally, SymBridge enables continuous robot learning by collecting human feedback and dynamically adapting the robotic interaction model. By leveraging a carefully designed system architecture and modules, SymBridge builds a bridge between humans and robots, as well as between cyber and physical spaces, providing a natural and realistic online interaction experience while facilitating the continuous evolution of robotic intelligence. Extensive experiments, user studies, and real robot testing demonstrate the promising performance of the system and highlight its potential to significantly advance research on human-robot symbiosis.
<div id='section'>Paperid: <span id='pid'>788, <a href='https://arxiv.org/pdf/2410.24037.pdf' target='_blank'>https://arxiv.org/pdf/2410.24037.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sunjae Yoon, Gwanhyeong Koo, Younghwan Lee, Chang D. Yoo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.24037">TPC: Test-time Procrustes Calibration for Diffusion-based Human Image Animation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human image animation aims to generate a human motion video from the inputs of a reference human image and a target motion video. Current diffusion-based image animation systems exhibit high precision in transferring human identity into targeted motion, yet they still exhibit irregular quality in their outputs. Their optimal precision is achieved only when the physical compositions (i.e., scale and rotation) of the human shapes in the reference image and target pose frame are aligned. In the absence of such alignment, there is a noticeable decline in fidelity and consistency. Especially, in real-world environments, this compositional misalignment commonly occurs, posing significant challenges to the practical usage of current systems. To this end, we propose Test-time Procrustes Calibration (TPC), which enhances the robustness of diffusion-based image animation systems by maintaining optimal performance even when faced with compositional misalignment, effectively addressing real-world scenarios. The TPC provides a calibrated reference image for the diffusion model, enhancing its capability to understand the correspondence between human shapes in the reference and target images. Our method is simple and can be applied to any diffusion-based image animation system in a model-agnostic manner, improving the effectiveness at test time without additional training.
<div id='section'>Paperid: <span id='pid'>789, <a href='https://arxiv.org/pdf/2410.03665.pdf' target='_blank'>https://arxiv.org/pdf/2410.03665.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Brent Yi, Vickie Ye, Maya Zheng, Yunqi Li, Lea MÃ¼ller, Georgios Pavlakos, Yi Ma, Jitendra Malik, Angjoo Kanazawa
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.03665">Estimating Body and Hand Motion in an Ego-sensed World</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present EgoAllo, a system for human motion estimation from a head-mounted device. Using only egocentric SLAM poses and images, EgoAllo guides sampling from a conditional diffusion model to estimate 3D body pose, height, and hand parameters that capture a device wearer's actions in the allocentric coordinate frame of the scene. To achieve this, our key insight is in representation: we propose spatial and temporal invariance criteria for improving model performance, from which we derive a head motion conditioning parameterization that improves estimation by up to 18%. We also show how the bodies estimated by our system can improve hand estimation: the resulting kinematic and temporal constraints can reduce world-frame errors in single-frame estimates by 40%. Project page: https://egoallo.github.io/
<div id='section'>Paperid: <span id='pid'>790, <a href='https://arxiv.org/pdf/2410.01968.pdf' target='_blank'>https://arxiv.org/pdf/2410.01968.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenshuai Zhao, Yi Zhao, Joni Pajarinen, Michael Muehlebach
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.01968">Bi-Level Motion Imitation for Humanoid Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Imitation learning from human motion capture (MoCap) data provides a promising way to train humanoid robots. However, due to differences in morphology, such as varying degrees of joint freedom and force limits, exact replication of human behaviors may not be feasible for humanoid robots. Consequently, incorporating physically infeasible MoCap data in training datasets can adversely affect the performance of the robot policy. To address this issue, we propose a bi-level optimization-based imitation learning framework that alternates between optimizing both the robot policy and the target MoCap data. Specifically, we first develop a generative latent dynamics model using a novel self-consistent auto-encoder, which learns sparse and structured motion representations while capturing desired motion patterns in the dataset. The dynamics model is then utilized to generate reference motions while the latent representation regularizes the bi-level motion imitation process. Simulations conducted with a realistic model of a humanoid robot demonstrate that our method enhances the robot policy by modifying reference motions to be physically consistent.
<div id='section'>Paperid: <span id='pid'>791, <a href='https://arxiv.org/pdf/2409.02634.pdf' target='_blank'>https://arxiv.org/pdf/2409.02634.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianwen Jiang, Chao Liang, Jiaqi Yang, Gaojie Lin, Tianyun Zhong, Yanbo Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.02634">Loopy: Taming Audio-Driven Portrait Avatar with Long-Term Motion Dependency</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the introduction of diffusion-based video generation techniques, audio-conditioned human video generation has recently achieved significant breakthroughs in both the naturalness of motion and the synthesis of portrait details. Due to the limited control of audio signals in driving human motion, existing methods often add auxiliary spatial signals to stabilize movements, which may compromise the naturalness and freedom of motion. In this paper, we propose an end-to-end audio-only conditioned video diffusion model named Loopy. Specifically, we designed an inter- and intra-clip temporal module and an audio-to-latents module, enabling the model to leverage long-term motion information from the data to learn natural motion patterns and improving audio-portrait movement correlation. This method removes the need for manually specified spatial motion templates used in existing methods to constrain motion during inference. Extensive experiments show that Loopy outperforms recent audio-driven portrait diffusion models, delivering more lifelike and high-quality results across various scenarios.
<div id='section'>Paperid: <span id='pid'>792, <a href='https://arxiv.org/pdf/2408.08202.pdf' target='_blank'>https://arxiv.org/pdf/2408.08202.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiao Han, Yiming Ren, Yichen Yao, Yujing Sun, Yuexin Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.08202">Towards Practical Human Motion Prediction with LiDAR Point Clouds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion prediction is crucial for human-centric multimedia understanding and interacting. Current methods typically rely on ground truth human poses as observed input, which is not practical for real-world scenarios where only raw visual sensor data is available. To implement these methods in practice, a pre-phrase of pose estimation is essential. However, such two-stage approaches often lead to performance degradation due to the accumulation of errors. Moreover, reducing raw visual data to sparse keypoint representations significantly diminishes the density of information, resulting in the loss of fine-grained features. In this paper, we propose \textit{LiDAR-HMP}, the first single-LiDAR-based 3D human motion prediction approach, which receives the raw LiDAR point cloud as input and forecasts future 3D human poses directly. Building upon our novel structure-aware body feature descriptor, LiDAR-HMP adaptively maps the observed motion manifold to future poses and effectively models the spatial-temporal correlations of human motions for further refinement of prediction results. Extensive experiments show that our method achieves state-of-the-art performance on two public benchmarks and demonstrates remarkable robustness and efficacy in real-world deployments.
<div id='section'>Paperid: <span id='pid'>793, <a href='https://arxiv.org/pdf/2407.21136.pdf' target='_blank'>https://arxiv.org/pdf/2407.21136.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxuan Bian, Ailing Zeng, Xuan Ju, Xian Liu, Zhaoyang Zhang, Wei Liu, Qiang Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.21136">MotionCraft: Crafting Whole-Body Motion with Plug-and-Play Multimodal Controls</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Whole-body multimodal motion generation, controlled by text, speech, or music, has numerous applications including video generation and character animation. However, employing a unified model to achieve various generation tasks with different condition modalities presents two main challenges: motion distribution drifts across different tasks (e.g., co-speech gestures and text-driven daily actions) and the complex optimization of mixed conditions with varying granularities (e.g., text and audio). Additionally, inconsistent motion formats across different tasks and datasets hinder effective training toward multimodal motion generation. In this paper, we propose MotionCraft, a unified diffusion transformer that crafts whole-body motion with plug-and-play multimodal control. Our framework employs a coarse-to-fine training strategy, starting with the first stage of text-to-motion semantic pre-training, followed by the second stage of multimodal low-level control adaptation to handle conditions of varying granularities. To effectively learn and transfer motion knowledge across different distributions, we design MC-Attn for parallel modeling of static and dynamic human topology graphs. To overcome the motion format inconsistency of existing benchmarks, we introduce MC-Bench, the first available multimodal whole-body motion generation benchmark based on the unified SMPL-X format. Extensive experiments show that MotionCraft achieves state-of-the-art performance on various standard motion generation tasks.
<div id='section'>Paperid: <span id='pid'>794, <a href='https://arxiv.org/pdf/2407.16341.pdf' target='_blank'>https://arxiv.org/pdf/2407.16341.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaodong Chen, Wu Liu, Qian Bao, Xinchen Liu, Quanwei Yang, Ruoli Dai, Tao Mei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.16341">Motion Capture from Inertial and Vision Sensors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion capture is the foundation for many computer vision and graphics tasks. While industrial motion capture systems with complex camera arrays or expensive wearable sensors have been widely adopted in movie and game production, consumer-affordable and easy-to-use solutions for personal applications are still far from mature. To utilize a mixture of a monocular camera and very few inertial measurement units (IMUs) for accurate multi-modal human motion capture in daily life, we contribute MINIONS in this paper, a large-scale Motion capture dataset collected from INertial and visION Sensors. MINIONS has several featured properties: 1) large scale of over five million frames and 400 minutes duration; 2) multi-modality data of IMUs signals and RGB videos labeled with joint positions, joint rotations, SMPL parameters, etc.; 3) a diverse set of 146 fine-grained single and interactive actions with textual descriptions. With the proposed MINIONS, we conduct experiments on multi-modal motion capture and explore the possibilities of consumer-affordable motion capture using a monocular camera and very few IMUs. The experiment results emphasize the unique advantages of inertial and vision sensors, showcasing the promise of consumer-affordable multi-modal motion capture and providing a valuable resource for further research and development.
<div id='section'>Paperid: <span id='pid'>795, <a href='https://arxiv.org/pdf/2407.09833.pdf' target='_blank'>https://arxiv.org/pdf/2407.09833.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiming Ren, Xiao Han, Yichen Yao, Xiaoxiao Long, Yujing Sun, Yuexin Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.09833">LiveHPS++: Robust and Coherent Motion Capture in Dynamic Free Environment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>LiDAR-based human motion capture has garnered significant interest in recent years for its practicability in large-scale and unconstrained environments. However, most methods rely on cleanly segmented human point clouds as input, the accuracy and smoothness of their motion results are compromised when faced with noisy data, rendering them unsuitable for practical applications. To address these limitations and enhance the robustness and precision of motion capture with noise interference, we introduce LiveHPS++, an innovative and effective solution based on a single LiDAR system. Benefiting from three meticulously designed modules, our method can learn dynamic and kinematic features from human movements, and further enable the precise capture of coherent human motions in open settings, making it highly applicable to real-world scenarios. Through extensive experiments, LiveHPS++ has proven to significantly surpass existing state-of-the-art methods across various datasets, establishing a new benchmark in the field.
<div id='section'>Paperid: <span id='pid'>796, <a href='https://arxiv.org/pdf/2407.05712.pdf' target='_blank'>https://arxiv.org/pdf/2407.05712.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianwen Jiang, Gaojie Lin, Zhengkun Rong, Chao Liang, Yongming Zhu, Jiaqi Yang, Tianyun Zhong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.05712">MobilePortrait: Real-Time One-Shot Neural Head Avatars on Mobile Devices</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing neural head avatars methods have achieved significant progress in the image quality and motion range of portrait animation. However, these methods neglect the computational overhead, and to the best of our knowledge, none is designed to run on mobile devices. This paper presents MobilePortrait, a lightweight one-shot neural head avatars method that reduces learning complexity by integrating external knowledge into both the motion modeling and image synthesis, enabling real-time inference on mobile devices. Specifically, we introduce a mixed representation of explicit and implicit keypoints for precise motion modeling and precomputed visual features for enhanced foreground and background synthesis. With these two key designs and using simple U-Nets as backbones, our method achieves state-of-the-art performance with less than one-tenth the computational demand. It has been validated to reach speeds of over 100 FPS on mobile devices and support both video and audio-driven inputs.
<div id='section'>Paperid: <span id='pid'>797, <a href='https://arxiv.org/pdf/2405.15439.pdf' target='_blank'>https://arxiv.org/pdf/2405.15439.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zichen Geng, Caren Han, Zeeshan Hayder, Jian Liu, Mubarak Shah, Ajmal Mian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.15439">Text-guided 3D Human Motion Generation with Keyframe-based Parallel Skip Transformer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-driven human motion generation is an emerging task in animation and humanoid robot design. Existing algorithms directly generate the full sequence which is computationally expensive and prone to errors as it does not pay special attention to key poses, a process that has been the cornerstone of animation for decades. We propose KeyMotion, that generates plausible human motion sequences corresponding to input text by first generating keyframes followed by in-filling. We use a Variational Autoencoder (VAE) with Kullback-Leibler regularization to project the keyframes into a latent space to reduce dimensionality and further accelerate the subsequent diffusion process. For the reverse diffusion, we propose a novel Parallel Skip Transformer that performs cross-modal attention between the keyframe latents and text condition. To complete the motion sequence, we propose a text-guided Transformer designed to perform motion-in-filling, ensuring the preservation of both fidelity and adherence to the physical constraints of human motion. Experiments show that our method achieves state-of-theart results on the HumanML3D dataset outperforming others on all R-precision metrics and MultiModal Distance. KeyMotion also achieves competitive performance on the KIT dataset, achieving the best results on Top3 R-precision, FID, and Diversity metrics.
<div id='section'>Paperid: <span id='pid'>798, <a href='https://arxiv.org/pdf/2402.07594.pdf' target='_blank'>https://arxiv.org/pdf/2402.07594.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Patrick Seifner, Kostadin Cvejoski, Antonia KÃ¶rner, RamsÃ©s J. SÃ¡nchez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.07594">Zero-shot Imputation with Foundation Inference Models for Dynamical Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dynamical systems governed by ordinary differential equations (ODEs) serve as models for a vast number of natural and social phenomena. In this work, we offer a fresh perspective on the classical problem of imputing missing time series data, whose underlying dynamics are assumed to be determined by ODEs. Specifically, we revisit ideas from amortized inference and neural operators, and propose a novel supervised learning framework for zero-shot time series imputation, through parametric functions satisfying some (hidden) ODEs. Our proposal consists of two components. First, a broad probability distribution over the space of ODE solutions, observation times and noise mechanisms, with which we generate a large, synthetic dataset of (hidden) ODE solutions, along with their noisy and sparse observations. Second, a neural recognition model that is trained offline, to map the generated time series onto the spaces of initial conditions and time derivatives of the (hidden) ODE solutions, which we then integrate to impute the missing data. We empirically demonstrate that one and the same (pretrained) recognition model can perform zero-shot imputation across 63 distinct time series with missing values, each sampled from widely different dynamical systems. Likewise, we demonstrate that it can perform zero-shot imputation of missing high-dimensional data in 10 vastly different settings, spanning human motion, air quality, traffic and electricity studies, as well as Navier-Stokes simulations -- without requiring any fine-tuning. What is more, our proposal often outperforms state-of-the-art methods, which are trained on the target datasets.
  Our pretrained model, repository and tutorials are available online.
<div id='section'>Paperid: <span id='pid'>799, <a href='https://arxiv.org/pdf/2512.21183.pdf' target='_blank'>https://arxiv.org/pdf/2512.21183.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenghao Xu, Guangtao Lyu, Qi Liu, Jiexi Yan, Muli Yang, Cheng Deng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.21183">Towards Arbitrary Motion Completing via Hierarchical Continuous Representation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Physical motions are inherently continuous, and higher camera frame rates typically contribute to improved smoothness and temporal coherence. For the first time, we explore continuous representations of human motion sequences, featuring the ability to interpolate, inbetween, and even extrapolate any input motion sequences at arbitrary frame rates. To achieve this, we propose a novel parametric activation-induced hierarchical implicit representation framework, referred to as NAME, based on Implicit Neural Representations (INRs). Our method introduces a hierarchical temporal encoding mechanism that extracts features from motion sequences at multiple temporal scales, enabling effective capture of intricate temporal patterns. Additionally, we integrate a custom parametric activation function, powered by Fourier transformations, into the MLP-based decoder to enhance the expressiveness of the continuous representation. This parametric formulation significantly augments the model's ability to represent complex motion behaviors with high accuracy. Extensive evaluations across several benchmark datasets demonstrate the effectiveness and robustness of our proposed approach.
<div id='section'>Paperid: <span id='pid'>800, <a href='https://arxiv.org/pdf/2511.19326.pdf' target='_blank'>https://arxiv.org/pdf/2511.19326.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Farnoosh Koleini, Hongfei Xue, Ahmed Helmy, Pu Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.19326">MonoMSK: Monocular 3D Musculoskeletal Dynamics Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reconstructing biomechanically realistic 3D human motion - recovering both kinematics (motion) and kinetics (forces) - is a critical challenge. While marker-based systems are lab-bound and slow, popular monocular methods use oversimplified, anatomically inaccurate models (e.g., SMPL) and ignore physics, fundamentally limiting their biomechanical fidelity. In this work, we introduce MonoMSK, a hybrid framework that bridges data-driven learning and physics-based simulation for biomechanically realistic 3D human motion estimation from monocular video. MonoMSK jointly recovers both kinematics (motions) and kinetics (forces and torques) through an anatomically accurate musculoskeletal model. By integrating transformer-based inverse dynamics with differentiable forward kinematics and dynamics layers governed by ODE-based simulation, MonoMSK establishes a physics-regulated inverse-forward loop that enforces biomechanical causality and physical plausibility. A novel forward-inverse consistency loss further aligns motion reconstruction with the underlying kinetic reasoning. Experiments on BML-MoVi, BEDLAM, and OpenCap show that MonoMSK significantly outperforms state-of-the-art methods in kinematic accuracy, while for the first time enabling precise monocular kinetics estimation.
<div id='section'>Paperid: <span id='pid'>801, <a href='https://arxiv.org/pdf/2509.21888.pdf' target='_blank'>https://arxiv.org/pdf/2509.21888.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Minjun Kang, Inkyu Shin, Taeyeop Lee, In So Kweon, Kuk-Jin Yoon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21888">Drag4D: Align Your Motion with Text-Driven 3D Scene Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce Drag4D, an interactive framework that integrates object motion control within text-driven 3D scene generation. This framework enables users to define 3D trajectories for the 3D objects generated from a single image, seamlessly integrating them into a high-quality 3D background. Our Drag4D pipeline consists of three stages. First, we enhance text-to-3D background generation by applying 2D Gaussian Splatting with panoramic images and inpainted novel views, resulting in dense and visually complete 3D reconstructions. In the second stage, given a reference image of the target object, we introduce a 3D copy-and-paste approach: the target instance is extracted in a full 3D mesh using an off-the-shelf image-to-3D model and seamlessly composited into the generated 3D scene. The object mesh is then positioned within the 3D scene via our physics-aware object position learning, ensuring precise spatial alignment. Lastly, the spatially aligned object is temporally animated along a user-defined 3D trajectory. To mitigate motion hallucination and ensure view-consistent temporal alignment, we develop a part-augmented, motion-conditioned video diffusion model that processes multiview image pairs together with their projected 2D trajectories. We demonstrate the effectiveness of our unified architecture through evaluations at each stage and in the final results, showcasing the harmonized alignment of user-controlled object motion within a high-quality 3D background.
<div id='section'>Paperid: <span id='pid'>802, <a href='https://arxiv.org/pdf/2509.15443.pdf' target='_blank'>https://arxiv.org/pdf/2509.15443.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xingyu Chen, Hanyu Wu, Sikai Wu, Mingliang Zhou, Diyun Xiang, Haodong Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15443">Implicit Kinodynamic Motion Retargeting for Human-to-humanoid Imitation Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human-to-humanoid imitation learning aims to learn a humanoid whole-body controller from human motion. Motion retargeting is a crucial step in enabling robots to acquire reference trajectories when exploring locomotion skills. However, current methods focus on motion retargeting frame by frame, which lacks scalability. Could we directly convert large-scale human motion into robot-executable motion through a more efficient approach? To address this issue, we propose Implicit Kinodynamic Motion Retargeting (IKMR), a novel efficient and scalable retargeting framework that considers both kinematics and dynamics. In kinematics, IKMR pretrains motion topology feature representation and a dual encoder-decoder architecture to learn a motion domain mapping. In dynamics, IKMR integrates imitation learning with the motion retargeting network to refine motion into physically feasible trajectories. After fine-tuning using the tracking results, IKMR can achieve large-scale physically feasible motion retargeting in real time, and a whole-body controller could be directly trained and deployed for tracking its retargeted trajectories. We conduct our experiments both in the simulator and the real robot on a full-size humanoid robot. Extensive experiments and evaluation results verify the effectiveness of our proposed framework.
<div id='section'>Paperid: <span id='pid'>803, <a href='https://arxiv.org/pdf/2507.19850.pdf' target='_blank'>https://arxiv.org/pdf/2507.19850.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bizhu Wu, Jinheng Xie, Meidan Ding, Zhe Kong, Jianfeng Ren, Ruibin Bai, Rong Qu, Linlin Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.19850">FineMotion: A Dataset and Benchmark with both Spatial and Temporal Annotation for Fine-grained Motion Generation and Editing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating realistic human motions from textual descriptions has undergone significant advancements. However, existing methods often overlook specific body part movements and their timing. In this paper, we address this issue by enriching the textual description with more details. Specifically, we propose the FineMotion dataset, which contains over 442,000 human motion snippets - short segments of human motion sequences - and their corresponding detailed descriptions of human body part movements. Additionally, the dataset includes about 95k detailed paragraphs describing the movements of human body parts of entire motion sequences. Experimental results demonstrate the significance of our dataset on the text-driven finegrained human motion generation task, especially with a remarkable +15.3% improvement in Top-3 accuracy for the MDM model. Notably, we further support a zero-shot pipeline of fine-grained motion editing, which focuses on detailed editing in both spatial and temporal dimensions via text. Dataset and code available at: CVI-SZU/FineMotion
<div id='section'>Paperid: <span id='pid'>804, <a href='https://arxiv.org/pdf/2507.07394.pdf' target='_blank'>https://arxiv.org/pdf/2507.07394.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhimin Zhang, Bi'an Du, Caoyuan Ma, Zheng Wang, Wei Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07394">Behave Your Motion: Habit-preserved Cross-category Animal Motion Transfer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Animal motion embodies species-specific behavioral habits, making the transfer of motion across categories a critical yet complex task for applications in animation and virtual reality. Existing motion transfer methods, primarily focused on human motion, emphasize skeletal alignment (motion retargeting) or stylistic consistency (motion style transfer), often neglecting the preservation of distinct habitual behaviors in animals. To bridge this gap, we propose a novel habit-preserved motion transfer framework for cross-category animal motion. Built upon a generative framework, our model introduces a habit-preservation module with category-specific habit encoder, allowing it to learn motion priors that capture distinctive habitual characteristics. Furthermore, we integrate a large language model (LLM) to facilitate the motion transfer to previously unobserved species. To evaluate the effectiveness of our approach, we introduce the DeformingThings4D-skl dataset, a quadruped dataset with skeletal bindings, and conduct extensive experiments and quantitative analyses, which validate the superiority of our proposed model.
<div id='section'>Paperid: <span id='pid'>805, <a href='https://arxiv.org/pdf/2506.22554.pdf' target='_blank'>https://arxiv.org/pdf/2506.22554.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vasu Agrawal, Akinniyi Akinyemi, Kathryn Alvero, Morteza Behrooz, Julia Buffalini, Fabio Maria Carlucci, Joy Chen, Junming Chen, Zhang Chen, Shiyang Cheng, Praveen Chowdary, Joe Chuang, Antony D'Avirro, Jon Daly, Ning Dong, Mark Duppenthaler, Cynthia Gao, Jeff Girard, Martin Gleize, Sahir Gomez, Hongyu Gong, Srivathsan Govindarajan, Brandon Han, Sen He, Denise Hernandez, Yordan Hristov, Rongjie Huang, Hirofumi Inaguma, Somya Jain, Raj Janardhan, Qingyao Jia, Christopher Klaiber, Dejan Kovachev, Moneish Kumar, Hang Li, Yilei Li, Pavel Litvin, Wei Liu, Guangyao Ma, Jing Ma, Martin Ma, Xutai Ma, Lucas Mantovani, Sagar Miglani, Sreyas Mohan, Louis-Philippe Morency, Evonne Ng, Kam-Woh Ng, Tu Anh Nguyen, Amia Oberai, Benjamin Peloquin, Juan Pino, Jovan Popovic, Omid Poursaeed, Fabian Prada, Alice Rakotoarison, Rakesh Ranjan, Alexander Richard, Christophe Ropers, Safiyyah Saleem, Vasu Sharma, Alex Shcherbyna, Jia Shen, Jie Shen, Anastasis Stathopoulos, Anna Sun, Paden Tomasello, Tuan Tran, Arina Turkatenko, Bo Wan, Chao Wang, Jeff Wang, Mary Williamson, Carleigh Wood, Tao Xiang, Yilin Yang, Julien Yao, Chen Zhang, Jiemin Zhang, Xinyue Zhang, Jason Zheng, Pavlo Zhyzheria, Jan Zikes, Michael Zollhoefer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.22554">Seamless Interaction: Dyadic Audiovisual Motion Modeling and Large-Scale Dataset</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human communication involves a complex interplay of verbal and nonverbal signals, essential for conveying meaning and achieving interpersonal goals. To develop socially intelligent AI technologies, it is crucial to develop models that can both comprehend and generate dyadic behavioral dynamics. To this end, we introduce the Seamless Interaction Dataset, a large-scale collection of over 4,000 hours of face-to-face interaction footage from over 4,000 participants in diverse contexts. This dataset enables the development of AI technologies that understand dyadic embodied dynamics, unlocking breakthroughs in virtual agents, telepresence experiences, and multimodal content analysis tools. We also develop a suite of models that utilize the dataset to generate dyadic motion gestures and facial expressions aligned with human speech. These models can take as input both the speech and visual behavior of their interlocutors. We present a variant with speech from an LLM model and integrations with 2D and 3D rendering methods, bringing us closer to interactive virtual agents. Additionally, we describe controllable variants of our motion models that can adapt emotional responses and expressivity levels, as well as generating more semantically-relevant gestures. Finally, we discuss methods for assessing the quality of these dyadic motion models, which are demonstrating the potential for more intuitive and responsive human-AI interactions.
<div id='section'>Paperid: <span id='pid'>806, <a href='https://arxiv.org/pdf/2505.24266.pdf' target='_blank'>https://arxiv.org/pdf/2505.24266.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guanren Qiao, Sixu Lin, Ronglai Zuo, Zhizheng Wu, Kui Jia, Guiliang Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.24266">SignBot: Learning Human-to-Humanoid Sign Language Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Sign language is a natural and visual form of language that uses movements and expressions to convey meaning, serving as a crucial means of communication for individuals who are deaf or hard-of-hearing (DHH). However, the number of people proficient in sign language remains limited, highlighting the need for technological advancements to bridge communication gaps and foster interactions with minorities. Based on recent advancements in embodied humanoid robots, we propose SignBot, a novel framework for human-robot sign language interaction. SignBot integrates a cerebellum-inspired motion control component and a cerebral-oriented module for comprehension and interaction. Specifically, SignBot consists of: 1) Motion Retargeting, which converts human sign language datasets into robot-compatible kinematics; 2) Motion Control, which leverages a learning-based paradigm to develop a robust humanoid control policy for tracking sign language gestures; and 3) Generative Interaction, which incorporates translator, responser, and generator of sign language, thereby enabling natural and effective communication between robots and humans. Simulation and real-world experimental results demonstrate that SignBot can effectively facilitate human-robot interaction and perform sign language motions with diverse robots and datasets. SignBot represents a significant advancement in automatic sign language interaction on embodied humanoid robot platforms, providing a promising solution to improve communication accessibility for the DHH community.
<div id='section'>Paperid: <span id='pid'>807, <a href='https://arxiv.org/pdf/2505.17333.pdf' target='_blank'>https://arxiv.org/pdf/2505.17333.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xin You, Minghui Zhang, Hanxiao Zhang, Jie Yang, Nassir Navab
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.17333">Temporal Differential Fields for 4D Motion Modeling via Image-to-Video Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Temporal modeling on regular respiration-induced motions is crucial to image-guided clinical applications. Existing methods cannot simulate temporal motions unless high-dose imaging scans including starting and ending frames exist simultaneously. However, in the preoperative data acquisition stage, the slight movement of patients may result in dynamic backgrounds between the first and last frames in a respiratory period. This additional deviation can hardly be removed by image registration, thus affecting the temporal modeling. To address that limitation, we pioneeringly simulate the regular motion process via the image-to-video (I2V) synthesis framework, which animates with the first frame to forecast future frames of a given length. Besides, to promote the temporal consistency of animated videos, we devise the Temporal Differential Diffusion Model to generate temporal differential fields, which measure the relative differential representations between adjacent frames. The prompt attention layer is devised for fine-grained differential fields, and the field augmented layer is adopted to better interact these fields with the I2V framework, promoting more accurate temporal variation of synthesized videos. Extensive results on ACDC cardiac and 4D Lung datasets reveal that our approach simulates 4D videos along the intrinsic motion trajectory, rivaling other competitive methods on perceptual similarity and temporal consistency. Codes will be available soon.
<div id='section'>Paperid: <span id='pid'>808, <a href='https://arxiv.org/pdf/2505.03779.pdf' target='_blank'>https://arxiv.org/pdf/2505.03779.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tao Liu, Tianyu Zhang, Yongxue Chen, Weiming Wang, Yu Jiang, Yuming Huang, Charlie C. L. Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.03779">Neural Co-Optimization of Structural Topology, Manufacturable Layers, and Path Orientations for Fiber-Reinforced Composites</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a neural network-based computational framework for the simultaneous optimization of structural topology, curved layers, and path orientations to achieve strong anisotropic strength in fiber-reinforced thermoplastic composites while ensuring manufacturability. Our framework employs three implicit neural fields to represent geometric shape, layer sequence, and fiber orientation. This enables the direct formulation of both design and manufacturability objectives - such as anisotropic strength, structural volume, machine motion control, layer curvature, and layer thickness - into an integrated and differentiable optimization process. By incorporating these objectives as loss functions, the framework ensures that the resultant composites exhibit optimized mechanical strength while remaining its manufacturability for filament-based multi-axis 3D printing across diverse hardware platforms. Physical experiments demonstrate that the composites generated by our co-optimization method can achieve an improvement of up to 33.1% in failure loads compared to composites with sequentially optimized structures and manufacturing sequences.
<div id='section'>Paperid: <span id='pid'>809, <a href='https://arxiv.org/pdf/2504.04634.pdf' target='_blank'>https://arxiv.org/pdf/2504.04634.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Foram Niravbhai Shah, Parshwa Shah, Muhammad Usama Saleem, Ekkasit Pinyoanuntapong, Pu Wang, Hongfei Xue, Ahmed Helmy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.04634">DanceMosaic: High-Fidelity Dance Generation with Multimodal Editability</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in dance generation have enabled automatic synthesis of 3D dance motions. However, existing methods still struggle to produce high-fidelity dance sequences that simultaneously deliver exceptional realism, precise dance-music synchronization, high motion diversity, and physical plausibility. Moreover, existing methods lack the flexibility to edit dance sequences according to diverse guidance signals, such as musical prompts, pose constraints, action labels, and genre descriptions, significantly restricting their creative utility and adaptability. Unlike the existing approaches, DanceMosaic enables fast and high-fidelity dance generation, while allowing multimodal motion editing. Specifically, we propose a multimodal masked motion model that fuses the text-to-motion model with music and pose adapters to learn probabilistic mapping from diverse guidance signals to high-quality dance motion sequences via progressive generative masking training. To further enhance the motion generation quality, we propose multimodal classifier-free guidance and inference-time optimization mechanism that further enforce the alignment between the generated motions and the multimodal guidance. Extensive experiments demonstrate that our method establishes a new state-of-the-art performance in dance generation, significantly advancing the quality and editability achieved by existing approaches.
<div id='section'>Paperid: <span id='pid'>810, <a href='https://arxiv.org/pdf/2504.02560.pdf' target='_blank'>https://arxiv.org/pdf/2504.02560.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yongqi Zhai, Luyang Tang, Wei Jiang, Jiayu Yang, Ronggang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.02560">L-LBVC: Long-Term Motion Estimation and Prediction for Learned Bi-Directional Video Compression</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, learned video compression (LVC) has shown superior performance under low-delay configuration. However, the performance of learned bi-directional video compression (LBVC) still lags behind traditional bi-directional coding. The performance gap mainly arises from inaccurate long-term motion estimation and prediction of distant frames, especially in large motion scenes. To solve these two critical problems, this paper proposes a novel LBVC framework, namely L-LBVC. Firstly, we propose an adaptive motion estimation module that can handle both short-term and long-term motions. Specifically, we directly estimate the optical flows for adjacent frames and non-adjacent frames with small motions. For non-adjacent frames with large motions, we recursively accumulate local flows between adjacent frames to estimate long-term flows. Secondly, we propose an adaptive motion prediction module that can largely reduce the bit cost for motion coding. To improve the accuracy of long-term motion prediction, we adaptively downsample reference frames during testing to match the motion ranges observed during training. Experiments show that our L-LBVC significantly outperforms previous state-of-the-art LVC methods and even surpasses VVC (VTM) on some test datasets under random access configuration.
<div id='section'>Paperid: <span id='pid'>811, <a href='https://arxiv.org/pdf/2503.04829.pdf' target='_blank'>https://arxiv.org/pdf/2503.04829.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tao Wang, Zhihua Wu, Qiaozhi He, Jiaming Chu, Ling Qian, Yu Cheng, Junliang Xing, Jian Zhao, Lei Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.04829">StickMotion: Generating 3D Human Motions by Drawing a Stickman</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-to-motion generation, which translates textual descriptions into human motions, has been challenging in accurately capturing detailed user-imagined motions from simple text inputs. This paper introduces StickMotion, an efficient diffusion-based network designed for multi-condition scenarios, which generates desired motions based on traditional text and our proposed stickman conditions for global and local control of these motions, respectively. We address the challenges introduced by the user-friendly stickman from three perspectives: 1) Data generation. We develop an algorithm to generate hand-drawn stickmen automatically across different dataset formats. 2) Multi-condition fusion. We propose a multi-condition module that integrates into the diffusion process and obtains outputs of all possible condition combinations, reducing computational complexity and enhancing StickMotion's performance compared to conventional approaches with the self-attention module. 3) Dynamic supervision. We empower StickMotion to make minor adjustments to the stickman's position within the output sequences, generating more natural movements through our proposed dynamic supervision strategy. Through quantitative experiments and user studies, sketching stickmen saves users about 51.5% of their time generating motions consistent with their imagination. Our codes, demos, and relevant data will be released to facilitate further research and validation within the scientific community.
<div id='section'>Paperid: <span id='pid'>812, <a href='https://arxiv.org/pdf/2412.19860.pdf' target='_blank'>https://arxiv.org/pdf/2412.19860.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenzhang Sun, Xiang Li, Donglin Di, Zhuding Liang, Qiyuan Zhang, Hao Li, Wei Chen, Jianxun Cui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.19860">UniAvatar: Taming Lifelike Audio-Driven Talking Head Generation with Comprehensive Motion and Lighting Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, animating portrait images using audio input is a popular task. Creating lifelike talking head videos requires flexible and natural movements, including facial and head dynamics, camera motion, realistic light and shadow effects. Existing methods struggle to offer comprehensive, multifaceted control over these aspects. In this work, we introduce UniAvatar, a designed method that provides extensive control over a wide range of motion and illumination conditions. Specifically, we use the FLAME model to render all motion information onto a single image, maintaining the integrity of 3D motion details while enabling fine-grained, pixel-level control. Beyond motion, this approach also allows for comprehensive global illumination control. We design independent modules to manage both 3D motion and illumination, permitting separate and combined control. Extensive experiments demonstrate that our method outperforms others in both broad-range motion control and lighting control. Additionally, to enhance the diversity of motion and environmental contexts in current datasets, we collect and plan to publicly release two datasets, DH-FaceDrasMvVid-100 and DH-FaceReliVid-200, which capture significant head movements during speech and various lighting scenarios.
<div id='section'>Paperid: <span id='pid'>813, <a href='https://arxiv.org/pdf/2412.19089.pdf' target='_blank'>https://arxiv.org/pdf/2412.19089.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Changwoon Choi, Jeongjun Kim, Geonho Cha, Minkwan Kim, Dongyoon Wee, Young Min Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.19089">Humans as a Calibration Pattern: Dynamic 3D Scene Reconstruction from Unsynchronized and Uncalibrated Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent works on dynamic 3D neural field reconstruction assume the input from synchronized multi-view videos whose poses are known. The input constraints are often not satisfied in real-world setups, making the approach impractical. We show that unsynchronized videos from unknown poses can generate dynamic neural fields as long as the videos capture human motion. Humans are one of the most common dynamic subjects captured in videos, and their shapes and poses can be estimated using state-of-the-art libraries. While noisy, the estimated human shape and pose parameters provide a decent initialization point to start the highly non-convex and under-constrained problem of training a consistent dynamic neural representation. Given the shape and pose parameters of humans in individual frames, we formulate methods to calculate the time offsets between videos, followed by camera pose estimations that analyze the 3D joint positions. Then, we train the dynamic neural fields employing multiresolution grids while we concurrently refine both time offsets and camera poses. The setup still involves optimizing many parameters; therefore, we introduce a robust progressive learning strategy to stabilize the process. Experiments show that our approach achieves accurate spatio-temporal calibration and high-quality scene reconstruction in challenging conditions.
<div id='section'>Paperid: <span id='pid'>814, <a href='https://arxiv.org/pdf/2412.17487.pdf' target='_blank'>https://arxiv.org/pdf/2412.17487.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yizhe Li, Linrui Zhang, Xueqian Wang, Houde Liu, Bin Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.17487">DeepMF: Deep Motion Factorization for Closed-Loop Safety-Critical Driving Scenario Simulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Safety-critical traffic scenarios are of great practical relevance to evaluating the robustness of autonomous driving (AD) systems. Given that these long-tail events are extremely rare in real-world traffic data, there is a growing body of work dedicated to the automatic traffic scenario generation. However, nearly all existing algorithms for generating safety-critical scenarios rely on snippets of previously recorded traffic events, transforming normal traffic flow into accident-prone situations directly. In other words, safety-critical traffic scenario generation is hindsight and not applicable to newly encountered and open-ended traffic events.In this paper, we propose the Deep Motion Factorization (DeepMF) framework, which extends static safety-critical driving scenario generation to closed-loop and interactive adversarial traffic simulation. DeepMF casts safety-critical traffic simulation as a Bayesian factorization that includes the assignment of hazardous traffic participants, the motion prediction of selected opponents, the reaction estimation of autonomous vehicle (AV) and the probability estimation of the accident occur. All the aforementioned terms are calculated using decoupled deep neural networks, with inputs limited to the current observation and historical states. Consequently, DeepMF can effectively and efficiently simulate safety-critical traffic scenarios at any triggered time and for any duration by maximizing the compounded posterior probability of traffic risk. Extensive experiments demonstrate that DeepMF excels in terms of risk management, flexibility, and diversity, showcasing outstanding performance in simulating a wide range of realistic, high-risk traffic scenarios.
<div id='section'>Paperid: <span id='pid'>815, <a href='https://arxiv.org/pdf/2412.10523.pdf' target='_blank'>https://arxiv.org/pdf/2412.10523.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Changan Chen, Juze Zhang, Shrinidhi K. Lakshmikanth, Yusu Fang, Ruizhi Shao, Gordon Wetzstein, Li Fei-Fei, Ehsan Adeli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.10523">The Language of Motion: Unifying Verbal and Non-verbal Language of 3D Human Motion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human communication is inherently multimodal, involving a combination of verbal and non-verbal cues such as speech, facial expressions, and body gestures. Modeling these behaviors is essential for understanding human interaction and for creating virtual characters that can communicate naturally in applications like games, films, and virtual reality. However, existing motion generation models are typically limited to specific input modalities -- either speech, text, or motion data -- and cannot fully leverage the diversity of available data. In this paper, we propose a novel framework that unifies verbal and non-verbal language using multimodal language models for human motion understanding and generation. This model is flexible in taking text, speech, and motion or any combination of them as input. Coupled with our novel pre-training strategy, our model not only achieves state-of-the-art performance on co-speech gesture generation but also requires much less data for training. Our model also unlocks an array of novel tasks such as editable gesture generation and emotion prediction from motion. We believe unifying the verbal and non-verbal language of human motion is essential for real-world applications, and language models offer a powerful approach to achieving this goal. Project page: languageofmotion.github.io.
<div id='section'>Paperid: <span id='pid'>816, <a href='https://arxiv.org/pdf/2410.09374.pdf' target='_blank'>https://arxiv.org/pdf/2410.09374.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junkai Niu, Sheng Zhong, Xiuyuan Lu, Shaojie Shen, Guillermo Gallego, Yi Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.09374">ESVO2: Direct Visual-Inertial Odometry with Stereo Event Cameras</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Event-based visual odometry is a specific branch of visual Simultaneous Localization and Mapping (SLAM) techniques, which aims at solving tracking and mapping subproblems (typically in parallel), by exploiting the special working principles of neuromorphic (i.e., event-based) cameras. Due to the motion-dependent nature of event data, explicit data association (i.e., feature matching) under large-baseline view-point changes is difficult to establish, making direct methods a more rational choice. However, state-of-the-art direct methods are limited by the high computational complexity of the mapping sub-problem and the degeneracy of camera pose tracking in certain degrees of freedom (DoF) in rotation. In this paper, we tackle these issues by building an event-based stereo visual-inertial odometry system on top of a direct pipeline. Specifically, to speed up the mapping operation, we propose an efficient strategy for sampling contour points according to the local dynamics of events. The mapping performance is also improved in terms of structure completeness and local smoothness by merging the temporal stereo and static stereo results. To circumvent the degeneracy of camera pose tracking in recovering the pitch and yaw components of general 6-DoF motion, we introduce IMU measurements as motion priors via pre-integration. To this end, a compact back-end is proposed for continuously updating the IMU bias and predicting the linear velocity, enabling an accurate motion prediction for camera pose tracking. The resulting system scales well with modern high-resolution event cameras and leads to better global positioning accuracy in large-scale outdoor environments. Extensive evaluations on five publicly available datasets featuring different resolutions and scenarios justify the superior performance of the proposed system against five state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>817, <a href='https://arxiv.org/pdf/2408.10073.pdf' target='_blank'>https://arxiv.org/pdf/2408.10073.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Oliver Cory, Ozge Mercanoglu Sincan, Matthew Vowels, Alessia Battisti, Franz Holzknecht, Katja Tissi, Sandra Sidler-Miserez, Tobias Haug, Sarah Ebling, Richard Bowden
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.10073">Modelling the Distribution of Human Motion for Sign Language Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Sign Language Assessment (SLA) tools are useful to aid in language learning and are underdeveloped. Previous work has focused on isolated signs or comparison against a single reference video to assess Sign Languages (SL). This paper introduces a novel SLA tool designed to evaluate the comprehensibility of SL by modelling the natural distribution of human motion. We train our pipeline on data from native signers and evaluate it using SL learners. We compare our results to ratings from a human raters study and find strong correlation between human ratings and our tool. We visually demonstrate our tools ability to detect anomalous results spatio-temporally, providing actionable feedback to aid in SL learning and assessment.
<div id='section'>Paperid: <span id='pid'>818, <a href='https://arxiv.org/pdf/2407.16591.pdf' target='_blank'>https://arxiv.org/pdf/2407.16591.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kan Chen, Zhen Meng, Xiangmin Xu, Changyang She, Philip G. Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.16591">Real-Time Interactions Between Human Controllers and Remote Devices in Metaverse</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Supporting real-time interactions between human controllers and remote devices remains a challenging goal in the Metaverse due to the stringent requirements on computing workload, communication throughput, and round-trip latency. In this paper, we establish a novel framework for real-time interactions through the virtual models in the Metaverse. Specifically, we jointly predict the motion of the human controller for 1) proactive rendering in the Metaverse and 2) generating control commands to the real-world remote device in advance. The virtual model is decoupled into two components for rendering and control, respectively. To dynamically adjust the prediction horizons for rendering and control, we develop a two-step human-in-the-loop continuous reinforcement learning approach and use an expert policy to improve the training efficiency. An experimental prototype is built to verify our algorithm with different communication latencies. Compared with the baseline policy without prediction, our proposed method can reduce 1) the Motion-To-Photon (MTP) latency between human motion and rendering feedback and 2) the root mean squared error (RMSE) between human motion and real-world remote devices significantly.
<div id='section'>Paperid: <span id='pid'>819, <a href='https://arxiv.org/pdf/2407.15122.pdf' target='_blank'>https://arxiv.org/pdf/2407.15122.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Konstantinos Gounis, Nikolaos Passalis, Anastasios Tefas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.15122">UAV Active Perception and Motion Control for Improving Navigation Using Low-Cost Sensors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this study a model pipeline is proposed that combines computer vision with control-theoretic methods and utilizes low cost sensors. The proposed work enables perception-aware motion control for a quadrotor UAV to detect and navigate to objects of interest such as wind turbines and electric towers. The distance to the object of interest was estimated utilizing RGB as the primary sensory input. For the needs of the study, the Microsoft AirSim simulator was used. As a first step, a YOLOv8 model was integrated providing the basic position setpoints towards the detection. From the YOLOv8 inference, a target yaw angle was derived. The subsequent algorithms, combining performant in computational terms computer vision methods and YOLOv8, actively drove the drone to measure the height of the detection. Based on the height, an estimate of the depth was retrieved. In addition to this step, a convolutional neural network was developed, namely ActvePerceptionNet aiming at active YOLOv8 inference. The latter was validated for wind turbines where the rotational motion of the propeller was found to affect object confidence in a near periodical fashion. The results of the simulation experiments conducted in this study showed efficient object height and distance estimation and effective localization.
<div id='section'>Paperid: <span id='pid'>820, <a href='https://arxiv.org/pdf/2405.15267.pdf' target='_blank'>https://arxiv.org/pdf/2405.15267.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoxuan Qu, Zhaoyang He, Zeyu Hu, Yujun Cai, Jun Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.15267">Off-the-shelf ChatGPT is a Good Few-shot Human Motion Predictor</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To facilitate the application of motion prediction in practice, recently, the few-shot motion prediction task has attracted increasing research attention. Yet, in existing few-shot motion prediction works, a specific model that is dedicatedly trained over human motions is generally required. In this work, rather than tackling this task through training a specific human motion prediction model, we instead propose a novel FMP-OC framework. In FMP-OC, in a totally training-free manner, we enable Few-shot Motion Prediction, which is a non-language task, to be performed directly via utilizing the Off-the-shelf language model ChatGPT. Specifically, to lead ChatGPT as a language model to become an accurate motion predictor, in FMP-OC, we first introduce several novel designs to facilitate extracting implicit knowledge from ChatGPT. Moreover, we also incorporate our framework with a motion-in-context learning mechanism. Extensive experiments demonstrate the efficacy of our proposed framework.
<div id='section'>Paperid: <span id='pid'>821, <a href='https://arxiv.org/pdf/2405.03803.pdf' target='_blank'>https://arxiv.org/pdf/2405.03803.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Massimiliano Pappa, Luca Collorone, Giovanni Ficarra, Indro Spinelli, Fabio Galasso
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.03803">MoDiPO: text-to-motion alignment via AI-feedback-driven Direct Preference Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion Models have revolutionized the field of human motion generation by offering exceptional generation quality and fine-grained controllability through natural language conditioning. Their inherent stochasticity, that is the ability to generate various outputs from a single input, is key to their success. However, this diversity should not be unrestricted, as it may lead to unlikely generations. Instead, it should be confined within the boundaries of text-aligned and realistic generations. To address this issue, we propose MoDiPO (Motion Diffusion DPO), a novel methodology that leverages Direct Preference Optimization (DPO) to align text-to-motion models. We streamline the laborious and expensive process of gathering human preferences needed in DPO by leveraging AI feedback instead. This enables us to experiment with novel DPO strategies, using both online and offline generated motion-preference pairs. To foster future research we contribute with a motion-preference dataset which we dub Pick-a-Move. We demonstrate, both qualitatively and quantitatively, that our proposed method yields significantly more realistic motions. In particular, MoDiPO substantially improves Frechet Inception Distance (FID) while retaining the same RPrecision and Multi-Modality performances.
<div id='section'>Paperid: <span id='pid'>822, <a href='https://arxiv.org/pdf/2404.01596.pdf' target='_blank'>https://arxiv.org/pdf/2404.01596.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhipeng Zhao, Bowen Li, Yi Du, Taimeng Fu, Chen Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.01596">PhysORD: A Neuro-Symbolic Approach for Physics-infused Motion Prediction in Off-road Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion prediction is critical for autonomous off-road driving, however, it presents significantly more challenges than on-road driving because of the complex interaction between the vehicle and the terrain. Traditional physics-based approaches encounter difficulties in accurately modeling dynamic systems and external disturbance. In contrast, data-driven neural networks require extensive datasets and struggle with explicitly capturing the fundamental physical laws, which can easily lead to poor generalization. By merging the advantages of both methods, neuro-symbolic approaches present a promising direction. These methods embed physical laws into neural models, potentially significantly improving generalization capabilities. However, no prior works were evaluated in real-world settings for off-road driving. To bridge this gap, we present PhysORD, a neural-symbolic approach integrating the conservation law, i.e., the Euler-Lagrange equation, into data-driven neural models for motion prediction in off-road driving. Our experiments showed that PhysORD can accurately predict vehicle motion and tolerate external disturbance by modeling uncertainties. The learned dynamics model achieves 46.7% higher accuracy using only 3.1% of the parameters compared to data-driven methods, demonstrating the data efficiency and superior generalization ability of our neural-symbolic method.
<div id='section'>Paperid: <span id='pid'>823, <a href='https://arxiv.org/pdf/2403.16080.pdf' target='_blank'>https://arxiv.org/pdf/2403.16080.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoyun Zheng, Liwei Liao, Xufeng Li, Jianbo Jiao, Rongjie Wang, Feng Gao, Shiqi Wang, Ronggang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.16080">PKU-DyMVHumans: A Multi-View Video Benchmark for High-Fidelity Dynamic Human Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>High-quality human reconstruction and photo-realistic rendering of a dynamic scene is a long-standing problem in computer vision and graphics. Despite considerable efforts invested in developing various capture systems and reconstruction algorithms, recent advancements still struggle with loose or oversized clothing and overly complex poses. In part, this is due to the challenges of acquiring high-quality human datasets. To facilitate the development of these fields, in this paper, we present PKU-DyMVHumans, a versatile human-centric dataset for high-fidelity reconstruction and rendering of dynamic human scenarios from dense multi-view videos. It comprises 8.2 million frames captured by more than 56 synchronized cameras across diverse scenarios. These sequences comprise 32 human subjects across 45 different scenarios, each with a high-detailed appearance and realistic human motion. Inspired by recent advancements in neural radiance field (NeRF)-based scene representations, we carefully set up an off-the-shelf framework that is easy to provide those state-of-the-art NeRF-based implementations and benchmark on PKU-DyMVHumans dataset. It is paving the way for various applications like fine-grained foreground/background decomposition, high-quality human reconstruction and photo-realistic novel view synthesis of a dynamic scene. Extensive studies are performed on the benchmark, demonstrating new observations and challenges that emerge from using such high-fidelity dynamic data.
<div id='section'>Paperid: <span id='pid'>824, <a href='https://arxiv.org/pdf/2403.14947.pdf' target='_blank'>https://arxiv.org/pdf/2403.14947.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoxuan Qu, Ziyan Guo, Jun Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.14947">GPT-Connect: Interaction between Text-Driven Human Motion Generator and 3D Scenes in a Training-free Manner</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, while text-driven human motion generation has received massive research attention, most existing text-driven motion generators are generally only designed to generate motion sequences in a blank background. While this is the case, in practice, human beings naturally perform their motions in 3D scenes, rather than in a blank background. Considering this, we here aim to perform scene-aware text-drive motion generation instead. Yet, intuitively training a separate scene-aware motion generator in a supervised way can require a large amount of motion samples to be troublesomely collected and annotated in a large scale of different 3D scenes. To handle this task rather in a relatively convenient manner, in this paper, we propose a novel GPT-connect framework. In GPT-connect, we enable scene-aware motion sequences to be generated directly utilizing the existing blank-background human motion generator, via leveraging ChatGPT to connect the existing motion generator with the 3D scene in a totally training-free manner. Extensive experiments demonstrate the efficacy and generalizability of our proposed framework.
<div id='section'>Paperid: <span id='pid'>825, <a href='https://arxiv.org/pdf/2401.11499.pdf' target='_blank'>https://arxiv.org/pdf/2401.11499.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shaoheng Fang, Zuhong Liu, Mingyu Wang, Chenxin Xu, Yiqi Zhong, Siheng Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.11499">Self-Supervised Bird's Eye View Motion Prediction with Cross-Modality Signals</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning the dense bird's eye view (BEV) motion flow in a self-supervised manner is an emerging research for robotics and autonomous driving. Current self-supervised methods mainly rely on point correspondences between point clouds, which may introduce the problems of fake flow and inconsistency, hindering the model's ability to learn accurate and realistic motion. In this paper, we introduce a novel cross-modality self-supervised training framework that effectively addresses these issues by leveraging multi-modality data to obtain supervision signals. We design three innovative supervision signals to preserve the inherent properties of scene motion, including the masked Chamfer distance loss, the piecewise rigidity loss, and the temporal consistency loss. Through extensive experiments, we demonstrate that our proposed self-supervised framework outperforms all previous self-supervision methods for the motion prediction task.
<div id='section'>Paperid: <span id='pid'>826, <a href='https://arxiv.org/pdf/2512.15126.pdf' target='_blank'>https://arxiv.org/pdf/2512.15126.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yupeng Zhu, Xiongzhen Zhang, Ye Chen, Bingbing Ni
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.15126">3DProxyImg: Controllable 3D-Aware Animation Synthesis from Single Image via 2D-3D Aligned Proxy Embedding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D animation is central to modern visual media, yet traditional production pipelines remain labor-intensive, expertise-demanding, and computationally expensive. Recent AIGC-based approaches partially automate asset creation and rigging, but they either inherit the heavy costs of full 3D pipelines or rely on video-synthesis paradigms that sacrifice 3D controllability and interactivity. We focus on single-image 3D animation generation and argue that progress is fundamentally constrained by a trade-off between rendering quality and 3D control. To address this limitation, we propose a lightweight 3D animation framework that decouples geometric control from appearance synthesis. The core idea is a 2D-3D aligned proxy representation that uses a coarse 3D estimate as a structural carrier, while delegating high-fidelity appearance and view synthesis to learned image-space generative priors. This proxy formulation enables 3D-aware motion control and interaction comparable to classical pipelines, without requiring accurate geometry or expensive optimization, and naturally extends to coherent background animation. Extensive experiments demonstrate that our method achieves efficient animation generation on low-power platforms and outperforms video-based 3D animation generation in identity preservation, geometric and textural consistency, and the level of precise, interactive control it offers to users.
<div id='section'>Paperid: <span id='pid'>827, <a href='https://arxiv.org/pdf/2512.11253.pdf' target='_blank'>https://arxiv.org/pdf/2512.11253.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiyuan Li, Chi-Man Pun, Chen Fang, Jue Wang, Xiaodong Cun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.11253">PersonaLive! Expressive Portrait Image Animation for Live Streaming</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current diffusion-based portrait animation models predominantly focus on enhancing visual quality and expression realism, while overlooking generation latency and real-time performance, which restricts their application range in the live streaming scenario. We propose PersonaLive, a novel diffusion-based framework towards streaming real-time portrait animation with multi-stage training recipes. Specifically, we first adopt hybrid implicit signals, namely implicit facial representations and 3D implicit keypoints, to achieve expressive image-level motion control. Then, a fewer-step appearance distillation strategy is proposed to eliminate appearance redundancy in the denoising process, greatly improving inference efficiency. Finally, we introduce an autoregressive micro-chunk streaming generation paradigm equipped with a sliding training strategy and a historical keyframe mechanism to enable low-latency and stable long-term video generation. Extensive experiments demonstrate that PersonaLive achieves state-of-the-art performance with up to 7-22x speedup over prior diffusion-based portrait animation models.
<div id='section'>Paperid: <span id='pid'>828, <a href='https://arxiv.org/pdf/2511.00153.pdf' target='_blank'>https://arxiv.org/pdf/2511.00153.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Justin Yu, Yide Shentu, Di Wu, Pieter Abbeel, Ken Goldberg, Philipp Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.00153">EgoMI: Learning Active Vision and Whole-Body Manipulation from Egocentric Human Demonstrations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Imitation learning from human demonstrations offers a promising approach for robot skill acquisition, but egocentric human data introduces fundamental challenges due to the embodiment gap. During manipulation, humans actively coordinate head and hand movements, continuously reposition their viewpoint and use pre-action visual fixation search strategies to locate relevant objects. These behaviors create dynamic, task-driven head motions that static robot sensing systems cannot replicate, leading to a significant distribution shift that degrades policy performance. We present EgoMI (Egocentric Manipulation Interface), a framework that captures synchronized end-effector and active head trajectories during manipulation tasks, resulting in data that can be retargeted to compatible semi-humanoid robot embodiments. To handle rapid and wide-spanning head viewpoint changes, we introduce a memory-augmented policy that selectively incorporates historical observations. We evaluate our approach on a bimanual robot equipped with an actuated camera head and find that policies with explicit head-motion modeling consistently outperform baseline methods. Results suggest that coordinated hand-eye learning with EgoMI effectively bridges the human-robot embodiment gap for robust imitation learning on semi-humanoid embodiments. Project page: https://egocentric-manipulation-interface.github.io
<div id='section'>Paperid: <span id='pid'>829, <a href='https://arxiv.org/pdf/2508.09003.pdf' target='_blank'>https://arxiv.org/pdf/2508.09003.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Filippo A. Spinelli, Yifan Zhai, Fang Nan, Pascal Egli, Julian Nubert, Thilo Bleumer, Lukas Miller, Ferdinand Hofmann, Marco Hutter
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09003">Large Scale Robotic Material Handling: Learning, Planning, and Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Bulk material handling involves the efficient and precise moving of large quantities of materials, a core operation in many industries, including cargo ship unloading, waste sorting, construction, and demolition. These repetitive, labor-intensive, and safety-critical operations are typically performed using large hydraulic material handlers equipped with underactuated grippers. In this work, we present a comprehensive framework for the autonomous execution of large-scale material handling tasks. The system integrates specialized modules for environment perception, pile attack point selection, path planning, and motion control. The main contributions of this work are two reinforcement learning-based modules: an attack point planner that selects optimal grasping locations on the material pile to maximize removal efficiency and minimize the number of scoops, and a robust trajectory following controller that addresses the precision and safety challenges associated with underactuated grippers in movement, while utilizing their free-swinging nature to release material through dynamic throwing. We validate our framework through real-world experiments on a 40 t material handler in a representative worksite, focusing on two key tasks: high-throughput bulk pile management and high-precision truck loading. Comparative evaluations against human operators demonstrate the system's effectiveness in terms of precision, repeatability, and operational safety. To the best of our knowledge, this is the first complete automation of material handling tasks on a full scale.
<div id='section'>Paperid: <span id='pid'>830, <a href='https://arxiv.org/pdf/2508.07376.pdf' target='_blank'>https://arxiv.org/pdf/2508.07376.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Huangbin Liang, Beatriz Moya, Francisco Chinesta, Eleni Chatzi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.07376">A Multi-Model Probabilistic Framework for Seismic Risk Assessment and Retrofit Planning of Electric Power Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Electric power networks are critical lifelines, and their disruption during earthquakes can lead to severe cascading failures and significantly hinder post-disaster recovery. Enhancing their seismic resilience requires identifying and strengthening vulnerable components in a cost-effective and system-aware manner. However, existing studies often overlook the systemic behavior of power networks under seismic loading. Common limitations include isolated component analyses that neglect network-wide interdependencies, oversimplified damage models assuming binary states or damage independence, and the exclusion of electrical operational constraints. These simplifications can result in inaccurate risk estimates and inefficient retrofit decisions. This study proposes a multi-model probabilistic framework for seismic risk assessment and retrofit planning of electric power systems. The approach integrates: (1) regional seismic hazard characterization with ground motion prediction and spatial correlation models; (2) component-level damage analysis using fragility functions and multi-state damage-functionality mappings; (3) system-level cascading impact evaluation through graph-based island detection and constrained optimal power flow analysis; and (4) retrofit planning via heuristic optimization to minimize expected annual functionality loss (EAFL) under budget constraints. Uncertainty is propagated throughout the framework using Monte Carlo simulation. The methodology is demonstrated on the IEEE 24-bus Reliability Test System, showcasing its ability to capture cascading failures, identify critical components, and generate effective retrofit strategies. Results underscore the potential of the framework as a scalable, data-informed decision-support tool for enhancing the seismic resilience of power infrastructure.
<div id='section'>Paperid: <span id='pid'>831, <a href='https://arxiv.org/pdf/2506.15290.pdf' target='_blank'>https://arxiv.org/pdf/2506.15290.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andela Ilic, Jiaxi Jiang, Paul Streli, Xintong Liu, Christian Holz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.15290">Human Motion Capture from Loose and Sparse Inertial Sensors with Garment-aware Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion capture using sparse inertial sensors has shown great promise due to its portability and lack of occlusion issues compared to camera-based tracking. Existing approaches typically assume that IMU sensors are tightly attached to the human body. However, this assumption often does not hold in real-world scenarios. In this paper, we present Garment Inertial Poser (GaIP), a method for estimating full-body poses from sparse and loosely attached IMU sensors. We first simulate IMU recordings using an existing garment-aware human motion dataset. Our transformer-based diffusion models synthesize loose IMU data and estimate human poses from this challenging loose IMU data. We also demonstrate that incorporating garment-related parameters during training on loose IMU data effectively maintains expressiveness and enhances the ability to capture variations introduced by looser or tighter garments. Our experiments show that our diffusion methods trained on simulated and synthetic data outperform state-of-the-art inertial full-body pose estimators, both quantitatively and qualitatively, opening up a promising direction for future research on motion capture from such realistic sensor placements.
<div id='section'>Paperid: <span id='pid'>832, <a href='https://arxiv.org/pdf/2505.06584.pdf' target='_blank'>https://arxiv.org/pdf/2505.06584.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziluo Ding, Haobin Jiang, Yuxuan Wang, Zhenguo Sun, Yu Zhang, Xiaojie Niu, Ming Yang, Weishuai Zeng, Xinrun Xu, Zongqing Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.06584">JAEGER: Dual-Level Humanoid Whole-Body Controller</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents JAEGER, a dual-level whole-body controller for humanoid robots that addresses the challenges of training a more robust and versatile policy. Unlike traditional single-controller approaches, JAEGER separates the control of the upper and lower bodies into two independent controllers, so that they can better focus on their distinct tasks. This separation alleviates the dimensionality curse and improves fault tolerance. JAEGER supports both root velocity tracking (coarse-grained control) and local joint angle tracking (fine-grained control), enabling versatile and stable movements. To train the controller, we utilize a human motion dataset (AMASS), retargeting human poses to humanoid poses through an efficient retargeting network, and employ a curriculum learning approach. This method performs supervised learning for initialization, followed by reinforcement learning for further exploration. We conduct our experiments on two humanoid platforms and demonstrate the superiority of our approach against state-of-the-art methods in both simulation and real environments.
<div id='section'>Paperid: <span id='pid'>833, <a href='https://arxiv.org/pdf/2505.01425.pdf' target='_blank'>https://arxiv.org/pdf/2505.01425.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiefeng Li, Jinkun Cao, Haotian Zhang, Davis Rempe, Jan Kautz, Umar Iqbal, Ye Yuan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.01425">GENMO: A GENeralist Model for Human MOtion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion modeling traditionally separates motion generation and estimation into distinct tasks with specialized models. Motion generation models focus on creating diverse, realistic motions from inputs like text, audio, or keyframes, while motion estimation models aim to reconstruct accurate motion trajectories from observations like videos. Despite sharing underlying representations of temporal dynamics and kinematics, this separation limits knowledge transfer between tasks and requires maintaining separate models. We present GENMO, a unified Generalist Model for Human Motion that bridges motion estimation and generation in a single framework. Our key insight is to reformulate motion estimation as constrained motion generation, where the output motion must precisely satisfy observed conditioning signals. Leveraging the synergy between regression and diffusion, GENMO achieves accurate global motion estimation while enabling diverse motion generation. We also introduce an estimation-guided training objective that exploits in-the-wild videos with 2D annotations and text descriptions to enhance generative diversity. Furthermore, our novel architecture handles variable-length motions and mixed multimodal conditions (text, audio, video) at different time intervals, offering flexible control. This unified approach creates synergistic benefits: generative priors improve estimated motions under challenging conditions like occlusions, while diverse video data enhances generation capabilities. Extensive experiments demonstrate GENMO's effectiveness as a generalist framework that successfully handles multiple human motion tasks within a single model.
<div id='section'>Paperid: <span id='pid'>834, <a href='https://arxiv.org/pdf/2503.05540.pdf' target='_blank'>https://arxiv.org/pdf/2503.05540.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Leonel Rozo, Miguel GonzÃ¡lez-Duque, NoÃ©mie Jaquier, SÃ¸ren Hauberg
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.05540">Riemann$^2$: Learning Riemannian Submanifolds from Riemannian Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Latent variable models are powerful tools for learning low-dimensional manifolds from high-dimensional data. However, when dealing with constrained data such as unit-norm vectors or symmetric positive-definite matrices, existing approaches ignore the underlying geometric constraints or fail to provide meaningful metrics in the latent space. To address these limitations, we propose to learn Riemannian latent representations of such geometric data. To do so, we estimate the pullback metric induced by a Wrapped Gaussian Process Latent Variable Model, which explicitly accounts for the data geometry. This enables us to define geometry-aware notions of distance and shortest paths in the latent space, while ensuring that our model only assigns probability mass to the data manifold. This generalizes previous work and allows us to handle complex tasks in various domains, including robot motion synthesis and analysis of brain connectomes.
<div id='section'>Paperid: <span id='pid'>835, <a href='https://arxiv.org/pdf/2502.18373.pdf' target='_blank'>https://arxiv.org/pdf/2502.18373.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dominik Hollidt, Paul Streli, Jiaxi Jiang, Yasaman Haghighi, Changlin Qian, Xintong Liu, Christian Holz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.18373">EgoSim: An Egocentric Multi-view Simulator and Real Dataset for Body-worn Cameras during Motion and Activity</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Research on egocentric tasks in computer vision has mostly focused on head-mounted cameras, such as fisheye cameras or embedded cameras inside immersive headsets. We argue that the increasing miniaturization of optical sensors will lead to the prolific integration of cameras into many more body-worn devices at various locations. This will bring fresh perspectives to established tasks in computer vision and benefit key areas such as human motion tracking, body pose estimation, or action recognition -- particularly for the lower body, which is typically occluded.
  In this paper, we introduce EgoSim, a novel simulator of body-worn cameras that generates realistic egocentric renderings from multiple perspectives across a wearer's body. A key feature of EgoSim is its use of real motion capture data to render motion artifacts, which are especially noticeable with arm- or leg-worn cameras. In addition, we introduce MultiEgoView, a dataset of egocentric footage from six body-worn cameras and ground-truth full-body 3D poses during several activities: 119 hours of data are derived from AMASS motion sequences in four high-fidelity virtual environments, which we augment with 5 hours of real-world motion data from 13 participants using six GoPro cameras and 3D body pose references from an Xsens motion capture suit.
  We demonstrate EgoSim's effectiveness by training an end-to-end video-only 3D pose estimation network. Analyzing its domain gap, we show that our dataset and simulator substantially aid training for inference on real-world data.
  EgoSim code & MultiEgoView dataset: https://siplab.org/projects/EgoSim
<div id='section'>Paperid: <span id='pid'>836, <a href='https://arxiv.org/pdf/2502.06212.pdf' target='_blank'>https://arxiv.org/pdf/2502.06212.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pandula Thennakoon, Mario De Silva, M. Mahesha Viduranga, Sashini Liyanage, Roshan Godaliyadda, Mervyn Parakrama Ekanayake, Vijitha Herath, Anuruddhika Rathnayake, Ganga Thilakarathne, Janaka Ekanayake, Samath Dharmarathne
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.06212">AVSim -- Realistic Simulation Framework for Airborne and Vector-Borne Disease Dynamics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Computational disease modeling plays a crucial role in understanding and controlling the transmission of infectious diseases. While agent-based models (ABMs) provide detailed insights into individual dynamics, accurately replicating human motion remains challenging due to its complex, multi-factorial nature. Most existing frameworks fail to model realistic human motion, leading to oversimplified and less realistic behavior modeling. Furthermore, many current models rely on synthetic assumptions and fail to account for realistic environmental structures, transportation systems, and behavioral heterogeneity across occupation groups. To address these limitations, we introduce AVSim, an agent-based simulation framework designed to model airborne and vector-borne disease dynamics under realistic conditions. A distinguishing feature of AVSim is its ability to accurately model the dual nature of human mobility (both the destinations individuals visit and the duration of their stay) by utilizing GPS traces from real-world participants, characterized by occupation. This enables a significantly more granular and realistic representation of human movement compared to existing approaches. Furthermore, spectral clustering combined with graph-theoretic analysis is used to uncover latent behavioral patterns within occupations, enabling fine-grained modeling of agent behavior. We validate the synthetic human mobility patterns against ground-truth GPS data and demonstrate AVSim's capabilities via simulations of COVID-19 and dengue. The results highlight AVSim's capacity to trace infection pathways, identify high-risk zones, and evaluate interventions such as vaccination, quarantine, and vector control with occupational and geographic specificity.
<div id='section'>Paperid: <span id='pid'>837, <a href='https://arxiv.org/pdf/2502.01357.pdf' target='_blank'>https://arxiv.org/pdf/2502.01357.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dong-In Kim, Dong-Hee Paek, Seung-Hyun Song, Seung-Hyun Kong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.01357">Bayesian Approximation-Based Trajectory Prediction and Tracking with 4D Radar</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate 3D multi-object tracking (MOT) is vital for autonomous vehicles, yet LiDAR and camera-based methods degrade in adverse weather. Meanwhile, Radar-based solutions remain robust but often suffer from limited vertical resolution and simplistic motion models. Existing Kalman filter-based approaches also rely on fixed noise covariance, hampering adaptability when objects make sudden maneuvers. We propose Bayes-4DRTrack, a 4D Radar-based MOT framework that adopts a transformer-based motion prediction network to capture nonlinear motion dynamics and employs Bayesian approximation in both detection and prediction steps. Moreover, our two-stage data association leverages Doppler measurements to better distinguish closely spaced targets. Evaluated on the K-Radar dataset (including adverse weather scenarios), Bayes-4DRTrack demonstrates a 5.7% gain in Average Multi-Object Tracking Accuracy (AMOTA) over methods with traditional motion models and fixed noise covariance. These results showcase enhanced robustness and accuracy in demanding, real-world conditions.
<div id='section'>Paperid: <span id='pid'>838, <a href='https://arxiv.org/pdf/2410.15797.pdf' target='_blank'>https://arxiv.org/pdf/2410.15797.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Julien Mellet, Andrea Berra, Achilleas Santi Seisa, Viswa Sankaranarayanan, Udayanga G. W. K. N. Gamage, Miguel Angel Trujillo Soto, Guillermo Heredia, George Nikolakopoulos, Vincenzo Lippiello, Fabio Ruggiero
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.15797">Design of a Flexible Robot Arm for Safe Aerial Physical Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces a novel compliant mechanism combining lightweight and energy dissipation for aerial physical interaction. Weighting 400~g at take-off, the mechanism is actuated in the forward body direction, enabling precise position control for force interaction and various other aerial manipulation tasks. The robotic arm, structured as a closed-loop kinematic chain, employs two deported servomotors. Each joint is actuated with a single tendon for active motion control in compression of the arm at the end-effector. Its elasto-mechanical design reduces weight and provides flexibility, allowing passive-compliant interactions without impacting the motors' integrity. Notably, the arm's damping can be adjusted based on the proposed inner frictional bulges. Experimental applications showcase the aerial system performance in both free-flight and physical interaction. The presented work may open safer applications for \ac{MAV} in real environments subject to perturbations during interaction.
<div id='section'>Paperid: <span id='pid'>839, <a href='https://arxiv.org/pdf/2410.15154.pdf' target='_blank'>https://arxiv.org/pdf/2410.15154.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yin Li, Liangwei Wang, Shiyuan Piao, Boo-Ho Yang, Ziyue Li, Wei Zeng, Fugee Tsung
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.15154">MCCoder: Streamlining Motion Control with LLM-Assisted Code Generation and Rigorous Verification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs) have demonstrated significant potential in code generation. However, in the factory automation sector, particularly motion control, manual programming, alongside inefficient and unsafe debugging practices, remains prevalent. This stems from the complex interplay of mechanical and electrical systems and stringent safety requirements. Moreover, most current AI-assisted motion control programming efforts focus on PLCs, with little attention given to high-level languages and function libraries. To address these challenges, we introduce MCCoder, an LLM-powered system tailored for generating motion control code, integrated with a soft-motion controller. MCCoder improves code generation through a structured workflow that combines multitask decomposition, hybrid retrieval-augmented generation (RAG), and iterative self-correction, utilizing a well-established motion library. Additionally, it integrates a 3D simulator for intuitive motion validation and logs of full motion trajectories for data verification, significantly enhancing accuracy and safety. In the absence of benchmark datasets and metrics tailored for evaluating motion control code generation, we propose MCEVAL, a dataset spanning motion tasks of varying complexity. Experiments show that MCCoder outperforms baseline models using Advanced RAG, achieving an overall performance gain of 33.09% and a 131.77% improvement on complex tasks in the MCEVAL dataset.
<div id='section'>Paperid: <span id='pid'>840, <a href='https://arxiv.org/pdf/2407.02633.pdf' target='_blank'>https://arxiv.org/pdf/2407.02633.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiming Hu, Zheming Yin, Daniel Haeufle, Syn Schmitt, Andreas Bulling
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.02633">HOIMotion: Forecasting Human Motion During Human-Object Interactions Using Egocentric 3D Object Bounding Boxes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present HOIMotion - a novel approach for human motion forecasting during human-object interactions that integrates information about past body poses and egocentric 3D object bounding boxes. Human motion forecasting is important in many augmented reality applications but most existing methods have only used past body poses to predict future motion. HOIMotion first uses an encoder-residual graph convolutional network (GCN) and multi-layer perceptrons to extract features from body poses and egocentric 3D object bounding boxes, respectively. Our method then fuses pose and object features into a novel pose-object graph and uses a residual-decoder GCN to forecast future body motion. We extensively evaluate our method on the Aria digital twin (ADT) and MoGaze datasets and show that HOIMotion consistently outperforms state-of-the-art methods by a large margin of up to 8.7% on ADT and 7.2% on MoGaze in terms of mean per joint position error. Complementing these evaluations, we report a human study (N=20) that shows that the improvements achieved by our method result in forecasted poses being perceived as both more precise and more realistic than those of existing methods. Taken together, these results reveal the significant information content available in egocentric 3D object bounding boxes for human motion forecasting and the effectiveness of our method in exploiting this information.
<div id='section'>Paperid: <span id='pid'>841, <a href='https://arxiv.org/pdf/2404.15383.pdf' target='_blank'>https://arxiv.org/pdf/2404.15383.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Markos Diomataris, Nikos Athanasiou, Omid Taheri, Xi Wang, Otmar Hilliges, Michael J. Black
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.15383">WANDR: Intention-guided Human Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Synthesizing natural human motions that enable a 3D human avatar to walk and reach for arbitrary goals in 3D space remains an unsolved problem with many applications. Existing methods (data-driven or using reinforcement learning) are limited in terms of generalization and motion naturalness. A primary obstacle is the scarcity of training data that combines locomotion with goal reaching. To address this, we introduce WANDR, a data-driven model that takes an avatar's initial pose and a goal's 3D position and generates natural human motions that place the end effector (wrist) on the goal location. To solve this, we introduce novel intention features that drive rich goal-oriented movement. Intention guides the agent to the goal, and interactively adapts the generation to novel situations without needing to define sub-goals or the entire motion path. Crucially, intention allows training on datasets that have goal-oriented motions as well as those that do not. WANDR is a conditional Variational Auto-Encoder (c-VAE), which we train using the AMASS and CIRCLE datasets. We evaluate our method extensively and demonstrate its ability to generate natural and long-term motions that reach 3D goals and generalize to unseen goal locations. Our models and code are available for research purposes at wandr.is.tue.mpg.de.
<div id='section'>Paperid: <span id='pid'>842, <a href='https://arxiv.org/pdf/2403.18344.pdf' target='_blank'>https://arxiv.org/pdf/2403.18344.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingxing Peng, Xusen Guo, Xianda Chen, Meixin Zhu, Kehua Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.18344">LC-LLM: Explainable Lane-Change Intention and Trajectory Predictions with Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To ensure safe driving in dynamic environments, autonomous vehicles should possess the capability to accurately predict lane change intentions of surrounding vehicles in advance and forecast their future trajectories. Existing motion prediction approaches have ample room for improvement, particularly in terms of long-term prediction accuracy and interpretability. In this paper, we address these challenges by proposing LC-LLM, an explainable lane change prediction model that leverages the strong reasoning capabilities and self-explanation abilities of Large Language Models (LLMs). Essentially, we reformulate the lane change prediction task as a language modeling problem, processing heterogeneous driving scenario information as natural language prompts for LLMs and employing supervised fine-tuning to tailor LLMs specifically for lane change prediction task. Additionally, we finetune the Chain-of-Thought (CoT) reasoning to improve prediction transparency and reliability, and include explanatory requirements in the prompts during inference stage. Therefore, our LC-LLM model not only predicts lane change intentions and trajectories but also provides CoT reasoning and explanations for its predictions, enhancing its interpretability. Extensive experiments based on the large-scale highD dataset demonstrate the superior performance and interpretability of our LC-LLM in lane change prediction task. To the best of our knowledge, this is the first attempt to utilize LLMs for predicting lane change behavior. Our study shows that LLMs can effectively encode comprehensive interaction information for driving behavior understanding.
<div id='section'>Paperid: <span id='pid'>843, <a href='https://arxiv.org/pdf/2403.13261.pdf' target='_blank'>https://arxiv.org/pdf/2403.13261.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kewei Wang, Yizheng Wu, Jun Cen, Zhiyu Pan, Xingyi Li, Zhe Wang, Zhiguo Cao, Guosheng Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.13261">Self-Supervised Class-Agnostic Motion Prediction with Spatial and Temporal Consistency Regularizations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The perception of motion behavior in a dynamic environment holds significant importance for autonomous driving systems, wherein class-agnostic motion prediction methods directly predict the motion of the entire point cloud. While most existing methods rely on fully-supervised learning, the manual labeling of point cloud data is laborious and time-consuming. Therefore, several annotation-efficient methods have been proposed to address this challenge. Although effective, these methods rely on weak annotations or additional multi-modal data like images, and the potential benefits inherent in the point cloud sequence are still underexplored. To this end, we explore the feasibility of self-supervised motion prediction with only unlabeled LiDAR point clouds. Initially, we employ an optimal transport solver to establish coarse correspondences between current and future point clouds as the coarse pseudo motion labels. Training models directly using such coarse labels leads to noticeable spatial and temporal prediction inconsistencies. To mitigate these issues, we introduce three simple spatial and temporal regularization losses, which facilitate the self-supervised training process effectively. Experimental results demonstrate the significant superiority of our approach over the state-of-the-art self-supervised methods.
<div id='section'>Paperid: <span id='pid'>844, <a href='https://arxiv.org/pdf/2403.09885.pdf' target='_blank'>https://arxiv.org/pdf/2403.09885.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiming Hu, Syn Schmitt, Daniel Haeufle, Andreas Bulling
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.09885">GazeMotion: Gaze-guided Human Motion Forecasting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present GazeMotion, a novel method for human motion forecasting that combines information on past human poses with human eye gaze. Inspired by evidence from behavioural sciences showing that human eye and body movements are closely coordinated, GazeMotion first predicts future eye gaze from past gaze, then fuses predicted future gaze and past poses into a gaze-pose graph, and finally uses a residual graph convolutional network to forecast body motion. We extensively evaluate our method on the MoGaze, ADT, and GIMO benchmark datasets and show that it outperforms state-of-the-art methods by up to 7.4% improvement in mean per joint position error. Using head direction as a proxy to gaze, our method still achieves an average improvement of 5.5%. We finally report an online user study showing that our method also outperforms prior methods in terms of perceived realism. These results show the significant information content available in eye gaze for human motion forecasting as well as the effectiveness of our method in exploiting this information.
<div id='section'>Paperid: <span id='pid'>845, <a href='https://arxiv.org/pdf/2403.00353.pdf' target='_blank'>https://arxiv.org/pdf/2403.00353.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaqiang Tang, Weigao Sun, Siyuan Hu, Yiyang Sun, Yafeng Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.00353">MS-Net: A Multi-Path Sparse Model for Motion Prediction in Multi-Scenes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The multi-modality and stochastic characteristics of human behavior make motion prediction a highly challenging task, which is critical for autonomous driving. While deep learning approaches have demonstrated their great potential in this area, it still remains unsolved to establish a connection between multiple driving scenes (e.g., merging, roundabout, intersection) and the design of deep learning models. Current learning-based methods typically use one unified model to predict trajectories in different scenarios, which may result in sub-optimal results for one individual scene. To address this issue, we propose Multi-Scenes Network (aka. MS-Net), which is a multi-path sparse model trained by an evolutionary process. MS-Net selectively activates a subset of its parameters during the inference stage to produce prediction results for each scene. In the training stage, the motion prediction task under differentiated scenes is abstracted as a multi-task learning problem, an evolutionary algorithm is designed to encourage the network search of the optimal parameters for each scene while sharing common knowledge between different scenes. Our experiment results show that with substantially reduced parameters, MS-Net outperforms existing state-of-the-art methods on well-established pedestrian motion prediction datasets, e.g., ETH and UCY, and ranks the 2nd place on the INTERACTION challenge.
<div id='section'>Paperid: <span id='pid'>846, <a href='https://arxiv.org/pdf/2512.22295.pdf' target='_blank'>https://arxiv.org/pdf/2512.22295.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tian Guo, Hui Yuan, Philip Xu, David Elizondo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.22295">Learning Dynamic Scene Reconstruction with Sinusoidal Geometric Priors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose SirenPose, a novel loss function that combines the periodic activation properties of sinusoidal representation networks with geometric priors derived from keypoint structures to improve the accuracy of dynamic 3D scene reconstruction. Existing approaches often struggle to maintain motion modeling accuracy and spatiotemporal consistency in fast moving and multi target scenes. By introducing physics inspired constraint mechanisms, SirenPose enforces coherent keypoint predictions across both spatial and temporal dimensions. We further expand the training dataset to 600,000 annotated instances to support robust learning. Experimental results demonstrate that models trained with SirenPose achieve significant improvements in spatiotemporal consistency metrics compared to prior methods, showing superior performance in handling rapid motion and complex scene changes.
<div id='section'>Paperid: <span id='pid'>847, <a href='https://arxiv.org/pdf/2512.00324.pdf' target='_blank'>https://arxiv.org/pdf/2512.00324.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinda Du, Jieji Ren, Qiaojun Yu, Ningbin Zhang, Yu Deng, Xingyu Wei, Yufei Liu, Guoying Gu, Xiangyang Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.00324">MILE: A Mechanically Isomorphic Exoskeleton Data Collection System with Fingertip Visuotactile Sensing for Dexterous Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Imitation learning provides a promising approach to dexterous hand manipulation, but its effectiveness is limited by the lack of large-scale, high-fidelity data. Existing data-collection pipelines suffer from inaccurate motion retargeting, low data-collection efficiency, and missing high-resolution fingertip tactile sensing. We address this gap with MILE, a mechanically isomorphic teleoperation and data-collection system co-designed from human hand to exoskeleton to robotic hand. The exoskeleton is anthropometrically derived from the human hand, and the robotic hand preserves one-to-one joint-position isomorphism, eliminating nonlinear retargeting and enabling precise, natural control. The exoskeleton achieves a multi-joint mean absolute angular error below one degree, while the robotic hand integrates compact fingertip visuotactile modules that provide high-resolution tactile observations. Built on this retargeting-free interface, we teleoperate complex, contact-rich in-hand manipulation and efficiently collect a multimodal dataset comprising high-resolution fingertip visuotactile signals, RGB-D images, and joint positions. The teleoperation pipeline achieves a mean success rate improvement of 64%. Incorporating fingertip tactile observations further increases the success rate by an average of 25% over the vision-only baseline, validating the fidelity and utility of the dataset. Further details are available at: https://sites.google.com/view/mile-system.
<div id='section'>Paperid: <span id='pid'>848, <a href='https://arxiv.org/pdf/2510.10206.pdf' target='_blank'>https://arxiv.org/pdf/2510.10206.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zuhong Liu, Junhao Ge, Minhao Xiong, Jiahao Gu, Bowei Tang, Wei Jing, Siheng Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.10206">It Takes Two: Learning Interactive Whole-Body Control Between Humanoid Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The true promise of humanoid robotics lies beyond single-agent autonomy: two or more humanoids must engage in physically grounded, socially meaningful whole-body interactions that echo the richness of human social interaction. However, single-humanoid methods suffer from the isolation issue, ignoring inter-agent dynamics and causing misaligned contacts, interpenetrations, and unrealistic motions. To address this, we present Harmanoid , a dual-humanoid motion imitation framework that transfers interacting human motions to two robots while preserving both kinematic fidelity and physical realism. Harmanoid comprises two key components: (i) contact-aware motion retargeting, which restores inter-body coordination by aligning SMPL contacts with robot vertices, and (ii) interaction-driven motion controller, which leverages interaction-specific rewards to enforce coordinated keypoints and physically plausible contacts. By explicitly modeling inter-agent contacts and interaction-aware dynamics, Harmanoid captures the coupled behaviors between humanoids that single-humanoid frameworks inherently overlook. Experiments demonstrate that Harmanoid significantly improves interactive motion imitation, surpassing existing single-humanoid frameworks that largely fail in such scenarios.
<div id='section'>Paperid: <span id='pid'>849, <a href='https://arxiv.org/pdf/2510.03423.pdf' target='_blank'>https://arxiv.org/pdf/2510.03423.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ethan Foss, Simone D'Amico
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03423">Efficient Input-Constrained Impulsive Optimal Control of Linear Systems with Application to Spacecraft Relative Motion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work presents a novel algorithm for impulsive optimal control of linear time-varying systems with the inclusion of input magnitude constraints. Impulsive optimal control problems, where the optimal input solution is a sum of delta functions, are typically formulated as an optimization over a normed function space subject to integral equality constraints and can be efficiently solved for linear time-varying systems in their dual formulation. In this dual setting, the problem takes the form of a semi-infinite program which is readily solvable in online scenarios for constructing maneuver plans. This work augments the approach with the inclusion of magnitude constraints on the input over time windows of interest, which is shown to preserve the impulsive nature of the optimal solution and enable efficient solution procedures via semi-infinite programming. The resulting algorithm is demonstrated on the highly relevant problem of relative motion control of spacecraft in Low Earth Orbit (LEO) and compared to several other proposed solutions from the literature.
<div id='section'>Paperid: <span id='pid'>850, <a href='https://arxiv.org/pdf/2509.24099.pdf' target='_blank'>https://arxiv.org/pdf/2509.24099.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Prerit Gupta, Shourya Verma, Ananth Grama, Aniket Bera
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24099">Unified Multi-Modal Interactive & Reactive 3D Motion Generation via Rectified Flow</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating realistic, context-aware two-person motion conditioned on diverse modalities remains a central challenge in computer graphics, animation, and human-computer interaction. We introduce DualFlow, a unified and efficient framework for multi-modal two-person motion generation. DualFlow conditions 3D motion synthesis on diverse inputs, including text, music, and prior motion sequences. Leveraging rectified flow, it achieves deterministic straight-line sampling paths between noise and data, reducing inference time and mitigating error accumulation common in diffusion-based models. To enhance semantic grounding, DualFlow employs a Retrieval-Augmented Generation (RAG) module that retrieves motion exemplars using music features and LLM-based text decompositions of spatial relations, body movements, and rhythmic patterns. We use contrastive objective that further strengthens alignment with conditioning signals and introduce synchronization loss that improves inter-person coordination. Extensive evaluations across text-to-motion, music-to-motion, and multi-modal interactive benchmarks show consistent gains in motion quality, responsiveness, and efficiency. DualFlow produces temporally coherent and rhythmically synchronized motions, setting state-of-the-art in multi-modal human motion generation.
<div id='section'>Paperid: <span id='pid'>851, <a href='https://arxiv.org/pdf/2509.23009.pdf' target='_blank'>https://arxiv.org/pdf/2509.23009.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Masato Kobayashi, Ning Ding, Toru Tamaki
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23009">Disentangling Static and Dynamic Information for Reducing Static Bias in Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Action recognition models rely excessively on static cues rather than dynamic human motion, which is known as static bias. This bias leads to poor performance in real-world applications and zero-shot action recognition. In this paper, we propose a method to reduce static bias by separating temporal dynamic information from static scene information. Our approach uses a statistical independence loss between biased and unbiased streams, combined with a scene prediction loss. Our experiments demonstrate that this method effectively reduces static bias and confirm the importance of scene prediction loss.
<div id='section'>Paperid: <span id='pid'>852, <a href='https://arxiv.org/pdf/2509.20263.pdf' target='_blank'>https://arxiv.org/pdf/2509.20263.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bingjie Chen, Zihan Wang, Zhe Han, Guoping Pan, Yi Cheng, Houde Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20263">HL-IK: A Lightweight Implementation of Human-Like Inverse Kinematics in Humanoid Arms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Traditional IK methods for redundant humanoid manipulators emphasize end-effector (EE) tracking, frequently producing configurations that are valid mechanically but not human-like. We present Human-Like Inverse Kinematics (HL-IK), a lightweight IK framework that preserves EE tracking while shaping whole-arm configurations to appear human-like, without full-body sensing at runtime. The key idea is a learned elbow prior: using large-scale human motion data retargeted to the robot, we train a FiLM-modulated spatio-temporal attention network (FiSTA) to predict the next-step elbow pose from the EE target and a short history of EE-elbow states.This prediction is incorporated as a small residual alongside EE and smoothness terms in a standard Levenberg-Marquardt optimizer, making HL-IK a drop-in addition to numerical IK stacks. Over 183k simulation steps, HL-IK reduces arm-similarity position and direction error by 30.6% and 35.4% on average, and by 42.2% and 47.4% on the most challenging trajectories. Hardware teleoperation on a robot distinct from simulation further confirms the gains in anthropomorphism. HL-IK is simple to integrate, adaptable across platforms via our pipeline, and adds minimal computation, enabling human-like motions for humanoid robots. Project page: https://hl-ik.github.io/
<div id='section'>Paperid: <span id='pid'>853, <a href='https://arxiv.org/pdf/2509.18427.pdf' target='_blank'>https://arxiv.org/pdf/2509.18427.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinyang Wu, Muheng Li, Xia Li, Orso Pusterla, Sairos Safai, Philippe C. Cattin, Antony J. Lomax, Ye Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.18427">CPT-4DMR: Continuous sPatial-Temporal Representation for 4D-MRI Reconstruction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Four-dimensional MRI (4D-MRI) is an promising technique for capturing respiratory-induced motion in radiation therapy planning and delivery. Conventional 4D reconstruction methods, which typically rely on phase binning or separate template scans, struggle to capture temporal variability, complicate workflows, and impose heavy computational loads. We introduce a neural representation framework that considers respiratory motion as a smooth, continuous deformation steered by a 1D surrogate signal, completely replacing the conventional discrete sorting approach. The new method fuses motion modeling with image reconstruction through two synergistic networks: the Spatial Anatomy Network (SAN) encodes a continuous 3D anatomical representation, while a Temporal Motion Network (TMN), guided by Transformer-derived respiratory signals, produces temporally consistent deformation fields. Evaluation using a free-breathing dataset of 19 volunteers demonstrates that our template- and phase-free method accurately captures both regular and irregular respiratory patterns, while preserving vessel and bronchial continuity with high anatomical fidelity. The proposed method significantly improves efficiency, reducing the total processing time from approximately five hours required by conventional discrete sorting methods to just 15 minutes of training. Furthermore, it enables inference of each 3D volume in under one second. The framework accurately reconstructs 3D images at any respiratory state, achieves superior performance compared to conventional methods, and demonstrates strong potential for application in 4D radiation therapy planning and real-time adaptive treatment.
<div id='section'>Paperid: <span id='pid'>854, <a href='https://arxiv.org/pdf/2509.17476.pdf' target='_blank'>https://arxiv.org/pdf/2509.17476.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mallikarjun B. R., Fei Yin, Vikram Voleti, Nikita Drobyshev, Maksim Lapin, Aaryaman Vasishta, Varun Jampani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17476">Stable Video-Driven Portraits</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Portrait animation aims to generate photo-realistic videos from a single source image by reenacting the expression and pose from a driving video. While early methods relied on 3D morphable models or feature warping techniques, they often suffered from limited expressivity, temporal inconsistency, and poor generalization to unseen identities or large pose variations. Recent advances using diffusion models have demonstrated improved quality but remain constrained by weak control signals and architectural limitations. In this work, we propose a novel diffusion based framework that leverages masked facial regions specifically the eyes, nose, and mouth from the driving video as strong motion control cues. To enable robust training without appearance leakage, we adopt cross identity supervision. To leverage the strong prior from the pretrained diffusion model, our novel architecture introduces minimal new parameters that converge faster and help in better generalization. We introduce spatial temporal attention mechanisms that allow inter frame and intra frame interactions, effectively capturing subtle motions and reducing temporal artifacts. Our model uses history frames to ensure continuity across segments. At inference, we propose a novel signal fusion strategy that balances motion fidelity with identity preservation. Our approach achieves superior temporal consistency and accurate expression control, enabling high-quality, controllable portrait animation suitable for real-world applications.
<div id='section'>Paperid: <span id='pid'>855, <a href='https://arxiv.org/pdf/2509.13116.pdf' target='_blank'>https://arxiv.org/pdf/2509.13116.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruibo Li, Hanyu Shi, Zhe Wang, Guosheng Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.13116">Weakly and Self-Supervised Class-Agnostic Motion Prediction for Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding motion in dynamic environments is critical for autonomous driving, thereby motivating research on class-agnostic motion prediction. In this work, we investigate weakly and self-supervised class-agnostic motion prediction from LiDAR point clouds. Outdoor scenes typically consist of mobile foregrounds and static backgrounds, allowing motion understanding to be associated with scene parsing. Based on this observation, we propose a novel weakly supervised paradigm that replaces motion annotations with fully or partially annotated (1%, 0.1%) foreground/background masks for supervision. To this end, we develop a weakly supervised approach utilizing foreground/background cues to guide the self-supervised learning of motion prediction models. Since foreground motion generally occurs in non-ground regions, non-ground/ground masks can serve as an alternative to foreground/background masks, further reducing annotation effort. Leveraging non-ground/ground cues, we propose two additional approaches: a weakly supervised method requiring fewer (0.01%) foreground/background annotations, and a self-supervised method without annotations. Furthermore, we design a Robust Consistency-aware Chamfer Distance loss that incorporates multi-frame information and robust penalty functions to suppress outliers in self-supervised learning. Experiments show that our weakly and self-supervised models outperform existing self-supervised counterparts, and our weakly supervised models even rival some supervised ones. This demonstrates that our approaches effectively balance annotation effort and performance.
<div id='section'>Paperid: <span id='pid'>856, <a href='https://arxiv.org/pdf/2508.12644.pdf' target='_blank'>https://arxiv.org/pdf/2508.12644.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Wen, Hongbo Kang, Jian Ma, Jing Huang, Yuanwang Yang, Haozhe Lin, Yu-Kun Lai, Kun Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.12644">DyCrowd: Towards Dynamic Crowd Reconstruction from a Large-scene Video</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D reconstruction of dynamic crowds in large scenes has become increasingly important for applications such as city surveillance and crowd analysis. However, current works attempt to reconstruct 3D crowds from a static image, causing a lack of temporal consistency and inability to alleviate the typical impact caused by occlusions. In this paper, we propose DyCrowd, the first framework for spatio-temporally consistent 3D reconstruction of hundreds of individuals' poses, positions and shapes from a large-scene video. We design a coarse-to-fine group-guided motion optimization strategy for occlusion-robust crowd reconstruction in large scenes. To address temporal instability and severe occlusions, we further incorporate a VAE (Variational Autoencoder)-based human motion prior along with a segment-level group-guided optimization. The core of our strategy leverages collective crowd behavior to address long-term dynamic occlusions. By jointly optimizing the motion sequences of individuals with similar motion segments and combining this with the proposed Asynchronous Motion Consistency (AMC) loss, we enable high-quality unoccluded motion segments to guide the motion recovery of occluded ones, ensuring robust and plausible motion recovery even in the presence of temporal desynchronization and rhythmic inconsistencies. Additionally, in order to fill the gap of no existing well-annotated large-scene video dataset, we contribute a virtual benchmark dataset, VirtualCrowd, for evaluating dynamic crowd reconstruction from large-scene videos. Experimental results demonstrate that the proposed method achieves state-of-the-art performance in the large-scene dynamic crowd reconstruction task. The code and dataset will be available for research purposes.
<div id='section'>Paperid: <span id='pid'>857, <a href='https://arxiv.org/pdf/2508.03313.pdf' target='_blank'>https://arxiv.org/pdf/2508.03313.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Libo Zhang, Xinyu Yi, Feng Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.03313">BaroPoser: Real-time Human Motion Tracking from IMUs and Barometers in Everyday Devices</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, tracking human motion using IMUs from everyday devices such as smartphones and smartwatches has gained increasing popularity. However, due to the sparsity of sensor measurements and the lack of datasets capturing human motion over uneven terrain, existing methods often struggle with pose estimation accuracy and are typically limited to recovering movements on flat terrain only. To this end, we present BaroPoser, the first method that combines IMU and barometric data recorded by a smartphone and a smartwatch to estimate human pose and global translation in real time. By leveraging barometric readings, we estimate sensor height changes, which provide valuable cues for both improving the accuracy of human pose estimation and predicting global translation on non-flat terrain. Furthermore, we propose a local thigh coordinate frame to disentangle local and global motion input for better pose representation learning. We evaluate our method on both public benchmark datasets and real-world recordings. Quantitative and qualitative results demonstrate that our approach outperforms the state-of-the-art (SOTA) methods that use IMUs only with the same hardware configuration.
<div id='section'>Paperid: <span id='pid'>858, <a href='https://arxiv.org/pdf/2507.16157.pdf' target='_blank'>https://arxiv.org/pdf/2507.16157.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weijia Peng, Mingtong Chen, Zhengbao Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.16157">Design and Optimization of Wearables for Human Motion Energy Harvesting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As wearable electronics become increasingly prevalent, there is a rise in interest and demand for sustainably designed systems that are also energy self-sufficient. The research described in this paper investigated a shoe-worn energy harvesting system designed use the mechanical energy from walking to output electrical energy. A spring is attached to electromagnetic generator embedded in the heel of the shoe to recover the vertical pressure caused by the foot strike. The simulated prototype consisted of a standard EM generator designed in MATLAB demonstrating a maximum voltage of 12V. The initial low fidelity prototype demonstrated testing the relationship between the EM generator and a simple electrical circuit, with energy output observed. Future research will explore enhancing the overall generator design, integrate a power management IC for battery protect and regulation, and combine the system into a final product, wearable footwear. This research lays a foundation for self-powered footwear and energy independent wearable electronic devices.
<div id='section'>Paperid: <span id='pid'>859, <a href='https://arxiv.org/pdf/2507.11464.pdf' target='_blank'>https://arxiv.org/pdf/2507.11464.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ajay Shankar, Keisuke Okumura, Amanda Prorok
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.11464">LF: Online Multi-Robot Path Planning Meets Optimal Trajectory Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a multi-robot control paradigm to solve point-to-point navigation tasks for a team of holonomic robots with access to the full environment information. The framework invokes two processes asynchronously at high frequency: (i) a centralized, discrete, and full-horizon planner for computing collision- and deadlock-free paths rapidly, leveraging recent advances in multi-agent pathfinding (MAPF), and (ii) dynamics-aware, robot-wise optimal trajectory controllers that ensure all robots independently follow their assigned paths reliably. This hierarchical shift in planning representation from (i) discrete and coupled to (ii) continuous and decoupled domains enables the framework to maintain long-term scalable motion synthesis. As an instantiation of this idea, we present LF, which combines a fast state-of-the-art MAPF solver (LaCAM), and a robust feedback control stack (Freyja) for executing agile robot maneuvers. LF provides a robust and versatile mechanism for lifelong multi-robot navigation even under asynchronous and partial goal updates, and adapts to dynamic workspaces simply by quick replanning. We present various multirotor and ground robot demonstrations, including the deployment of 15 real multirotors with random, consecutive target updates while a person walks through the operational workspace.
<div id='section'>Paperid: <span id='pid'>860, <a href='https://arxiv.org/pdf/2506.23114.pdf' target='_blank'>https://arxiv.org/pdf/2506.23114.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhanxiang Cao, Buqing Nie, Yang Zhang, Yue Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.23114">Minimizing Acoustic Noise: Enhancing Quiet Locomotion for Quadruped Robots in Indoor Applications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in quadruped robot research have significantly improved their ability to traverse complex and unstructured outdoor environments. However, the issue of noise generated during locomotion is generally overlooked, which is critically important in noise-sensitive indoor environments, such as service and healthcare settings, where maintaining low noise levels is essential. This study aims to optimize the acoustic noise generated by quadruped robots during locomotion through the development of advanced motion control algorithms. To achieve this, we propose a novel approach that minimizes noise emissions by integrating optimized gait design with tailored control strategies. This method achieves an average noise reduction of approximately 8 dBA during movement, thereby enhancing the suitability of quadruped robots for deployment in noise-sensitive indoor environments. Experimental results demonstrate the effectiveness of this approach across various indoor settings, highlighting the potential of quadruped robots for quiet operation in noise-sensitive environments.
<div id='section'>Paperid: <span id='pid'>861, <a href='https://arxiv.org/pdf/2506.22907.pdf' target='_blank'>https://arxiv.org/pdf/2506.22907.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunzhe Shao, Xinyu Yi, Lu Yin, Shihui Guo, Junhai Yong, Feng Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.22907">MagShield: Towards Better Robustness in Sparse Inertial Motion Capture Under Magnetic Disturbances</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper proposes a novel method called MagShield, designed to address the issue of magnetic interference in sparse inertial motion capture (MoCap) systems. Existing Inertial Measurement Unit (IMU) systems are prone to orientation estimation errors in magnetically disturbed environments, limiting their practical application in real-world scenarios. To address this problem, MagShield employs a "detect-then-correct" strategy, first detecting magnetic disturbances through multi-IMU joint analysis, and then correcting orientation errors using human motion priors. MagShield can be integrated with most existing sparse inertial MoCap systems, improving their performance in magnetically disturbed environments. Experimental results demonstrate that MagShield significantly enhances the accuracy of motion capture under magnetic interference and exhibits good compatibility across different sparse inertial MoCap systems.
<div id='section'>Paperid: <span id='pid'>862, <a href='https://arxiv.org/pdf/2506.10016.pdf' target='_blank'>https://arxiv.org/pdf/2506.10016.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Longzhen Han, Awes Mubarak, Almas Baimagambetov, Nikolaos Polatidis, Thar Baker
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.10016">A Survey of Generative Categories and Techniques in Multimodal Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multimodal Large Language Models (MLLMs) have rapidly evolved beyond text generation, now spanning diverse output modalities including images, music, video, human motion, and 3D objects, by integrating language with other sensory modalities under unified architectures. This survey categorises six primary generative modalities and examines how foundational techniques, namely Self-Supervised Learning (SSL), Mixture of Experts (MoE), Reinforcement Learning from Human Feedback (RLHF), and Chain-of-Thought (CoT) prompting, enable cross-modal capabilities. We analyze key models, architectural trends, and emergent cross-modal synergies, while highlighting transferable techniques and unresolved challenges. Architectural innovations like transformers and diffusion models underpin this convergence, enabling cross-modal transfer and modular specialization. We highlight emerging patterns of synergy, and identify open challenges in evaluation, modularity, and structured reasoning. This survey offers a unified perspective on MLLM development and identifies critical paths toward more general-purpose, adaptive, and interpretable multimodal systems.
<div id='section'>Paperid: <span id='pid'>863, <a href='https://arxiv.org/pdf/2505.13140.pdf' target='_blank'>https://arxiv.org/pdf/2505.13140.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Takahiro Maeda, Jinkun Cao, Norimichi Ukita, Kris Kitani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.13140">CacheFlow: Fast Human Motion Prediction by Cached Normalizing Flow</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Many density estimation techniques for 3D human motion prediction require a significant amount of inference time, often exceeding the duration of the predicted time horizon. To address the need for faster density estimation for 3D human motion prediction, we introduce a novel flow-based method for human motion prediction called CacheFlow. Unlike previous conditional generative models that suffer from time efficiency, CacheFlow takes advantage of an unconditional flow-based generative model that transforms a Gaussian mixture into the density of future motions. The results of the computation of the flow-based generative model can be precomputed and cached. Then, for conditional prediction, we seek a mapping from historical trajectories to samples in the Gaussian mixture. This mapping can be done by a much more lightweight model, thus saving significant computation overhead compared to a typical conditional flow model. In such a two-stage fashion and by caching results from the slow flow model computation, we build our CacheFlow without loss of prediction accuracy and model expressiveness. This inference process is completed in approximately one millisecond, making it 4 times faster than previous VAE methods and 30 times faster than previous diffusion-based methods on standard benchmarks such as Human3.6M and AMASS datasets. Furthermore, our method demonstrates improved density estimation accuracy and comparable prediction accuracy to a SOTA method on Human3.6M. Our code and models will be publicly available.
<div id='section'>Paperid: <span id='pid'>864, <a href='https://arxiv.org/pdf/2412.06174.pdf' target='_blank'>https://arxiv.org/pdf/2412.06174.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuzhu Ji, Chuanxia Zheng, Tat-Jen Cham
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.06174">One-shot Human Motion Transfer via Occlusion-Robust Flow Prediction and Neural Texturing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion transfer aims at animating a static source image with a driving video. While recent advances in one-shot human motion transfer have led to significant improvement in results, it remains challenging for methods with 2D body landmarks, skeleton and semantic mask to accurately capture correspondences between source and driving poses due to the large variation in motion and articulation complexity. In addition, the accuracy and precision of DensePose degrade the image quality for neural-rendering-based methods. To address the limitations and by both considering the importance of appearance and geometry for motion transfer, in this work, we proposed a unified framework that combines multi-scale feature warping and neural texture mapping to recover better 2D appearance and 2.5D geometry, partly by exploiting the information from DensePose, yet adapting to its inherent limited accuracy. Our model takes advantage of multiple modalities by jointly training and fusing them, which allows it to robust neural texture features that cope with geometric errors as well as multi-scale dense motion flow that better preserves appearance. Experimental results with full and half-view body video datasets demonstrate that our model can generalize well and achieve competitive results, and that it is particularly effective in handling challenging cases such as those with substantial self-occlusions.
<div id='section'>Paperid: <span id='pid'>865, <a href='https://arxiv.org/pdf/2412.05515.pdf' target='_blank'>https://arxiv.org/pdf/2412.05515.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Runhao Zeng, Dingjie Zhou, Qiwei Liang, Junlin Liu, Hui Li, Changxin Huang, Jianqiang Li, Xiping Hu, Fuchun Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.05515">Video2Reward: Generating Reward Function from Videos for Legged Robot Behavior Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning behavior in legged robots presents a significant challenge due to its inherent instability and complex constraints. Recent research has proposed the use of a large language model (LLM) to generate reward functions in reinforcement learning, thereby replacing the need for manually designed rewards by experts. However, this approach, which relies on textual descriptions to define learning objectives, fails to achieve controllable and precise behavior learning with clear directionality. In this paper, we introduce a new video2reward method, which directly generates reward functions from videos depicting the behaviors to be mimicked and learned. Specifically, we first process videos containing the target behaviors, converting the motion information of individuals in the videos into keypoint trajectories represented as coordinates through a video2text transforming module. These trajectories are then fed into an LLM to generate the reward function, which in turn is used to train the policy. To enhance the quality of the reward function, we develop a video-assisted iterative reward refinement scheme that visually assesses the learned behaviors and provides textual feedback to the LLM. This feedback guides the LLM to continually refine the reward function, ultimately facilitating more efficient behavior learning. Experimental results on tasks involving bipedal and quadrupedal robot motion control demonstrate that our method surpasses the performance of state-of-the-art LLM-based reward generation methods by over 37.6% in terms of human normalized score. More importantly, by switching video inputs, we find our method can rapidly learn diverse motion behaviors such as walking and running.
<div id='section'>Paperid: <span id='pid'>866, <a href='https://arxiv.org/pdf/2412.02419.pdf' target='_blank'>https://arxiv.org/pdf/2412.02419.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingyi Shi, Dafei Qin, Leo Ho, Zhouyingcheng Liao, Yinghao Huang, Junichi Yamagishi, Taku Komura
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.02419">It Takes Two: Real-time Co-Speech Two-person's Interaction Generation via Reactive Auto-regressive Diffusion Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Conversational scenarios are very common in real-world settings, yet existing co-speech motion synthesis approaches often fall short in these contexts, where one person's audio and gestures will influence the other's responses. Additionally, most existing methods rely on offline sequence-to-sequence frameworks, which are unsuitable for online applications. In this work, we introduce an audio-driven, auto-regressive system designed to synthesize dynamic movements for two characters during a conversation. At the core of our approach is a diffusion-based full-body motion synthesis model, which is conditioned on the past states of both characters, speech audio, and a task-oriented motion trajectory input, allowing for flexible spatial control. To enhance the model's ability to learn diverse interactions, we have enriched existing two-person conversational motion datasets with more dynamic and interactive motions. We evaluate our system through multiple experiments to show it outperforms across a variety of tasks, including single and two-person co-speech motion generation, as well as interactive motion generation. To the best of our knowledge, this is the first system capable of generating interactive full-body motions for two characters from speech in an online manner.
<div id='section'>Paperid: <span id='pid'>867, <a href='https://arxiv.org/pdf/2410.20358.pdf' target='_blank'>https://arxiv.org/pdf/2410.20358.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingjiang Liang, Yongkang Cheng, Hualin Liang, Shaoli Huang, Wei Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.20358">RopeTP: Global Human Motion Recovery via Integrating Robust Pose Estimation with Diffusion Trajectory Prior</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present RopeTP, a novel framework that combines Robust pose estimation with a diffusion Trajectory Prior to reconstruct global human motion from videos. At the heart of RopeTP is a hierarchical attention mechanism that significantly improves context awareness, which is essential for accurately inferring the posture of occluded body parts. This is achieved by exploiting the relationships with visible anatomical structures, enhancing the accuracy of local pose estimations. The improved robustness of these local estimations allows for the reconstruction of precise and stable global trajectories. Additionally, RopeTP incorporates a diffusion trajectory model that predicts realistic human motion from local pose sequences. This model ensures that the generated trajectories are not only consistent with observed local actions but also unfold naturally over time, thereby improving the realism and stability of 3D human motion reconstruction. Extensive experimental validation shows that RopeTP surpasses current methods on two benchmark datasets, particularly excelling in scenarios with occlusions. It also outperforms methods that rely on SLAM for initial camera estimates and extensive optimization, delivering more accurate and realistic trajectories.
<div id='section'>Paperid: <span id='pid'>868, <a href='https://arxiv.org/pdf/2410.07296.pdf' target='_blank'>https://arxiv.org/pdf/2410.07296.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gaoge Han, Mingjiang Liang, Jinglei Tang, Yongkang Cheng, Wei Liu, Shaoli Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.07296">ReinDiffuse: Crafting Physically Plausible Motions with Reinforced Diffusion Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating human motion from textual descriptions is a challenging task. Existing methods either struggle with physical credibility or are limited by the complexities of physics simulations. In this paper, we present \emph{ReinDiffuse} that combines reinforcement learning with motion diffusion model to generate physically credible human motions that align with textual descriptions. Our method adapts Motion Diffusion Model to output a parameterized distribution of actions, making them compatible with reinforcement learning paradigms. We employ reinforcement learning with the objective of maximizing physically plausible rewards to optimize motion generation for physical fidelity. Our approach outperforms existing state-of-the-art models on two major datasets, HumanML3D and KIT-ML, achieving significant improvements in physical plausibility and motion quality. Project: https://reindiffuse.github.io/
<div id='section'>Paperid: <span id='pid'>869, <a href='https://arxiv.org/pdf/2408.11814.pdf' target='_blank'>https://arxiv.org/pdf/2408.11814.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinsub Yim, Hyungtae Lee, Sungmin Eum, Yi-Ting Shen, Yan Zhang, Heesung Kwon, Shuvra S. Bhattacharyya
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.11814">SynPlay: Large-Scale Synthetic Human Data with Real-World Diversity for Aerial-View Perception</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce SynPlay, a large-scale synthetic human dataset purpose-built for advancing multi-perspective human localization, with a predominant focus on aerial-view perception. SynPlay departs from traditional synthetic datasets by addressing a critical but underexplored challenge: localizing humans in aerial scenes where subjects often occupy only tens of pixels in the image. In such scenarios, fine-grained details like facial features or textures become irrelevant, shifting the burden of recognition to human motion, behavior, and interactions. To meet this need, SynPlay implements a novel rule-guided motion generation framework that combines real-world motion capture with motion evolution graphs. This design enables human actions to evolve dynamically through high-level game rules rather than predefined scripts, resulting in effectively uncountable motion variations. Unlike existing synthetic datasets-which either focus on static visual traits or reuse a limited set of mocap-driven actions-SynPlay captures a wide spectrum of spontaneous behaviors, including complex interactions that naturally emerge from unscripted gameplay scenarios. SynPlay also introduces an extensive multi-camera setup that spans UAVs at random altitudes, CCTVs, and a freely roaming UGV, achieving true near-to-far perspective coverage in a single dataset. The majority of instances are captured from aerial viewpoints at varying scales, directly supporting the development of models for long-range human analysis-a setting where existing datasets fall short. Our data contains over 73k images and 6.5M human instances, with detailed annotations for detection, segmentation, and keypoint tasks. Extensive experiments demonstrate that training with SynPlay significantly improves human localization performance, especially in few-shot and data-scarce scenarios.
<div id='section'>Paperid: <span id='pid'>870, <a href='https://arxiv.org/pdf/2407.14220.pdf' target='_blank'>https://arxiv.org/pdf/2407.14220.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunfan Gao, Florian Messerer, Niels van Duijkeren, Moritz Diehl
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.14220">Stochastic Model Predictive Control with Optimal Linear Feedback for Mobile Robots in Dynamic Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robot navigation around humans can be a challenging problem since human movements are hard to predict. Stochastic model predictive control (MPC) can account for such uncertainties and approximately bound the probability of a collision to take place. In this paper, to counteract the rapidly growing human motion uncertainty over time, we incorporate state feedback in the stochastic MPC. This allows the robot to more closely track reference trajectories. To this end the feedback policy is left as a degree of freedom in the optimal control problem. The stochastic MPC with feedback is validated in simulation experiments and is compared against nominal MPC and stochastic MPC without feedback. The added computation time can be limited by reducing the number of additional variables for the feedback law with a small compromise in control performance.
<div id='section'>Paperid: <span id='pid'>871, <a href='https://arxiv.org/pdf/2405.15385.pdf' target='_blank'>https://arxiv.org/pdf/2405.15385.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xia Li, Runzhao Yang, Xiangtai Li, Antony Lomax, Ye Zhang, Joachim Buhmann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.15385">CPT-Interp: Continuous sPatial and Temporal Motion Modeling for 4D Medical Image Interpolation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion information from 4D medical imaging offers critical insights into dynamic changes in patient anatomy for clinical assessments and radiotherapy planning and, thereby, enhances the capabilities of 3D image analysis. However, inherent physical and technical constraints of imaging hardware often necessitate a compromise between temporal resolution and image quality. Frame interpolation emerges as a pivotal solution to this challenge. Previous methods often suffer from discretion when they estimate the intermediate motion and execute the forward warping. In this study, we draw inspiration from fluid mechanics to propose a novel approach for continuously modeling patient anatomic motion using implicit neural representation. It ensures both spatial and temporal continuity, effectively bridging Eulerian and Lagrangian specifications together to naturally facilitate continuous frame interpolation. Our experiments across multiple datasets underscore the method's superior accuracy and speed. Furthermore, as a case-specific optimization (training-free) approach, it circumvents the need for extensive datasets and addresses model generalization issues.
<div id='section'>Paperid: <span id='pid'>872, <a href='https://arxiv.org/pdf/2405.00430.pdf' target='_blank'>https://arxiv.org/pdf/2405.00430.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xia Li, Runzhao Yang, Muheng Li, Xiangtai Li, Antony J. Lomax, Joachim M. Buhmann, Ye Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.00430">Continuous sPatial-Temporal Deformable Image Registration (CPT-DIR) for motion modelling in radiotherapy: beyond classic voxel-based methods</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deformable image registration (DIR) is a crucial tool in radiotherapy for analyzing anatomical changes and motion patterns. Current DIR implementations rely on discrete volumetric motion representation, which often leads to compromised accuracy and uncertainty when handling significant anatomical changes and sliding boundaries. This limitation affects the reliability of subsequent contour propagation and dose accumulation procedures, particularly in regions with complex anatomical interfaces such as the lung-chest wall boundary. Given that organ motion is inherently a continuous process in both space and time, we aimed to develop a model that preserves these fundamental properties. Drawing inspiration from fluid mechanics, we propose a novel approach using implicit neural representation (INR) for continuous modeling of patient anatomical motion. This approach ensures spatial and temporal continuity while effectively unifying Eulerian and Lagrangian specifications to enable natural continuous motion modeling and frame interpolation. The integration of these specifications provides a more comprehensive understanding of anatomical deformation patterns. By leveraging the continuous representations, the CPT-DIR method significantly enhances registration and interpolation accuracy, automation, and speed. The method demonstrates superior performance in landmark and contour precision, particularly in challenging anatomical regions, representing a substantial advancement over conventional approaches in deformable image registration. The improved efficiency and accuracy of CPT-DIR make it particularly suitable for real-time adaptive radiotherapy applications.
<div id='section'>Paperid: <span id='pid'>873, <a href='https://arxiv.org/pdf/2403.13238.pdf' target='_blank'>https://arxiv.org/pdf/2403.13238.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qitong Yang, Mingtao Feng, Zijie Wu, Shijie Sun, Weisheng Dong, Yaonan Wang, Ajmal Mian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.13238">Learning Coherent Matrixized Representation in Latent Space for Volumetric 4D Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Directly learning to model 4D content, including shape, color, and motion, is challenging. Existing methods rely on pose priors for motion control, resulting in limited motion diversity and continuity in details. To address this, we propose a framework that generates volumetric 4D sequences, where 3D shapes are animated under given conditions (text-image guidance) with dynamic evolution in shape and color across spatial and temporal dimensions, allowing for free navigation and rendering from any direction. We first use a coherent 3D shape and color modeling to encode the shape and color of each detailed 3D geometry frame into a latent space. Then we propose a matrixized 4D sequence representation allowing efficient diffusion model operation. Finally, we introduce spatio-temporal diffusion for 4D volumetric generation under given images and text prompts. Extensive experiments on the ShapeNet, 3DBiCar, DeformingThings4D and Objaverse datasets for several tasks demonstrate that our method effectively learns to generate high quality 3D shapes with consistent color and coherent mesh animations, improving over the current methods. Our code will be publicly available.
<div id='section'>Paperid: <span id='pid'>874, <a href='https://arxiv.org/pdf/2402.12099.pdf' target='_blank'>https://arxiv.org/pdf/2402.12099.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haiming Zhu, Yangyang Xu, Shengfeng He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.12099">Human Video Translation via Query Warping</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we present QueryWarp, a novel framework for temporally coherent human motion video translation. Existing diffusion-based video editing approaches that rely solely on key and value tokens to ensure temporal consistency, which scarifies the preservation of local and structural regions. In contrast, we aim to consider complementary query priors by constructing the temporal correlations among query tokens from different frames. Initially, we extract appearance flows from source poses to capture continuous human foreground motion. Subsequently, during the denoising process of the diffusion model, we employ appearance flows to warp the previous frame's query token, aligning it with the current frame's query. This query warping imposes explicit constraints on the outputs of self-attention layers, effectively guaranteeing temporally coherent translation. We perform experiments on various human motion video translation tasks, and the results demonstrate that our QueryWarp framework surpasses state-of-the-art methods both qualitatively and quantitatively.
<div id='section'>Paperid: <span id='pid'>875, <a href='https://arxiv.org/pdf/2402.02624.pdf' target='_blank'>https://arxiv.org/pdf/2402.02624.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Baha Zarrouki, Marios Spanakakis, Johannes Betz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.02624">A Safe Reinforcement Learning driven Weights-varying Model Predictive Control for Autonomous Vehicle Motion Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Determining the optimal cost function parameters of Model Predictive Control (MPC) to optimize multiple control objectives is a challenging and time-consuming task. Multiobjective Bayesian Optimization (BO) techniques solve this problem by determining a Pareto optimal parameter set for an MPC with static weights. However, a single parameter set may not deliver the most optimal closed-loop control performance when the context of the MPC operating conditions changes during its operation, urging the need to adapt the cost function weights at runtime. Deep Reinforcement Learning (RL) algorithms can automatically learn context-dependent optimal parameter sets and dynamically adapt for a Weightsvarying MPC (WMPC). However, learning cost function weights from scratch in a continuous action space may lead to unsafe operating states. To solve this, we propose a novel approach limiting the RL actions within a safe learning space representing a catalog of pre-optimized BO Pareto-optimal weight sets. We conceive a RL agent not to learn in a continuous space but to proactively anticipate upcoming control tasks and to choose the most optimal discrete actions, each corresponding to a single set of Pareto optimal weights, context-dependent. Hence, even an untrained RL agent guarantees a safe and optimal performance. Experimental results demonstrate that an untrained RL-WMPC shows Pareto-optimal closed-loop behavior and training the RL-WMPC helps exhibit a performance beyond the Pareto-front.
<div id='section'>Paperid: <span id='pid'>876, <a href='https://arxiv.org/pdf/2401.16189.pdf' target='_blank'>https://arxiv.org/pdf/2401.16189.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sungmin Woo, Minjung Kim, Donghyeong Kim, Sungjun Jang, Sangyoun Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.16189">FIMP: Future Interaction Modeling for Multi-Agent Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent motion prediction is a crucial concern in autonomous driving, yet it remains a challenge owing to the ambiguous intentions of dynamic agents and their intricate interactions. Existing studies have attempted to capture interactions between road entities by using the definite data in history timesteps, as future information is not available and involves high uncertainty. However, without sufficient guidance for capturing future states of interacting agents, they frequently produce unrealistic trajectory overlaps. In this work, we propose Future Interaction modeling for Motion Prediction (FIMP), which captures potential future interactions in an end-to-end manner. FIMP adopts a future decoder that implicitly extracts the potential future information in an intermediate feature-level, and identifies the interacting entity pairs through future affinity learning and top-k filtering strategy. Experiments show that our future interaction modeling improves the performance remarkably, leading to superior performance on the Argoverse motion forecasting benchmark.
<div id='section'>Paperid: <span id='pid'>877, <a href='https://arxiv.org/pdf/2401.03599.pdf' target='_blank'>https://arxiv.org/pdf/2401.03599.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Victoria M. Dax, Jiachen Li, Enna Sachdeva, Nakul Agarwal, Mykel J. Kochenderfer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.03599">Disentangled Neural Relational Inference for Interpretable Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Effective interaction modeling and behavior prediction of dynamic agents play a significant role in interactive motion planning for autonomous robots. Although existing methods have improved prediction accuracy, few research efforts have been devoted to enhancing prediction model interpretability and out-of-distribution (OOD) generalizability. This work addresses these two challenging aspects by designing a variational auto-encoder framework that integrates graph-based representations and time-sequence models to efficiently capture spatio-temporal relations between interactive agents and predict their dynamics. Our model infers dynamic interaction graphs in a latent space augmented with interpretable edge features that characterize the interactions. Moreover, we aim to enhance model interpretability and performance in OOD scenarios by disentangling the latent space of edge features, thereby strengthening model versatility and robustness. We validate our approach through extensive experiments on both simulated and real-world datasets. The results show superior performance compared to existing methods in modeling spatio-temporal relations, motion prediction, and identifying time-invariant latent features.
<div id='section'>Paperid: <span id='pid'>878, <a href='https://arxiv.org/pdf/2512.18814.pdf' target='_blank'>https://arxiv.org/pdf/2512.18814.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxiao Yang, Hualian Sheng, Sijia Cai, Jing Lin, Jiahao Wang, Bing Deng, Junzhe Lu, Haoqian Wang, Jieping Ye
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.18814">EchoMotion: Unified Human Video and Motion Generation via Dual-Modality Diffusion Transformer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video generation models have advanced significantly, yet they still struggle to synthesize complex human movements due to the high degrees of freedom in human articulation. This limitation stems from the intrinsic constraints of pixel-only training objectives, which inherently bias models toward appearance fidelity at the expense of learning underlying kinematic principles. To address this, we introduce EchoMotion, a framework designed to model the joint distribution of appearance and human motion, thereby improving the quality of complex human action video generation. EchoMotion extends the DiT (Diffusion Transformer) framework with a dual-branch architecture that jointly processes tokens concatenated from different modalities. Furthermore, we propose MVS-RoPE (Motion-Video Syncronized RoPE), which offers unified 3D positional encoding for both video and motion tokens. By providing a synchronized coordinate system for the dual-modal latent sequence, MVS-RoPE establishes an inductive bias that fosters temporal alignment between the two modalities. We also propose a Motion-Video Two-Stage Training Strategy. This strategy enables the model to perform both the joint generation of complex human action videos and their corresponding motion sequences, as well as versatile cross-modal conditional generation tasks. To facilitate the training of a model with these capabilities, we construct HuMoVe, a large-scale dataset of approximately 80,000 high-quality, human-centric video-motion pairs. Our findings reveal that explicitly representing human motion is complementary to appearance, significantly boosting the coherence and plausibility of human-centric video generation.
<div id='section'>Paperid: <span id='pid'>879, <a href='https://arxiv.org/pdf/2512.16199.pdf' target='_blank'>https://arxiv.org/pdf/2512.16199.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jerrin Bright, Zhibo Wang, Dmytro Klepachevskyi, Yuhao Chen, Sirisha Rambhatla, David Clausi, John Zelek
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.16199">Avatar4D: Synthesizing Domain-Specific 4D Humans for Real-World Pose Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present Avatar4D, a real-world transferable pipeline for generating customizable synthetic human motion datasets tailored to domain-specific applications. Unlike prior works, which focus on general, everyday motions and offer limited flexibility, our approach provides fine-grained control over body pose, appearance, camera viewpoint, and environmental context, without requiring any manual annotations. To validate the impact of Avatar4D, we focus on sports, where domain-specific human actions and movement patterns pose unique challenges for motion understanding. In this setting, we introduce Syn2Sport, a large-scale synthetic dataset spanning sports, including baseball and ice hockey. Avatar4D features high-fidelity 4D (3D geometry over time) human motion sequences with varying player appearances rendered in diverse environments. We benchmark several state-of-the-art pose estimation models on Syn2Sport and demonstrate their effectiveness for supervised learning, zero-shot transfer to real-world data, and generalization across sports. Furthermore, we evaluate how closely the generated synthetic data aligns with real-world datasets in feature space. Our results highlight the potential of such systems to generate scalable, controllable, and transferable human datasets for diverse domain-specific tasks without relying on domain-specific real data.
<div id='section'>Paperid: <span id='pid'>880, <a href='https://arxiv.org/pdf/2512.09423.pdf' target='_blank'>https://arxiv.org/pdf/2512.09423.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Marco Pegoraro, Evan Atherton, Bruno Roy, Aliasghar Khani, Arianna Rampini
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.09423">FunPhase: A Periodic Functional Autoencoder for Motion Generation via Phase Manifolds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning natural body motion remains challenging due to the strong coupling between spatial geometry and temporal dynamics. Embedding motion in phase manifolds, latent spaces that capture local periodicity, has proven effective for motion prediction; however, existing approaches lack scalability and remain confined to specific settings. We introduce FunPhase, a functional periodic autoencoder that learns a phase manifold for motion and replaces discrete temporal decoding with a function-space formulation, enabling smooth trajectories that can be sampled at arbitrary temporal resolutions. FunPhase supports downstream tasks such as super-resolution and partial-body motion completion, generalizes across skeletons and datasets, and unifies motion prediction and generation within a single interpretable manifold. Our model achieves substantially lower reconstruction error than prior periodic autoencoder baselines while enabling a broader range of applications and performing on par with state-of-the-art motion generation methods.
<div id='section'>Paperid: <span id='pid'>881, <a href='https://arxiv.org/pdf/2512.06306.pdf' target='_blank'>https://arxiv.org/pdf/2512.06306.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoxian Zhou, Chuanzhi Xu, Langyi Chen, Haodong Chen, Yuk Ying Chung, Qiang Qu, Xaoming Chen, Weidong Cai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.06306">Exploiting Spatiotemporal Properties for Efficient Event-Driven Human Pose Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human pose estimation focuses on predicting body keypoints to analyze human motion. Event cameras provide high temporal resolution and low latency, enabling robust estimation under challenging conditions. However, most existing methods convert event streams into dense event frames, which adds extra computation and sacrifices the high temporal resolution of the event signal. In this work, we aim to exploit the spatiotemporal properties of event streams based on point cloud-based framework, designed to enhance human pose estimation performance. We design Event Temporal Slicing Convolution module to capture short-term dependencies across event slices, and combine it with Event Slice Sequencing module for structured temporal modeling. We also apply edge enhancement in point cloud-based event representation to enhance spatial edge information under sparse event conditions to further improve performance. Experiments on the DHP19 dataset show our proposed method consistently improves performance across three representative point cloud backbones: PointNet, DGCNN, and Point Transformer.
<div id='section'>Paperid: <span id='pid'>882, <a href='https://arxiv.org/pdf/2512.03936.pdf' target='_blank'>https://arxiv.org/pdf/2512.03936.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aron Distelzweig, Yiwei Wang, Faris Janjoš, Marcel Hallgarten, Mihai Dobre, Alexander Langmann, Joschka Boedecker, Johannes Betz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.03936">Driving is a Game: Combining Planning and Prediction with Bayesian Iterative Best Response</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous driving planning systems perform nearly perfectly in routine scenarios using lightweight, rule-based methods but still struggle in dense urban traffic, where lane changes and merges require anticipating and influencing other agents. Modern motion predictors offer highly accurate forecasts, yet their integration into planning is mostly rudimental: discarding unsafe plans. Similarly, end-to-end models offer a one-way integration that avoids the challenges of joint prediction and planning modeling under uncertainty. In contrast, game-theoretic formulations offer a principled alternative but have seen limited adoption in autonomous driving. We present Bayesian Iterative Best Response (BIBeR), a framework that unifies motion prediction and game-theoretic planning into a single interaction-aware process. BIBeR is the first to integrate a state-of-the-art predictor into an Iterative Best Response (IBR) loop, repeatedly refining the strategies of the ego vehicle and surrounding agents. This repeated best-response process approximates a Nash equilibrium, enabling bidirectional adaptation where the ego both reacts to and shapes the behavior of others. In addition, our proposed Bayesian confidence estimation quantifies prediction reliability and modulates update strength, more conservative under low confidence and more decisive under high confidence. BIBeR is compatible with modern predictors and planners, combining the transparency of structured planning with the flexibility of learned models. Experiments show that BIBeR achieves an 11% improvement over state-of-the-art planners on highly interactive interPlan lane-change scenarios, while also outperforming existing approaches on standard nuPlan benchmarks.
<div id='section'>Paperid: <span id='pid'>883, <a href='https://arxiv.org/pdf/2511.16264.pdf' target='_blank'>https://arxiv.org/pdf/2511.16264.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sinan Mutlu, Georgios F. Angelis, Savas Ozkan, Paul Wisbey, Anastasios Drosou, Mete Ozay
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.16264">Mem-MLP: Real-Time 3D Human Motion Generation from Sparse Inputs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Realistic and smooth full-body tracking is crucial for immersive AR/VR applications. Existing systems primarily track head and hands via Head Mounted Devices (HMDs) and controllers, making the 3D full-body reconstruction in-complete. One potential approach is to generate the full-body motions from sparse inputs collected from limited sensors using a Neural Network (NN) model. In this paper, we propose a novel method based on a multi-layer perceptron (MLP) backbone that is enhanced with residual connections and a novel NN-component called Memory-Block. In particular, Memory-Block represents missing sensor data with trainable code-vectors, which are combined with the sparse signals from previous time instances to improve the temporal consistency. Furthermore, we formulate our solution as a multi-task learning problem, allowing our MLP-backbone to learn robust representations that boost accuracy. Our experiments show that our method outperforms state-of-the-art baselines by substantially reducing prediction errors. Moreover, it achieves 72 FPS on mobile HMDs that ultimately improves the accuracy-running time tradeoff.
<div id='section'>Paperid: <span id='pid'>884, <a href='https://arxiv.org/pdf/2510.24767.pdf' target='_blank'>https://arxiv.org/pdf/2510.24767.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guorui Song, Guocun Wang, Zhe Huang, Jing Lin, Xuefei Zhe, Jian Li, Haoqian Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.24767">Towards Fine-Grained Human Motion Video Captioning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating accurate descriptions of human actions in videos remains a challenging task for video captioning models. Existing approaches often struggle to capture fine-grained motion details, resulting in vague or semantically inconsistent captions. In this work, we introduce the Motion-Augmented Caption Model (M-ACM), a novel generative framework that enhances caption quality by incorporating motion-aware decoding. At its core, M-ACM leverages motion representations derived from human mesh recovery to explicitly highlight human body dynamics, thereby reducing hallucinations and improving both semantic fidelity and spatial alignment in the generated captions. To support research in this area, we present the Human Motion Insight (HMI) Dataset, comprising 115K video-description pairs focused on human movement, along with HMI-Bench, a dedicated benchmark for evaluating motion-focused video captioning. Experimental results demonstrate that M-ACM significantly outperforms previous methods in accurately describing complex human motions and subtle temporal variations, setting a new standard for motion-centric video captioning.
<div id='section'>Paperid: <span id='pid'>885, <a href='https://arxiv.org/pdf/2510.11107.pdf' target='_blank'>https://arxiv.org/pdf/2510.11107.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiahui Lei, Kyle Genova, George Kopanas, Noah Snavely, Leonidas Guibas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.11107">MoMaps: Semantics-Aware Scene Motion Generation with Motion Maps</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper addresses the challenge of learning semantically and functionally meaningful 3D motion priors from real-world videos, in order to enable prediction of future 3D scene motion from a single input image. We propose a novel pixel-aligned Motion Map (MoMap) representation for 3D scene motion, which can be generated from existing generative image models to facilitate efficient and effective motion prediction. To learn meaningful distributions over motion, we create a large-scale database of MoMaps from over 50,000 real videos and train a diffusion model on these representations. Our motion generation not only synthesizes trajectories in 3D but also suggests a new pipeline for 2D video synthesis: first generate a MoMap, then warp an image accordingly and complete the warped point-based renderings. Experimental results demonstrate that our approach generates plausible and semantically consistent 3D scene motion.
<div id='section'>Paperid: <span id='pid'>886, <a href='https://arxiv.org/pdf/2510.11017.pdf' target='_blank'>https://arxiv.org/pdf/2510.11017.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Runyang Feng, Hyung Jin Chang, Tze Ho Elden Tse, Boeun Kim, Yi Chang, Yixing Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.11017">High-Resolution Spatiotemporal Modeling with Global-Local State Space Models for Video-Based Human Pose Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modeling high-resolution spatiotemporal representations, including both global dynamic contexts (e.g., holistic human motion tendencies) and local motion details (e.g., high-frequency changes of keypoints), is essential for video-based human pose estimation (VHPE). Current state-of-the-art methods typically unify spatiotemporal learning within a single type of modeling structure (convolution or attention-based blocks), which inherently have difficulties in balancing global and local dynamic modeling and may bias the network to one of them, leading to suboptimal performance. Moreover, existing VHPE models suffer from quadratic complexity when capturing global dependencies, limiting their applicability especially for high-resolution sequences. Recently, the state space models (known as Mamba) have demonstrated significant potential in modeling long-range contexts with linear complexity; however, they are restricted to 1D sequential data. In this paper, we present a novel framework that extends Mamba from two aspects to separately learn global and local high-resolution spatiotemporal representations for VHPE. Specifically, we first propose a Global Spatiotemporal Mamba, which performs 6D selective space-time scan and spatial- and temporal-modulated scan merging to efficiently extract global representations from high-resolution sequences. We further introduce a windowed space-time scan-based Local Refinement Mamba to enhance the high-frequency details of localized keypoint motions. Extensive experiments on four benchmark datasets demonstrate that the proposed model outperforms state-of-the-art VHPE approaches while achieving better computational trade-offs.
<div id='section'>Paperid: <span id='pid'>887, <a href='https://arxiv.org/pdf/2510.02732.pdf' target='_blank'>https://arxiv.org/pdf/2510.02732.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianing Chen, Zehao Li, Yujun Cai, Hao Jiang, Shuqin Gao, Honglong Zhao, Tianlu Mao, Yucheng Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.02732">From Tokens to Nodes: Semantic-Guided Motion Control for Dynamic 3D Gaussian Splatting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dynamic 3D reconstruction from monocular videos remains difficult due to the ambiguity inferring 3D motion from limited views and computational demands of modeling temporally varying scenes. While recent sparse control methods alleviate computation by reducing millions of Gaussians to thousands of control points, they suffer from a critical limitation: they allocate points purely by geometry, leading to static redundancy and dynamic insufficiency. We propose a motion-adaptive framework that aligns control density with motion complexity. Leveraging semantic and motion priors from vision foundation models, we establish patch-token-node correspondences and apply motion-adaptive compression to concentrate control points in dynamic regions while suppressing redundancy in static backgrounds. Our approach achieves flexible representational density adaptation through iterative voxelization and motion tendency scoring, directly addressing the fundamental mismatch between control point allocation and motion complexity. To capture temporal evolution, we introduce spline-based trajectory parameterization initialized by 2D tracklets, replacing MLP-based deformation fields to achieve smoother motion representation and more stable optimization. Extensive experiments demonstrate significant improvements in reconstruction quality and efficiency over existing state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>888, <a href='https://arxiv.org/pdf/2509.21006.pdf' target='_blank'>https://arxiv.org/pdf/2509.21006.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Konstantin Gubernatorov, Artem Voronov, Roman Voronov, Sergei Pasynkov, Stepan Perminov, Ziang Guo, Dzmitry Tsetserukou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21006">AnywhereVLA: Language-Conditioned Exploration and Mobile Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We address natural language pick-and-place in unseen, unpredictable indoor environments with AnywhereVLA, a modular framework for mobile manipulation. A user text prompt serves as an entry point and is parsed into a structured task graph that conditions classical SLAM with LiDAR and cameras, metric semantic mapping, and a task-aware frontier exploration policy. An approach planner then selects visibility and reachability aware pre grasp base poses. For interaction, a compact SmolVLA manipulation head is fine tuned on platform pick and place trajectories for the SO-101 by TheRobotStudio, grounding local visual context and sub-goals into grasp and place proposals. The full system runs fully onboard on consumer-level hardware, with Jetson Orin NX for perception and VLA and an Intel NUC for SLAM, exploration, and control, sustaining real-time operation. We evaluated AnywhereVLA in a multi-room lab under static scenes and normal human motion. In this setting, the system achieves a $46\%$ overall task success rate while maintaining throughput on embedded compute. By combining a classical stack with a fine-tuned VLA manipulation, the system inherits the reliability of geometry-based navigation with the agility and task generalization of language-conditioned manipulation.
<div id='section'>Paperid: <span id='pid'>889, <a href='https://arxiv.org/pdf/2509.12430.pdf' target='_blank'>https://arxiv.org/pdf/2509.12430.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mayank Patel, Rahul Jain, Asim Unmesh, Karthik Ramani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.12430">DYNAMO: Dependency-Aware Deep Learning Framework for Articulated Assembly Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding the motion of articulated mechanical assemblies from static geometry remains a core challenge in 3D perception and design automation. Prior work on everyday articulated objects such as doors and laptops typically assumes simplified kinematic structures or relies on joint annotations. However, in mechanical assemblies like gears, motion arises from geometric coupling, through meshing teeth or aligned axes, making it difficult for existing methods to reason about relational motion from geometry alone. To address this gap, we introduce MechBench, a benchmark dataset of 693 diverse synthetic gear assemblies with part-wise ground-truth motion trajectories. MechBench provides a structured setting to study coupled motion, where part dynamics are induced by contact and transmission rather than predefined joints. Building on this, we propose DYNAMO, a dependency-aware neural model that predicts per-part SE(3) motion trajectories directly from segmented CAD point clouds. Experiments show that DYNAMO outperforms strong baselines, achieving accurate and temporally consistent predictions across varied gear configurations. Together, MechBench and DYNAMO establish a novel systematic framework for data-driven learning of coupled mechanical motion in CAD assemblies.
<div id='section'>Paperid: <span id='pid'>890, <a href='https://arxiv.org/pdf/2509.10096.pdf' target='_blank'>https://arxiv.org/pdf/2509.10096.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Saeed Saadatnejad, Reyhaneh Hosseininejad, Jose Barreiros, Katherine M. Tsui, Alexandre Alahi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.10096">HHI-Assist: A Dataset and Benchmark of Human-Human Interaction in Physical Assistance Scenario</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The increasing labor shortage and aging population underline the need for assistive robots to support human care recipients. To enable safe and responsive assistance, robots require accurate human motion prediction in physical interaction scenarios. However, this remains a challenging task due to the variability of assistive settings and the complexity of coupled dynamics in physical interactions. In this work, we address these challenges through two key contributions: (1) HHI-Assist, a dataset comprising motion capture clips of human-human interactions in assistive tasks; and (2) a conditional Transformer-based denoising diffusion model for predicting the poses of interacting agents. Our model effectively captures the coupled dynamics between caregivers and care receivers, demonstrating improvements over baselines and strong generalization to unseen scenarios. By advancing interaction-aware motion prediction and introducing a new dataset, our work has the potential to significantly enhance robotic assistance policies. The dataset and code are available at: https://sites.google.com/view/hhi-assist/home
<div id='section'>Paperid: <span id='pid'>891, <a href='https://arxiv.org/pdf/2508.20812.pdf' target='_blank'>https://arxiv.org/pdf/2508.20812.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lorenzo Busellato, Federico Cunico, Diego Dall'Alba, Marco Emporio, Andrea Giachetti, Riccardo Muradore, Marco Cristani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.20812">Uncertainty Aware-Predictive Control Barrier Functions: Safer Human Robot Interaction through Probabilistic Motion Forecasting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To enable flexible, high-throughput automation in settings where people and robots share workspaces, collaborative robotic cells must reconcile stringent safety guarantees with the need for responsive and effective behavior. A dynamic obstacle is the stochastic, task-dependent variability of human motion: when robots fall back on purely reactive or worst-case envelopes, they brake unnecessarily, stall task progress, and tamper with the fluidity that true Human-Robot Interaction demands. In recent years, learning-based human-motion prediction has rapidly advanced, although most approaches produce worst-case scenario forecasts that often do not treat prediction uncertainty in a well-structured way, resulting in over-conservative planning algorithms, limiting their flexibility. We introduce Uncertainty-Aware Predictive Control Barrier Functions (UA-PCBFs), a unified framework that fuses probabilistic human hand motion forecasting with the formal safety guarantees of Control Barrier Functions. In contrast to other variants, our framework allows for dynamic adjustment of the safety margin thanks to the human motion uncertainty estimation provided by a forecasting module. Thanks to uncertainty estimation, UA-PCBFs empower collaborative robots with a deeper understanding of future human states, facilitating more fluid and intelligent interactions through informed motion planning. We validate UA-PCBFs through comprehensive real-world experiments with an increasing level of realism, including automated setups (to perform exactly repeatable motions) with a robotic hand and direct human-robot interactions (to validate promptness, usability, and human confidence). Relative to state-of-the-art HRI architectures, UA-PCBFs show better performance in task-critical metrics, significantly reducing the number of violations of the robot's safe space during interaction with respect to the state-of-the-art.
<div id='section'>Paperid: <span id='pid'>892, <a href='https://arxiv.org/pdf/2508.08179.pdf' target='_blank'>https://arxiv.org/pdf/2508.08179.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sihan Zhao, Zixuan Wang, Tianyu Luan, Jia Jia, Wentao Zhu, Jiebo Luo, Junsong Yuan, Nan Xi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08179">PP-Motion: Physical-Perceptual Fidelity Evaluation for Human Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion generation has found widespread applications in AR/VR, film, sports, and medical rehabilitation, offering a cost-effective alternative to traditional motion capture systems. However, evaluating the fidelity of such generated motions is a crucial, multifaceted task. Although previous approaches have attempted at motion fidelity evaluation using human perception or physical constraints, there remains an inherent gap between human-perceived fidelity and physical feasibility. Moreover, the subjective and coarse binary labeling of human perception further undermines the development of a robust data-driven metric. We address these issues by introducing a physical labeling method. This method evaluates motion fidelity by calculating the minimum modifications needed for a motion to align with physical laws. With this approach, we are able to produce fine-grained, continuous physical alignment annotations that serve as objective ground truth. With these annotations, we propose PP-Motion, a novel data-driven metric to evaluate both physical and perceptual fidelity of human motion. To effectively capture underlying physical priors, we employ Pearson's correlation loss for the training of our metric. Additionally, by incorporating a human-based perceptual fidelity loss, our metric can capture fidelity that simultaneously considers both human perception and physical alignment. Experimental results demonstrate that our metric, PP-Motion, not only aligns with physical laws but also aligns better with human perception of motion fidelity than previous work.
<div id='section'>Paperid: <span id='pid'>893, <a href='https://arxiv.org/pdf/2508.03068.pdf' target='_blank'>https://arxiv.org/pdf/2508.03068.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sirui Chen, Yufei Ye, Zi-Ang Cao, Jennifer Lew, Pei Xu, C. Karen Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.03068">Hand-Eye Autonomous Delivery: Learning Humanoid Navigation, Locomotion and Reaching</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose Hand-Eye Autonomous Delivery (HEAD), a framework that learns navigation, locomotion, and reaching skills for humanoids, directly from human motion and vision perception data. We take a modular approach where the high-level planner commands the target position and orientation of the hands and eyes of the humanoid, delivered by the low-level policy that controls the whole-body movements. Specifically, the low-level whole-body controller learns to track the three points (eyes, left hand, and right hand) from existing large-scale human motion capture data while high-level policy learns from human data collected by Aria glasses. Our modular approach decouples the ego-centric vision perception from physical actions, promoting efficient learning and scalability to novel scenes. We evaluate our method both in simulation and in the real-world, demonstrating humanoid's capabilities to navigate and reach in complex environments designed for humans.
<div id='section'>Paperid: <span id='pid'>894, <a href='https://arxiv.org/pdf/2508.01730.pdf' target='_blank'>https://arxiv.org/pdf/2508.01730.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianbo Ma, Hui Luo, Qi Chen, Yuankai Qi, Yumei Sun, Amin Beheshti, Jianlin Zhang, Ming-Hsuan Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01730">Tracking the Unstable: Appearance-Guided Motion Modeling for Robust Multi-Object Tracking in UAV-Captured Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) aims to track multiple objects while maintaining consistent identities across frames of a given video. In unmanned aerial vehicle (UAV) recorded videos, frequent viewpoint changes and complex UAV-ground relative motion dynamics pose significant challenges, which often lead to unstable affinity measurement and ambiguous association. Existing methods typically model motion and appearance cues separately, overlooking their spatio-temporal interplay and resulting in suboptimal tracking performance. In this work, we propose AMOT, which jointly exploits appearance and motion cues through two key components: an Appearance-Motion Consistency (AMC) matrix and a Motion-aware Track Continuation (MTC) module. Specifically, the AMC matrix computes bi-directional spatial consistency under the guidance of appearance features, enabling more reliable and context-aware identity association. The MTC module complements AMC by reactivating unmatched tracks through appearance-guided predictions that align with Kalman-based predictions, thereby reducing broken trajectories caused by missed detections. Extensive experiments on three UAV benchmarks, including VisDrone2019, UAVDT, and VT-MOT-UAV, demonstrate that our AMOT outperforms current state-of-the-art methods and generalizes well in a plug-and-play and training-free manner.
<div id='section'>Paperid: <span id='pid'>895, <a href='https://arxiv.org/pdf/2506.21912.pdf' target='_blank'>https://arxiv.org/pdf/2506.21912.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinghan Wang, Kun Xu, Fei Li, Cao Sheng, Jiazhong Yu, Yadong Mu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.21912">Generating Attribute-Aware Human Motions from Textual Prompt</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-driven human motion generation has recently attracted considerable attention, allowing models to generate human motions based on textual descriptions. However, current methods neglect the influence of human attributes (such as age, gender, weight, and height) which are key factors shaping human motion patterns. This work represents a pilot exploration for bridging this gap. We conceptualize each motion as comprising both attribute information and action semantics, where textual descriptions align exclusively with action semantics. To achieve this, a new framework inspired by Structural Causal Models is proposed to decouple action semantics from human attributes, enabling text-to-semantics prediction and attribute-controlled generation. The resulting model is capable of generating realistic, attribute-aware motion aligned with the user's text and attribute inputs. For evaluation, we introduce HumanAttr, a comprehensive dataset containing attribute annotations for text-motion pairs, setting the first benchmark for attribute-aware text-to-motion generation. Extensive experiments on the new dataset validate our model's effectiveness.
<div id='section'>Paperid: <span id='pid'>896, <a href='https://arxiv.org/pdf/2506.14233.pdf' target='_blank'>https://arxiv.org/pdf/2506.14233.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Amirreza Payandeh, Anuj Pokhrel, Daeun Song, Marcos Zampieri, Xuesu Xiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.14233">Narrate2Nav: Real-Time Visual Navigation with Implicit Language Reasoning in Human-Centric Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Vision-Language Models (VLMs) have demonstrated potential in enhancing mobile robot navigation in human-centric environments by understanding contextual cues, human intentions, and social dynamics while exhibiting reasoning capabilities. However, their computational complexity and limited sensitivity to continuous numerical data impede real-time performance and precise motion control. To this end, we propose Narrate2Nav, a novel real-time vision-action model that leverages a novel self-supervised learning framework based on the Barlow Twins redundancy reduction loss to embed implicit natural language reasoning, social cues, and human intentions within a visual encoder-enabling reasoning in the model's latent space rather than token space. The model combines RGB inputs, motion commands, and textual signals of scene context during training to bridge from robot observations to low-level motion commands for short-horizon point-goal navigation during deployment. Extensive evaluation of Narrate2Nav across various challenging scenarios in both offline unseen dataset and real-world experiments demonstrates an overall improvement of 52.94 percent and 41.67 percent, respectively, over the next best baseline. Additionally, qualitative comparative analysis of Narrate2Nav's visual encoder attention map against four other baselines demonstrates enhanced attention to navigation-critical scene elements, underscoring its effectiveness in human-centric navigation tasks.
<div id='section'>Paperid: <span id='pid'>897, <a href='https://arxiv.org/pdf/2506.07565.pdf' target='_blank'>https://arxiv.org/pdf/2506.07565.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinlu Zhang, Zixi Kang, Libin Liu, Jianlong Chang, Qi Tian, Feng Gao, Yizhou Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.07565">OpenDance: Multimodal Controllable 3D Dance Generation with Large-scale Internet Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Music-driven 3D dance generation offers significant creative potential, yet practical applications demand versatile and multimodal control. As the highly dynamic and complex human motion covering various styles and genres, dance generation requires satisfying diverse conditions beyond just music (e.g., spatial trajectories, keyframe gestures, or style descriptions). However, the absence of a large-scale and richly annotated dataset severely hinders progress. In this paper, we build OpenDanceSet, an extensive human dance dataset comprising over 100 hours across 14 genres and 147 subjects. Each sample has rich annotations to facilitate robust cross-modal learning: 3D motion, paired music, 2D keypoints, trajectories, and expert-annotated text descriptions. Furthermore, we propose OpenDanceNet, a unified masked modeling framework for controllable dance generation, including a disentangled auto-encoder and a multimodal joint-prediction Transformer. OpenDanceNet supports generation conditioned on music and arbitrary combinations of text, keypoints, or trajectories. Comprehensive experiments demonstrate that our work achieves high-fidelity synthesis with strong diversity and realistic physical contacts, while also offering flexible control over spatial and stylistic conditions. Project Page: https://open-dance.github.io
<div id='section'>Paperid: <span id='pid'>898, <a href='https://arxiv.org/pdf/2505.21864.pdf' target='_blank'>https://arxiv.org/pdf/2505.21864.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mengda Xu, Han Zhang, Yifan Hou, Zhenjia Xu, Linxi Fan, Manuela Veloso, Shuran Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.21864">DexUMI: Using Human Hand as the Universal Manipulation Interface for Dexterous Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present DexUMI - a data collection and policy learning framework that uses the human hand as the natural interface to transfer dexterous manipulation skills to various robot hands. DexUMI includes hardware and software adaptations to minimize the embodiment gap between the human hand and various robot hands. The hardware adaptation bridges the kinematics gap using a wearable hand exoskeleton. It allows direct haptic feedback in manipulation data collection and adapts human motion to feasible robot hand motion. The software adaptation bridges the visual gap by replacing the human hand in video data with high-fidelity robot hand inpainting. We demonstrate DexUMI's capabilities through comprehensive real-world experiments on two different dexterous robot hand hardware platforms, achieving an average task success rate of 86%.
<div id='section'>Paperid: <span id='pid'>899, <a href='https://arxiv.org/pdf/2505.19463.pdf' target='_blank'>https://arxiv.org/pdf/2505.19463.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoyu Zhao, Sixu Lin, Qingwei Ben, Minyue Dai, Hao Fei, Jingbo Wang, Hua Zou, Junting Dong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.19463">SMAP: Self-supervised Motion Adaptation for Physically Plausible Humanoid Whole-body Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a novel framework that enables real-world humanoid robots to maintain stability while performing human-like motion. Current methods train a policy which allows humanoid robots to follow human body using the massive retargeted human data via reinforcement learning. However, due to the heterogeneity between human and humanoid robot motion, directly using retargeted human motion reduces training efficiency and stability. To this end, we introduce SMAP, a novel whole-body tracking framework that bridges the gap between human and humanoid action spaces, enabling accurate motion mimicry by humanoid robots. The core idea is to use a vector-quantized periodic autoencoder to capture generic atomic behaviors and adapt human motion into physically plausible humanoid motion. This adaptation accelerates training convergence and improves stability when handling novel or challenging motions. We then employ a privileged teacher to distill precise mimicry skills into the student policy with a proposed decoupled reward. We conduct experiments in simulation and real world to demonstrate the superiority stability and performance of SMAP over SOTA methods, offering practical guidelines for advancing whole-body control in humanoid robots.
<div id='section'>Paperid: <span id='pid'>900, <a href='https://arxiv.org/pdf/2505.08238.pdf' target='_blank'>https://arxiv.org/pdf/2505.08238.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunyue Wei, Shanning Zhuang, Vincent Zhuang, Yanan Sui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.08238">Motion Control of High-Dimensional Musculoskeletal Systems with Hierarchical Model-Based Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Controlling high-dimensional nonlinear systems, such as those found in biological and robotic applications, is challenging due to large state and action spaces. While deep reinforcement learning has achieved a number of successes in these domains, it is computationally intensive and time consuming, and therefore not suitable for solving large collections of tasks that require significant manual tuning. In this work, we introduce Model Predictive Control with Morphology-aware Proportional Control (MPC^2), a hierarchical model-based learning algorithm for zero-shot and near-real-time control of high-dimensional complex dynamical systems. MPC^2 uses a sampling-based model predictive controller for target posture planning, and enables robust control for high-dimensional tasks by incorporating a morphology-aware proportional controller for actuator coordination. The algorithm enables motion control of a high-dimensional human musculoskeletal model in a variety of motion tasks, such as standing, walking on different terrains, and imitating sports activities. The reward function of MPC^2 can be tuned via black-box optimization, drastically reducing the need for human-intensive reward engineering.
<div id='section'>Paperid: <span id='pid'>901, <a href='https://arxiv.org/pdf/2503.07390.pdf' target='_blank'>https://arxiv.org/pdf/2503.07390.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Boeun Kim, Hea In Jeong, JungHoon Sung, Yihua Cheng, Jeongmin Lee, Ju Yong Chang, Sang-Il Choi, Younggeun Choi, Saim Shin, Jungho Kim, Hyung Jin Chang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.07390">PersonaBooth: Personalized Text-to-Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces Motion Personalization, a new task that generates personalized motions aligned with text descriptions using several basic motions containing Persona. To support this novel task, we introduce a new large-scale motion dataset called PerMo (PersonaMotion), which captures the unique personas of multiple actors. We also propose a multi-modal finetuning method of a pretrained motion diffusion model called PersonaBooth. PersonaBooth addresses two main challenges: i) A significant distribution gap between the persona-focused PerMo dataset and the pretraining datasets, which lack persona-specific data, and ii) the difficulty of capturing a consistent persona from the motions vary in content (action type). To tackle the dataset distribution gap, we introduce a persona token to accept new persona features and perform multi-modal adaptation for both text and visuals during finetuning. To capture a consistent persona, we incorporate a contrastive learning technique to enhance intra-cohesion among samples with the same persona. Furthermore, we introduce a context-aware fusion mechanism to maximize the integration of persona cues from multiple input motions. PersonaBooth outperforms state-of-the-art motion style transfer methods, establishing a new benchmark for motion personalization.
<div id='section'>Paperid: <span id='pid'>902, <a href='https://arxiv.org/pdf/2503.03474.pdf' target='_blank'>https://arxiv.org/pdf/2503.03474.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Varsha Suresh, M. Hamza Mughal, Christian Theobalt, Vera Demberg
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.03474">Enhancing Spoken Discourse Modeling in Language Models Using Gestural Cues</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Research in linguistics shows that non-verbal cues, such as gestures, play a crucial role in spoken discourse. For example, speakers perform hand gestures to indicate topic shifts, helping listeners identify transitions in discourse. In this work, we investigate whether the joint modeling of gestures using human motion sequences and language can improve spoken discourse modeling in language models. To integrate gestures into language models, we first encode 3D human motion sequences into discrete gesture tokens using a VQ-VAE. These gesture token embeddings are then aligned with text embeddings through feature alignment, mapping them into the text embedding space. To evaluate the gesture-aligned language model on spoken discourse, we construct text infilling tasks targeting three key discourse cues grounded in linguistic research: discourse connectives, stance markers, and quantifiers. Results show that incorporating gestures enhances marker prediction accuracy across the three tasks, highlighting the complementary information that gestures can offer in modeling spoken discourse. We view this work as an initial step toward leveraging non-verbal cues to advance spoken language modeling in language models.
<div id='section'>Paperid: <span id='pid'>903, <a href='https://arxiv.org/pdf/2502.11515.pdf' target='_blank'>https://arxiv.org/pdf/2502.11515.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junxian Ma, Shiwen Wang, Jian Yang, Junyi Hu, Jian Liang, Guosheng Lin, Jingbo chen, Kai Li, Yu Meng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.11515">SayAnything: Audio-Driven Lip Synchronization with Conditional Video Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in diffusion models have led to significant progress in audio-driven lip synchronization. However, existing methods typically rely on constrained audio-visual alignment priors or multi-stage learning of intermediate representations to force lip motion synthesis. This leads to complex training pipelines and limited motion naturalness. In this paper, we present SayAnything, a conditional video diffusion framework that directly synthesizes lip movements from audio input while preserving speaker identity. Specifically, we propose three specialized modules including identity preservation module, audio guidance module, and editing control module. Our novel design effectively balances different condition signals in the latent space, enabling precise control over appearance, motion, and region-specific generation without requiring additional supervision signals or intermediate representations. Extensive experiments demonstrate that SayAnything generates highly realistic videos with improved lip-teeth coherence, enabling unseen characters to say anything, while effectively generalizing to animated characters.
<div id='section'>Paperid: <span id='pid'>904, <a href='https://arxiv.org/pdf/2501.16753.pdf' target='_blank'>https://arxiv.org/pdf/2501.16753.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hy Nguyen, Srikanth Thudumu, Hung Du, Rajesh Vasa, Kon Mouzakis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.16753">Overcoming Semantic Dilution in Transformer-Based Next Frame Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Next-frame prediction in videos is crucial for applications such as autonomous driving, object tracking, and motion prediction. The primary challenge in next-frame prediction lies in effectively capturing and processing both spatial and temporal information from previous video sequences. The transformer architecture, known for its prowess in handling sequence data, has made remarkable progress in this domain. However, transformer-based next-frame prediction models face notable issues: (a) The multi-head self-attention (MHSA) mechanism requires the input embedding to be split into $N$ chunks, where $N$ is the number of heads. Each segment captures only a fraction of the original embeddings information, which distorts the representation of the embedding in the latent space, resulting in a semantic dilution problem; (b) These models predict the embeddings of the next frames rather than the frames themselves, but the loss function based on the errors of the reconstructed frames, not the predicted embeddings -- this creates a discrepancy between the training objective and the model output. We propose a Semantic Concentration Multi-Head Self-Attention (SCMHSA) architecture, which effectively mitigates semantic dilution in transformer-based next-frame prediction. Additionally, we introduce a loss function that optimizes SCMHSA in the latent space, aligning the training objective more closely with the model output. Our method demonstrates superior performance compared to the original transformer-based predictors.
<div id='section'>Paperid: <span id='pid'>905, <a href='https://arxiv.org/pdf/2412.20350.pdf' target='_blank'>https://arxiv.org/pdf/2412.20350.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunyue Wei, Zeji Yi, Hongda Li, Saraswati Soedarmadji, Yanan Sui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.20350">Safe Bayesian Optimization for the Control of High-Dimensional Embodied Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning to move is a primary goal for animals and robots, where ensuring safety is often important when optimizing control policies on the embodied systems. For complex tasks such as the control of human or humanoid control, the high-dimensional parameter space adds complexity to the safe optimization effort. Current safe exploration algorithms exhibit inefficiency and may even become infeasible with large high-dimensional input spaces. Furthermore, existing high-dimensional constrained optimization methods neglect safety in the search process. In this paper, we propose High-dimensional Safe Bayesian Optimization with local optimistic exploration (HdSafeBO), a novel approach designed to handle high-dimensional sampling problems under probabilistic safety constraints. We introduce a local optimistic strategy to efficiently and safely optimize the objective function, providing a probabilistic safety guarantee and a cumulative safety violation bound. Through the use of isometric embedding, HdSafeBO addresses problems ranging from a few hundred to several thousand dimensions while maintaining safety guarantees. To our knowledge, HdSafeBO is the first algorithm capable of optimizing the control of high-dimensional musculoskeletal systems with high safety probability. We also demonstrate the real-world applicability of HdSafeBO through its use in the safe online optimization of neural stimulation induced human motion control.
<div id='section'>Paperid: <span id='pid'>906, <a href='https://arxiv.org/pdf/2412.04343.pdf' target='_blank'>https://arxiv.org/pdf/2412.04343.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhouyingcheng Liao, Mingyuan Zhang, Wenjia Wang, Lei Yang, Taku Komura
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.04343">RMD: A Simple Baseline for More General Human Motion Generation via Training-free Retrieval-Augmented Motion Diffuse</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While motion generation has made substantial progress, its practical application remains constrained by dataset diversity and scale, limiting its ability to handle out-of-distribution scenarios. To address this, we propose a simple and effective baseline, RMD, which enhances the generalization of motion generation through retrieval-augmented techniques. Unlike previous retrieval-based methods, RMD requires no additional training and offers three key advantages: (1) the external retrieval database can be flexibly replaced; (2) body parts from the motion database can be reused, with an LLM facilitating splitting and recombination; and (3) a pre-trained motion diffusion model serves as a prior to improve the quality of motions obtained through retrieval and direct combination. Without any training, RMD achieves state-of-the-art performance, with notable advantages on out-of-distribution data.
<div id='section'>Paperid: <span id='pid'>907, <a href='https://arxiv.org/pdf/2411.18303.pdf' target='_blank'>https://arxiv.org/pdf/2411.18303.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenjie Zhuo, Fan Ma, Hehe Fan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.18303">InfiniDreamer: Arbitrarily Long Human Motion Generation via Segment Score Distillation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present InfiniDreamer, a novel framework for arbitrarily long human motion generation. InfiniDreamer addresses the limitations of current motion generation methods, which are typically restricted to short sequences due to the lack of long motion training data. To achieve this, we first generate sub-motions corresponding to each textual description and then assemble them into a coarse, extended sequence using randomly initialized transition segments. We then introduce an optimization-based method called Segment Score Distillation (SSD) to refine the entire long motion sequence. SSD is designed to utilize an existing motion prior, which is trained only on short clips, in a training-free manner. Specifically, SSD iteratively refines overlapping short segments sampled from the coarsely extended long motion sequence, progressively aligning them with the pre-trained motion diffusion prior. This process ensures local coherence within each segment, while the refined transitions between segments maintain global consistency across the entire sequence. Extensive qualitative and quantitative experiments validate the superiority of our framework, showcasing its ability to generate coherent, contextually aware motion sequences of arbitrary length.
<div id='section'>Paperid: <span id='pid'>908, <a href='https://arxiv.org/pdf/2411.11911.pdf' target='_blank'>https://arxiv.org/pdf/2411.11911.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zikang Zhou, Hengjian Zhou, Haibo Hu, Zihao Wen, Jianping Wang, Yung-Hui Li, Yu-Kai Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.11911">ModeSeq: Taming Sparse Multimodal Motion Prediction with Sequential Mode Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Anticipating the multimodality of future events lays the foundation for safe autonomous driving. However, multimodal motion prediction for traffic agents has been clouded by the lack of multimodal ground truth. Existing works predominantly adopt the winner-take-all training strategy to tackle this challenge, yet still suffer from limited trajectory diversity and uncalibrated mode confidence. While some approaches address these limitations by generating excessive trajectory candidates, they necessitate a post-processing stage to identify the most representative modes, a process lacking universal principles and compromising trajectory accuracy. We are thus motivated to introduce ModeSeq, a new multimodal prediction paradigm that models modes as sequences. Unlike the common practice of decoding multiple plausible trajectories in one shot, ModeSeq requires motion decoders to infer the next mode step by step, thereby more explicitly capturing the correlation between modes and significantly enhancing the ability to reason about multimodality. Leveraging the inductive bias of sequential mode prediction, we also propose the Early-Match-Take-All (EMTA) training strategy to diversify the trajectories further. Without relying on dense mode prediction or heuristic post-processing, ModeSeq considerably improves the diversity of multimodal output while attaining satisfactory trajectory accuracy, resulting in balanced performance on motion prediction benchmarks. Moreover, ModeSeq naturally emerges with the capability of mode extrapolation, which supports forecasting more behavior modes when the future is highly uncertain.
<div id='section'>Paperid: <span id='pid'>909, <a href='https://arxiv.org/pdf/2410.20974.pdf' target='_blank'>https://arxiv.org/pdf/2410.20974.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Di Qiu, Zheng Chen, Rui Wang, Mingyuan Fan, Changqian Yu, Junshi Huang, Xiang Wen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.20974">MovieCharacter: A Tuning-Free Framework for Controllable Character Video Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in character video synthesis still depend on extensive fine-tuning or complex 3D modeling processes, which can restrict accessibility and hinder real-time applicability. To address these challenges, we propose a simple yet effective tuning-free framework for character video synthesis, named MovieCharacter, designed to streamline the synthesis process while ensuring high-quality outcomes. Our framework decomposes the synthesis task into distinct, manageable modules: character segmentation and tracking, video object removal, character motion imitation, and video composition. This modular design not only facilitates flexible customization but also ensures that each component operates collaboratively to effectively meet user needs. By leveraging existing open-source models and integrating well-established techniques, MovieCharacter achieves impressive synthesis results without necessitating substantial resources or proprietary datasets. Experimental results demonstrate that our framework enhances the efficiency, accessibility, and adaptability of character video synthesis, paving the way for broader creative and interactive applications.
<div id='section'>Paperid: <span id='pid'>910, <a href='https://arxiv.org/pdf/2410.06437.pdf' target='_blank'>https://arxiv.org/pdf/2410.06437.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kojiro Takeyama, Yimeng Liu, Misha Sra
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.06437">LocoVR: Multiuser Indoor Locomotion Dataset in Virtual Reality</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding human locomotion is crucial for AI agents such as robots, particularly in complex indoor home environments. Modeling human trajectories in these spaces requires insight into how individuals maneuver around physical obstacles and manage social navigation dynamics. These dynamics include subtle behaviors influenced by proxemics - the social use of space, such as stepping aside to allow others to pass or choosing longer routes to avoid collisions. Previous research has developed datasets of human motion in indoor scenes, but these are often limited in scale and lack the nuanced social navigation dynamics common in home environments. To address this, we present LocoVR, a dataset of 7000+ two-person trajectories captured in virtual reality from over 130 different indoor home environments. LocoVR provides accurate trajectory data and precise spatial information, along with rich examples of socially-motivated movement behaviors. For example, the dataset captures instances of individuals navigating around each other in narrow spaces, adjusting paths to respect personal boundaries in living areas, and coordinating movements in high-traffic zones like entryways and kitchens. Our evaluation shows that LocoVR significantly enhances model performance in three practical indoor tasks utilizing human trajectories, and demonstrates predicting socially-aware navigation patterns in home environments.
<div id='section'>Paperid: <span id='pid'>911, <a href='https://arxiv.org/pdf/2409.09536.pdf' target='_blank'>https://arxiv.org/pdf/2409.09536.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Teun van de Laar, Zengjie Zhang, Shuhao Qi, Sofie Haesaert, Zhiyong Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.09536">VernaCopter: Disambiguated Natural-Language-Driven Robot via Formal Specifications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>It has been an ambition of many to control a robot for a complex task using natural language (NL). The rise of large language models (LLMs) makes it closer to coming true. However, an LLM-powered system still suffers from the ambiguity inherent in an NL and the uncertainty brought up by LLMs. This paper proposes a novel LLM-based robot motion planner, named \textit{VernaCopter}, with signal temporal logic (STL) specifications serving as a bridge between NL commands and specific task objectives. The rigorous and abstract nature of formal specifications allows the planner to generate high-quality and highly consistent paths to guide the motion control of a robot. Compared to a conventional NL-prompting-based planner, the proposed VernaCopter planner is more stable and reliable due to less ambiguous uncertainty. Its efficacy and advantage have been validated by two small but challenging experimental scenarios, implying its potential in designing NL-driven robots.
<div id='section'>Paperid: <span id='pid'>912, <a href='https://arxiv.org/pdf/2407.08049.pdf' target='_blank'>https://arxiv.org/pdf/2407.08049.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lei Cheng, Arindam Sengupta, Siyang Cao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.08049">Deep Learning-Based Robust Multi-Object Tracking via Fusion of mmWave Radar and Camera Sensors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous driving holds great promise in addressing traffic safety concerns by leveraging artificial intelligence and sensor technology. Multi-Object Tracking plays a critical role in ensuring safer and more efficient navigation through complex traffic scenarios. This paper presents a novel deep learning-based method that integrates radar and camera data to enhance the accuracy and robustness of Multi-Object Tracking in autonomous driving systems. The proposed method leverages a Bi-directional Long Short-Term Memory network to incorporate long-term temporal information and improve motion prediction. An appearance feature model inspired by FaceNet is used to establish associations between objects across different frames, ensuring consistent tracking. A tri-output mechanism is employed, consisting of individual outputs for radar and camera sensors and a fusion output, to provide robustness against sensor failures and produce accurate tracking results. Through extensive evaluations of real-world datasets, our approach demonstrates remarkable improvements in tracking accuracy, ensuring reliable performance even in low-visibility scenarios.
<div id='section'>Paperid: <span id='pid'>913, <a href='https://arxiv.org/pdf/2407.01593.pdf' target='_blank'>https://arxiv.org/pdf/2407.01593.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sariah Mghames, Luca Castri, Marc Hanheide, Nicola Bellotto
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.01593">neuROSym: Deployment and Evaluation of a ROS-based Neuro-Symbolic Model for Human Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous mobile robots can rely on several human motion detection and prediction systems for safe and efficient navigation in human environments, but the underline model architectures can have different impacts on the trustworthiness of the robot in the real world. Among existing solutions for context-aware human motion prediction, some approaches have shown the benefit of integrating symbolic knowledge with state-of-the-art neural networks. In particular, a recent neuro-symbolic architecture (NeuroSyM) has successfully embedded context with a Qualitative Trajectory Calculus (QTC) for spatial interactions representation. This work achieved better performance than neural-only baseline architectures on offline datasets. In this paper, we extend the original architecture to provide neuROSym, a ROS package for robot deployment in real-world scenarios, which can run, visualise, and evaluate previous neural-only and neuro-symbolic models for motion prediction online. We evaluated these models, NeuroSyM and a baseline SGAN, on a TIAGo robot in two scenarios with different human motion patterns. We assessed accuracy and runtime performance of the prediction models, showing a general improvement in case our neuro-symbolic architecture is used. We make the neuROSym package1 publicly available to the robotics community.
<div id='section'>Paperid: <span id='pid'>914, <a href='https://arxiv.org/pdf/2406.10740.pdf' target='_blank'>https://arxiv.org/pdf/2406.10740.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhikai Zhang, Yitang Li, Haofeng Huang, Mingxian Lin, Li Yi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.10740">FreeMotion: MoCap-Free Human Motion Synthesis with Multimodal Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion synthesis is a fundamental task in computer animation. Despite recent progress in this field utilizing deep learning and motion capture data, existing methods are always limited to specific motion categories, environments, and styles. This poor generalizability can be partially attributed to the difficulty and expense of collecting large-scale and high-quality motion data. At the same time, foundation models trained with internet-scale image and text data have demonstrated surprising world knowledge and reasoning ability for various downstream tasks. Utilizing these foundation models may help with human motion synthesis, which some recent works have superficially explored. However, these methods didn't fully unveil the foundation models' potential for this task and only support several simple actions and environments. In this paper, we for the first time, without any motion data, explore open-set human motion synthesis using natural language instructions as user control signals based on MLLMs across any motion task and environment. Our framework can be split into two stages: 1) sequential keyframe generation by utilizing MLLMs as a keyframe designer and animator; 2) motion filling between keyframes through interpolation and motion tracking. Our method can achieve general human motion synthesis for many downstream tasks. The promising results demonstrate the worth of mocap-free human motion synthesis aided by MLLMs and pave the way for future research.
<div id='section'>Paperid: <span id='pid'>915, <a href='https://arxiv.org/pdf/2406.05613.pdf' target='_blank'>https://arxiv.org/pdf/2406.05613.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenhang Liu, Meng Ren, Kun Song, Gaoming Chen, Michael Yu Wang, Zhenhua Xiong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.05613">Distributed Motion Control of Multiple Mobile Manipulators for Reducing Interaction Wrench in Object Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In real-world cooperative manipulation of objects, multiple mobile manipulator systems may suffer from disturbances and asynchrony, leading to excessive interaction wrenches and potentially causing object damage or emergency stops. Existing methods often rely on torque control and dynamic models, which are uncommon in many industrial robots and settings. Additionally, dynamic models often neglect joint friction forces and are not accurate. These methods are challenging to implement and validate in physical systems. To address the problems, this paper presents a novel distributed motion control approach aimed at reducing these unnecessary interaction wrenches. The control law is only based on local information and joint velocity control to enhance practical applicability. The communication delays within the distributed architecture are considered. The stability of the control law is rigorously proven by the Lyapunov theorem. In the simulations, the effectiveness is shown, and the impact of communication graph connectivity and communication delays has been studied. A comparison with other methods shows the advantages of the proposed control law in terms of convergence speed and robustness. Finally, the control law has been validated in physical experiments. It does not require dynamic modeling or torque control, and thus is more user-friendly for physical robots.
<div id='section'>Paperid: <span id='pid'>916, <a href='https://arxiv.org/pdf/2404.19531.pdf' target='_blank'>https://arxiv.org/pdf/2404.19531.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Norman Mu, Jingwei Ji, Zhenpei Yang, Nate Harada, Haotian Tang, Kan Chen, Charles R. Qi, Runzhou Ge, Kratarth Goel, Zoey Yang, Scott Ettinger, Rami Al-Rfou, Dragomir Anguelov, Yin Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.19531">MoST: Multi-modality Scene Tokenization for Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Many existing motion prediction approaches rely on symbolic perception outputs to generate agent trajectories, such as bounding boxes, road graph information and traffic lights. This symbolic representation is a high-level abstraction of the real world, which may render the motion prediction model vulnerable to perception errors (e.g., failures in detecting open-vocabulary obstacles) while missing salient information from the scene context (e.g., poor road conditions). An alternative paradigm is end-to-end learning from raw sensors. However, this approach suffers from the lack of interpretability and requires significantly more training resources. In this work, we propose tokenizing the visual world into a compact set of scene elements and then leveraging pre-trained image foundation models and LiDAR neural networks to encode all the scene elements in an open-vocabulary manner. The image foundation model enables our scene tokens to encode the general knowledge of the open world while the LiDAR neural network encodes geometry information. Our proposed representation can efficiently encode the multi-frame multi-modality observations with a few hundred tokens and is compatible with most transformer-based architectures. To evaluate our method, we have augmented Waymo Open Motion Dataset with camera embeddings. Experiments over Waymo Open Motion Dataset show that our approach leads to significant performance improvements over the state-of-the-art.
<div id='section'>Paperid: <span id='pid'>917, <a href='https://arxiv.org/pdf/2404.19283.pdf' target='_blank'>https://arxiv.org/pdf/2404.19283.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Marlon Steiner, Marvin Klemp, Christoph Stiller
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.19283">MAP-Former: Multi-Agent-Pair Gaussian Joint Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>There is a gap in risk assessment of trajectories between the trajectory information coming from a traffic motion prediction module and what is actually needed. Closing this gap necessitates advancements in prediction beyond current practices. Existing prediction models yield joint predictions of agents' future trajectories with uncertainty weights or marginal Gaussian probability density functions (PDFs) for single agents. Although, these methods achieve high accurate trajectory predictions, they only provide little or no information about the dependencies of interacting agents. Since traffic is a process of highly interdependent agents, whose actions directly influence their mutual behavior, the existing methods are not sufficient to reliably assess the risk of future trajectories. This paper addresses that gap by introducing a novel approach to motion prediction, focusing on predicting agent-pair covariance matrices in a ``scene-centric'' manner, which can then be used to model Gaussian joint PDFs for all agent-pairs in a scene. We propose a model capable of predicting those agent-pair covariance matrices, leveraging an enhanced awareness of interactions. Utilizing the prediction results of our model, this work forms the foundation for comprehensive risk assessment with statistically based methods for analyzing agents' relations by their joint PDFs.
<div id='section'>Paperid: <span id='pid'>918, <a href='https://arxiv.org/pdf/2404.12942.pdf' target='_blank'>https://arxiv.org/pdf/2404.12942.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nicolas Ugrinovic, Thomas Lucas, Fabien Baradel, Philippe Weinzaepfel, Gregory Rogez, Francesc Moreno-Noguer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.12942">Purposer: Putting Human Motion Generation in Context</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a novel method to generate human motion to populate 3D indoor scenes. It can be controlled with various combinations of conditioning signals such as a path in a scene, target poses, past motions, and scenes represented as 3D point clouds. State-of-the-art methods are either models specialized to one single setting, require vast amounts of high-quality and diverse training data, or are unconditional models that do not integrate scene or other contextual information. As a consequence, they have limited applicability and rely on costly training data. To address these limitations, we propose a new method ,dubbed Purposer, based on neural discrete representation learning. Our model is capable of exploiting, in a flexible manner, different types of information already present in open access large-scale datasets such as AMASS. First, we encode unconditional human motion into a discrete latent space. Second, an autoregressive generative model, conditioned with key contextual information, either with prompting or additive tokens, and trained for next-step prediction in this space, synthesizes sequences of latent indices. We further design a novel conditioning block to handle future conditioning information in such a causal model by using a network with two branches to compute separate stacks of features. In this manner, Purposer can generate realistic motion sequences in diverse test scenes. Through exhaustive evaluation, we demonstrate that our multi-contextual solution outperforms existing specialized approaches for specific contextual information, both in terms of quality and diversity. Our model is trained with short sequences, but a byproduct of being able to use various conditioning signals is that at test time different combinations can be used to chain short sequences together and generate long motions within a context scene.
<div id='section'>Paperid: <span id='pid'>919, <a href='https://arxiv.org/pdf/2402.17434.pdf' target='_blank'>https://arxiv.org/pdf/2402.17434.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tong Hui, Eugenio Cuniato, Michael Pantic, Marco Tognon, Matteo Fumagalli, Roland Siegwart
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.17434">Passive Aligning Physical Interaction of Fully-Actuated Aerial Vehicles for Pushing Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, the utilization of aerial manipulators for performing pushing tasks in non-destructive testing (NDT) applications has seen significant growth. Such operations entail physical interactions between the aerial robotic system and the environment. End-effectors with multiple contact points are often used for placing NDT sensors in contact with a surface to be inspected. Aligning the NDT sensor and the work surface while preserving contact, requires that all available contact points at the end-effector tip are in contact with the work surface. With a standard full-pose controller, attitude errors often occur due to perturbations caused by modeling uncertainties, sensor noise, and environmental uncertainties. Even small attitude errors can cause a loss of contact points between the end-effector tip and the work surface. To preserve full alignment amidst these uncertainties, we propose a control strategy which selectively deactivates angular motion control and enables direct force control in specific directions. In particular, we derive two essential conditions to be met, such that the robot can passively align with flat work surfaces achieving full alignment through the rotation along non-actively controlled axes. Additionally, these conditions serve as hardware design and control guidelines for effectively integrating the proposed control method for practical usage. Real world experiments are conducted to validate both the control design and the guidelines.
<div id='section'>Paperid: <span id='pid'>920, <a href='https://arxiv.org/pdf/2512.20847.pdf' target='_blank'>https://arxiv.org/pdf/2512.20847.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Parag Khanna, Karen Jane Dsouza, Chunyu Wang, Mårten Björkman, Christian Smith
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.20847">YCB-Handovers Dataset: Analyzing Object Weight Impact on Human Handovers to Adapt Robotic Handover Motion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces the YCB-Handovers dataset, capturing motion data of 2771 human-human handovers with varying object weights. The dataset aims to bridge a gap in human-robot collaboration research, providing insights into the impact of object weight in human handovers and readiness cues for intuitive robotic motion planning. The underlying dataset for object recognition and tracking is the YCB (Yale-CMU-Berkeley) dataset, which is an established standard dataset used in algorithms for robotic manipulation, including grasping and carrying objects. The YCB-Handovers dataset incorporates human motion patterns in handovers, making it applicable for data-driven, human-inspired models aimed at weight-sensitive motion planning and adaptive robotic behaviors. This dataset covers an extensive range of weights, allowing for a more robust study of handover behavior and weight variation. Some objects also require careful handovers, highlighting contrasts with standard handovers. We also provide a detailed analysis of the object's weight impact on the human reaching motion in these handovers.
<div id='section'>Paperid: <span id='pid'>921, <a href='https://arxiv.org/pdf/2512.13560.pdf' target='_blank'>https://arxiv.org/pdf/2512.13560.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shun Maeda, Chunzhi Gu, Koichiro Kamide, Katsuya Hotta, Shangce Gao, Chao Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.13560">3D Human-Human Interaction Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human-centric anomaly detection (AD) has been primarily studied to specify anomalous behaviors in a single person. However, as humans by nature tend to act in a collaborative manner, behavioral anomalies can also arise from human-human interactions. Detecting such anomalies using existing single-person AD models is prone to low accuracy, as these approaches are typically not designed to capture the complex and asymmetric dynamics of interactions. In this paper, we introduce a novel task, Human-Human Interaction Anomaly Detection (H2IAD), which aims to identify anomalous interactive behaviors within collaborative 3D human actions. To address H2IAD, we then propose Interaction Anomaly Detection Network (IADNet), which is formalized with a Temporal Attention Sharing Module (TASM). Specifically, in designing TASM, we share the encoded motion embeddings across both people such that collaborative motion correlations can be effectively synchronized. Moreover, we notice that in addition to temporal dynamics, human interactions are also characterized by spatial configurations between two people. We thus introduce a Distance-Based Relational Encoding Module (DREM) to better reflect social cues in H2IAD. The normalizing flow is eventually employed for anomaly scoring. Extensive experiments on human-human motion benchmarks demonstrate that IADNet outperforms existing Human-centric AD baselines in H2IAD.
<div id='section'>Paperid: <span id='pid'>922, <a href='https://arxiv.org/pdf/2511.00248.pdf' target='_blank'>https://arxiv.org/pdf/2511.00248.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shurui Gui, Deep Anil Patel, Xiner Li, Martin Renqiang Min
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.00248">Object-Aware 4D Human Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in video diffusion models have enabled the generation of high-quality videos. However, these videos still suffer from unrealistic deformations, semantic violations, and physical inconsistencies that are largely rooted in the absence of 3D physical priors. To address these challenges, we propose an object-aware 4D human motion generation framework grounded in 3D Gaussian representations and motion diffusion priors. With pre-generated 3D humans and objects, our method, Motion Score Distilled Interaction (MSDI), employs the spatial and prompt semantic information in large language models (LLMs) and motion priors through the proposed Motion Diffusion Score Distillation Sampling (MSDS). The combination of MSDS and LLMs enables our spatial-aware motion optimization, which distills score gradients from pre-trained motion diffusion models, to refine human motion while respecting object and semantic constraints. Unlike prior methods requiring joint training on limited interaction datasets, our zero-shot approach avoids retraining and generalizes to out-of-distribution object aware human motions. Experiments demonstrate that our framework produces natural and physically plausible human motions that respect 3D spatial context, offering a scalable solution for realistic 4D generation.
<div id='section'>Paperid: <span id='pid'>923, <a href='https://arxiv.org/pdf/2510.22712.pdf' target='_blank'>https://arxiv.org/pdf/2510.22712.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jose Luis Ponton, Eduardo Alvarado, Lin Geng Foo, Nuria Pelechano, Carlos Andujar, Marc Habermann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.22712">Step2Motion: Locomotion Reconstruction from Pressure Sensing Insoles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion is fundamentally driven by continuous physical interaction with the environment. Whether walking, running, or simply standing, the forces exchanged between our feet and the ground provide crucial insights for understanding and reconstructing human movement. Recent advances in wearable insole devices offer a compelling solution for capturing these forces in diverse, real-world scenarios. Sensor insoles pose no constraint on the users' motion (unlike mocap suits) and are unaffected by line-of-sight limitations (in contrast to optical systems). These qualities make sensor insoles an ideal choice for robust, unconstrained motion capture, particularly in outdoor environments. Surprisingly, leveraging these devices with recent motion reconstruction methods remains largely unexplored. Aiming to fill this gap, we present Step2Motion, the first approach to reconstruct human locomotion from multi-modal insole sensors. Our method utilizes pressure and inertial data-accelerations and angular rates-captured by the insoles to reconstruct human motion. We evaluate the effectiveness of our approach across a range of experiments to show its versatility for diverse locomotion styles, from simple ones like walking or jogging up to moving sideways, on tiptoes, slightly crouching, or dancing.
<div id='section'>Paperid: <span id='pid'>924, <a href='https://arxiv.org/pdf/2510.08406.pdf' target='_blank'>https://arxiv.org/pdf/2510.08406.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Filip Bečanović, Kosta Jovanović, Vincent Bonnet
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.08406">Reliability of Single-Level Equality-Constrained Inverse Optimal Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Inverse optimal control (IOC) allows the retrieval of optimal cost function weights, or behavioral parameters, from human motion. The literature on IOC uses methods that are either based on a slow bilevel process or a fast but noise-sensitive minimization of optimality condition violation. Assuming equality-constrained optimal control models of human motion, this article presents a faster but robust approach to solving IOC using a single-level reformulation of the bilevel method and yields equivalent results. Through numerical experiments in simulation, we analyze the robustness to noise of the proposed single-level reformulation to the bilevel IOC formulation with a human-like planar reaching task that is used across recent studies. The approach shows resilience to very large levels of noise and reduces the computation time of the IOC on this task by a factor of 15 when compared to a classical bilevel implementation.
<div id='section'>Paperid: <span id='pid'>925, <a href='https://arxiv.org/pdf/2510.04076.pdf' target='_blank'>https://arxiv.org/pdf/2510.04076.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Amin Vahidi-Moghaddam, Sayed Pedram Haeri Boroujeni, Iman Jebellat, Ehsan Jebellat, Niloufar Mehrabi, Zhaojian Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04076">From Shadow to Light: Toward Safe and Efficient Policy Learning Across MPC, DeePC, RL, and LLM Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>One of the main challenges in modern control applications, particularly in robot and vehicle motion control, is achieving accurate, fast, and safe movement. To address this, optimal control policies have been developed to enforce safety while ensuring high performance. Since basic first-principles models of real systems are often available, model-based controllers are widely used. Model predictive control (MPC) is a leading approach that optimizes performance while explicitly handling safety constraints. However, obtaining accurate models for complex systems is difficult, which motivates data-driven alternatives. ML-based MPC leverages learned models to reduce reliance on hand-crafted dynamics, while reinforcement learning (RL) can learn near-optimal policies directly from interaction data. Data-enabled predictive control (DeePC) goes further by bypassing modeling altogether, directly learning safe policies from raw input-output data. Recently, large language model (LLM) agents have also emerged, translating natural language instructions into structured formulations of optimal control problems. Despite these advances, data-driven policies face significant limitations. They often suffer from slow response times, high computational demands, and large memory needs, making them less practical for real-world systems with fast dynamics, limited onboard computing, or strict memory constraints. To address this, various technique, such as reduced-order modeling, function-approximated policy learning, and convex relaxations, have been proposed to reduce computational complexity. In this paper, we present eight such approaches and demonstrate their effectiveness across real-world applications, including robotic arms, soft robots, and vehicle motion control.
<div id='section'>Paperid: <span id='pid'>926, <a href='https://arxiv.org/pdf/2510.02968.pdf' target='_blank'>https://arxiv.org/pdf/2510.02968.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Amir Habel, Fawad Mehboob, Jeffrin Sam, Clement Fortin, Dzmitry Tsetserukou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.02968">YawSitter: Modeling and Controlling a Tail-Sitter UAV with Enhanced Yaw Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Achieving precise lateral motion modeling and decoupled control in hover remains a significant challenge for tail-sitter Unmanned Aerial Vehicles (UAVs), primarily due to complex aerodynamic couplings and the absence of welldefined lateral dynamics. This paper presents a novel modeling and control strategy that enhances yaw authority and lateral motion by introducing a sideslip force model derived from differential propeller slipstream effects acting on the fuselage under differential thrust. The resulting lateral force along the body y-axis enables yaw-based lateral position control without inducing roll coupling. The control framework employs a YXZ Euler rotation formulation to accurately represent attitude and incorporate gravitational components while directly controlling yaw in the yaxis, thereby improving lateral dynamic behavior and avoiding singularities. The proposed approach is validated through trajectory-tracking simulations conducted in a Unity-based environment. Tests on both rectangular and circular paths in hover mode demonstrate stable performance, with low mean absolute position errors and yaw deviations constrained within 5.688 degrees. These results confirm the effectiveness of the proposed lateral force generation model and provide a foundation for the development of agile, hover-capable tail-sitter UAVs.
<div id='section'>Paperid: <span id='pid'>927, <a href='https://arxiv.org/pdf/2508.20553.pdf' target='_blank'>https://arxiv.org/pdf/2508.20553.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alexander GrÃ¤fe, Joram Eickhoff, Marco Zimmerling, Sebastian Trimpe
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.20553">DMPC-Swarm: Distributed Model Predictive Control on Nano UAV Swarms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Swarms of unmanned aerial vehicles (UAVs) are increasingly becoming vital to our society, undertaking tasks such as search and rescue, surveillance and delivery. A special variant of Distributed Model Predictive Control (DMPC) has emerged as a promising approach for the safe management of these swarms by combining the scalability of distributed computation with dynamic swarm motion control. In this DMPC method, multiple agents solve local optimization problems with coupled anti-collision constraints, periodically exchanging their solutions. Despite its potential, existing methodologies using this DMPC variant have yet to be deployed on distributed hardware that fully utilize true distributed computation and wireless communication. This is primarily due to the lack of a communication system tailored to meet the unique requirements of mobile swarms and an architecture that supports distributed computation while adhering to the payload constraints of UAVs. We present DMPC-SWARM, a new swarm control methodology that integrates an efficient, stateless low-power wireless communication protocol with a novel DMPC algorithm that provably avoids UAV collisions even under message loss. By utilizing event-triggered and distributed off-board computing, DMPC-SWARM supports nano UAVs, allowing them to benefit from additional computational resources while retaining scalability and fault tolerance. In a detailed theoretical analysis, we prove that DMPC-SWARM guarantees collision avoidance under realistic conditions, including communication delays and message loss. Finally, we present DMPC-SWARM's implementation on a swarm of up to 16 nano-quadcopters, demonstrating the first realization of these DMPC variants with computation distributed on multiple physical devices interconnected by a real wireless mesh networks. A video showcasing DMPC-SWARM is available at http://tiny.cc/DMPCSwarm.
<div id='section'>Paperid: <span id='pid'>928, <a href='https://arxiv.org/pdf/2506.22488.pdf' target='_blank'>https://arxiv.org/pdf/2506.22488.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xi Fu, Weibang Jiang, Rui Liu, Gernot R. MÃ¼ller-Putz, Cuntai Guan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.22488">Zero-Shot EEG-to-Gait Decoding via Phase-Aware Representation Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate decoding of lower-limb motion from EEG signals is essential for advancing brain-computer interface (BCI) applications in movement intent recognition and control. However, challenges persist in achieving causal, phase-consistent predictions and in modeling both inter- and intra-subject variability. To address these issues, we propose NeuroDyGait, a domain-generalizable EEG-to-motion decoding framework that leverages structured contrastive representation learning and relational domain modeling. The proposed method employs relative contrastive learning to achieve semantic alignment between EEG and motion embeddings. Furthermore, a multi-cycle gait reconstruction objective is introduced to enforce temporal coherence and maintain biomechanical consistency. To promote inter-session generalization, during fine-tuning, a domain dynamic decoding mechanism adaptively assigns session-specific prediction heads and learns to mix their outputs based on inter-session relationships. NeuroDyGait enables zero-shot motion prediction for unseen individuals without requiring adaptation and achieves superior performance in cross-subject gait decoding on benchmark datasets. Additionally, it demonstrates strong phase-detection capabilities even without explicit phase supervision during training. These findings highlight the potential of relational domain learning in enabling scalable, target-free deployment of BCIs.
<div id='section'>Paperid: <span id='pid'>929, <a href='https://arxiv.org/pdf/2505.21531.pdf' target='_blank'>https://arxiv.org/pdf/2505.21531.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kunhang Li, Jason Naradowsky, Yansong Feng, Yusuke Miyao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.21531">How Much Do Large Language Models Know about Human Motion? A Case Study in 3D Avatar Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We explore the human motion knowledge of Large Language Models (LLMs) through 3D avatar control. Given a motion instruction, we prompt LLMs to first generate a high-level movement plan with consecutive steps (High-level Planning), then specify body part positions in each step (Low-level Planning), which we linearly interpolate into avatar animations. Using 20 representative motion instructions that cover fundamental movements and balance body part usage, we conduct comprehensive evaluations, including human and automatic scoring of both high-level movement plans and generated animations, as well as automatic comparison with oracle positions in low-level planning. Our findings show that LLMs are strong at interpreting high-level body movements but struggle with precise body part positioning. While decomposing motion queries into atomic components improves planning, LLMs face challenges in multi-step movements involving high-degree-of-freedom body parts. Furthermore, LLMs provide reasonable approximations for general spatial descriptions, but fall short in handling precise spatial specifications. Notably, LLMs demonstrate promise in conceptualizing creative motions and distinguishing culturally specific motion patterns.
<div id='section'>Paperid: <span id='pid'>930, <a href='https://arxiv.org/pdf/2505.20582.pdf' target='_blank'>https://arxiv.org/pdf/2505.20582.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yizhou Zhao, Chunjiang Liu, Haoyu Chen, Bhiksha Raj, Min Xu, Tadas Baltrusaitis, Mitch Rundle, HsiangTao Wu, Kamran Ghasedi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.20582">Total-Editing: Head Avatar with Editable Appearance, Motion, and Lighting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Face reenactment and portrait relighting are essential tasks in portrait editing, yet they are typically addressed independently, without much synergy. Most face reenactment methods prioritize motion control and multiview consistency, while portrait relighting focuses on adjusting shading effects. To take advantage of both geometric consistency and illumination awareness, we introduce Total-Editing, a unified portrait editing framework that enables precise control over appearance, motion, and lighting. Specifically, we design a neural radiance field decoder with intrinsic decomposition capabilities. This allows seamless integration of lighting information from portrait images or HDR environment maps into synthesized portraits. We also incorporate a moving least squares based deformation field to enhance the spatiotemporal coherence of avatar motion and shading effects. With these innovations, our unified framework significantly improves the quality and realism of portrait editing results. Further, the multi-source nature of Total-Editing supports more flexible applications, such as illumination transfer from one portrait to another, or portrait animation with customized backgrounds.
<div id='section'>Paperid: <span id='pid'>931, <a href='https://arxiv.org/pdf/2505.19086.pdf' target='_blank'>https://arxiv.org/pdf/2505.19086.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chen Tessler, Yifeng Jiang, Erwin Coumans, Zhengyi Luo, Gal Chechik, Xue Bin Peng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.19086">MaskedManipulator: Versatile Whole-Body Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We tackle the challenges of synthesizing versatile, physically simulated human motions for full-body object manipulation. Unlike prior methods that are focused on detailed motion tracking, trajectory following, or teleoperation, our framework enables users to specify versatile high-level objectives such as target object poses or body poses. To achieve this, we introduce MaskedManipulator, a generative control policy distilled from a tracking controller trained on large-scale human motion capture data. This two-stage learning process allows the system to perform complex interaction behaviors, while providing intuitive user control over both character and object motions. MaskedManipulator produces goal-directed manipulation behaviors that expand the scope of interactive animation systems beyond task-specific solutions.
<div id='section'>Paperid: <span id='pid'>932, <a href='https://arxiv.org/pdf/2504.02478.pdf' target='_blank'>https://arxiv.org/pdf/2504.02478.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bizhu Wu, Jinheng Xie, Keming Shen, Zhe Kong, Jianfeng Ren, Ruibin Bai, Rong Qu, Linlin Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.02478">MG-MotionLLM: A Unified Framework for Motion Comprehension and Generation across Multiple Granularities</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent motion-aware large language models have demonstrated promising potential in unifying motion comprehension and generation. However, existing approaches primarily focus on coarse-grained motion-text modeling, where text describes the overall semantics of an entire motion sequence in just a few words. This limits their ability to handle fine-grained motion-relevant tasks, such as understanding and controlling the movements of specific body parts. To overcome this limitation, we pioneer MG-MotionLLM, a unified motion-language model for multi-granular motion comprehension and generation. We further introduce a comprehensive multi-granularity training scheme by incorporating a set of novel auxiliary tasks, such as localizing temporal boundaries of motion segments via detailed text as well as motion detailed captioning, to facilitate mutual reinforcement for motion-text modeling across various levels of granularity. Extensive experiments show that our MG-MotionLLM achieves superior performance on classical text-to-motion and motion-to-text tasks, and exhibits potential in novel fine-grained motion comprehension and editing tasks. Project page: CVI-SZU/MG-MotionLLM
<div id='section'>Paperid: <span id='pid'>933, <a href='https://arxiv.org/pdf/2503.14919.pdf' target='_blank'>https://arxiv.org/pdf/2503.14919.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junyu Shi, Lijiang Liu, Yong Sun, Zhiyuan Zhang, Jinni Zhou, Qiang Nie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.14919">GenM$^3$: Generative Pretrained Multi-path Motion Model for Text Conditional Human Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scaling up motion datasets is crucial to enhance motion generation capabilities. However, training on large-scale multi-source datasets introduces data heterogeneity challenges due to variations in motion content. To address this, we propose Generative Pretrained Multi-path Motion Model (GenM\(^3\)), a comprehensive framework designed to learn unified motion representations. GenM\(^3\) comprises two components: 1) a Multi-Expert VQ-VAE (MEVQ-VAE) that adapts to different dataset distributions to learn a unified discrete motion representation, and 2) a Multi-path Motion Transformer (MMT) that improves intra-modal representations by using separate modality-specific pathways, each with densely activated experts to accommodate variations within that modality, and improves inter-modal alignment by the text-motion shared pathway. To enable large-scale training, we integrate and unify 11 high-quality motion datasets (approximately 220 hours of motion data) and augment it with textual annotations (nearly 10,000 motion sequences labeled by a large language model and 300+ by human experts). After training on our integrated dataset, GenM\(^3\) achieves a state-of-the-art FID of 0.035 on the HumanML3D benchmark, surpassing state-of-the-art methods by a large margin. It also demonstrates strong zero-shot generalization on IDEA400 dataset, highlighting its effectiveness and adaptability across diverse motion scenarios.
<div id='section'>Paperid: <span id='pid'>934, <a href='https://arxiv.org/pdf/2503.09950.pdf' target='_blank'>https://arxiv.org/pdf/2503.09950.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxiang Fu, Qi Yan, Lele Wang, Ke Li, Renjie Liao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.09950">MoFlow: One-Step Flow Matching for Human Trajectory Forecasting via Implicit Maximum Likelihood Estimation based Distillation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we address the problem of human trajectory forecasting, which aims to predict the inherently multi-modal future movements of humans based on their past trajectories and other contextual cues. We propose a novel motion prediction conditional flow matching model, termed MoFlow, to predict K-shot future trajectories for all agents in a given scene. We design a novel flow matching loss function that not only ensures at least one of the $K$ sets of future trajectories is accurate but also encourages all $K$ sets of future trajectories to be diverse and plausible. Furthermore, by leveraging the implicit maximum likelihood estimation (IMLE), we propose a novel distillation method for flow models that only requires samples from the teacher model. Extensive experiments on the real-world datasets, including SportVU NBA games, ETH-UCY, and SDD, demonstrate that both our teacher flow model and the IMLE-distilled student model achieve state-of-the-art performance. These models can generate diverse trajectories that are physically and socially plausible. Moreover, our one-step student model is $\textbf{100}$ times faster than the teacher flow model during sampling. The code, model, and data are available at our project page: https://moflow-imle.github.io
<div id='section'>Paperid: <span id='pid'>935, <a href='https://arxiv.org/pdf/2502.17834.pdf' target='_blank'>https://arxiv.org/pdf/2502.17834.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Parag Khanna, MÃ¥rten BjÃ¶rkman, Christian Smith
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.17834">Impact of Object Weight in Handovers: Inspiring Robotic Grip Release and Motion from Human Handovers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work explores the effect of object weight on human motion and grip release during handovers to enhance the naturalness, safety, and efficiency of robot-human interactions. We introduce adaptive robotic strategies based on the analysis of human handover behavior with varying object weights. The key contributions of this work includes the development of an adaptive grip-release strategy for robots, a detailed analysis of how object weight influences human motion to guide robotic motion adaptations, and the creation of handover-datasets incorporating various object weights, including the YCB handover dataset. By aligning robotic grip release and motion with human behavior, this work aims to improve robot-human handovers for different weighted objects. We also evaluate these human-inspired adaptive robotic strategies in robot-to-human handovers to assess their effectiveness and performance and demonstrate that they outperform the baseline approaches in terms of naturalness, efficiency, and user perception.
<div id='section'>Paperid: <span id='pid'>936, <a href='https://arxiv.org/pdf/2502.16913.pdf' target='_blank'>https://arxiv.org/pdf/2502.16913.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kedi Lyu, Haipeng Chen, Zhenguang Liu, Yifang Yin, Yukang Lin, Yingying Jiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.16913">HVIS: A Human-like Vision and Inference System for Human Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Grasping the intricacies of human motion, which involve perceiving spatio-temporal dependence and multi-scale effects, is essential for predicting human motion. While humans inherently possess the requisite skills to navigate this issue, it proves to be markedly more challenging for machines to emulate. To bridge the gap, we propose the Human-like Vision and Inference System (HVIS) for human motion prediction, which is designed to emulate human observation and forecast future movements. HVIS comprises two components: the human-like vision encode (HVE) module and the human-like motion inference (HMI) module. The HVE module mimics and refines the human visual process, incorporating a retina-analog component that captures spatiotemporal information separately to avoid unnecessary crosstalk. Additionally, a visual cortex-analogy component is designed to hierarchically extract and treat complex motion features, focusing on both global and local features of human poses. The HMI is employed to simulate the multi-stage learning model of the human brain. The spontaneous learning network simulates the neuronal fracture generation process for the adversarial generation of future motions. Subsequently, the deliberate learning network is optimized for hard-to-train joints to prevent misleading learning. Experimental results demonstrate that our method achieves new state-of-the-art performance, significantly outperforming existing methods by 19.8% on Human3.6M, 15.7% on CMU Mocap, and 11.1% on G3D.
<div id='section'>Paperid: <span id='pid'>937, <a href='https://arxiv.org/pdf/2502.12546.pdf' target='_blank'>https://arxiv.org/pdf/2502.12546.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sang-Eun Lee, Ko Nishino, Shohei Nobuhara
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.12546">Spatiotemporal Multi-Camera Calibration using Freely Moving People</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a novel method for spatiotemporal multi-camera calibration using freely moving people in multiview videos. Since calibrating multiple cameras and finding matches across their views are inherently interdependent, performing both in a unified framework poses a significant challenge. We address these issues as a single registration problem of matching two sets of 3D points, leveraging human motion in dynamic multi-person scenes. To this end, we utilize 3D human poses obtained from an off-the-shelf monocular 3D human pose estimator and transform them into 3D points on a unit sphere, to solve the rotation, time offset, and the association alternatingly. We employ a probabilistic approach that can jointly solve both problems of aligning spatiotemporal data and establishing correspondences through soft assignment between two views. The translation is determined by applying coplanarity constraints. The pairwise registration results are integrated into a multiview setup, and then a nonlinear optimization method is used to improve the accuracy of the camera poses, temporal offsets, and multi-person associations. Extensive experiments on synthetic and real data demonstrate the effectiveness and flexibility of the proposed method as a practical marker-free calibration tool.
<div id='section'>Paperid: <span id='pid'>938, <a href='https://arxiv.org/pdf/2501.15860.pdf' target='_blank'>https://arxiv.org/pdf/2501.15860.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lei Wan, Hannan Ejaz Keen, Alexey Vinel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.15860">The Components of Collaborative Joint Perception and Prediction -- A Conceptual Framework</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Connected Autonomous Vehicles (CAVs) benefit from Vehicle-to-Everything (V2X) communication, which enables the exchange of sensor data to achieve Collaborative Perception (CP). To reduce cumulative errors in perception modules and mitigate the visual occlusion, this paper introduces a new task, Collaborative Joint Perception and Prediction (Co-P&P), and provides a conceptual framework for its implementation to improve motion prediction of surrounding objects, thereby enhancing vehicle awareness in complex traffic scenarios. The framework consists of two decoupled core modules, Collaborative Scene Completion (CSC) and Joint Perception and Prediction (P&P) module, which simplify practical deployment and enhance scalability. Additionally, we outline the challenges in Co-P&P and discuss future directions for this research area.
<div id='section'>Paperid: <span id='pid'>939, <a href='https://arxiv.org/pdf/2412.20104.pdf' target='_blank'>https://arxiv.org/pdf/2412.20104.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenkun He, Yun Liu, Ruitao Liu, Li Yi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.20104">SyncDiff: Synchronized Motion Diffusion for Multi-Body Human-Object Interaction Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Synthesizing realistic human-object interaction motions is a critical problem in VR/AR and human animation. Unlike the commonly studied scenarios involving a single human or hand interacting with one object, we address a more generic multi-body setting with arbitrary numbers of humans, hands, and objects. This complexity introduces significant challenges in synchronizing motions due to the high correlations and mutual influences among bodies. To address these challenges, we introduce SyncDiff, a novel method for multi-body interaction synthesis using a synchronized motion diffusion strategy. SyncDiff employs a single diffusion model to capture the joint distribution of multi-body motions. To enhance motion fidelity, we propose a frequency-domain motion decomposition scheme. Additionally, we introduce a new set of alignment scores to emphasize the synchronization of different body motions. SyncDiff jointly optimizes both data sample likelihood and alignment likelihood through an explicit synchronization strategy. Extensive experiments across four datasets with various multi-body configurations demonstrate the superiority of SyncDiff over existing state-of-the-art motion synthesis methods.
<div id='section'>Paperid: <span id='pid'>940, <a href='https://arxiv.org/pdf/2412.15664.pdf' target='_blank'>https://arxiv.org/pdf/2412.15664.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaohan Zhang, Sebastian Starke, Vladimir Guzov, Zhensong Zhang, Eduardo PÃ©rez Pellitero, Gerard Pons-Moll
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.15664">SCENIC: Scene-aware Semantic Navigation with Instruction-guided Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Synthesizing natural human motion that adapts to complex environments while allowing creative control remains a fundamental challenge in motion synthesis. Existing models often fall short, either by assuming flat terrain or lacking the ability to control motion semantics through text. To address these limitations, we introduce SCENIC, a diffusion model designed to generate human motion that adapts to dynamic terrains within virtual scenes while enabling semantic control through natural language. The key technical challenge lies in simultaneously reasoning about complex scene geometry while maintaining text control. This requires understanding both high-level navigation goals and fine-grained environmental constraints. The model must ensure physical plausibility and precise navigation across varied terrain, while also preserving user-specified text control, such as ``carefully stepping over obstacles" or ``walking upstairs like a zombie." Our solution introduces a hierarchical scene reasoning approach. At its core is a novel scene-dependent, goal-centric canonicalization that handles high-level goal constraint, and is complemented by an ego-centric distance field that captures local geometric details. This dual representation enables our model to generate physically plausible motion across diverse 3D scenes. By implementing frame-wise text alignment, our system achieves seamless transitions between different motion styles while maintaining scene constraints. Experiments demonstrate our novel diffusion model generates arbitrarily long human motions that both adapt to complex scenes with varying terrain surfaces and respond to textual prompts. Additionally, we show SCENIC can generalize to four real-scene datasets. Our code, dataset, and models will be released at \url{https://virtualhumans.mpi-inf.mpg.de/scenic/}.
<div id='section'>Paperid: <span id='pid'>941, <a href='https://arxiv.org/pdf/2409.14103.pdf' target='_blank'>https://arxiv.org/pdf/2409.14103.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kanghao Chen, Zeyu Wang, Lin Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.14103">ExFMan: Rendering 3D Dynamic Humans with Hybrid Monocular Blurry Frames and Events</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent years have witnessed tremendous progress in the 3D reconstruction of dynamic humans from a monocular video with the advent of neural rendering techniques. This task has a wide range of applications, including the creation of virtual characters for virtual reality (VR) environments. However, it is still challenging to reconstruct clear humans when the monocular video is affected by motion blur, particularly caused by rapid human motion (e.g., running, dancing), as often occurs in the wild. This leads to distinct inconsistency of shape and appearance for the rendered 3D humans, especially in the blurry regions with rapid motion, e.g., hands and legs. In this paper, we propose ExFMan, the first neural rendering framework that unveils the possibility of rendering high-quality humans in rapid motion with a hybrid frame-based RGB and bio-inspired event camera. The ``out-of-the-box'' insight is to leverage the high temporal information of event data in a complementary manner and adaptively reweight the effect of losses for both RGB frames and events in the local regions, according to the velocity of the rendered human. This significantly mitigates the inconsistency associated with motion blur in the RGB frames. Specifically, we first formulate a velocity field of the 3D body in the canonical space and render it to image space to identify the body parts with motion blur. We then propose two novel losses, i.e., velocity-aware photometric loss and velocity-relative event loss, to optimize the neural human for both modalities under the guidance of the estimated velocity. In addition, we incorporate novel pose regularization and alpha losses to facilitate continuous pose and clear boundary. Extensive experiments on synthetic and real-world datasets demonstrate that ExFMan can reconstruct sharper and higher quality humans.
<div id='section'>Paperid: <span id='pid'>942, <a href='https://arxiv.org/pdf/2409.13426.pdf' target='_blank'>https://arxiv.org/pdf/2409.13426.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vladimir Guzov, Yifeng Jiang, Fangzhou Hong, Gerard Pons-Moll, Richard Newcombe, C. Karen Liu, Yuting Ye, Lingni Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.13426">HMD^2: Environment-aware Motion Generation from Single Egocentric Head-Mounted Device</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper investigates the generation of realistic full-body human motion using a single head-mounted device with an outward-facing color camera and the ability to perform visual SLAM. To address the ambiguity of this setup, we present HMD^2, a novel system that balances motion reconstruction and generation. From a reconstruction standpoint, it aims to maximally utilize the camera streams to produce both analytical and learned features, including head motion, SLAM point cloud, and image embeddings. On the generative front, HMD^2 employs a multi-modal conditional motion diffusion model with a Transformer backbone to maintain temporal coherence of generated motions, and utilizes autoregressive inpainting to facilitate online motion inference with minimal latency (0.17 seconds). We show that our system provides an effective and robust solution that scales to a diverse dataset of over 200 hours of motion in complex indoor and outdoor environments.
<div id='section'>Paperid: <span id='pid'>943, <a href='https://arxiv.org/pdf/2409.04639.pdf' target='_blank'>https://arxiv.org/pdf/2409.04639.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sylvain Bertrand, Luigi Penco, Dexton Anderson, Duncan Calvert, Valentine Roy, Stephen McCrory, Khizar Mohammed, Sebastian Sanchez, Will Griffith, Steve Morfey, Alexis Maslyczyk, Achintya Mohan, Cody Castello, Bingyin Ma, Kartik Suryavanshi, Patrick Dills, Jerry Pratt, Victor Ragusila, Brandon Shrewsbury, Robert Griffin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.04639">High-Speed and Impact Resilient Teleoperation of Humanoid Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Teleoperation of humanoid robots has long been a challenging domain, necessitating advances in both hardware and software to achieve seamless and intuitive control. This paper presents an integrated solution based on several elements: calibration-free motion capture and retargeting, low-latency fast whole-body kinematics streaming toolbox and high-bandwidth cycloidal actuators. Our motion retargeting approach stands out for its simplicity, requiring only 7 IMUs to generate full-body references for the robot. The kinematics streaming toolbox, ensures real-time, responsive control of the robot's movements, significantly reducing latency and enhancing operational efficiency. Additionally, the use of cycloidal actuators makes it possible to withstand high speeds and impacts with the environment. Together, these approaches contribute to a teleoperation framework that offers unprecedented performance. Experimental results on the humanoid robot Nadia demonstrate the effectiveness of the integrated system.
<div id='section'>Paperid: <span id='pid'>944, <a href='https://arxiv.org/pdf/2406.19353.pdf' target='_blank'>https://arxiv.org/pdf/2406.19353.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yun Liu, Chengwen Zhang, Ruofan Xing, Bingda Tang, Bowen Yang, Li Yi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.19353">CORE4D: A 4D Human-Object-Human Interaction Dataset for Collaborative Object REarrangement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding how humans cooperatively rearrange household objects is critical for VR/AR and human-robot interaction. However, in-depth studies on modeling these behaviors are under-researched due to the lack of relevant datasets. We fill this gap by presenting CORE4D, a novel large-scale 4D human-object-human interaction dataset focusing on collaborative object rearrangement, which encompasses diverse compositions of various object geometries, collaboration modes, and 3D scenes. With 1K human-object-human motion sequences captured in the real world, we enrich CORE4D by contributing an iterative collaboration retargeting strategy to augment motions to a variety of novel objects. Leveraging this approach, CORE4D comprises a total of 11K collaboration sequences spanning 3K real and virtual object shapes. Benefiting from extensive motion patterns provided by CORE4D, we benchmark two tasks aiming at generating human-object interaction: human-object motion forecasting and interaction synthesis. Extensive experiments demonstrate the effectiveness of our collaboration retargeting strategy and indicate that CORE4D has posed new challenges to existing human-object interaction generation methodologies.
<div id='section'>Paperid: <span id='pid'>945, <a href='https://arxiv.org/pdf/2404.10685.pdf' target='_blank'>https://arxiv.org/pdf/2404.10685.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongwei Yi, Justus Thies, Michael J. Black, Xue Bin Peng, Davis Rempe
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.10685">Generating Human Interaction Motions in Scenes with Text Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present TeSMo, a method for text-controlled scene-aware motion generation based on denoising diffusion models. Previous text-to-motion methods focus on characters in isolation without considering scenes due to the limited availability of datasets that include motion, text descriptions, and interactive scenes. Our approach begins with pre-training a scene-agnostic text-to-motion diffusion model, emphasizing goal-reaching constraints on large-scale motion-capture datasets. We then enhance this model with a scene-aware component, fine-tuned using data augmented with detailed scene information, including ground plane and object shapes. To facilitate training, we embed annotated navigation and interaction motions within scenes. The proposed method produces realistic and diverse human-object interactions, such as navigation and sitting, in different scenes with various object shapes, orientations, initial body positions, and poses. Extensive experiments demonstrate that our approach surpasses prior techniques in terms of the plausibility of human-scene interactions, as well as the realism and variety of the generated motions. Code will be released upon publication of this work at https://research.nvidia.com/labs/toronto-ai/tesmo.
<div id='section'>Paperid: <span id='pid'>946, <a href='https://arxiv.org/pdf/2403.19026.pdf' target='_blank'>https://arxiv.org/pdf/2403.19026.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weizhuo Wang, C. Karen Liu, Monroe Kennedy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.19026">EgoNav: Egocentric Scene-aware Human Trajectory Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Wearable collaborative robots stand to assist human wearers who need fall prevention assistance or wear exoskeletons. Such a robot needs to be able to constantly adapt to the surrounding scene based on egocentric vision, and predict the ego motion of the wearer. In this work, we leveraged body-mounted cameras and sensors to anticipate the trajectory of human wearers through complex surroundings. To facilitate research in ego-motion prediction, we have collected a comprehensive walking scene navigation dataset centered on the user's perspective. We then present a method to predict human motion conditioning on the surrounding static scene. Our method leverages a diffusion model to produce a distribution of potential future trajectories, taking into account the user's observation of the environment. To that end, we introduce a compact representation to encode the user's visual memory of the surroundings, as well as an efficient sample-generating technique to speed up real-time inference of a diffusion model. We ablate our model and compare it to baselines, and results show that our model outperforms existing methods on key metrics of collision avoidance and trajectory mode coverage.
<div id='section'>Paperid: <span id='pid'>947, <a href='https://arxiv.org/pdf/2403.11237.pdf' target='_blank'>https://arxiv.org/pdf/2403.11237.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaohan Zhang, Bharat Lal Bhatnagar, Sebastian Starke, Ilya Petrov, Vladimir Guzov, Helisa Dhamo, Eduardo PÃ©rez-Pellitero, Gerard Pons-Moll
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.11237">FORCE: Physics-aware Human-object Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Interactions between human and objects are influenced not only by the object's pose and shape, but also by physical attributes such as object mass and surface friction. They introduce important motion nuances that are essential for diversity and realism. Despite advancements in recent human-object interaction methods, this aspect has been overlooked. Generating nuanced human motion presents two challenges. First, it is non-trivial to learn from multi-modal human and object information derived from both the physical and non-physical attributes. Second, there exists no dataset capturing nuanced human interactions with objects of varying physical properties, hampering model development. This work addresses the gap by introducing the FORCE model, an approach for synthesizing diverse, nuanced human-object interactions by modeling physical attributes. Our key insight is that human motion is dictated by the interrelation between the force exerted by the human and the perceived resistance. Guided by a novel intuitive physics encoding, the model captures the interplay between human force and resistance. Experiments also demonstrate incorporating human force facilitates learning multi-class motion. Accompanying our model, we contribute a dataset, which features diverse, different-styled motion through interactions with varying resistances.
<div id='section'>Paperid: <span id='pid'>948, <a href='https://arxiv.org/pdf/2402.14780.pdf' target='_blank'>https://arxiv.org/pdf/2402.14780.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yixuan Ren, Yang Zhou, Jimei Yang, Jing Shi, Difan Liu, Feng Liu, Mingi Kwon, Abhinav Shrivastava
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.14780">Customize-A-Video: One-Shot Motion Customization of Text-to-Video Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image customization has been extensively studied in text-to-image (T2I) diffusion models, leading to impressive outcomes and applications. With the emergence of text-to-video (T2V) diffusion models, its temporal counterpart, motion customization, has not yet been well investigated. To address the challenge of one-shot video motion customization, we propose Customize-A-Video that models the motion from a single reference video and adapts it to new subjects and scenes with both spatial and temporal varieties. It leverages low-rank adaptation (LoRA) on temporal attention layers to tailor the pre-trained T2V diffusion model for specific motion modeling. To disentangle the spatial and temporal information during training, we introduce a novel concept of appearance absorbers that detach the original appearance from the reference video prior to motion learning. The proposed modules are trained in a staged pipeline and inferred in a plug-and-play fashion, enabling easy extensions to various downstream tasks such as custom video generation and editing, video appearance customization and multiple motion combination. Our project page can be found at https://customize-a-video.github.io.
<div id='section'>Paperid: <span id='pid'>949, <a href='https://arxiv.org/pdf/2401.05365.pdf' target='_blank'>https://arxiv.org/pdf/2401.05365.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cheng Guo, Lorenzo Rapetti, Kourosh Darvish, Riccardo Grieco, Francesco Draicchio, Daniele Pucci
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.05365">Online Action Recognition for Human Risk Prediction with Anticipated Haptic Alert via Wearables</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper proposes a framework that combines online human state estimation, action recognition and motion prediction to enable early assessment and prevention of worker biomechanical risk during lifting tasks. The framework leverages the NIOSH index to perform online risk assessment, thus fitting real-time applications. In particular, the human state is retrieved via inverse kinematics/dynamics algorithms from wearable sensor data. Human action recognition and motion prediction are achieved by implementing an LSTM-based Guided Mixture of Experts architecture, which is trained offline and inferred online. With the recognized actions, a single lifting activity is divided into a series of continuous movements and the Revised NIOSH Lifting Equation can be applied for risk assessment. Moreover, the predicted motions enable anticipation of future risks. A haptic actuator, embedded in the wearable system, can alert the subject of potential risk, acting as an active prevention device. The performance of the proposed framework is validated by executing real lifting tasks, while the subject is equipped with the iFeel wearable system.
<div id='section'>Paperid: <span id='pid'>950, <a href='https://arxiv.org/pdf/2512.22214.pdf' target='_blank'>https://arxiv.org/pdf/2512.22214.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Naichuan Zheng, Xiahai Lun, Weiyi Li, Yuchen Du
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.22214">Signal-SGN++: Topology-Enhanced Time-Frequency Spiking Graph Network for Skeleton-Based Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Graph Convolutional Networks (GCNs) demonstrate strong capability in modeling skeletal topology for action recognition, yet their dense floating-point computations incur high energy costs. Spiking Neural Networks (SNNs), characterized by event-driven and sparse activation, offer energy efficiency but remain limited in capturing coupled temporal-frequency and topological dependencies of human motion. To bridge this gap, this article proposes Signal-SGN++, a topology-aware spiking graph framework that integrates structural adaptivity with time-frequency spiking dynamics. The network employs a backbone composed of 1D Spiking Graph Convolution (1D-SGC) and Frequency Spiking Convolution (FSC) for joint spatiotemporal and spectral feature extraction. Within this backbone, a Topology-Shift Self-Attention (TSSA) mechanism is embedded to adaptively route attention across learned skeletal topologies, enhancing graph-level sensitivity without increasing computational complexity. Moreover, an auxiliary Multi-Scale Wavelet Transform Fusion (MWTF) branch decomposes spiking features into multi-resolution temporal-frequency representations, wherein a Topology-Aware Time-Frequency Fusion (TATF) unit incorporates structural priors to preserve topology-consistent spectral fusion. Comprehensive experiments on large-scale benchmarks validate that Signal-SGN++ achieves superior accuracy-efficiency trade-offs, outperforming existing SNN-based methods and achieving competitive results against state-of-the-art GCNs under substantially reduced energy consumption.
<div id='section'>Paperid: <span id='pid'>951, <a href='https://arxiv.org/pdf/2512.21209.pdf' target='_blank'>https://arxiv.org/pdf/2512.21209.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Siqi Zhu, Yixuan Li, Junfu Li, Qi Wu, Zan Wang, Haozhe Ma, Wei Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.21209">Human Motion Estimation with Everyday Wearables</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While on-body device-based human motion estimation is crucial for applications such as XR interaction, existing methods often suffer from poor wearability, expensive hardware, and cumbersome calibration, which hinder their adoption in daily life. To address these challenges, we present EveryWear, a lightweight and practical human motion capture approach based entirely on everyday wearables: a smartphone, smartwatch, earbuds, and smart glasses equipped with one forward-facing and two downward-facing cameras, requiring no explicit calibration before use. We introduce Ego-Elec, a 9-hour real-world dataset covering 56 daily activities across 17 diverse indoor and outdoor environments, with ground-truth 3D annotations provided by the motion capture (MoCap), to facilitate robust research and benchmarking in this direction. Our approach employs a multimodal teacher-student framework that integrates visual cues from egocentric cameras with inertial signals from consumer devices. By training directly on real-world data rather than synthetic data, our model effectively eliminates the sim-to-real gap that constrains prior work. Experiments demonstrate that our method outperforms baseline models, validating its effectiveness for practical full-body motion estimation.
<div id='section'>Paperid: <span id='pid'>952, <a href='https://arxiv.org/pdf/2512.19927.pdf' target='_blank'>https://arxiv.org/pdf/2512.19927.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alexey Yermakov, Yue Zhao, Marine Denolle, Yiyu Ni, Philippe M. Wyder, Judah Goldfeder, Stefano Riva, Jan Williams, David Zoro, Amy Sara Rude, Matteo Tomasetto, Joe Germany, Joseph Bakarji, Georg Maierhofer, Miles Cranmer, J. Nathan Kutz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.19927">The Seismic Wavefield Common Task Framework</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Seismology faces fundamental challenges in state forecasting and reconstruction (e.g., earthquake early warning and ground motion prediction) and managing the parametric variability of source locations, mechanisms, and Earth models (e.g., subsurface structure and topography effects). Addressing these with simulations is hindered by their massive scale, both in synthetic data volumes and numerical complexity, while real-data efforts are constrained by models that inadequately reflect the Earth's complexity and by sparse sensor measurements from the field. Recent machine learning (ML) efforts offer promise, but progress is obscured by a lack of proper characterization, fair reporting, and rigorous comparisons. To address this, we introduce a Common Task Framework (CTF) for ML for seismic wavefields, starting with three distinct wavefield datasets. Our CTF features a curated set of datasets at various scales (global, crustal, and local) and task-specific metrics spanning forecasting, reconstruction, and generalization under realistic constraints such as noise and limited data. Inspired by CTFs in fields like natural language processing, this framework provides a structured and rigorous foundation for head-to-head algorithm evaluation. We illustrate the evaluation procedure with scores reported for two of the datasets, showcasing the performance of various methods and foundation models for reconstructing seismic wavefields from both simulated and real-world sensor measurements. The CTF scores reveal the strengths, limitations, and suitability for specific problem classes. Our vision is to replace ad hoc comparisons with standardized evaluations on hidden test sets, raising the bar for rigor and reproducibility in scientific ML.
<div id='section'>Paperid: <span id='pid'>953, <a href='https://arxiv.org/pdf/2512.17446.pdf' target='_blank'>https://arxiv.org/pdf/2512.17446.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chunggi Lee, Ut Gong, Tica Lin, Stefanie Zollmann, Scott A Epsley, Adam Petway, Hanspeter Pfister
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.17446">VAIR: Visual Analytics for Injury Risk Exploration in Sports</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Injury prevention in sports requires understanding how bio-mechanical risks emerge from movement patterns captured in real-world scenarios. However, identifying and interpreting injury prone events from raw video remains difficult and time-consuming. We present VAIR, a visual analytics system that supports injury risk analysis using 3D human motion reconstructed from sports video. VAIR combines pose estimation, bio-mechanical simulation, and synchronized visualizations to help users explore how joint-level risk indicators evolve over time. Domain experts can inspect movement segments through temporally aligned joint angles, angular velocity, and internal forces to detect patterns associated with known injury mechanisms. Through case studies involving Achilles tendon and Anterior cruciate ligament (ACL) injuries in basketball, we show that VAIR enables more efficient identification and interpretation of risky movements. Expert feedback confirms that VAIR improves diagnostic reasoning and supports both retrospective analysis and proactive intervention planning.
<div id='section'>Paperid: <span id='pid'>954, <a href='https://arxiv.org/pdf/2512.14234.pdf' target='_blank'>https://arxiv.org/pdf/2512.14234.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Juze Zhang, Changan Chen, Xin Chen, Heng Yu, Tiange Xiang, Ali Sartaz Khan, Shrinidhi K. Lakshmikanth, Ehsan Adeli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.14234">ViBES: A Conversational Agent with Behaviorally-Intelligent 3D Virtual Body</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human communication is inherently multimodal and social: words, prosody, and body language jointly carry intent. Yet most prior systems model human behavior as a translation task co-speech gesture or text-to-motion that maps a fixed utterance to motion clips-without requiring agentic decision-making about when to move, what to do, or how to adapt across multi-turn dialogue. This leads to brittle timing, weak social grounding, and fragmented stacks where speech, text, and motion are trained or inferred in isolation. We introduce ViBES (Voice in Behavioral Expression and Synchrony), a conversational 3D agent that jointly plans language and movement and executes dialogue-conditioned body actions. Concretely, ViBES is a speech-language-behavior (SLB) model with a mixture-of-modality-experts (MoME) backbone: modality-partitioned transformer experts for speech, facial expression, and body motion. The model processes interleaved multimodal token streams with hard routing by modality (parameters are split per expert), while sharing information through cross-expert attention. By leveraging strong pretrained speech-language models, the agent supports mixed-initiative interaction: users can speak, type, or issue body-action directives mid-conversation, and the system exposes controllable behavior hooks for streaming responses. We further benchmark on multi-turn conversation with automatic metrics of dialogue-motion alignment and behavior quality, and observe consistent gains over strong co-speech and text-to-motion baselines. ViBES goes beyond "speech-conditioned motion generation" toward agentic virtual bodies where language, prosody, and movement are jointly generated, enabling controllable, socially competent 3D interaction. Code and data will be made available at: ai.stanford.edu/~juze/ViBES/
<div id='section'>Paperid: <span id='pid'>955, <a href='https://arxiv.org/pdf/2512.11894.pdf' target='_blank'>https://arxiv.org/pdf/2512.11894.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mahathir Monjur, Shahriar Nirjon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.11894">mmWEAVER: Environment-Specific mmWave Signal Synthesis from a Photo and Activity Description</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Realistic signal generation and dataset augmentation are essential for advancing mmWave radar applications such as activity recognition and pose estimation, which rely heavily on diverse, and environment-specific signal datasets. However, mmWave signals are inherently complex, sparse, and high-dimensional, making physical simulation computationally expensive. This paper presents mmWeaver, a novel framework that synthesizes realistic, environment-specific complex mmWave signals by modeling them as continuous functions using Implicit Neural Representations (INRs), achieving up to 49-fold compression. mmWeaver incorporates hypernetworks that dynamically generate INR parameters based on environmental context (extracted from RGB-D images) and human motion features (derived from text-to-pose generation via MotionGPT), enabling efficient and adaptive signal synthesis. By conditioning on these semantic and geometric priors, mmWeaver generates diverse I/Q signals at multiple resolutions, preserving phase information critical for downstream tasks such as point cloud estimation and activity classification. Extensive experiments show that mmWeaver achieves a complex SSIM of 0.88 and a PSNR of 35 dB, outperforming existing methods in signal realism while improving activity recognition accuracy by up to 7% and reducing human pose estimation error by up to 15%, all while operating 6-35 times faster than simulation-based approaches.
<div id='section'>Paperid: <span id='pid'>956, <a href='https://arxiv.org/pdf/2511.10079.pdf' target='_blank'>https://arxiv.org/pdf/2511.10079.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yizheng Wang, Timon Rabczuk, Yinghua Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.10079">Physics-informed Machine Learning for Static Friction Modeling in Robotic Manipulators Based on Kolmogorov-Arnold Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Friction modeling plays a crucial role in achieving high-precision motion control in robotic operating systems. Traditional static friction models (such as the Stribeck model) are widely used due to their simple forms; however, they typically require predefined functional assumptions, which poses significant challenges when dealing with unknown functional structures. To address this issue, this paper proposes a physics-inspired machine learning approach based on the Kolmogorov Arnold Network (KAN) for static friction modeling of robotic joints. The method integrates spline activation functions with a symbolic regression mechanism, enabling model simplification and physical expression extraction through pruning and attribute scoring, while maintaining both high prediction accuracy and interpretability. We first validate the method's capability to accurately identify key parameters under known functional models, and further demonstrate its robustness and generalization ability under conditions with unknown functional structures and noisy data. Experiments conducted on both synthetic data and real friction data collected from a six-degree-of-freedom industrial manipulator show that the proposed method achieves a coefficient of determination greater than 0.95 across various tasks and successfully extracts concise and physically meaningful friction expressions. This study provides a new perspective for interpretable and data-driven robotic friction modeling with promising engineering applicability.
<div id='section'>Paperid: <span id='pid'>957, <a href='https://arxiv.org/pdf/2510.16258.pdf' target='_blank'>https://arxiv.org/pdf/2510.16258.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Claire McLean, Makenzie Meendering, Tristan Swartz, Orri Gabbay, Alexandra Olsen, Rachel Jacobs, Nicholas Rosen, Philippe de Bree, Tony Garcia, Gadsden Merrill, Jake Sandakly, Julia Buffalini, Neham Jain, Steven Krenn, Moneish Kumar, Dejan Markovic, Evonne Ng, Fabian Prada, Andrew Saba, Siwei Zhang, Vasu Agrawal, Tim Godisart, Alexander Richard, Michael Zollhoefer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.16258">Embody 3D: A Large-scale Multimodal Motion and Behavior Dataset</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Codec Avatars Lab at Meta introduces Embody 3D, a multimodal dataset of 500 individual hours of 3D motion data from 439 participants collected in a multi-camera collection stage, amounting to over 54 million frames of tracked 3D motion. The dataset features a wide range of single-person motion data, including prompted motions, hand gestures, and locomotion; as well as multi-person behavioral and conversational data like discussions, conversations in different emotional states, collaborative activities, and co-living scenarios in an apartment-like space. We provide tracked human motion including hand tracking and body shape, text annotations, and a separate audio track for each participant.
<div id='section'>Paperid: <span id='pid'>958, <a href='https://arxiv.org/pdf/2510.15342.pdf' target='_blank'>https://arxiv.org/pdf/2510.15342.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Joshua Li, Brendan Chharawala, Chang Shu, Xue Bin Peng, Pengcheng Xi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.15342">SHARE: Scene-Human Aligned Reconstruction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Animating realistic character interactions with the surrounding environment is important for autonomous agents in gaming, AR/VR, and robotics. However, current methods for human motion reconstruction struggle with accurately placing humans in 3D space. We introduce Scene-Human Aligned REconstruction (SHARE), a technique that leverages the scene geometry's inherent spatial cues to accurately ground human motion reconstruction. Each reconstruction relies solely on a monocular RGB video from a stationary camera. SHARE first estimates a human mesh and segmentation mask for every frame, alongside a scene point map at keyframes. It iteratively refines the human's positions at these keyframes by comparing the human mesh against the human point map extracted from the scene using the mask. Crucially, we also ensure that non-keyframe human meshes remain consistent by preserving their relative root joint positions to keyframe root joints during optimization. Our approach enables more accurate 3D human placement while reconstructing the surrounding scene, facilitating use cases on both curated datasets and in-the-wild web videos. Extensive experiments demonstrate that SHARE outperforms existing methods.
<div id='section'>Paperid: <span id='pid'>959, <a href='https://arxiv.org/pdf/2508.21095.pdf' target='_blank'>https://arxiv.org/pdf/2508.21095.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Thomas Besnier, Sylvain ArguillÃ¨re, Mohamed Daoudi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.21095">ScanMove: Motion Prediction and Transfer for Unregistered Body Meshes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unregistered surface meshes, especially raw 3D scans, present significant challenges for automatic computation of plausible deformations due to the lack of established point-wise correspondences and the presence of noise in the data. In this paper, we propose a new, rig-free, data-driven framework for motion prediction and transfer on such body meshes. Our method couples a robust motion embedding network with a learned per-vertex feature field to generate a spatio-temporal deformation field, which drives the mesh deformation. Extensive evaluations, including quantitative benchmarks and qualitative visuals on tasks such as walking and running, demonstrate the effectiveness and versatility of our approach on challenging unregistered meshes.
<div id='section'>Paperid: <span id='pid'>960, <a href='https://arxiv.org/pdf/2508.06093.pdf' target='_blank'>https://arxiv.org/pdf/2508.06093.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chen Zhu, Buzhen Huang, Zijing Wu, Binghui Zuo, Yangang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.06093">E-React: Towards Emotionally Controlled Synthesis of Human Reactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Emotion serves as an essential component in daily human interactions. Existing human motion generation frameworks do not consider the impact of emotions, which reduces naturalness and limits their application in interactive tasks, such as human reaction synthesis. In this work, we introduce a novel task: generating diverse reaction motions in response to different emotional cues. However, learning emotion representation from limited motion data and incorporating it into a motion generation framework remains a challenging problem. To address the above obstacles, we introduce a semi-supervised emotion prior in an actor-reactor diffusion model to facilitate emotion-driven reaction synthesis. Specifically, based on the observation that motion clips within a short sequence tend to share the same emotion, we first devise a semi-supervised learning framework to train an emotion prior. With this prior, we further train an actor-reactor diffusion model to generate reactions by considering both spatial interaction and emotional response. Finally, given a motion sequence of an actor, our approach can generate realistic reactions under various emotional conditions. Experimental results demonstrate that our model outperforms existing reaction generation methods. The code and data will be made publicly available at https://ereact.github.io/
<div id='section'>Paperid: <span id='pid'>961, <a href='https://arxiv.org/pdf/2508.02632.pdf' target='_blank'>https://arxiv.org/pdf/2508.02632.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Italo Napolitano, Stefano Covone, Andrea Lama, Francesco De Lellis, Mario di Bernardo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02632">Hierarchical Learning-Based Control for Multi-Agent Shepherding of Stochastic Autonomous Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent shepherding represents a challenging distributed control problem where herder agents must coordinate to guide independently moving targets to desired spatial configurations. Most existing control strategies assume cohesive target behavior, which frequently fails in practical applications where targets exhibit stochastic autonomous behavior. This paper presents a hierarchical learning-based control architecture that decomposes the shepherding problem into a high-level decision-making module and a low-level motion control component. The proposed distributed control system synthesizes effective control policies directly from closed-loop experience without requiring explicit inter-agent communication or prior knowledge of target dynamics. The decentralized architecture achieves cooperative control behavior through emergent coordination without centralized supervision. Experimental validation demonstrates superior closed-loop performance compared to state-of-the-art heuristic control methods, achieving 100\% success rates with improved settling times and control efficiency. The control architecture scales beyond its design conditions, adapts to time-varying goal regions, and demonstrates practical implementation feasibility through real-time experiments on the Robotarium platform.
<div id='section'>Paperid: <span id='pid'>962, <a href='https://arxiv.org/pdf/2508.02264.pdf' target='_blank'>https://arxiv.org/pdf/2508.02264.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Markus Buchholz, Ignacio Carlucho, Michele Grimaldi, Yvan R. Petillot
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02264">Tethered Multi-Robot Systems in Marine Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces a novel simulation framework for evaluating motion control in tethered multi-robot systems within dynamic marine environments. Specifically, it focuses on the coordinated operation of an Autonomous Underwater Vehicle (AUV) and an Autonomous Surface Vehicle(ASV). The framework leverages GazeboSim, enhanced with realistic marine environment plugins and ArduPilots SoftwareIn-The-Loop (SITL) mode, to provide a high-fidelity simulation platform. A detailed tether model, combining catenary equations and physical simulation, is integrated to accurately represent the dynamic interactions between the vehicles and the environment. This setup facilitates the development and testing of advanced control strategies under realistic conditions, demonstrating the frameworks capability to analyze complex tether interactions and their impact on system performance.
<div id='section'>Paperid: <span id='pid'>963, <a href='https://arxiv.org/pdf/2507.22792.pdf' target='_blank'>https://arxiv.org/pdf/2507.22792.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guoping Xu, Jayaram K. Udupa, Yajun Yu, Hua-Chieh Shao, Songlin Zhao, Wei Liu, You Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.22792">Segment Anything for Video: A Comprehensive Review of Video Object Segmentation and Tracking from Past to Future</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video Object Segmentation and Tracking (VOST) presents a complex yet critical challenge in computer vision, requiring robust integration of segmentation and tracking across temporally dynamic frames. Traditional methods have struggled with domain generalization, temporal consistency, and computational efficiency. The emergence of foundation models like the Segment Anything Model (SAM) and its successor, SAM2, has introduced a paradigm shift, enabling prompt-driven segmentation with strong generalization capabilities. Building upon these advances, this survey provides a comprehensive review of SAM/SAM2-based methods for VOST, structured along three temporal dimensions: past, present, and future. We examine strategies for retaining and updating historical information (past), approaches for extracting and optimizing discriminative features from the current frame (present), and motion prediction and trajectory estimation mechanisms for anticipating object dynamics in subsequent frames (future). In doing so, we highlight the evolution from early memory-based architectures to the streaming memory and real-time segmentation capabilities of SAM2. We also discuss recent innovations such as motion-aware memory selection and trajectory-guided prompting, which aim to enhance both accuracy and efficiency. Finally, we identify remaining challenges including memory redundancy, error accumulation, and prompt inefficiency, and suggest promising directions for future research. This survey offers a timely and structured overview of the field, aiming to guide researchers and practitioners in advancing the state of VOST through the lens of foundation models.
<div id='section'>Paperid: <span id='pid'>964, <a href='https://arxiv.org/pdf/2507.03227.pdf' target='_blank'>https://arxiv.org/pdf/2507.03227.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruoshi Wen, Jiajun Zhang, Guangzeng Chen, Zhongren Cui, Min Du, Yang Gou, Zhigang Han, Junkai Hu, Liqun Huang, Hao Niu, Wei Xu, Haoxiang Zhang, Zhengming Zhu, Hang Li, Zeyu Ren
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.03227">Dexterous Teleoperation of 20-DoF ByteDexter Hand via Human Motion Retargeting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Replicating human--level dexterity remains a fundamental robotics challenge, requiring integrated solutions from mechatronic design to the control of high degree--of--freedom (DoF) robotic hands. While imitation learning shows promise in transferring human dexterity to robots, the efficacy of trained policies relies on the quality of human demonstration data. We bridge this gap with a hand--arm teleoperation system featuring: (1) a 20--DoF linkage--driven anthropomorphic robotic hand for biomimetic dexterity, and (2) an optimization--based motion retargeting for real--time, high--fidelity reproduction of intricate human hand motions and seamless hand--arm coordination. We validate the system via extensive empirical evaluations, including dexterous in-hand manipulation tasks and a long--horizon task requiring the organization of a cluttered makeup table randomly populated with nine objects. Experimental results demonstrate its intuitive teleoperation interface with real--time control and the ability to generate high--quality demonstration data. Please refer to the accompanying video for further details.
<div id='section'>Paperid: <span id='pid'>965, <a href='https://arxiv.org/pdf/2506.20668.pdf' target='_blank'>https://arxiv.org/pdf/2506.20668.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sungjae Park, Homanga Bharadhwaj, Shubham Tulsiani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.20668">DemoDiffusion: One-Shot Human Imitation using pre-trained Diffusion Policy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose DemoDiffusion, a simple and scalable method for enabling robots to perform manipulation tasks in natural environments by imitating a single human demonstration. Our approach is based on two key insights. First, the hand motion in a human demonstration provides a useful prior for the robot's end-effector trajectory, which we can convert into a rough open-loop robot motion trajectory via kinematic retargeting. Second, while this retargeted motion captures the overall structure of the task, it may not align well with plausible robot actions in-context. To address this, we leverage a pre-trained generalist diffusion policy to modify the trajectory, ensuring it both follows the human motion and remains within the distribution of plausible robot actions. Our approach avoids the need for online reinforcement learning or paired human-robot data, enabling robust adaptation to new tasks and scenes with minimal manual effort. Experiments in both simulation and real-world settings show that DemoDiffusion outperforms both the base policy and the retargeted trajectory, enabling the robot to succeed even on tasks where the pre-trained generalist policy fails entirely. Project page: https://demodiffusion.github.io/
<div id='section'>Paperid: <span id='pid'>966, <a href='https://arxiv.org/pdf/2506.10574.pdf' target='_blank'>https://arxiv.org/pdf/2506.10574.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qing Wang, Xiaohang Yang, Yilan Dong, Naveen Raj Govindaraj, Gregory Slabaugh, Shanxin Yuan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.10574">DanceChat: Large Language Model-Guided Music-to-Dance Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Music-to-dance generation aims to synthesize human dance motion conditioned on musical input. Despite recent progress, significant challenges remain due to the semantic gap between music and dance motion, as music offers only abstract cues, such as melody, groove, and emotion, without explicitly specifying the physical movements. Moreover, a single piece of music can produce multiple plausible dance interpretations. This one-to-many mapping demands additional guidance, as music alone provides limited information for generating diverse dance movements. The challenge is further amplified by the scarcity of paired music and dance data, which restricts the modelÃ¢ÄÅ¹s ability to learn diverse dance patterns. In this paper, we introduce DanceChat, a Large Language Model (LLM)-guided music-to-dance generation approach. We use an LLM as a choreographer that provides textual motion instructions, offering explicit, high-level guidance for dance generation. This approach goes beyond implicit learning from music alone, enabling the model to generate dance that is both more diverse and better aligned with musical styles. Our approach consists of three components: (1) an LLM-based pseudo instruction generation module that produces textual dance guidance based on music style and structure, (2) a multi-modal feature extraction and fusion module that integrates music, rhythm, and textual guidance into a shared representation, and (3) a diffusion-based motion synthesis module together with a multi-modal alignment loss, which ensures that the generated dance is aligned with both musical and textual cues. Extensive experiments on AIST++ and human evaluations show that DanceChat outperforms state-of-the-art methods both qualitatively and quantitatively.
<div id='section'>Paperid: <span id='pid'>967, <a href='https://arxiv.org/pdf/2504.03639.pdf' target='_blank'>https://arxiv.org/pdf/2504.03639.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ting-Hsuan Liao, Yi Zhou, Yu Shen, Chun-Hao Paul Huang, Saayan Mitra, Jia-Bin Huang, Uttaran Bhattacharya
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.03639">Shape My Moves: Text-Driven Shape-Aware Synthesis of Human Motions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We explore how body shapes influence human motion synthesis, an aspect often overlooked in existing text-to-motion generation methods due to the ease of learning a homogenized, canonical body shape. However, this homogenization can distort the natural correlations between different body shapes and their motion dynamics. Our method addresses this gap by generating body-shape-aware human motions from natural language prompts. We utilize a finite scalar quantization-based variational autoencoder (FSQ-VAE) to quantize motion into discrete tokens and then leverage continuous body shape information to de-quantize these tokens back into continuous, detailed motion. Additionally, we harness the capabilities of a pretrained language model to predict both continuous shape parameters and motion tokens, facilitating the synthesis of text-aligned motions and decoding them into shape-aware motions. We evaluate our method quantitatively and qualitatively, and also conduct a comprehensive perceptual study to demonstrate its efficacy in generating shape-aware motions.
<div id='section'>Paperid: <span id='pid'>968, <a href='https://arxiv.org/pdf/2504.01019.pdf' target='_blank'>https://arxiv.org/pdf/2504.01019.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pablo Ruiz-Ponce, German Barquero, Cristina Palmero, Sergio Escalera, JosÃ© GarcÃ­a-RodrÃ­guez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.01019">MixerMDM: Learnable Composition of Human Motion Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating human motion guided by conditions such as textual descriptions is challenging due to the need for datasets with pairs of high-quality motion and their corresponding conditions. The difficulty increases when aiming for finer control in the generation. To that end, prior works have proposed to combine several motion diffusion models pre-trained on datasets with different types of conditions, thus allowing control with multiple conditions. However, the proposed merging strategies overlook that the optimal way to combine the generation processes might depend on the particularities of each pre-trained generative model and also the specific textual descriptions. In this context, we introduce MixerMDM, the first learnable model composition technique for combining pre-trained text-conditioned human motion diffusion models. Unlike previous approaches, MixerMDM provides a dynamic mixing strategy that is trained in an adversarial fashion to learn to combine the denoising process of each model depending on the set of conditions driving the generation. By using MixerMDM to combine single- and multi-person motion diffusion models, we achieve fine-grained control on the dynamics of every person individually, and also on the overall interaction. Furthermore, we propose a new evaluation technique that, for the first time in this task, measures the interaction and individual quality by computing the alignment between the mixed generated motions and their conditions as well as the capabilities of MixerMDM to adapt the mixing throughout the denoising process depending on the motions to mix.
<div id='section'>Paperid: <span id='pid'>969, <a href='https://arxiv.org/pdf/2503.19351.pdf' target='_blank'>https://arxiv.org/pdf/2503.19351.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingyu Liu, Zijie Xin, Yuhan Fu, Ruixiang Zhao, Bangxiang Lan, Xirong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.19351">Multi-Object Sketch Animation by Scene Decomposition and Motion Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Sketch animation, which brings static sketches to life by generating dynamic video sequences, has found widespread applications in GIF design, cartoon production, and daily entertainment. While current methods for sketch animation perform well in single-object sketch animation, they struggle in multi-object scenarios. By analyzing their failures, we identify two major challenges of transitioning from single-object to multi-object sketch animation: object-aware motion modeling and complex motion optimization. For multi-object sketch animation, we propose MoSketch based on iterative optimization through Score Distillation Sampling (SDS) and thus animating a multi-object sketch in a training-data free manner. To tackle the two challenges in a divide-and-conquer strategy, MoSketch has four novel modules, i.e., LLM-based scene decomposition, LLM-based motion planning, multi-grained motion refinement, and compositional SDS. Extensive qualitative and quantitative experiments demonstrate the superiority of our method over existing sketch animation approaches. MoSketch takes a pioneering step towards multi-object sketch animation, opening new avenues for future research and applications.
<div id='section'>Paperid: <span id='pid'>970, <a href='https://arxiv.org/pdf/2503.15819.pdf' target='_blank'>https://arxiv.org/pdf/2503.15819.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junyi Shen, Tetsuro Miyazaki, Kenji Kawashima
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.15819">Control Pneumatic Soft Bending Actuator with Online Learning Pneumatic Physical Reservoir Computing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The intrinsic nonlinearities of soft robots present significant control but simultaneously provide them with rich computational potential. Reservoir computing (RC) has shown effectiveness in online learning systems for controlling nonlinear systems such as soft actuators. Conventional RC can be extended into physical reservoir computing (PRC) by leveraging the nonlinear dynamics of soft actuators for computation. This paper introduces a PRC-based online learning framework to control the motion of a pneumatic soft bending actuator, utilizing another pneumatic soft actuator as the PRC model. Unlike conventional designs requiring two RC models, the proposed control system employs a more compact architecture with a single RC model. Additionally, the framework enables zero-shot online learning, addressing limitations of previous PRC-based control systems reliant on offline training. Simulations and experiments validated the performance of the proposed system. Experimental results indicate that the PRC model achieved superior control performance compared to a linear model, reducing the root-mean-square error (RMSE) by an average of over 37% in bending motion control tasks. The proposed PRC-based online learning control framework provides a novel approach for harnessing physical systems' inherent nonlinearities to enhance the control of soft actuators.
<div id='section'>Paperid: <span id='pid'>971, <a href='https://arxiv.org/pdf/2503.14736.pdf' target='_blank'>https://arxiv.org/pdf/2503.14736.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yilan Dong, Haohe Liu, Qing Wang, Jiahao Yang, Wenqing Wang, Gregory Slabaugh, Shanxin Yuan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.14736">HandSplat: Embedding-Driven Gaussian Splatting for High-Fidelity Hand Rendering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing 3D Gaussian Splatting (3DGS) methods for hand rendering rely on rigid skeletal motion with an oversimplified non-rigid motion model, which fails to capture fine geometric and appearance details. Additionally, they perform densification based solely on per-point gradients and process poses independently, ignoring spatial and temporal correlations. These limitations lead to geometric detail loss, temporal instability, and inefficient point distribution. To address these issues, we propose HandSplat, a novel Gaussian Splatting-based framework that enhances both fidelity and stability for hand rendering. To improve fidelity, we extend standard 3DGS attributes with implicit geometry and appearance embeddings for finer non-rigid motion modeling while preserving the static hand characteristic modeled by original 3DGS attributes. Additionally, we introduce a local gradient-aware densification strategy that dynamically refines Gaussian density in high-variation regions. To improve stability, we incorporate pose-conditioned attribute regularization to encourage attribute consistency across similar poses, mitigating temporal artifacts. Extensive experiments on InterHand2.6M demonstrate that HandSplat surpasses existing methods in fidelity and stability while achieving real-time performance. We will release the code and pre-trained models upon acceptance.
<div id='section'>Paperid: <span id='pid'>972, <a href='https://arxiv.org/pdf/2412.01747.pdf' target='_blank'>https://arxiv.org/pdf/2412.01747.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyun Wang, Ruijun Zhang, Zi-Yan Liu, Yufu Wang, Kostas Daniilidis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.01747">Continuous-Time Human Motion Field from Events</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper addresses the challenges of estimating a continuous-time human motion field from a stream of events. Existing Human Mesh Recovery (HMR) methods rely predominantly on frame-based approaches, which are prone to aliasing and inaccuracies due to limited temporal resolution and motion blur. In this work, we predict a continuous-time human motion field directly from events by leveraging a recurrent feed-forward neural network to predict human motion in the latent space of possible human motions. Prior state-of-the-art event-based methods rely on computationally intensive optimization across a fixed number of poses at high frame rates, which becomes prohibitively expensive as we increase the temporal resolution. In comparison, we present the first work that replaces traditional discrete-time predictions with a continuous human motion field represented as a time-implicit function, enabling parallel pose queries at arbitrary temporal resolutions. Despite the promises of event cameras, few benchmarks have tested the limit of high-speed human motion estimation. We introduce Beam-splitter Event Agile Human Motion Dataset-a hardware-synchronized high-speed human dataset to fill this gap. On this new data, our method improves joint errors by 23.8% compared to previous event human methods while reducing the computational time by 69%.
<div id='section'>Paperid: <span id='pid'>973, <a href='https://arxiv.org/pdf/2411.03289.pdf' target='_blank'>https://arxiv.org/pdf/2411.03289.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ananya Trivedi, Sarvesh Prajapati, Anway Shirgaonkar, Mark Zolotas, Taskin Padir
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.03289">Data-Driven Sampling Based Stochastic MPC for Skid-Steer Mobile Robot Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Traditional approaches to motion modeling for skid-steer robots struggle with capturing nonlinear tire-terrain dynamics, especially during high-speed maneuvers. In this paper, we tackle such nonlinearities by enhancing a dynamic unicycle model with Gaussian Process (GP) regression outputs. This enables us to develop an adaptive, uncertainty-informed navigation formulation. We solve the resultant stochastic optimal control problem using a chance-constrained Model Predictive Path Integral (MPPI) control method. This approach formulates both obstacle avoidance and path-following as chance constraints, accounting for residual uncertainties from the GP to ensure safety and reliability in control. Leveraging GPU acceleration, we efficiently manage the non-convex nature of the problem, ensuring real-time performance. Our approach unifies path-following and obstacle avoidance across different terrains, unlike prior works which typically focus on one or the other. We compare our GP-MPPI method against unicycle and data-driven kinematic models within the MPPI framework. In simulations, our approach shows superior tracking accuracy and obstacle avoidance. We further validate our approach through hardware experiments on a skid-steer robot platform, demonstrating its effectiveness in high-speed navigation. The GPU implementation of the proposed method and supplementary video footage are available at https: //stochasticmppi.github.io.
<div id='section'>Paperid: <span id='pid'>974, <a href='https://arxiv.org/pdf/2410.10802.pdf' target='_blank'>https://arxiv.org/pdf/2410.10802.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Soon Yau Cheong, Duygu Ceylan, Armin Mustafa, Andrew Gilbert, Chun-Hao Paul Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.10802">Boosting Camera Motion Control for Video Diffusion Transformers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in diffusion models have significantly enhanced the quality of video generation. However, fine-grained control over camera pose remains a challenge. While U-Net-based models have shown promising results for camera control, transformer-based diffusion models (DiT)-the preferred architecture for large-scale video generation - suffer from severe degradation in camera motion accuracy. In this paper, we investigate the underlying causes of this issue and propose solutions tailored to DiT architectures. Our study reveals that camera control performance depends heavily on the choice of conditioning methods rather than camera pose representations that is commonly believed. To address the persistent motion degradation in DiT, we introduce Camera Motion Guidance (CMG), based on classifier-free guidance, which boosts camera control by over 400%. Additionally, we present a sparse camera control pipeline, significantly simplifying the process of specifying camera poses for long videos. Our method universally applies to both U-Net and DiT models, offering improved camera control for video generation tasks.
<div id='section'>Paperid: <span id='pid'>975, <a href='https://arxiv.org/pdf/2410.05628.pdf' target='_blank'>https://arxiv.org/pdf/2410.05628.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jeongeun Park, Sungjoon Choi, Sangdoo Yun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.05628">A Unified Framework for Motion Reasoning and Generation in Human Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in large language models (LLMs) have significantly improved their ability to generate natural and contextually relevant text, enabling more human-like AI interactions. However, generating and understanding interactive human-like motion, where multiple individuals engage in coordinated movements, remains challenging due to the complexity of modeling these interactions. Additionally, a unified and versatile model is needed to handle diverse interactive scenarios, such as chat systems that dynamically adapt to user instructions and assigned roles. To address these challenges, we introduce VIM, the Versatile Interactive Motion-language model, which integrates both language and motion modalities to effectively understand, generate, and control interactive motions in multi-turn conversational contexts. Unlike previous studies that primarily focus on uni-directional tasks such as text-to-motion or motion-to-text, VIM employs a unified architecture capable of simultaneously understanding and generating both motion and text modalities. Given the absence of an appropriate dataset to support this task, we introduce Inter-MT2, a large-scale instruction-tuning dataset containing 82.7K multi-turn interactive motion instructions, covering 153K interactive motion samples. Inter-MT2 spans diverse instructional scenarios, including motion editing, question answering, and story generation, leveraging off-the-shelf large language models and motion diffusion models to construct a broad set of interactive motion instructions. We extensively evaluate the versatility of VIM across multiple interactive motion-related tasks, including motion-to-text, text-to-motion, reaction generation, motion editing, and reasoning about motion sequences.
<div id='section'>Paperid: <span id='pid'>976, <a href='https://arxiv.org/pdf/2408.03302.pdf' target='_blank'>https://arxiv.org/pdf/2408.03302.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Siyuan Fan, Bo Du, Xiantao Cai, Bo Peng, Longling Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.03302">TextIM: Part-aware Interactive Motion Synthesis from Text</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we propose TextIM, a novel framework for synthesizing TEXT-driven human Interactive Motions, with a focus on the precise alignment of part-level semantics. Existing methods often overlook the critical roles of interactive body parts and fail to adequately capture and align part-level semantics, resulting in inaccuracies and even erroneous movement outcomes. To address these issues, TextIM utilizes a decoupled conditional diffusion framework to enhance the detailed alignment between interactive movements and corresponding semantic intents from textual descriptions. Our approach leverages large language models, functioning as a human brain, to identify interacting human body parts and to comprehend interaction semantics to generate complicated and subtle interactive motion. Guided by the refined movements of the interacting parts, TextIM further extends these movements into a coherent whole-body motion. We design a spatial coherence module to complement the entire body movements while maintaining consistency and harmony across body parts using a part graph convolutional network. For training and evaluation, we carefully selected and re-labeled interactive motions from HUMANML3D to develop a specialized dataset. Experimental results demonstrate that TextIM produces semantically accurate human interactive motions, significantly enhancing the realism and applicability of synthesized interactive motions in diverse scenarios, even including interactions with deformable and dynamically changing objects.
<div id='section'>Paperid: <span id='pid'>977, <a href='https://arxiv.org/pdf/2407.19071.pdf' target='_blank'>https://arxiv.org/pdf/2407.19071.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Minjun Sung, Hunmin Kim, Naira Hovakimyan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.19071">Addressing Behavior Model Inaccuracies for Safe Motion Control in Uncertain Dynamic Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Uncertainties in the environment and behavior model inaccuracies compromise the state estimation of a dynamic obstacle and its trajectory predictions, introducing biases in estimation and shifts in predictive distributions. Addressing these challenges is crucial to safely control an autonomous system. In this paper, we propose a novel algorithm SIED-MPC, which synergistically integrates Simultaneous State and Input Estimation (SSIE) and Distributionally Robust Model Predictive Control (DR-MPC) using model confidence evaluation. The SSIE process produces unbiased state estimates and optimal input gap estimates to assess the confidence of the behavior model, defining the ambiguity radius for DR-MPC to handle predictive distribution shifts. This systematic confidence evaluation leads to producing safe inputs with an adequate level of conservatism. Our algorithm demonstrated a reduced collision rate in autonomous driving simulations through improved state estimation, with a 54% shorter average computation time.
<div id='section'>Paperid: <span id='pid'>978, <a href='https://arxiv.org/pdf/2407.14502.pdf' target='_blank'>https://arxiv.org/pdf/2407.14502.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Seunggeun Chi, Hyung-gun Chi, Hengbo Ma, Nakul Agarwal, Faizan Siddiqui, Karthik Ramani, Kwonjoon Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.14502">M2D2M: Multi-Motion Generation from Text with Discrete Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce the Multi-Motion Discrete Diffusion Models (M2D2M), a novel approach for human motion generation from textual descriptions of multiple actions, utilizing the strengths of discrete diffusion models. This approach adeptly addresses the challenge of generating multi-motion sequences, ensuring seamless transitions of motions and coherence across a series of actions. The strength of M2D2M lies in its dynamic transition probability within the discrete diffusion model, which adapts transition probabilities based on the proximity between motion tokens, encouraging mixing between different modes. Complemented by a two-phase sampling strategy that includes independent and joint denoising steps, M2D2M effectively generates long-term, smooth, and contextually coherent human motion sequences, utilizing a model trained for single-motion generation. Extensive experiments demonstrate that M2D2M surpasses current state-of-the-art benchmarks for motion generation from text descriptions, showcasing its efficacy in interpreting language semantics and generating dynamic, realistic motions.
<div id='section'>Paperid: <span id='pid'>979, <a href='https://arxiv.org/pdf/2407.08428.pdf' target='_blank'>https://arxiv.org/pdf/2407.08428.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wentao Lei, Jinting Wang, Fengji Ma, Guanjie Huang, Li Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.08428">A Comprehensive Survey on Human Video Generation: Challenges, Methods, and Insights</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human video generation is a dynamic and rapidly evolving task that aims to synthesize 2D human body video sequences with generative models given control conditions such as text, audio, and pose. With the potential for wide-ranging applications in film, gaming, and virtual communication, the ability to generate natural and realistic human video is critical. Recent advancements in generative models have laid a solid foundation for the growing interest in this area. Despite the significant progress, the task of human video generation remains challenging due to the consistency of characters, the complexity of human motion, and difficulties in their relationship with the environment. This survey provides a comprehensive review of the current state of human video generation, marking, to the best of our knowledge, the first extensive literature review in this domain. We start with an introduction to the fundamentals of human video generation and the evolution of generative models that have facilitated the field's growth. We then examine the main methods employed for three key sub-tasks within human video generation: text-driven, audio-driven, and pose-driven motion generation. These areas are explored concerning the conditions that guide the generation process. Furthermore, we offer a collection of the most commonly utilized datasets and the evaluation metrics that are crucial in assessing the quality and realism of generated videos. The survey concludes with a discussion of the current challenges in the field and suggests possible directions for future research. The goal of this survey is to offer the research community a clear and holistic view of the advancements in human video generation, highlighting the milestones achieved and the challenges that lie ahead.
<div id='section'>Paperid: <span id='pid'>980, <a href='https://arxiv.org/pdf/2405.08726.pdf' target='_blank'>https://arxiv.org/pdf/2405.08726.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yashuai Yan, Esteve Valls Mascaro, Tobias Egle, Dongheui Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.08726">I-CTRL: Imitation to Control Humanoid Robots Through Constrained Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humanoid robots have the potential to mimic human motions with high visual fidelity, yet translating these motions into practical, physical execution remains a significant challenge. Existing techniques in the graphics community often prioritize visual fidelity over physics-based feasibility, posing a significant challenge for deploying bipedal systems in practical applications. This paper addresses these issues through bounded residual reinforcement learning to produce physics-based high-quality motion imitation onto legged humanoid robots that enhance motion resemblance while successfully following the reference human trajectory. Our framework, Imitation to Control Humanoid Robots Through Bounded Residual Reinforcement Learning (I-CTRL), reformulates motion imitation as a constrained refinement over non-physics-based retargeted motions. I-CTRL excels in motion imitation with simple and unique rewards that generalize across five robots. Moreover, our framework introduces an automatic priority scheduler to manage large-scale motion datasets when efficiently training a unified RL policy across diverse motions. The proposed approach signifies a crucial step forward in advancing the control of bipedal robots, emphasizing the importance of aligning visual and physical realism for successful motion imitation.
<div id='section'>Paperid: <span id='pid'>981, <a href='https://arxiv.org/pdf/2404.07505.pdf' target='_blank'>https://arxiv.org/pdf/2404.07505.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Thies Oelerich, Christian Hartl-Nesic, Andreas Kugi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.07505">Model Predictive Trajectory Planning for Human-Robot Handovers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work develops a novel trajectory planner for human-robot handovers. The handover requirements can naturally be handled by a path-following-based model predictive controller, where the path progress serves as a progress measure of the handover. Moreover, the deviations from the path are used to follow human motion by adapting the path deviation bounds with a handover location prediction. A Gaussian process regression model, which is trained on known handover trajectories, is employed for this prediction. Experiments with a collaborative 7-DoF robotic manipulator show the effectiveness and versatility of the proposed approach.
<div id='section'>Paperid: <span id='pid'>982, <a href='https://arxiv.org/pdf/2404.06892.pdf' target='_blank'>https://arxiv.org/pdf/2404.06892.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Diankun Zhang, Guoan Wang, Runwen Zhu, Jianbo Zhao, Xiwu Chen, Siyu Zhang, Jiahao Gong, Qibin Zhou, Wenyuan Zhang, Ningzi Wang, Feiyang Tan, Hangning Zhou, Ziyao Xu, Haotian Yao, Chi Zhang, Xiaojun Liu, Xiaoguang Di, Bin Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.06892">SparseAD: Sparse Query-Centric Paradigm for Efficient End-to-End Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>End-to-End paradigms use a unified framework to implement multi-tasks in an autonomous driving system. Despite simplicity and clarity, the performance of end-to-end autonomous driving methods on sub-tasks is still far behind the single-task methods. Meanwhile, the widely used dense BEV features in previous end-to-end methods make it costly to extend to more modalities or tasks. In this paper, we propose a Sparse query-centric paradigm for end-to-end Autonomous Driving (SparseAD), where the sparse queries completely represent the whole driving scenario across space, time and tasks without any dense BEV representation. Concretely, we design a unified sparse architecture for perception tasks including detection, tracking, and online mapping. Moreover, we revisit motion prediction and planning, and devise a more justifiable motion planner framework. On the challenging nuScenes dataset, SparseAD achieves SOTA full-task performance among end-to-end methods and significantly narrows the performance gap between end-to-end paradigms and single-task methods. Codes will be released soon.
<div id='section'>Paperid: <span id='pid'>983, <a href='https://arxiv.org/pdf/2404.06171.pdf' target='_blank'>https://arxiv.org/pdf/2404.06171.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Oxana Shamilyan, Ievgen Kabin, Zoya Dyka, Oleksandr Sudakov, Andrii Cherninskyi, Marcin Brzozowski, Peter Langendoerfer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.06171">Intelligence and Motion Models of Continuum Robots: an Overview</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Many technical solutions are bio-inspired. Octopus-inspired robotic arms belong to continuum robots which are used in minimally invasive surgery or for technical system restoration in areas difficult-toaccess. Continuum robot missions are bounded with their motions, whereby the motion of the robots is controlled by humans via wireless communication. In case of a lost connection, robot autonomy is required. Distributed control and distributed decision-making mechanisms based on artificial intelligence approaches can be a promising solution to achieve autonomy of technical systems and to increase their resilience. However these methods are not well investigated yet. Octopuses are the living example of natural distributed intelligence but their learning and decision-making mechanisms are also not fully investigated and understood yet. Our major interest is investigating mechanisms of Distributed Artificial Intelligence as a basis for improving resilience of complex systems. We decided to use a physical continuum robot prototype that is able to perform some basic movements for our research. The idea is to research how a technical system can be empowered to combine movements into sequences of motions by itself. For the experimental investigations a suitable physical prototype has to be selected, its motion control has to be implemented and automated. In this paper, we give an overview combining different fields of research, such as Distributed Artificial Intelligence and continuum robots based on 98 publications. We provide a detailed description of the basic motion control models of continuum robots based on the literature reviewed, discuss different aspects of autonomy and give an overview of physical prototypes of continuum robots.
<div id='section'>Paperid: <span id='pid'>984, <a href='https://arxiv.org/pdf/2403.05081.pdf' target='_blank'>https://arxiv.org/pdf/2403.05081.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kanghyun Ryu, Negar Mehr
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.05081">Integrating Predictive Motion Uncertainties with Distributionally Robust Risk-Aware Control for Safe Robot Navigation in Crowds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Ensuring safe navigation in human-populated environments is crucial for autonomous mobile robots. Although recent advances in machine learning offer promising methods to predict human trajectories in crowded areas, it remains unclear how one can safely incorporate these learned models into a control loop due to the uncertain nature of human motion, which can make predictions of these models imprecise. In this work, we address this challenge and introduce a distributionally robust chance-constrained model predictive control (DRCC-MPC) which: (i) adopts a probability of collision as a pre-specified, interpretable risk metric, and (ii) offers robustness against discrepancies between actual human trajectories and their predictions. We consider the risk of collision in the form of a chance constraint, providing an interpretable measure of robot safety. To enable real-time evaluation of chance constraints, we consider conservative approximations of chance constraints in the form of distributionally robust Conditional Value at Risk constraints. The resulting formulation offers computational efficiency as well as robustness with respect to out-of-distribution human motion. With the parallelization of a sampling-based optimization technique, our method operates in real-time, demonstrating successful and safe navigation in a number of case studies with real-world pedestrian data.
<div id='section'>Paperid: <span id='pid'>985, <a href='https://arxiv.org/pdf/2512.22957.pdf' target='_blank'>https://arxiv.org/pdf/2512.22957.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mengyu Ji, Shiliang Guo, Zhengzhen Li, Jiahao Shen, Huazi Cao, Shiyu Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.22957">PreGME: Prescribed Performance Control of Aerial Manipulators based on Variable-Gain ESO</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>An aerial manipulator, comprising a multirotor base and a robotic arm, is subject to significant dynamic coupling between these two components. Therefore, achieving precise and robust motion control is a challenging yet important objective. Here, we propose a novel prescribed performance motion control framework based on variable-gain extended state observers (ESOs), referred to as PreGME. The method includes variable-gain ESOs for real-time estimation of dynamic coupling and a prescribed performance flight control that incorporates error trajectory constraints. Compared with existing methods, the proposed approach exhibits the following two characteristics. First, the adopted variable-gain ESOs can accurately estimate rapidly varying dynamic coupling. This enables the proposed method to handle manipulation tasks that require aggressive motion of the robotic arm. Second, by prescribing the performance, a preset error trajectory is generated to guide the system evolution along this trajectory. This strategy allows the proposed method to ensure the tracking error remains within the prescribed performance envelope, thereby achieving high-precision control. Experiments on a real platform, including aerial staff twirling, aerial mixology, and aerial cart-pulling experiments, are conducted to validate the effectiveness of the proposed method. Experimental results demonstrate that even under the dynamic coupling caused by rapid robotic arm motion (end-effector velocity: 1.02 m/s, acceleration: 5.10 m/s$^2$), the proposed method achieves high tracking performance.
<div id='section'>Paperid: <span id='pid'>986, <a href='https://arxiv.org/pdf/2512.10617.pdf' target='_blank'>https://arxiv.org/pdf/2512.10617.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bishoy Galoaa, Xiangyu Bai, Sarah Ostadabbas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.10617">Lang2Motion: Bridging Language and Motion through Joint Embedding Spaces</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present Lang2Motion, a framework for language-guided point trajectory generation by aligning motion manifolds with joint embedding spaces. Unlike prior work focusing on human motion or video synthesis, we generate explicit trajectories for arbitrary objects using motion extracted from real-world videos via point tracking. Our transformer-based auto-encoder learns trajectory representations through dual supervision: textual motion descriptions and rendered trajectory visualizations, both mapped through CLIP's frozen encoders. Lang2Motion achieves 34.2% Recall@1 on text-to-trajectory retrieval, outperforming video-based methods by 12.5 points, and improves motion accuracy by 33-52% (12.4 ADE vs 18.3-25.3) compared to video generation baselines. We demonstrate 88.3% Top-1 accuracy on human action recognition despite training only on diverse object motions, showing effective transfer across motion domains. Lang2Motion supports style transfer, semantic interpolation, and latent-space editing through CLIP-aligned trajectory representations.
<div id='section'>Paperid: <span id='pid'>987, <a href='https://arxiv.org/pdf/2512.07821.pdf' target='_blank'>https://arxiv.org/pdf/2512.07821.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shaoheng Fang, Hanwen Jiang, Yunpeng Bai, Niloy J. Mitra, Qixing Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.07821">WorldReel: 4D Video Generation with Consistent Geometry and Motion Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent video generators achieve striking photorealism, yet remain fundamentally inconsistent in 3D. We present WorldReel, a 4D video generator that is natively spatio-temporally consistent. WorldReel jointly produces RGB frames together with 4D scene representations, including pointmaps, camera trajectory, and dense flow mapping, enabling coherent geometry and appearance modeling over time. Our explicit 4D representation enforces a single underlying scene that persists across viewpoints and dynamic content, yielding videos that remain consistent even under large non-rigid motion and significant camera movement. We train WorldReel by carefully combining synthetic and real data: synthetic data providing precise 4D supervision (geometry, motion, and camera), while real videos contribute visual diversity and realism. This blend allows WorldReel to generalize to in-the-wild footage while preserving strong geometric fidelity. Extensive experiments demonstrate that WorldReel sets a new state-of-the-art for consistent video generation with dynamic scenes and moving cameras, improving metrics of geometric consistency, motion coherence, and reducing view-time artifacts over competing methods. We believe that WorldReel brings video generation closer to 4D-consistent world modeling, where agents can render, interact, and reason about scenes through a single and stable spatiotemporal representation.
<div id='section'>Paperid: <span id='pid'>988, <a href='https://arxiv.org/pdf/2512.02453.pdf' target='_blank'>https://arxiv.org/pdf/2512.02453.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kerui Chen, Jianrong Zhang, Ming Li, Zhonglong Zheng, Hehe Fan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.02453">ClusterStyle: Modeling Intra-Style Diversity with Prototypical Clustering for Stylized Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing stylized motion generation models have shown their remarkable ability to understand specific style information from the style motion, and insert it into the content motion. However, capturing intra-style diversity, where a single style should correspond to diverse motion variations, remains a significant challenge. In this paper, we propose a clustering-based framework, ClusterStyle, to address this limitation. Instead of learning an unstructured embedding from each style motion, we leverage a set of prototypes to effectively model diverse style patterns across motions belonging to the same style category. We consider two types of style diversity: global-level diversity among style motions of the same category, and local-level diversity within the temporal dynamics of motion sequences. These components jointly shape two structured style embedding spaces, i.e., global and local, optimized via alignment with non-learnable prototype anchors. Furthermore, we augment the pretrained text-to-motion generation model with the Stylistic Modulation Adapter (SMA) to integrate the style features. Extensive experiments demonstrate that our approach outperforms existing state-of-the-art models in stylized motion generation and motion style transfer.
<div id='section'>Paperid: <span id='pid'>989, <a href='https://arxiv.org/pdf/2512.02015.pdf' target='_blank'>https://arxiv.org/pdf/2512.02015.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yao-Chih Lee, Zhoutong Zhang, Jiahui Huang, Jui-Hsien Wang, Joon-Young Lee, Jia-Bin Huang, Eli Shechtman, Zhengqi Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.02015">Generative Video Motion Editing with 3D Point Tracks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Camera and object motions are central to a video's narrative. However, precisely editing these captured motions remains a significant challenge, especially under complex object movements. Current motion-controlled image-to-video (I2V) approaches often lack full-scene context for consistent video editing, while video-to-video (V2V) methods provide viewpoint changes or basic object translation, but offer limited control over fine-grained object motion. We present a track-conditioned V2V framework that enables joint editing of camera and object motion. We achieve this by conditioning a video generation model on a source video and paired 3D point tracks representing source and target motions. These 3D tracks establish sparse correspondences that transfer rich context from the source video to new motions while preserving spatiotemporal coherence. Crucially, compared to 2D tracks, 3D tracks provide explicit depth cues, allowing the model to resolve depth order and handle occlusions for precise motion editing. Trained in two stages on synthetic and real data, our model supports diverse motion edits, including joint camera/object manipulation, motion transfer, and non-rigid deformation, unlocking new creative potential in video editing.
<div id='section'>Paperid: <span id='pid'>990, <a href='https://arxiv.org/pdf/2511.03996.pdf' target='_blank'>https://arxiv.org/pdf/2511.03996.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yushi Wang, Changsheng Luo, Penghui Chen, Jianran Liu, Weijian Sun, Tong Guo, Kechang Yang, Biao Hu, Yangang Zhang, Mingguo Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.03996">Learning Vision-Driven Reactive Soccer Skills for Humanoid Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humanoid soccer poses a representative challenge for embodied intelligence, requiring robots to operate within a tightly coupled perception-action loop. However, existing systems typically rely on decoupled modules, resulting in delayed responses and incoherent behaviors in dynamic environments, while real-world perceptual limitations further exacerbate these issues. In this work, we present a unified reinforcement learning-based controller that enables humanoid robots to acquire reactive soccer skills through the direct integration of visual perception and motion control. Our approach extends Adversarial Motion Priors to perceptual settings in real-world dynamic environments, bridging motion imitation and visually grounded dynamic control. We introduce an encoder-decoder architecture combined with a virtual perception system that models real-world visual characteristics, allowing the policy to recover privileged states from imperfect observations and establish active coordination between perception and action. The resulting controller demonstrates strong reactivity, consistently executing coherent and robust soccer behaviors across various scenarios, including real RoboCup matches.
<div id='section'>Paperid: <span id='pid'>991, <a href='https://arxiv.org/pdf/2510.13044.pdf' target='_blank'>https://arxiv.org/pdf/2510.13044.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jungbin Cho, Minsu Kim, Jisoo Kim, Ce Zheng, Laszlo A. Jeni, Ming-Hsuan Yang, Youngjae Yu, Seonjoo Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.13044">SceneAdapt: Scene-aware Adaptation of Human Motion Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion is inherently diverse and semantically rich, while also shaped by the surrounding scene. However, existing motion generation approaches address either motion semantics or scene-awareness in isolation, since constructing large-scale datasets with both rich text--motion coverage and precise scene interactions is extremely challenging. In this work, we introduce SceneAdapt, a framework that injects scene awareness into text-conditioned motion models by leveraging disjoint scene--motion and text--motion datasets through two adaptation stages: inbetweening and scene-aware inbetweening. The key idea is to use motion inbetweening, learnable without text, as a proxy task to bridge two distinct datasets and thereby inject scene-awareness to text-to-motion models. In the first stage, we introduce keyframing layers that modulate motion latents for inbetweening while preserving the latent manifold. In the second stage, we add a scene-conditioning layer that injects scene geometry by adaptively querying local context through cross-attention. Experimental results show that SceneAdapt effectively injects scene awareness into text-to-motion models, and we further analyze the mechanisms through which this awareness emerges. Code and models will be released.
<div id='section'>Paperid: <span id='pid'>992, <a href='https://arxiv.org/pdf/2510.10650.pdf' target='_blank'>https://arxiv.org/pdf/2510.10650.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Peiyin Chen, Zhuowei Yang, Hui Feng, Sheng Jiang, Rui Yan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.10650">DEMO: Disentangled Motion Latent Flow Matching for Fine-Grained Controllable Talking Portrait Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Audio-driven talking-head generation has advanced rapidly with diffusion-based generative models, yet producing temporally coherent videos with fine-grained motion control remains challenging. We propose DEMO, a flow-matching generative framework for audio-driven talking-portrait video synthesis that delivers disentangled, high-fidelity control of lip motion, head pose, and eye gaze. The core contribution is a motion auto-encoder that builds a structured latent space in which motion factors are independently represented and approximately orthogonalized. On this disentangled motion space, we apply optimal-transport-based flow matching with a transformer predictor to generate temporally smooth motion trajectories conditioned on audio. Extensive experiments across multiple benchmarks show that DEMO outperforms prior methods in video realism, lip-audio synchronization, and motion fidelity. These results demonstrate that combining fine-grained motion disentanglement with flow-based generative modeling provides a powerful new paradigm for controllable talking-head video synthesis.
<div id='section'>Paperid: <span id='pid'>993, <a href='https://arxiv.org/pdf/2510.08482.pdf' target='_blank'>https://arxiv.org/pdf/2510.08482.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Onur Keleş, Aslı Özyürek, Gerardo Ortega, Kadir Gökgö, Esam Ghaleb
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.08482">The Visual Iconicity Challenge: Evaluating Vision-Language Models on Sign Language Form-Meaning Mapping</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Iconicity, the resemblance between linguistic form and meaning, is pervasive in signed languages, offering a natural testbed for visual grounding. For vision-language models (VLMs), the challenge is to recover such essential mappings from dynamic human motion rather than static context. We introduce the \textit{Visual Iconicity Challenge}, a novel video-based benchmark that adapts psycholinguistic measures to evaluate VLMs on three tasks: (i) phonological sign-form prediction (e.g., handshape, location), (ii) transparency (inferring meaning from visual form), and (iii) graded iconicity ratings. We assess $13$ state-of-the-art VLMs in zero- and few-shot settings on Sign Language of the Netherlands and compare them to human baselines. On \textit{phonological form prediction}, VLMs recover some handshape and location detail but remain below human performance; on \textit{transparency}, they are far from human baselines; and only top models correlate moderately with human \textit{iconicity ratings}. Interestingly, \textit{models with stronger phonological form prediction correlate better with human iconicity judgment}, indicating shared sensitivity to visually grounded structure. Our findings validate these diagnostic tasks and motivate human-centric signals and embodied learning methods for modelling iconicity and improving visual grounding in multimodal models.
<div id='section'>Paperid: <span id='pid'>994, <a href='https://arxiv.org/pdf/2510.02526.pdf' target='_blank'>https://arxiv.org/pdf/2510.02526.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anamika J H, Anujith Muraleedharan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.02526">U-LAG: Uncertainty-Aware, Lag-Adaptive Goal Retargeting for Robotic Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robots manipulating in changing environments must act on percepts that are late, noisy, or stale. We present U-LAG, a mid-execution goal-retargeting layer that leaves the low-level controller unchanged while re-aiming task goals (pre-contact, contact, post) as new observations arrive. Unlike motion retargeting or generic visual servoing, U-LAG treats in-flight goal re-aiming as a first-class, pluggable module between perception and control. Our main technical contribution is UAR-PF, an uncertainty-aware retargeter that maintains a distribution over object pose under sensing lag and selects goals that maximize expected progress. We instantiate a reproducible Shift x Lag stress test in PyBullet/PandaGym for pick, push, stacking, and peg insertion, where the object undergoes abrupt in-plane shifts while synthetic perception lag is injected during approach. Across 0-10 cm shifts and 0-400 ms lags, UAR-PF and ICP degrade gracefully relative to a no-retarget baseline, achieving higher success with modest end-effector travel and fewer aborts; simple operational safeguards further improve stability. Contributions: (1) UAR-PF for lag-adaptive, uncertainty-aware goal retargeting; (2) a pluggable retargeting interface; and (3) a reproducible Shift x Lag benchmark with evaluation on pick, push, stacking, and peg insertion.
<div id='section'>Paperid: <span id='pid'>995, <a href='https://arxiv.org/pdf/2509.25443.pdf' target='_blank'>https://arxiv.org/pdf/2509.25443.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zewen He, Chenyuan Chen, Dilshod Azizov, Yoshihiko Nakamura
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25443">CoTaP: Compliant Task Pipeline and Reinforcement Learning of Its Controller with Compliance Modulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humanoid whole-body locomotion control is a critical approach for humanoid robots to leverage their inherent advantages. Learning-based control methods derived from retargeted human motion data provide an effective means of addressing this issue. However, because most current human datasets lack measured force data, and learning-based robot control is largely position-based, achieving appropriate compliance during interaction with real environments remains challenging. This paper presents Compliant Task Pipeline (CoTaP): a pipeline that leverages compliance information in the learning-based structure of humanoid robots. A two-stage dual-agent reinforcement learning framework combined with model-based compliance control for humanoid robots is proposed. In the training process, first a base policy with a position-based controller is trained; then in the distillation, the upper-body policy is combined with model-based compliance control, and the lower-body agent is guided by the base policy. In the upper-body control, adjustable task-space compliance can be specified and integrated with other controllers through compliance modulation on the symmetric positive definite (SPD) manifold, ensuring system stability. We validated the feasibility of the proposed strategy in simulation, primarily comparing the responses to external disturbances under different compliance settings.
<div id='section'>Paperid: <span id='pid'>996, <a href='https://arxiv.org/pdf/2509.19545.pdf' target='_blank'>https://arxiv.org/pdf/2509.19545.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Min Dai, Aaron D. Ames
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.19545">RoMoCo: Robotic Motion Control Toolbox for Reduced-Order Model-Based Locomotion on Bipedal and Humanoid Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present RoMoCo, an open-source C++ toolbox for the synthesis and evaluation of reduced-order model-based planners and whole-body controllers for bipedal and humanoid robots. RoMoCo's modular architecture unifies state-of-the-art planners and whole-body locomotion controllers under a consistent API, enabling rapid prototyping and reproducible benchmarking. By leveraging reduced-order models for platform-agnostic gait generation, RoMoCo enables flexible controller design across diverse robots. We demonstrate its versatility and performance through extensive simulations on the Cassie, Unitree H1, and G1 robots, and validate its real-world efficacy with hardware experiments on the Cassie and G1 humanoids.
<div id='section'>Paperid: <span id='pid'>997, <a href='https://arxiv.org/pdf/2509.15130.pdf' target='_blank'>https://arxiv.org/pdf/2509.15130.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenxi Song, Yanming Yang, Tong Zhao, Ruibo Li, Chi Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15130">WorldForge: Unlocking Emergent 3D/4D Generation in Video Diffusion Model via Training-Free Guidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent video diffusion models show immense potential for spatial intelligence tasks due to their rich world priors, but this is undermined by limited controllability, poor spatial-temporal consistency, and entangled scene-camera dynamics. Existing solutions, such as model fine-tuning and warping-based repainting, struggle with scalability, generalization, and robustness against artifacts. To address this, we propose WorldForge, a training-free, inference-time framework composed of three tightly coupled modules. 1) Intra-Step Recursive Refinement injects fine-grained trajectory guidance at denoising steps through a recursive correction loop, ensuring motion remains aligned with the target path. 2) Flow-Gated Latent Fusion leverages optical flow similarity to decouple motion from appearance in the latent space and selectively inject trajectory guidance into motion-related channels. 3) Dual-Path Self-Corrective Guidance compares guided and unguided denoising paths to adaptively correct trajectory drift caused by noisy or misaligned structural signals. Together, these components inject fine-grained, trajectory-aligned guidance without training, achieving both accurate motion control and photorealistic content generation. Our framework is plug-and-play and model-agnostic, enabling broad applicability across various 3D/4D tasks. Extensive experiments demonstrate that our method achieves state-of-the-art performance in trajectory adherence, geometric consistency, and perceptual quality, outperforming both training-intensive and inference-only baselines.
<div id='section'>Paperid: <span id='pid'>998, <a href='https://arxiv.org/pdf/2508.06547.pdf' target='_blank'>https://arxiv.org/pdf/2508.06547.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Heran Wu, Zirun Zhou, Jingfeng Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.06547">A tutorial note on collecting simulated data for vision-language-action models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Traditional robotic systems typically decompose intelligence into independent modules for computer vision, natural language processing, and motion control. Vision-Language-Action (VLA) models fundamentally transform this approach by employing a single neural network that can simultaneously process visual observations, understand human instructions, and directly output robot actions -- all within a unified framework. However, these systems are highly dependent on high-quality training datasets that can capture the complex relationships between visual observations, language instructions, and robotic actions. This tutorial reviews three representative systems: the PyBullet simulation framework for flexible customized data generation, the LIBERO benchmark suite for standardized task definition and evaluation, and the RT-X dataset collection for large-scale multi-robot data acquisition. We demonstrated dataset generation approaches in PyBullet simulation and customized data collection within LIBERO, and provide an overview of the characteristics and roles of the RT-X dataset for large-scale multi-robot data acquisition.
<div id='section'>Paperid: <span id='pid'>999, <a href='https://arxiv.org/pdf/2508.00917.pdf' target='_blank'>https://arxiv.org/pdf/2508.00917.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiayuan Wang, Farhad Pourpanah, Q. M. Jonathan Wu, Ning Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.00917">A Survey on Deep Multi-Task Learning in Connected Autonomous Vehicles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Connected autonomous vehicles (CAVs) must simultaneously perform multiple tasks, such as object detection, semantic segmentation, depth estimation, trajectory prediction, motion prediction, and behaviour prediction, to ensure safe and reliable navigation in complex environments. Vehicle-to-everything (V2X) communication enables cooperative driving among CAVs, thereby mitigating the limitations of individual sensors, reducing occlusions, and improving perception over long distances. Traditionally, these tasks are addressed using distinct models, which leads to high deployment costs, increased computational overhead, and challenges in achieving real-time performance. Multi-task learning (MTL) has recently emerged as a promising solution that enables the joint learning of multiple tasks within a single unified model. This offers improved efficiency and resource utilization. To the best of our knowledge, this survey is the first comprehensive review focused on MTL in the context of CAVs. We begin with an overview of CAVs and MTL to provide foundational background. We then explore the application of MTL across key functional modules, including perception, prediction, planning, control, and multi-agent collaboration. Finally, we discuss the strengths and limitations of existing methods, identify key research gaps, and provide directions for future research aimed at advancing MTL methodologies for CAV systems.
<div id='section'>Paperid: <span id='pid'>1000, <a href='https://arxiv.org/pdf/2507.21069.pdf' target='_blank'>https://arxiv.org/pdf/2507.21069.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andreas Spilz, Heiko Oppel, Jochen Werner, Kathrin Stucke-Straub, Felix Capanni, Michael Munz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.21069">GAITEX: Human motion dataset from impaired gait and rehabilitation exercises of inertial and optical sensor data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Wearable inertial measurement units (IMUs) offer a cost-effective and scalable means to assess human movement quality in clinical and everyday settings. However, the development of robust sensor-based classification models for physiotherapeutic exercises and gait analysis requires large, diverse datasets, which are costly and time-consuming to collect. Here, we present a multimodal dataset of physiotherapeutic exercises - including correct and clinically relevant variants - and gait-related exercises - including both normal and impaired gait patterns - recorded from 19 participants using synchronized IMUs and marker-based motion capture (MoCap). The dataset includes raw data from nine IMUs and thirty-five optical markers capturing full-body kinematics. Each IMU is additionally equipped with four optical markers, enabling precise comparison between IMU-derived orientation estimates and reference values from the MoCap system. To support further analysis, we also provide processed IMU orientations aligned with common segment coordinate systems, subject-specific OpenSim models, inverse kinematics results, and tools for visualizing IMU orientations in the musculoskeletal context. Detailed annotations of movement execution quality and time-stamped segmentations support diverse analysis goals. This dataset supports the development and benchmarking of machine learning models for tasks such as automatic exercise evaluation, gait analysis, temporal activity segmentation, and biomechanical parameter estimation. To facilitate reproducibility, we provide code for postprocessing, sensor-to-segment alignment, inverse kinematics computation, and technical validation. This resource is intended to accelerate research in machine learning-driven human movement analysis.
<div id='section'>Paperid: <span id='pid'>1001, <a href='https://arxiv.org/pdf/2506.00411.pdf' target='_blank'>https://arxiv.org/pdf/2506.00411.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi Yang, Jiaxuan Sun, Siqi Kou, Yihan Wang, Zhijie Deng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.00411">LoHoVLA: A Unified Vision-Language-Action Model for Long-Horizon Embodied Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Real-world embodied agents face long-horizon tasks, characterized by high-level goals demanding multi-step solutions beyond single actions. Successfully navigating these requires both high-level task planning (i.e., decomposing goals into sub-tasks) and low-level motion control (i.e., generating precise robot actions). While existing vision language action (VLA) models and hierarchical architectures offer potential in embodied tasks, the former often falter in planning, and the latter can suffer from coordination issues, both hampering performance. We introduce a new unified VLA framework for long-horizon tasks, dubbed LoHoVLA, to overcome these limitations. LoHoVLA leverages a large pretrained vision language model (VLM) as the backbone to jointly generate language and action tokens for sub-task generation and robot action prediction, respectively. This shared representation promotes better generalization across tasks. Additionally, LoHoVLA embraces a hierarchical closed-loop control mechanism to mitigate errors originating from both high-level planning and low-level control. To train LoHoVLA, we introduce LoHoSet, a dataset built on the Ravens simulator, containing 20 long-horizon tasks, each with 1,000 expert demonstrations composed of visual observations, linguistic goals, sub-tasks, and robot actions. Experimental results show that LoHoVLA significantly surpasses both hierarchical and standard VLA approaches on long-horizon embodied tasks in the Ravens simulator. These findings underscore the promise of unified architectures for advancing generalizable embodied intelligence.
<div id='section'>Paperid: <span id='pid'>1002, <a href='https://arxiv.org/pdf/2505.20744.pdf' target='_blank'>https://arxiv.org/pdf/2505.20744.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Zhang, Zhan Zhuang, Xuehao Wang, Xiaodong Yang, Yu Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.20744">MoPFormer: Motion-Primitive Transformer for Wearable-Sensor Activity Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human Activity Recognition (HAR) with wearable sensors is challenged by limited interpretability, which significantly impacts cross-dataset generalization. To address this challenge, we propose Motion-Primitive Transformer (MoPFormer), a novel self-supervised framework that enhances interpretability by tokenizing inertial measurement unit signals into semantically meaningful motion primitives and leverages a Transformer architecture to learn rich temporal representations. MoPFormer comprises two-stages. first stage is to partition multi-channel sensor streams into short segments and quantizing them into discrete "motion primitive" codewords, while the second stage enriches those tokenized sequences through a context-aware embedding module and then processes them with a Transformer encoder. The proposed MoPFormer can be pre-trained using a masked motion-modeling objective that reconstructs missing primitives, enabling it to develop robust representations across diverse sensor configurations. Experiments on six HAR benchmarks demonstrate that MoPFormer not only outperforms state-of-the-art methods but also successfully generalizes across multiple datasets. Most importantly, the learned motion primitives significantly enhance both interpretability and cross-dataset performance by capturing fundamental movement patterns that remain consistent across similar activities regardless of dataset origin.
<div id='section'>Paperid: <span id='pid'>1003, <a href='https://arxiv.org/pdf/2505.02108.pdf' target='_blank'>https://arxiv.org/pdf/2505.02108.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maksym Ivashechkin, Oscar Mendez, Richard Bowden
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.02108">SignSplat: Rendering Sign Language via Gaussian Splatting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>State-of-the-art approaches for conditional human body rendering via Gaussian splatting typically focus on simple body motions captured from many views. This is often in the context of dancing or walking. However, for more complex use cases, such as sign language, we care less about large body motion and more about subtle and complex motions of the hands and face. The problems of building high fidelity models are compounded by the complexity of capturing multi-view data of sign. The solution is to make better use of sequence data, ensuring that we can overcome the limited information from only a few views by exploiting temporal variability. Nevertheless, learning from sequence-level data requires extremely accurate and consistent model fitting to ensure that appearance is consistent across complex motions. We focus on how to achieve this, constraining mesh parameters to build an accurate Gaussian splatting framework from few views capable of modelling subtle human motion. We leverage regularization techniques on the Gaussian parameters to mitigate overfitting and rendering artifacts. Additionally, we propose a new adaptive control method to densify Gaussians and prune splat points on the mesh surface. To demonstrate the accuracy of our approach, we render novel sequences of sign language video, building on neural machine translation approaches to sign stitching. On benchmark datasets, our approach achieves state-of-the-art performance; and on highly articulated and complex sign language motion, we significantly outperform competing approaches.
<div id='section'>Paperid: <span id='pid'>1004, <a href='https://arxiv.org/pdf/2504.20685.pdf' target='_blank'>https://arxiv.org/pdf/2504.20685.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zesheng Wang, Alexandre Bruckert, Patrick Le Callet, Guangtao Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.20685">Efficient Listener: Dyadic Facial Motion Synthesis via Action Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating realistic listener facial motions in dyadic conversations remains challenging due to the high-dimensional action space and temporal dependency requirements. Existing approaches usually consider extracting 3D Morphable Model (3DMM) coefficients and modeling in the 3DMM space. However, this makes the computational speed of the 3DMM a bottleneck, making it difficult to achieve real-time interactive responses. To tackle this problem, we propose Facial Action Diffusion (FAD), which introduces the diffusion methods from the field of image generation to achieve efficient facial action generation. We further build the Efficient Listener Network (ELNet) specially designed to accommodate both the visual and audio information of the speaker as input. Considering of FAD and ELNet, the proposed method learns effective listener facial motion representations and leads to improvements of performance over the state-of-the-art methods while reducing 99% computational time.
<div id='section'>Paperid: <span id='pid'>1005, <a href='https://arxiv.org/pdf/2504.11733.pdf' target='_blank'>https://arxiv.org/pdf/2504.11733.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Li Yu, Situo Wang, Wei Zhou, Moncef Gabbouj
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.11733">DVLTA-VQA: Decoupled Vision-Language Modeling with Text-Guided Adaptation for Blind Video Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Inspired by the dual-stream theory of the human visual system (HVS) - where the ventral stream is responsible for object recognition and detail analysis, while the dorsal stream focuses on spatial relationships and motion perception - an increasing number of video quality assessment (VQA) works built upon this framework are proposed. Recent advancements in large multi-modal models, notably Contrastive Language-Image Pretraining (CLIP), have motivated researchers to incorporate CLIP into dual-stream-based VQA methods. This integration aims to harness the model's superior semantic understanding capabilities to replicate the object recognition and detail analysis in ventral stream, as well as spatial relationship analysis in dorsal stream. However, CLIP is originally designed for images and lacks the ability to capture temporal and motion information inherent in videos. To address the limitation, this paper propose a Decoupled Vision-Language Modeling with Text-Guided Adaptation for Blind Video Quality Assessment (DVLTA-VQA), which decouples CLIP's visual and textual components, and integrates them into different stages of the NR-VQA pipeline. Specifically, a Video-Based Temporal CLIP module is proposed to explicitly model temporal dynamics and enhance motion perception, aligning with the dorsal stream. Additionally, a Temporal Context Module is developed to refine inter-frame dependencies, further improving motion modeling. On the ventral stream side, a Basic Visual Feature Extraction Module is employed to strengthen detail analysis. Finally, a text-guided adaptive fusion strategy is proposed to enable dynamic weighting of features, facilitating more effective integration of spatial and temporal information.
<div id='section'>Paperid: <span id='pid'>1006, <a href='https://arxiv.org/pdf/2504.10190.pdf' target='_blank'>https://arxiv.org/pdf/2504.10190.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaushik Bhargav Sivangi, Idris Zakariyya, Paul Henderson, Fani Deligianni
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.10190">Differentially Private 2D Human Pose Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human pose estimation (HPE) has become essential in numerous applications including healthcare, activity recognition, and human-computer interaction. However, the privacy implications of processing sensitive visual data present significant deployment barriers in critical domains. While traditional anonymization techniques offer limited protection and often compromise data utility for broader motion analysis, Differential Privacy (DP) provides formal privacy guarantees but typically degrades model performance when applied naively. In this work, we present the first differentially private 2D human pose estimation (2D-HPE) by applying Differentially Private Stochastic Gradient Descent (DP-SGD) to this task. To effectively balance privacy with performance, we adopt Projected DP-SGD (PDP-SGD), which projects the noisy gradients to a low-dimensional subspace. Additionally, we adapt TinyViT, a compact and efficient vision transformer for coordinate classification in HPE, providing a lightweight yet powerful backbone that enhances privacy-preserving deployment feasibility on resource-limited devices. Our approach is particularly valuable for multimedia interpretation tasks, enabling privacy-safe analysis and understanding of human motion across diverse visual media while preserving the semantic meaning required for downstream applications. Comprehensive experiments on the MPII Human Pose Dataset demonstrate significant performance enhancement with PDP-SGD achieving 78.48% PCKh@0.5 at a strict privacy budget ($Îµ=0.2$), compared to 63.85% for standard DP-SGD. This work lays foundation for privacy-preserving human pose estimation in real-world, sensitive applications.
<div id='section'>Paperid: <span id='pid'>1007, <a href='https://arxiv.org/pdf/2504.09209.pdf' target='_blank'>https://arxiv.org/pdf/2504.09209.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiangyue Zhang, Jianfang Li, Jiaxu Zhang, Jianqiang Ren, Liefeng Bo, Zhigang Tu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.09209">EchoMask: Speech-Queried Attention-based Mask Modeling for Holistic Co-Speech Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Masked modeling framework has shown promise in co-speech motion generation. However, it struggles to identify semantically significant frames for effective motion masking. In this work, we propose a speech-queried attention-based mask modeling framework for co-speech motion generation. Our key insight is to leverage motion-aligned speech features to guide the masked motion modeling process, selectively masking rhythm-related and semantically expressive motion frames. Specifically, we first propose a motion-audio alignment module (MAM) to construct a latent motion-audio joint space. In this space, both low-level and high-level speech features are projected, enabling motion-aligned speech representation using learnable speech queries. Then, a speech-queried attention mechanism (SQA) is introduced to compute frame-level attention scores through interactions between motion keys and speech queries, guiding selective masking toward motion frames with high attention scores. Finally, the motion-aligned speech features are also injected into the generation network to facilitate co-speech motion generation. Qualitative and quantitative evaluations confirm that our method outperforms existing state-of-the-art approaches, successfully producing high-quality co-speech motion.
<div id='section'>Paperid: <span id='pid'>1008, <a href='https://arxiv.org/pdf/2504.08959.pdf' target='_blank'>https://arxiv.org/pdf/2504.08959.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yilin Wang, Chuan Guo, Yuxuan Mu, Muhammad Gohar Javed, Xinxin Zuo, Juwei Lu, Hai Jiang, Li Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.08959">MotionDreamer: One-to-Many Motion Synthesis with Localized Generative Masked Transformer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generative masked transformers have demonstrated remarkable success across various content generation tasks, primarily due to their ability to effectively model large-scale dataset distributions with high consistency. However, in the animation domain, large datasets are not always available. Applying generative masked modeling to generate diverse instances from a single MoCap reference may lead to overfitting, a challenge that remains unexplored. In this work, we present MotionDreamer, a localized masked modeling paradigm designed to learn internal motion patterns from a given motion with arbitrary topology and duration. By embedding the given motion into quantized tokens with a novel distribution regularization method, MotionDreamer constructs a robust and informative codebook for local motion patterns. Moreover, a sliding window local attention is introduced in our masked transformer, enabling the generation of natural yet diverse animations that closely resemble the reference motion patterns. As demonstrated through comprehensive experiments, MotionDreamer outperforms the state-of-the-art methods that are typically GAN or Diffusion-based in both faithfulness and diversity. Thanks to the consistency and robustness of the quantization-based approach, MotionDreamer can also effectively perform downstream tasks such as temporal motion editing, \textcolor{update}{crowd animation}, and beat-aligned dance generation, all using a single reference motion. Visit our project page: https://motiondreamer.github.io/
<div id='section'>Paperid: <span id='pid'>1009, <a href='https://arxiv.org/pdf/2503.23381.pdf' target='_blank'>https://arxiv.org/pdf/2503.23381.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiexin Wang, Wenwen Qiang, Zhao Yang, Bing Su
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.23381">Enhancing Human Motion Prediction via Multi-range Decoupling Decoding with Gating-adjusting Aggregation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Expressive representation of pose sequences is crucial for accurate motion modeling in human motion prediction (HMP). While recent deep learning-based methods have shown promise in learning motion representations, these methods tend to overlook the varying relevance and dependencies between historical information and future moments, with a stronger correlation for short-term predictions and weaker for distant future predictions. This limits the learning of motion representation and then hampers prediction performance. In this paper, we propose a novel approach called multi-range decoupling decoding with gating-adjusting aggregation ($MD2GA$), which leverages the temporal correlations to refine motion representation learning. This approach employs a two-stage strategy for HMP. In the first stage, a multi-range decoupling decoding adeptly adjusts feature learning by decoding the shared features into distinct future lengths, where different decoders offer diverse insights into motion patterns. In the second stage, a gating-adjusting aggregation dynamically combines the diverse insights guided by input motion data. Extensive experiments demonstrate that the proposed method can be easily integrated into other motion prediction methods and enhance their prediction performance.
<div id='section'>Paperid: <span id='pid'>1010, <a href='https://arxiv.org/pdf/2503.12782.pdf' target='_blank'>https://arxiv.org/pdf/2503.12782.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiming Wang, Yulong Gao, Yang Wang, Xiongwei Zhao, Yijiao Sun, Xiangyan Kong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.12782">DART: Dual-level Autonomous Robotic Topology for Efficient Exploration in Unknown Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Conventional algorithms in autonomous exploration face challenges due to their inability to accurately and efficiently identify the spatial distribution of convex regions in the real-time map. These methods often prioritize navigation toward the nearest or information-rich frontiers -- the boundaries between known and unknown areas -- resulting in incomplete convex region exploration and requiring excessive backtracking to revisit these missed areas. To address these limitations, this paper introduces an innovative dual-level topological analysis approach. First, we introduce a Low-level Topological Graph (LTG), generated through uniform sampling of the original map data, which captures essential geometric and connectivity details. Next, the LTG is transformed into a High-level Topological Graph (HTG), representing the spatial layout and exploration completeness of convex regions, prioritizing the exploration of convex regions that are not fully explored and minimizing unnecessary backtracking. Finally, an novel Local Artificial Potential Field (LAPF) method is employed for motion control, replacing conventional path planning and boosting overall efficiency. Experimental results highlight the effectiveness of our approach. Simulation tests reveal that our framework significantly reduces exploration time and travel distance, outperforming existing methods in both speed and efficiency. Ablation studies confirm the critical role of each framework component. Real-world tests demonstrate the robustness of our method in environments with poor mapping quality, surpassing other approaches in adaptability to mapping inaccuracies and inaccessible areas.
<div id='section'>Paperid: <span id='pid'>1011, <a href='https://arxiv.org/pdf/2502.04299.pdf' target='_blank'>https://arxiv.org/pdf/2502.04299.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinbo Xing, Long Mai, Cusuh Ham, Jiahui Huang, Aniruddha Mahapatra, Chi-Wing Fu, Tien-Tsin Wong, Feng Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.04299">MotionCanvas: Cinematic Shot Design with Controllable Image-to-Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a method that allows users to design cinematic video shots in the context of image-to-video generation. Shot design, a critical aspect of filmmaking, involves meticulously planning both camera movements and object motions in a scene. However, enabling intuitive shot design in modern image-to-video generation systems presents two main challenges: first, effectively capturing user intentions on the motion design, where both camera movements and scene-space object motions must be specified jointly; and second, representing motion information that can be effectively utilized by a video diffusion model to synthesize the image animations. To address these challenges, we introduce MotionCanvas, a method that integrates user-driven controls into image-to-video (I2V) generation models, allowing users to control both object and camera motions in a scene-aware manner. By connecting insights from classical computer graphics and contemporary video generation techniques, we demonstrate the ability to achieve 3D-aware motion control in I2V synthesis without requiring costly 3D-related training data. MotionCanvas enables users to intuitively depict scene-space motion intentions, and translates them into spatiotemporal motion-conditioning signals for video diffusion models. We demonstrate the effectiveness of our method on a wide range of real-world image content and shot-design scenarios, highlighting its potential to enhance the creative workflows in digital content creation and adapt to various image and video editing applications.
<div id='section'>Paperid: <span id='pid'>1012, <a href='https://arxiv.org/pdf/2412.15166.pdf' target='_blank'>https://arxiv.org/pdf/2412.15166.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junjia Liu, Zhuo Li, Minghao Yu, Zhipeng Dong, Sylvain Calinon, Darwin Caldwell, Fei Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.15166">Human-Humanoid Robots Cross-Embodiment Behavior-Skill Transfer Using Decomposed Adversarial Learning from Demonstration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humanoid robots are envisioned as embodied intelligent agents capable of performing a wide range of human-level loco-manipulation tasks, particularly in scenarios requiring strenuous and repetitive labor. However, learning these skills is challenging due to the high degrees of freedom of humanoid robots, and collecting sufficient training data for humanoid is a laborious process. Given the rapid introduction of new humanoid platforms, a cross-embodiment framework that allows generalizable skill transfer is becoming increasingly critical. To address this, we propose a transferable framework that reduces the data bottleneck by using a unified digital human model as a common prototype and bypassing the need for re-training on every new robot platform. The model learns behavior primitives from human demonstrations through adversarial imitation, and the complex robot structures are decomposed into functional components, each trained independently and dynamically coordinated. Task generalization is achieved through a human-object interaction graph, and skills are transferred to different robots via embodiment-specific kinematic motion retargeting and dynamic fine-tuning. Our framework is validated on five humanoid robots with diverse configurations, demonstrating stable loco-manipulation and highlighting its effectiveness in reducing data requirements and increasing the efficiency of skill transfer across platforms.
<div id='section'>Paperid: <span id='pid'>1013, <a href='https://arxiv.org/pdf/2411.12773.pdf' target='_blank'>https://arxiv.org/pdf/2411.12773.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Youyuan Zhang, Zehua Liu, Zenan Li, Zhaoyu Li, James J. Clark, Xujie Si
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.12773">Decoupling Training-Free Guided Diffusion by ADMM</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we consider the conditional generation problem by guiding off-the-shelf unconditional diffusion models with differentiable loss functions in a plug-and-play fashion. While previous research has primarily focused on balancing the unconditional diffusion model and the guided loss through a tuned weight hyperparameter, we propose a novel framework that distinctly decouples these two components. Specifically, we introduce two variables ${x}$ and ${z}$, to represent the generated samples governed by the unconditional generation model and the guidance function, respectively. This decoupling reformulates conditional generation into two manageable subproblems, unified by the constraint ${x} = {z}$. Leveraging this setup, we develop a new algorithm based on the Alternating Direction Method of Multipliers (ADMM) to adaptively balance these components. Additionally, we establish the equivalence between the diffusion reverse step and the proximal operator of ADMM and provide a detailed convergence analysis of our algorithm under certain mild assumptions. Our experiments demonstrate that our proposed method ADMMDiff consistently generates high-quality samples while ensuring strong adherence to the conditioning criteria. It outperforms existing methods across a range of conditional generation tasks, including image generation with various guidance and controllable motion synthesis.
<div id='section'>Paperid: <span id='pid'>1014, <a href='https://arxiv.org/pdf/2411.02099.pdf' target='_blank'>https://arxiv.org/pdf/2411.02099.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Idris Zakariyya, Linda Tran, Kaushik Bhargav Sivangi, Paul Henderson, Fani Deligianni
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.02099">Differentially Private Integrated Decision Gradients (IDG-DP) for Radar-based Human Activity Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion analysis offers significant potential for healthcare monitoring and early detection of diseases. The advent of radar-based sensing systems has captured the spotlight for they are able to operate without physical contact and they can integrate with pre-existing Wi-Fi networks. They are also seen as less privacy-invasive compared to camera-based systems. However, recent research has shown high accuracy in recognizing subjects or gender from radar gait patterns, raising privacy concerns. This study addresses these issues by investigating privacy vulnerabilities in radar-based Human Activity Recognition (HAR) systems and proposing a novel method for privacy preservation using Differential Privacy (DP) driven by attributions derived with Integrated Decision Gradient (IDG) algorithm. We investigate Black-box Membership Inference Attack (MIA) Models in HAR settings across various levels of attacker-accessible information. We extensively evaluated the effectiveness of the proposed IDG-DP method by designing a CNN-based HAR model and rigorously assessing its resilience against MIAs. Experimental results demonstrate the potential of IDG-DP in mitigating privacy attacks while maintaining utility across all settings, particularly excelling against label-only and shadow model black-box MIA attacks. This work represents a crucial step towards balancing the need for effective radar-based HAR with robust privacy protection in healthcare environments.
<div id='section'>Paperid: <span id='pid'>1015, <a href='https://arxiv.org/pdf/2407.18159.pdf' target='_blank'>https://arxiv.org/pdf/2407.18159.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Max Emerick, Stacy Patterson, Bassam Bamieh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.18159">Optimal Assignment and Motion Control in Two-Class Continuum Swarms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We consider optimal swarm control problems where two different classes of agents are present. Continuum idealizations of large-scale swarms are used where the dynamics describe the evolution of the spatially-distributed densities of each agent class. The problem formulation we adopt is motivated by applications where agents of one class are assigned to agents of the other class, which we refer to as demand and resource agents respectively. Assignments have costs related to the distances between mutually assigned agents, and the overall cost of an assignment is quantified by a Wasserstein distance between the densities of the two agent classes. When agents can move, the assignment cost can decrease at the expense of a physical motion cost, and this tradeoff sets up a nonlinear, infinite-dimensional optimal control problem. We show that in one spatial dimension, this problem can be converted to an infinite-dimensional, but decoupled, linear-quadratic (LQ) tracking problem when expressed in terms of the respective quantile functions. Solutions are given in the general one-dimensional case, as well as in the special cases of constant and periodically time-varying demands.
<div id='section'>Paperid: <span id='pid'>1016, <a href='https://arxiv.org/pdf/2407.00955.pdf' target='_blank'>https://arxiv.org/pdf/2407.00955.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiang Jiao, Dingzhu Wen, Guangxu Zhu, Wei Jiang, Wu Luo, Yuanming Shi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.00955">Task-oriented Over-the-air Computation for Edge-device Co-inference with Balanced Classification Accuracy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Edge-device co-inference, which concerns the cooperation between edge devices and an edge server for completing inference tasks over wireless networks, has been a promising technique for enabling various kinds of intelligent services at the network edge, e.g., auto-driving. In this paradigm, the concerned design objective of the network shifts from the traditional communication throughput to the effective and efficient execution of the inference task underpinned by the network, measured by, e.g., the inference accuracy and latency. In this paper, a task-oriented over-the-air computation scheme is proposed for a multidevice artificial intelligence system. Particularly, a novel tractable inference accuracy metric is proposed for classification tasks, which is called minimum pair-wise discriminant gain. Unlike prior work measuring the average of all class pairs in feature space, it measures the minimum distance of all class pairs. By maximizing the minimum pair-wise discriminant gain instead of its average counterpart, any pair of classes can be better separated in the feature space, and thus leading to a balanced and improved inference accuracy for all classes. Besides, this paper jointly optimizes the minimum discriminant gain of all feature elements instead of separately maximizing that of each element in the existing designs. As a result, the transmit power can be adaptively allocated to the feature elements according to their different contributions to the inference accuracy, opening an extra degree of freedom to improve inference performance. Extensive experiments are conducted using a concrete use case of human motion recognition to verify the superiority of the proposed design over the benchmarking scheme.
<div id='section'>Paperid: <span id='pid'>1017, <a href='https://arxiv.org/pdf/2406.19852.pdf' target='_blank'>https://arxiv.org/pdf/2406.19852.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guillem Capellera, Luis Ferraz, Antonio Rubio, Antonio Agudo, Francesc Moreno-Noguer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.19852">FootBots: A Transformer-based Architecture for Motion Prediction in Soccer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion prediction in soccer involves capturing complex dynamics from player and ball interactions. We present FootBots, an encoder-decoder transformer-based architecture addressing motion prediction and conditioned motion prediction through equivariance properties. FootBots captures temporal and social dynamics using set attention blocks and multi-attention block decoder. Our evaluation utilizes two datasets: a real soccer dataset and a tailored synthetic one. Insights from the synthetic dataset highlight the effectiveness of FootBots' social attention mechanism and the significance of conditioned motion prediction. Empirical results on real soccer data demonstrate that FootBots outperforms baselines in motion prediction and excels in conditioned tasks, such as predicting the players based on the ball position, predicting the offensive (defensive) team based on the ball and the defensive (offensive) team, and predicting the ball position based on all players. Our evaluation connects quantitative and qualitative findings. https://youtu.be/9kaEkfzG3L8
<div id='section'>Paperid: <span id='pid'>1018, <a href='https://arxiv.org/pdf/2406.10454.pdf' target='_blank'>https://arxiv.org/pdf/2406.10454.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zipeng Fu, Qingqing Zhao, Qi Wu, Gordon Wetzstein, Chelsea Finn
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.10454">HumanPlus: Humanoid Shadowing and Imitation from Humans</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>One of the key arguments for building robots that have similar form factors to human beings is that we can leverage the massive human data for training. Yet, doing so has remained challenging in practice due to the complexities in humanoid perception and control, lingering physical gaps between humanoids and humans in morphologies and actuation, and lack of a data pipeline for humanoids to learn autonomous skills from egocentric vision. In this paper, we introduce a full-stack system for humanoids to learn motion and autonomous skills from human data. We first train a low-level policy in simulation via reinforcement learning using existing 40-hour human motion datasets. This policy transfers to the real world and allows humanoid robots to follow human body and hand motion in real time using only a RGB camera, i.e. shadowing. Through shadowing, human operators can teleoperate humanoids to collect whole-body data for learning different tasks in the real world. Using the data collected, we then perform supervised behavior cloning to train skill policies using egocentric vision, allowing humanoids to complete different tasks autonomously by imitating human skills. We demonstrate the system on our customized 33-DoF 180cm humanoid, autonomously completing tasks such as wearing a shoe to stand up and walk, unloading objects from warehouse racks, folding a sweatshirt, rearranging objects, typing, and greeting another robot with 60-100% success rates using up to 40 demonstrations. Project website: https://humanoid-ai.github.io/
<div id='section'>Paperid: <span id='pid'>1019, <a href='https://arxiv.org/pdf/2405.19283.pdf' target='_blank'>https://arxiv.org/pdf/2405.19283.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hanchao Liu, Xiaohang Zhan, Shaoli Huang, Tai-Jiang Mu, Ying Shan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.19283">Programmable Motion Generation for Open-Set Motion Control Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Character animation in real-world scenarios necessitates a variety of constraints, such as trajectories, key-frames, interactions, etc. Existing methodologies typically treat single or a finite set of these constraint(s) as separate control tasks. They are often specialized, and the tasks they address are rarely extendable or customizable. We categorize these as solutions to the close-set motion control problem. In response to the complexity of practical motion control, we propose and attempt to solve the open-set motion control problem. This problem is characterized by an open and fully customizable set of motion control tasks. To address this, we introduce a new paradigm, programmable motion generation. In this paradigm, any given motion control task is broken down into a combination of atomic constraints. These constraints are then programmed into an error function that quantifies the degree to which a motion sequence adheres to them. We utilize a pre-trained motion generation model and optimize its latent code to minimize the error function of the generated motion. Consequently, the generated motion not only inherits the prior of the generative model but also satisfies the required constraints. Experiments show that we can generate high-quality motions when addressing a wide range of unseen tasks. These tasks encompass motion control by motion dynamics, geometric constraints, physical laws, interactions with scenes, objects or the character own body parts, etc. All of these are achieved in a unified approach, without the need for ad-hoc paired training data collection or specialized network designs. During the programming of novel tasks, we observed the emergence of new skills beyond those of the prior model. With the assistance of large language models, we also achieved automatic programming. We hope that this work will pave the way for the motion control of general AI agents.
<div id='section'>Paperid: <span id='pid'>1020, <a href='https://arxiv.org/pdf/2405.03971.pdf' target='_blank'>https://arxiv.org/pdf/2405.03971.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiwei Li, Bozhen Zhang, Lei Yang, Tianyu Shen, Nuo Xu, Ruosen Hao, Weiting Li, Tao Yan, Huaping Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.03971">Unified End-to-End V2X Cooperative Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>V2X cooperation, through the integration of sensor data from both vehicles and infrastructure, is considered a pivotal approach to advancing autonomous driving technology. Current research primarily focuses on enhancing perception accuracy, often overlooking the systematic improvement of accident prediction accuracy through end-to-end learning, leading to insufficient attention to the safety issues of autonomous driving. To address this challenge, this paper introduces the UniE2EV2X framework, a V2X-integrated end-to-end autonomous driving system that consolidates key driving modules within a unified network. The framework employs a deformable attention-based data fusion strategy, effectively facilitating cooperation between vehicles and infrastructure. The main advantages include: 1) significantly enhancing agents' perception and motion prediction capabilities, thereby improving the accuracy of accident predictions; 2) ensuring high reliability in the data fusion process; 3) superior end-to-end perception compared to modular approaches. Furthermore, We implement the UniE2EV2X framework on the challenging DeepAccident, a simulation dataset designed for V2X cooperative driving.
<div id='section'>Paperid: <span id='pid'>1021, <a href='https://arxiv.org/pdf/2404.14745.pdf' target='_blank'>https://arxiv.org/pdf/2404.14745.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Runqi Wang, Caoyuan Ma, Guopeng Li, Hanrui Xu, Yuke Li, Zheng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.14745">You Think, You ACT: The New Task of Arbitrary Text to Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text to Motion aims to generate human motions from texts. Existing settings rely on limited Action Texts that include action labels, which limits flexibility and practicability in scenarios difficult to describe directly. This paper extends limited Action Texts to arbitrary ones. Scene texts without explicit action labels can enhance the practicality of models in complex and diverse industries such as virtual human interaction, robot behavior generation, and film production, while also supporting the exploration of potential implicit behavior patterns. However, newly introduced Scene Texts may yield multiple reasonable output results, causing significant challenges in existing data, framework, and evaluation. To address this practical issue, we first create a new dataset HUMANML3D++ by extending texts of the largest existing dataset HUMANML3D. Secondly, we propose a simple yet effective framework that extracts action instructions from arbitrary texts and subsequently generates motions. Furthermore, we also benchmark this new setting with multi-solution metrics to address the inadequacies of existing single-solution metrics. Extensive experiments indicate that Text to Motion in this realistic setting is challenging, fostering new research in this practical direction.
<div id='section'>Paperid: <span id='pid'>1022, <a href='https://arxiv.org/pdf/2404.14713.pdf' target='_blank'>https://arxiv.org/pdf/2404.14713.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinhao Liang, Kaidi Yang, Chaopeng Tan, Jinxiang Wang, Guodong Yin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.14713">Enhancing High-Speed Cruising Performance of Autonomous Vehicles through Integrated Deep Reinforcement Learning Framework</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>High-speed cruising scenarios with mixed traffic greatly challenge the road safety of autonomous vehicles (AVs). Unlike existing works that only look at fundamental modules in isolation, this work enhances AV safety in mixed-traffic high-speed cruising scenarios by proposing an integrated framework that synthesizes three fundamental modules, i.e., behavioral decision-making, path-planning, and motion-control modules. Considering that the integrated framework would increase the system complexity, a bootstrapped deep Q-Network (DQN) is employed to enhance the deep exploration of the reinforcement learning method and achieve adaptive decision making of AVs. Moreover, to make AV behavior understandable by surrounding HDVs to prevent unexpected operations caused by misinterpretations, we derive an inverse reinforcement learning (IRL) approach to learn the reward function of skilled drivers for the path planning of lane-changing maneuvers. Such a design enables AVs to achieve a human-like tradeoff between multi-performance requirements. Simulations demonstrate that the proposed integrated framework can guide AVs to take safe actions while guaranteeing high-speed cruising performance.
<div id='section'>Paperid: <span id='pid'>1023, <a href='https://arxiv.org/pdf/2512.07248.pdf' target='_blank'>https://arxiv.org/pdf/2512.07248.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhaorui Meng, Lu Yin, Xinrui Chen, Anjun Chen, Shihui Guo, Yipeng Qin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.07248">Benchmarking Humanoid Imitation Learning with Motion Difficulty</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Physics-based motion imitation is central to humanoid control, yet current evaluation metrics (e.g., joint position error) only measure how well a policy imitates but not how difficult the motion itself is. This conflates policy performance with motion difficulty, obscuring whether failures stem from poor learning or inherently challenging motions. In this work, we address this gap with Motion Difficulty Score (MDS), a novel metric that defines and quantifies imitation difficulty independent of policy performance. Grounded in rigid-body dynamics, MDS interprets difficulty as the torque variation induced by small pose perturbations: larger torque-to-pose variation yields flatter reward landscapes and thus higher learning difficulty. MDS captures this through three properties of the perturbation-induced torque space: volume, variance, and temporal variability. We also use it to construct MD-AMASS, a difficulty-aware repartitioning of the AMASS dataset. Empirically, we rigorously validate MDS by demonstrating its explanatory power on the performance of state-of-the-art motion imitation policies. We further demonstrate the utility of MDS through two new MDS-based metrics: Maximum Imitable Difficulty (MID) and Difficulty-Stratified Joint Error (DSJE), providing fresh insights into imitation learning.
<div id='section'>Paperid: <span id='pid'>1024, <a href='https://arxiv.org/pdf/2511.22288.pdf' target='_blank'>https://arxiv.org/pdf/2511.22288.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhaorui Meng, Lu Yin, Yangqing Hou, Anjun Chen, Shihui Guo, Yipeng Qin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.22288">Improving Sparse IMU-based Motion Capture with Motion Label Smoothing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Sparse Inertial Measurement Units (IMUs) based human motion capture has gained significant momentum, driven by the adaptation of fundamental AI tools such as recurrent neural networks (RNNs) and transformers that are tailored for temporal and spatial modeling. Despite these achievements, current research predominantly focuses on pipeline and architectural designs, with comparatively little attention given to regularization methods, highlighting a critical gap in developing a comprehensive AI toolkit for this task. To bridge this gap, we propose motion label smoothing, a novel method that adapts the classic label smoothing strategy from classification to the sparse IMU-based motion capture task. Specifically, we first demonstrate that a naive adaptation of label smoothing, including simply blending a uniform vector or a ``uniform'' motion representation (e.g., dataset-average motion or a canonical T-pose), is suboptimal; and argue that a proper adaptation requires increasing the entropy of the smoothed labels. Second, we conduct a thorough analysis of human motion labels, identifying three critical properties: 1) Temporal Smoothness, 2) Joint Correlation, and 3) Low-Frequency Dominance, and show that conventional approaches to entropy enhancement (e.g., blending Gaussian noise) are ineffective as they disrupt these properties. Finally, we propose the blend of a novel skeleton-based Perlin noise for motion label smoothing, designed to raise label entropy while satisfying motion properties. Extensive experiments applying our motion label smoothing to three state-of-the-art methods across four real-world IMU datasets demonstrate its effectiveness and robust generalization (plug-and-play) capability.
<div id='section'>Paperid: <span id='pid'>1025, <a href='https://arxiv.org/pdf/2511.18173.pdf' target='_blank'>https://arxiv.org/pdf/2511.18173.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Enrico Pallotta, Sina Mokhtarzadeh Azar, Lars Doorenbos, Serdar Ozsoy, Umar Iqbal, Juergen Gall
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.18173">EgoControl: Controllable Egocentric Video Generation via 3D Full-Body Poses</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Egocentric video generation with fine-grained control through body motion is a key requirement towards embodied AI agents that can simulate, predict, and plan actions. In this work, we propose EgoControl, a pose-controllable video diffusion model trained on egocentric data. We train a video prediction model to condition future frame generation on explicit 3D body pose sequences. To achieve precise motion control, we introduce a novel pose representation that captures both global camera dynamics and articulated body movements, and integrate it through a dedicated control mechanism within the diffusion process. Given a short sequence of observed frames and a sequence of target poses, EgoControl generates temporally coherent and visually realistic future frames that align with the provided pose control. Experimental results demonstrate that EgoControl produces high-quality, pose-consistent egocentric videos, paving the way toward controllable embodied video simulation and understanding.
<div id='section'>Paperid: <span id='pid'>1026, <a href='https://arxiv.org/pdf/2511.06172.pdf' target='_blank'>https://arxiv.org/pdf/2511.06172.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hua Chang, Xin Xu, Wei Liu, Wei Wang, Xin Yuan, Kui Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.06172">MambaOVSR: Multiscale Fusion with Global Motion Modeling for Chinese Opera Video Super-Resolution</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Chinese opera is celebrated for preserving classical art. However, early filming equipment limitations have degraded videos of last-century performances by renowned artists (e.g., low frame rates and resolution), hindering archival efforts. Although space-time video super-resolution (STVSR) has advanced significantly, applying it directly to opera videos remains challenging. The scarcity of datasets impedes the recovery of high frequency details, and existing STVSR methods lack global modeling capabilities, compromising visual quality when handling opera's characteristic large motions. To address these challenges, we pioneer a large scale Chinese Opera Video Clip (COVC) dataset and propose the Mamba-based multiscale fusion network for space-time Opera Video Super-Resolution (MambaOVSR). Specifically, MambaOVSR involves three novel components: the Global Fusion Module (GFM) for motion modeling through a multiscale alternating scanning mechanism, and the Multiscale Synergistic Mamba Module (MSMM) for alignment across different sequence lengths. Additionally, our MambaVR block resolves feature artifacts and positional information loss during alignment. Experimental results on the COVC dataset show that MambaOVSR significantly outperforms the SOTA STVSR method by an average of 1.86 dB in terms of PSNR. Dataset and Code will be publicly released.
<div id='section'>Paperid: <span id='pid'>1027, <a href='https://arxiv.org/pdf/2511.05369.pdf' target='_blank'>https://arxiv.org/pdf/2511.05369.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shiyao Xu, Benedetta Liberatori, Gül Varol, Paolo Rota
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.05369">Dense Motion Captioning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in 3D human motion and language integration have primarily focused on text-to-motion generation, leaving the task of motion understanding relatively unexplored. We introduce Dense Motion Captioning, a novel task that aims to temporally localize and caption actions within 3D human motion sequences. Current datasets fall short in providing detailed temporal annotations and predominantly consist of short sequences featuring few actions. To overcome these limitations, we present the Complex Motion Dataset (CompMo), the first large-scale dataset featuring richly annotated, complex motion sequences with precise temporal boundaries. Built through a carefully designed data generation pipeline, CompMo includes 60,000 motion sequences, each composed of multiple actions ranging from at least two to ten, accurately annotated with their temporal extents. We further present DEMO, a model that integrates a large language model with a simple motion adapter, trained to generate dense, temporally grounded captions. Our experiments show that DEMO substantially outperforms existing methods on CompMo as well as on adapted benchmarks, establishing a robust baseline for future research in 3D motion understanding and captioning.
<div id='section'>Paperid: <span id='pid'>1028, <a href='https://arxiv.org/pdf/2510.14250.pdf' target='_blank'>https://arxiv.org/pdf/2510.14250.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lianzi Jiang, Jianxin Zhang, Xinyu Han, Huanhe Dong, Xiangrong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.14250">A Physics Prior-Guided Dual-Stream Attention Network for Motion Prediction of Elastic Bragg Breakwaters</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate motion response prediction for elastic Bragg breakwaters is critical for their structural safety and operational integrity in marine environments. However, conventional deep learning models often exhibit limited generalization capabilities when presented with unseen sea states. These deficiencies stem from the neglect of natural decay observed in marine systems and inadequate modeling of wave-structure interaction (WSI). To overcome these challenges, this study proposes a novel Physics Prior-Guided Dual-Stream Attention Network (PhysAttnNet). First, the decay bidirectional self-attention (DBSA) module incorporates a learnable temporal decay to assign higher weights to recent states, aiming to emulate the natural decay phenomenon. Meanwhile, the phase differences guided bidirectional cross-attention (PDG-BCA) module explicitly captures the bidirectional interaction and phase relationship between waves and the structure using a cosine-based bias within a bidirectional cross-computation paradigm. These streams are synergistically integrated through a global context fusion (GCF) module. Finally, PhysAttnNet is trained with a hybrid time-frequency loss that jointly minimizes time-domain prediction errors and frequency-domain spectral discrepancies. Comprehensive experiments on wave flume datasets demonstrate that PhysAttnNet significantly outperforms mainstream models. Furthermore,cross-scenario generalization tests validate the model's robustness and adaptability to unseen environments, highlighting its potential as a framework to develop predictive models for complex systems in ocean engineering.
<div id='section'>Paperid: <span id='pid'>1029, <a href='https://arxiv.org/pdf/2510.06988.pdf' target='_blank'>https://arxiv.org/pdf/2510.06988.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Girolamo Macaluso, Lorenzo Mandelli, Mirko Bicchierai, Stefano Berretti, Andrew D. Bagdanov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.06988">No MoCap Needed: Post-Training Motion Diffusion Models with Reinforcement Learning using Only Textual Prompts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion models have recently advanced human motion generation, producing realistic and diverse animations from textual prompts. However, adapting these models to unseen actions or styles typically requires additional motion capture data and full retraining, which is costly and difficult to scale. We propose a post-training framework based on Reinforcement Learning that fine-tunes pretrained motion diffusion models using only textual prompts, without requiring any motion ground truth. Our approach employs a pretrained text-motion retrieval network as a reward signal and optimizes the diffusion policy with Denoising Diffusion Policy Optimization, effectively shifting the model's generative distribution toward the target domain without relying on paired motion data. We evaluate our method on cross-dataset adaptation and leave-one-out motion experiments using the HumanML3D and KIT-ML datasets across both latent- and joint-space diffusion architectures. Results from quantitative metrics and user studies show that our approach consistently improves the quality and diversity of generated motions, while preserving performance on the original distribution. Our approach is a flexible, data-efficient, and privacy-preserving solution for motion adaptation.
<div id='section'>Paperid: <span id='pid'>1030, <a href='https://arxiv.org/pdf/2509.09667.pdf' target='_blank'>https://arxiv.org/pdf/2509.09667.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhengdi Yu, Simone Foti, Linguang Zhang, Amy Zhao, Cem Keskin, Stefanos Zafeiriou, Tolga Birdal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09667">Geometric Neural Distance Fields for Learning Human Motion Priors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce Neural Riemannian Motion Fields (NRMF), a novel 3D generative human motion prior that enables robust, temporally consistent, and physically plausible 3D motion recovery. Unlike existing VAE or diffusion-based methods, our higher-order motion prior explicitly models the human motion in the zero level set of a collection of neural distance fields (NDFs) corresponding to pose, transition (velocity), and acceleration dynamics. Our framework is rigorous in the sense that our NDFs are constructed on the product space of joint rotations, their angular velocities, and angular accelerations, respecting the geometry of the underlying articulations. We further introduce: (i) a novel adaptive-step hybrid algorithm for projecting onto the set of plausible motions, and (ii) a novel geometric integrator to "roll out" realistic motion trajectories during test-time-optimization and generation. Our experiments show significant and consistent gains: trained on the AMASS dataset, NRMF remarkably generalizes across multiple input modalities and to diverse tasks ranging from denoising to motion in-betweening and fitting to partial 2D / 3D observations.
<div id='section'>Paperid: <span id='pid'>1031, <a href='https://arxiv.org/pdf/2509.09210.pdf' target='_blank'>https://arxiv.org/pdf/2509.09210.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xing Gao, Zherui Huang, Weiyao Lin, Xiao Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09210">ProgD: Progressive Multi-scale Decoding with Dynamic Graphs for Joint Multi-agent Motion Forecasting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate motion prediction of surrounding agents is crucial for the safe planning of autonomous vehicles. Recent advancements have extended prediction techniques from individual agents to joint predictions of multiple interacting agents, with various strategies to address complex interactions within future motions of agents. However, these methods overlook the evolving nature of these interactions. To address this limitation, we propose a novel progressive multi-scale decoding strategy, termed ProgD, with the help of dynamic heterogeneous graph-based scenario modeling. In particular, to explicitly and comprehensively capture the evolving social interactions in future scenarios, given their inherent uncertainty, we design a progressive modeling of scenarios with dynamic heterogeneous graphs. With the unfolding of such dynamic heterogeneous graphs, a factorized architecture is designed to process the spatio-temporal dependencies within future scenarios and progressively eliminate uncertainty in future motions of multiple agents. Furthermore, a multi-scale decoding procedure is incorporated to improve on the future scenario modeling and consistent prediction of agents' future motion. The proposed ProgD achieves state-of-the-art performance on the INTERACTION multi-agent prediction benchmark, ranking $1^{st}$, and the Argoverse 2 multi-world forecasting benchmark.
<div id='section'>Paperid: <span id='pid'>1032, <a href='https://arxiv.org/pdf/2508.14033.pdf' target='_blank'>https://arxiv.org/pdf/2508.14033.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shaoshu Yang, Zhe Kong, Feng Gao, Meng Cheng, Xiangyu Liu, Yong Zhang, Zhuoliang Kang, Wenhan Luo, Xunliang Cai, Ran He, Xiaoming Wei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.14033">InfiniteTalk: Audio-driven Video Generation for Sparse-Frame Video Dubbing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent breakthroughs in video AIGC have ushered in a transformative era for audio-driven human animation. However, conventional video dubbing techniques remain constrained to mouth region editing, resulting in discordant facial expressions and body gestures that compromise viewer immersion. To overcome this limitation, we introduce sparse-frame video dubbing, a novel paradigm that strategically preserves reference keyframes to maintain identity, iconic gestures, and camera trajectories while enabling holistic, audio-synchronized full-body motion editing. Through critical analysis, we identify why naive image-to-video models fail in this task, particularly their inability to achieve adaptive conditioning. Addressing this, we propose InfiniteTalk, a streaming audio-driven generator designed for infinite-length long sequence dubbing. This architecture leverages temporal context frames for seamless inter-chunk transitions and incorporates a simple yet effective sampling strategy that optimizes control strength via fine-grained reference frame positioning. Comprehensive evaluations on HDTF, CelebV-HQ, and EMTD datasets demonstrate state-of-the-art performance. Quantitative metrics confirm superior visual realism, emotional coherence, and full-body motion synchronization.
<div id='section'>Paperid: <span id='pid'>1033, <a href='https://arxiv.org/pdf/2508.05162.pdf' target='_blank'>https://arxiv.org/pdf/2508.05162.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuan Wang, Kai Ruan, Liyang Qian, Zhizhi Guo, Chang Su, Gaoang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05162">X-MoGen: Unified Motion Generation across Humans and Animals</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-driven motion generation has attracted increasing attention due to its broad applications in virtual reality, animation, and robotics. While existing methods typically model human and animal motion separately, a joint cross-species approach offers key advantages, such as a unified representation and improved generalization. However, morphological differences across species remain a key challenge, often compromising motion plausibility. To address this, we propose \textbf{X-MoGen}, the first unified framework for cross-species text-driven motion generation covering both humans and animals. X-MoGen adopts a two-stage architecture. First, a conditional graph variational autoencoder learns canonical T-pose priors, while an autoencoder encodes motion into a shared latent space regularized by morphological loss. In the second stage, we perform masked motion modeling to generate motion embeddings conditioned on textual descriptions. During training, a morphological consistency module is employed to promote skeletal plausibility across species. To support unified modeling, we construct \textbf{UniMo4D}, a large-scale dataset of 115 species and 119k motion sequences, which integrates human and animal motions under a shared skeletal topology for joint training. Extensive experiments on UniMo4D demonstrate that X-MoGen outperforms state-of-the-art methods on both seen and unseen species.
<div id='section'>Paperid: <span id='pid'>1034, <a href='https://arxiv.org/pdf/2508.03695.pdf' target='_blank'>https://arxiv.org/pdf/2508.03695.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pulkit Kumar, Shuaiyi Huang, Matthew Walmer, Sai Saketh Rambhatla, Abhinav Shrivastava
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.03695">Trokens: Semantic-Aware Relational Trajectory Tokens for Few-Shot Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video understanding requires effective modeling of both motion and appearance information, particularly for few-shot action recognition. While recent advances in point tracking have been shown to improve few-shot action recognition, two fundamental challenges persist: selecting informative points to track and effectively modeling their motion patterns. We present Trokens, a novel approach that transforms trajectory points into semantic-aware relational tokens for action recognition. First, we introduce a semantic-aware sampling strategy to adaptively distribute tracking points based on object scale and semantic relevance. Second, we develop a motion modeling framework that captures both intra-trajectory dynamics through the Histogram of Oriented Displacements (HoD) and inter-trajectory relationships to model complex action patterns. Our approach effectively combines these trajectory tokens with semantic features to enhance appearance features with motion information, achieving state-of-the-art performance across six diverse few-shot action recognition benchmarks: Something-Something-V2 (both full and small splits), Kinetics, UCF101, HMDB51, and FineGym. For project page see https://trokens-iccv25.github.io
<div id='section'>Paperid: <span id='pid'>1035, <a href='https://arxiv.org/pdf/2507.23188.pdf' target='_blank'>https://arxiv.org/pdf/2507.23188.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shiyao Yu, Zi-An Wang, Kangning Yin, Zheng Tian, Mingyuan Zhang, Weixin Si, Shihao Zou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.23188">Multi-Modal Motion Retrieval by Learning a Fine-Grained Joint Embedding Space</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion retrieval is crucial for motion acquisition, offering superior precision, realism, controllability, and editability compared to motion generation. Existing approaches leverage contrastive learning to construct a unified embedding space for motion retrieval from text or visual modality. However, these methods lack a more intuitive and user-friendly interaction mode and often overlook the sequential representation of most modalities for improved retrieval performance. To address these limitations, we propose a framework that aligns four modalities -- text, audio, video, and motion -- within a fine-grained joint embedding space, incorporating audio for the first time in motion retrieval to enhance user immersion and convenience. This fine-grained space is achieved through a sequence-level contrastive learning approach, which captures critical details across modalities for better alignment. To evaluate our framework, we augment existing text-motion datasets with synthetic but diverse audio recordings, creating two multi-modal motion retrieval datasets. Experimental results demonstrate superior performance over state-of-the-art methods across multiple sub-tasks, including an 10.16% improvement in R@10 for text-to-motion retrieval and a 25.43% improvement in R@1 for video-to-motion retrieval on the HumanML3D dataset. Furthermore, our results show that our 4-modal framework significantly outperforms its 3-modal counterpart, underscoring the potential of multi-modal motion retrieval for advancing motion acquisition.
<div id='section'>Paperid: <span id='pid'>1036, <a href='https://arxiv.org/pdf/2507.21018.pdf' target='_blank'>https://arxiv.org/pdf/2507.21018.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ali Ismail-Fawaz, Maxime Devanne, Stefano Berretti, Jonathan Weber, Germain Forestier
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.21018">Deep Learning for Skeleton Based Human Motion Rehabilitation Assessment: A Benchmark</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automated assessment of human motion plays a vital role in rehabilitation, enabling objective evaluation of patient performance and progress. Unlike general human activity recognition, rehabilitation motion assessment focuses on analyzing the quality of movement within the same action class, requiring the detection of subtle deviations from ideal motion. Recent advances in deep learning and video-based skeleton extraction have opened new possibilities for accessible, scalable motion assessment using affordable devices such as smartphones or webcams. However, the field lacks standardized benchmarks, consistent evaluation protocols, and reproducible methodologies, limiting progress and comparability across studies. In this work, we address these gaps by (i) aggregating existing rehabilitation datasets into a unified archive called Rehab-Pile, (ii) proposing a general benchmarking framework for evaluating deep learning methods in this domain, and (iii) conducting extensive benchmarking of multiple architectures across classification and regression tasks. All datasets and implementations are released to the community to support transparency and reproducibility. This paper aims to establish a solid foundation for future research in automated rehabilitation assessment and foster the development of reliable, accessible, and personalized rehabilitation solutions. The datasets, source-code and results of this article are all publicly available.
<div id='section'>Paperid: <span id='pid'>1037, <a href='https://arxiv.org/pdf/2507.20557.pdf' target='_blank'>https://arxiv.org/pdf/2507.20557.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingting Li, Yu Qian, Lin Zhao, Su-Jing Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.20557">FED-PsyAU: Privacy-Preserving Micro-Expression Recognition via Psychological AU Coordination and Dynamic Facial Motion Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Micro-expressions (MEs) are brief, low-intensity, often localized facial expressions. They could reveal genuine emotions individuals may attempt to conceal, valuable in contexts like criminal interrogation and psychological counseling. However, ME recognition (MER) faces challenges, such as small sample sizes and subtle features, which hinder efficient modeling. Additionally, real-world applications encounter ME data privacy issues, leaving the task of enhancing recognition across settings under privacy constraints largely unexplored. To address these issues, we propose a FED-PsyAU research framework. We begin with a psychological study on the coordination of upper and lower facial action units (AUs) to provide structured prior knowledge of facial muscle dynamics. We then develop a DPK-GAT network that combines these psychological priors with statistical AU patterns, enabling hierarchical learning of facial motion features from regional to global levels, effectively enhancing MER performance. Additionally, our federated learning framework advances MER capabilities across multiple clients without data sharing, preserving privacy and alleviating the limited-sample issue for each client. Extensive experiments on commonly-used ME databases demonstrate the effectiveness of our approach.
<div id='section'>Paperid: <span id='pid'>1038, <a href='https://arxiv.org/pdf/2506.02733.pdf' target='_blank'>https://arxiv.org/pdf/2506.02733.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoyi Feng, Kaifeng Zou, Caichun Cen, Tao Huang, Hui Guo, Zizhou Huang, Yingli Zhao, Mingqing Zhang, Ziyuan Zheng, Diwei Wang, Yuntao Zou, Dagang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.02733">LinkTo-Anime: A 2D Animation Optical Flow Dataset from 3D Model Rendering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing optical flow datasets focus primarily on real-world simulation or synthetic human motion, but few are tailored to Celluloid(cel) anime character motion: a domain with unique visual and motion characteristics. To bridge this gap and facilitate research in optical flow estimation and downstream tasks such as anime video generation and line drawing colorization, we introduce LinkTo-Anime, the first high-quality dataset specifically designed for cel anime character motion generated with 3D model rendering. LinkTo-Anime provides rich annotations including forward and backward optical flow, occlusion masks, and Mixamo Skeleton. The dataset comprises 395 video sequences, totally 24,230 training frames, 720 validation frames, and 4,320 test frames. Furthermore, a comprehensive benchmark is constructed with various optical flow estimation methods to analyze the shortcomings and limitations across multiple datasets.
<div id='section'>Paperid: <span id='pid'>1039, <a href='https://arxiv.org/pdf/2505.19580.pdf' target='_blank'>https://arxiv.org/pdf/2505.19580.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Masaki Murooka, Kensuke Fukumitsu, Marwan Hamze, Mitsuharu Morisawa, Hiroshi Kaminaga, Fumio Kanehiro, Eiichi Yoshida
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.19580">Whole-body Multi-contact Motion Control for Humanoid Robots Based on Distributed Tactile Sensors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To enable humanoid robots to work robustly in confined environments, multi-contact motion that makes contacts not only at extremities, such as hands and feet, but also at intermediate areas of the limbs, such as knees and elbows, is essential. We develop a method to realize such whole-body multi-contact motion involving contacts at intermediate areas by a humanoid robot. Deformable sheet-shaped distributed tactile sensors are mounted on the surface of the robot's limbs to measure the contact force without significantly changing the robot body shape. The multi-contact motion controller developed earlier, which is dedicated to contact at extremities, is extended to handle contact at intermediate areas, and the robot motion is stabilized by feedback control using not only force/torque sensors but also distributed tactile sensors. Through verification on dynamics simulations, we show that the developed tactile feedback improves the stability of whole-body multi-contact motion against disturbances and environmental errors. Furthermore, the life-sized humanoid RHP Kaleido demonstrates whole-body multi-contact motions, such as stepping forward while supporting the body with forearm contact and balancing in a sitting posture with thigh contacts.
<div id='section'>Paperid: <span id='pid'>1040, <a href='https://arxiv.org/pdf/2505.00998.pdf' target='_blank'>https://arxiv.org/pdf/2505.00998.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Hua, Weiming Liu, Gui Xu, Yaqing Hou, Yew-Soon Ong, Qiang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.00998">Deterministic-to-Stochastic Diverse Latent Feature Mapping for Human Motion Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion synthesis aims to generate plausible human motion sequences, which has raised widespread attention in computer animation. Recent score-based generative models (SGMs) have demonstrated impressive results on this task. However, their training process involves complex curvature trajectories, leading to unstable training process. In this paper, we propose a Deterministic-to-Stochastic Diverse Latent Feature Mapping (DSDFM) method for human motion synthesis. DSDFM consists of two stages. The first human motion reconstruction stage aims to learn the latent space distribution of human motions. The second diverse motion generation stage aims to build connections between the Gaussian distribution and the latent space distribution of human motions, thereby enhancing the diversity and accuracy of the generated human motions. This stage is achieved by the designed deterministic feature mapping procedure with DerODE and stochastic diverse output generation procedure with DivSDE.DSDFM is easy to train compared to previous SGMs-based methods and can enhance diversity without introducing additional training parameters.Through qualitative and quantitative experiments, DSDFM achieves state-of-the-art results surpassing the latest methods, validating its superiority in human motion synthesis.
<div id='section'>Paperid: <span id='pid'>1041, <a href='https://arxiv.org/pdf/2505.00237.pdf' target='_blank'>https://arxiv.org/pdf/2505.00237.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ze Zhang, Georg Hess, Junjie Hu, Emmanuel Dean, Lennart Svensson, Knut Ãkesson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.00237">Future-Oriented Navigation: Dynamic Obstacle Avoidance with One-Shot Energy-Based Multimodal Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper proposes an integrated approach for the safe and efficient control of mobile robots in dynamic and uncertain environments. The approach consists of two key steps: one-shot multimodal motion prediction to anticipate motions of dynamic obstacles and model predictive control to incorporate these predictions into the motion planning process. Motion prediction is driven by an energy-based neural network that generates high-resolution, multi-step predictions in a single operation. The prediction outcomes are further utilized to create geometric shapes formulated as mathematical constraints. Instead of treating each dynamic obstacle individually, predicted obstacles are grouped by proximity in an unsupervised way to improve performance and efficiency. The overall collision-free navigation is handled by model predictive control with a specific design for proactive dynamic obstacle avoidance. The proposed approach allows mobile robots to navigate effectively in dynamic environments. Its performance is accessed across various scenarios that represent typical warehouse settings. The results demonstrate that the proposed approach outperforms other existing dynamic obstacle avoidance methods.
<div id='section'>Paperid: <span id='pid'>1042, <a href='https://arxiv.org/pdf/2504.08181.pdf' target='_blank'>https://arxiv.org/pdf/2504.08181.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruineng Li, Daitao Xing, Huiming Sun, Yuanzhou Ha, Jinglin Shen, Chiuman Ho
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.08181">TokenMotion: Decoupled Motion Control via Token Disentanglement for Human-centric Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human-centric motion control in video generation remains a critical challenge, particularly when jointly controlling camera movements and human poses in scenarios like the iconic Grammy Glambot moment. While recent video diffusion models have made significant progress, existing approaches struggle with limited motion representations and inadequate integration of camera and human motion controls. In this work, we present TokenMotion, the first DiT-based video diffusion framework that enables fine-grained control over camera motion, human motion, and their joint interaction. We represent camera trajectories and human poses as spatio-temporal tokens to enable local control granularity. Our approach introduces a unified modeling framework utilizing a decouple-and-fuse strategy, bridged by a human-aware dynamic mask that effectively handles the spatially-and-temporally varying nature of combined motion signals. Through extensive experiments, we demonstrate TokenMotion's effectiveness across both text-to-video and image-to-video paradigms, consistently outperforming current state-of-the-art methods in human-centric motion control tasks. Our work represents a significant advancement in controllable video generation, with particular relevance for creative production applications.
<div id='section'>Paperid: <span id='pid'>1043, <a href='https://arxiv.org/pdf/2503.16455.pdf' target='_blank'>https://arxiv.org/pdf/2503.16455.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiwen Dong, Jessica Rose, Hae Young Noh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.16455">Bridging Structural Dynamics and Biomechanics: Human Motion Estimation through Footstep-Induced Floor Vibrations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Quantitative estimation of human joint motion in daily living spaces is essential for early detection and rehabilitation tracking of neuromusculoskeletal disorders (e.g., Parkinson's) and mitigating trip and fall risks for older adults. Existing approaches involve monitoring devices such as cameras, wearables, and pressure mats, but have operational constraints such as direct line-of-sight, carrying devices, and dense deployment. To overcome these limitations, we leverage gait-induced floor vibration to estimate lower-limb joint motion (e.g., ankle, knee, and hip flexion angles), allowing non-intrusive and contactless gait health monitoring in people's living spaces. To overcome the high uncertainty in lower-limb movement given the limited information provided by the gait-induced floor vibrations, we formulate a physics-informed graph to integrate domain knowledge of gait biomechanics and structural dynamics into the model. Specifically, different types of nodes represent heterogeneous information from joint motions and floor vibrations; Their connecting edges represent the physiological relationships between joints and forces governed by gait biomechanics, as well as the relationships between forces and floor responses governed by the structural dynamics. As a result, our model poses physical constraints to reduce uncertainty while allowing information sharing between the body and the floor to make more accurate predictions. We evaluate our approach with 20 participants through a real-world walking experiment. We achieved an average of 3.7 degrees of mean absolute error in estimating 12 joint flexion angles (38% error reduction from baseline), which is comparable to the performance of cameras and wearables in current medical practices.
<div id='section'>Paperid: <span id='pid'>1044, <a href='https://arxiv.org/pdf/2503.15557.pdf' target='_blank'>https://arxiv.org/pdf/2503.15557.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Inwoo Hwang, Jinseok Bae, Donggeun Lim, Young Min Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.15557">Motion Synthesis with Sparse and Flexible Keyjoint Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Creating expressive character animations is labor-intensive, requiring intricate manual adjustment of animators across space and time. Previous works on controllable motion generation often rely on a predefined set of dense spatio-temporal specifications (e.g., dense pelvis trajectories with exact per-frame timing), limiting practicality for animators. To process high-level intent and intuitive control in diverse scenarios, we propose a practical controllable motions synthesis framework that respects sparse and flexible keyjoint signals. Our approach employs a decomposed diffusion-based motion synthesis framework that first synthesizes keyjoint movements from sparse input control signals and then synthesizes full-body motion based on the completed keyjoint trajectories. The low-dimensional keyjoint movements can easily adapt to various control signal types, such as end-effector position for diverse goal-driven motion synthesis, or incorporate functional constraints on a subset of keyjoints. Additionally, we introduce a time-agnostic control formulation, eliminating the need for frame-specific timing annotations and enhancing control flexibility. Then, the shared second stage can synthesize a natural whole-body motion that precisely satisfies the task requirement from dense keyjoint movements. We demonstrate the effectiveness of sparse and flexible keyjoint control through comprehensive experiments on diverse datasets and scenarios.
<div id='section'>Paperid: <span id='pid'>1045, <a href='https://arxiv.org/pdf/2503.15127.pdf' target='_blank'>https://arxiv.org/pdf/2503.15127.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tommaso Van Der Meer, Andrea Garulli, Antonio Giannitrapani, Renato Quartullo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.15127">A Comparative Study of Human Motion Models in Reinforcement Learning Algorithms for Social Robot Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Social robot navigation is an evolving research field that aims to find efficient strategies to safely navigate dynamic environments populated by humans. A critical challenge in this domain is the accurate modeling of human motion, which directly impacts the design and evaluation of navigation algorithms. This paper presents a comparative study of two popular categories of human motion models used in social robot navigation, namely velocity-based models and force-based models. A system-theoretic representation of both model types is presented, which highlights their common feedback structure, although with different state variables. Several navigation policies based on reinforcement learning are trained and tested in various simulated environments involving pedestrian crowds modeled with these approaches. A comparative study is conducted to assess performance across multiple factors, including human motion model, navigation policy, scenario complexity and crowd density. The results highlight advantages and challenges of different approaches to modeling human behavior, as well as their role during training and testing of learning-based navigation policies. The findings offer valuable insights and guidelines for selecting appropriate human motion models when designing socially-aware robot navigation systems.
<div id='section'>Paperid: <span id='pid'>1046, <a href='https://arxiv.org/pdf/2503.13859.pdf' target='_blank'>https://arxiv.org/pdf/2503.13859.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinseok Bae, Inwoo Hwang, Young Yoon Lee, Ziyu Guo, Joseph Liu, Yizhak Ben-Shabat, Young Min Kim, Mubbasir Kapadia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.13859">Less is More: Improving Motion Diffusion Models with Sparse Keyframes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in motion diffusion models have led to remarkable progress in diverse motion generation tasks, including text-to-motion synthesis. However, existing approaches represent motions as dense frame sequences, requiring the model to process redundant or less informative frames. The processing of dense animation frames imposes significant training complexity, especially when learning intricate distributions of large motion datasets even with modern neural architectures. This severely limits the performance of generative motion models for downstream tasks. Inspired by professional animators who mainly focus on sparse keyframes, we propose a novel diffusion framework explicitly designed around sparse and geometrically meaningful keyframes. Our method reduces computation by masking non-keyframes and efficiently interpolating missing frames. We dynamically refine the keyframe mask during inference to prioritize informative frames in later diffusion steps. Extensive experiments show that our approach consistently outperforms state-of-the-art methods in text alignment and motion realism, while also effectively maintaining high performance at significantly fewer diffusion steps. We further validate the robustness of our framework by using it as a generative prior and adapting it to different downstream tasks.
<div id='section'>Paperid: <span id='pid'>1047, <a href='https://arxiv.org/pdf/2503.06151.pdf' target='_blank'>https://arxiv.org/pdf/2503.06151.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zixi Kang, Xinghan Wang, Yadong Mu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.06151">Biomechanics-Guided Residual Approach to Generalizable Human Motion Generation and Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human pose, action, and motion generation are critical for applications in digital humans, character animation, and humanoid robotics. However, many existing methods struggle to produce physically plausible movements that are consistent with biomechanical principles. Although recent autoregressive and diffusion models deliver impressive visual quality, they often neglect key biodynamic features and fail to ensure physically realistic motions. Reinforcement Learning (RL) approaches can address these shortcomings but are highly dependent on simulation environments, limiting their generalizability. To overcome these challenges, we propose BioVAE, a biomechanics-aware framework with three core innovations: (1) integration of muscle electromyography (EMG) signals and kinematic features with acceleration constraints to enable physically plausible motion without simulations; (2) seamless coupling with diffusion models for stable end-to-end training; and (3) biomechanical priors that promote strong generalization across diverse motion generation and estimation tasks. Extensive experiments demonstrate that BioVAE achieves state-of-the-art performance on multiple benchmarks, bridging the gap between data-driven motion synthesis and biomechanical authenticity while setting new standards for physically accurate motion generation and pose estimation.
<div id='section'>Paperid: <span id='pid'>1048, <a href='https://arxiv.org/pdf/2502.10724.pdf' target='_blank'>https://arxiv.org/pdf/2502.10724.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiuxia Lin, Rongyu Chen, Kerui Gu, Angela Yao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.10724">Semantics-aware Test-time Adaptation for 3D Human Pose Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work highlights a semantics misalignment in 3D human pose estimation. For the task of test-time adaptation, the misalignment manifests as overly smoothed and unguided predictions. The smoothing settles predictions towards some average pose. Furthermore, when there are occlusions or truncations, the adaptation becomes fully unguided. To this end, we pioneer the integration of a semantics-aware motion prior for the test-time adaptation of 3D pose estimation. We leverage video understanding and a well-structured motion-text space to adapt the model motion prediction to adhere to video semantics during test time. Additionally, we incorporate a missing 2D pose completion based on the motion-text similarity. The pose completion strengthens the motion prior's guidance for occlusions and truncations. Our method significantly improves state-of-the-art 3D human pose estimation TTA techniques, with more than 12% decrease in PA-MPJPE on 3DPW and 3DHP.
<div id='section'>Paperid: <span id='pid'>1049, <a href='https://arxiv.org/pdf/2502.08377.pdf' target='_blank'>https://arxiv.org/pdf/2502.08377.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Liying Yang, Chen Liu, Zhenwei Zhu, Ajian Liu, Hui Ma, Jian Nong, Yanyan Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.08377">Not All Frame Features Are Equal: Video-to-4D Generation via Decoupling Dynamic-Static Features</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, the generation of dynamic 3D objects from a video has shown impressive results. Existing methods directly optimize Gaussians using whole information in frames. However, when dynamic regions are interwoven with static regions within frames, particularly if the static regions account for a large proportion, existing methods often overlook information in dynamic regions and are prone to overfitting on static regions. This leads to producing results with blurry textures. We consider that decoupling dynamic-static features to enhance dynamic representations can alleviate this issue. Thus, we propose a dynamic-static feature decoupling module (DSFD). Along temporal axes, it regards the regions of current frame features that possess significant differences relative to reference frame features as dynamic features. Conversely, the remaining parts are the static features. Then, we acquire decoupled features driven by dynamic features and current frame features. Moreover, to further enhance the dynamic representation of decoupled features from different viewpoints and ensure accurate motion prediction, we design a temporal-spatial similarity fusion module (TSSF). Along spatial axes, it adaptively selects similar information of dynamic regions. Hinging on the above, we construct a novel approach, DS4D. Experimental results verify our method achieves state-of-the-art (SOTA) results in video-to-4D. In addition, the experiments on a real-world scenario dataset demonstrate its effectiveness on the 4D scene. Our code will be publicly available.
<div id='section'>Paperid: <span id='pid'>1050, <a href='https://arxiv.org/pdf/2501.07149.pdf' target='_blank'>https://arxiv.org/pdf/2501.07149.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Simon Hanisch, Julian Todt, Thorsten Strufe
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.07149">Pantomime: Motion Data Anonymization using Foundation Motion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion is a behavioral biometric trait that can be used to identify individuals and infer private attributes such as medical conditions. This poses a serious threat to privacy as motion extraction from video and motion capture are increasingly used for a variety of applications, including mixed reality, robotics, medicine, and the quantified self. In order to protect the privacy of the tracked individuals, anonymization techniques that preserve the utility of the data are required. However, anonymizing motion data is a challenging task because there are many dependencies in motion sequences (such as physiological constraints) that, if ignored, make the anonymized motion sequence appear unnatural. In this paper, we propose Pantomime, a full-body anonymization technique for motion data, which uses foundation motion models to generate motion sequences that adhere to the dependencies in the data, thus keeping the utility of the anonymized data high. Our results show that Pantomime can maintain the naturalness of the motion sequences while reducing the identification accuracy to 10%.
<div id='section'>Paperid: <span id='pid'>1051, <a href='https://arxiv.org/pdf/2411.12831.pdf' target='_blank'>https://arxiv.org/pdf/2411.12831.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Paul Janson, Tiberiu Popa, Eugene Belilovsky
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.12831">Towards motion from video diffusion models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-conditioned video diffusion models have emerged as a powerful tool in the realm of video generation and editing. But their ability to capture the nuances of human movement remains under-explored. Indeed the ability of these models to faithfully model an array of text prompts can lead to a wide host of applications in human and character animation. In this work, we take initial steps to investigate whether these models can effectively guide the synthesis of realistic human body animations. Specifically we propose to synthesize human motion by deforming an SMPL-X body representation guided by Score distillation sampling (SDS) calculated using a video diffusion model. By analyzing the fidelity of the resulting animations, we gain insights into the extent to which we can obtain motion using publicly available text-to-video diffusion models using SDS. Our findings shed light on the potential and limitations of these models for generating diverse and plausible human motions, paving the way for further research in this exciting area.
<div id='section'>Paperid: <span id='pid'>1052, <a href='https://arxiv.org/pdf/2410.10646.pdf' target='_blank'>https://arxiv.org/pdf/2410.10646.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>James R. Han, Hugues Thomas, Jian Zhang, Nicholas Rhinehart, Timothy D. Barfoot
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.10646">DR-MPC: Deep Residual Model Predictive Control for Real-world Social Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>How can a robot safely navigate around people with complex motion patterns? Deep Reinforcement Learning (DRL) in simulation holds some promise, but much prior work relies on simulators that fail to capture the nuances of real human motion. Thus, we propose Deep Residual Model Predictive Control (DR-MPC) to enable robots to quickly and safely perform DRL from real-world crowd navigation data. By blending MPC with model-free DRL, DR-MPC overcomes the DRL challenges of large data requirements and unsafe initial behavior. DR-MPC is initialized with MPC-based path tracking, and gradually learns to interact more effectively with humans. To further accelerate learning, a safety component estimates out-of-distribution states to guide the robot away from likely collisions. In simulation, we show that DR-MPC substantially outperforms prior work, including traditional DRL and residual DRL models. Hardware experiments show our approach successfully enables a robot to navigate a variety of crowded situations with few errors using less than 4 hours of training data.
<div id='section'>Paperid: <span id='pid'>1053, <a href='https://arxiv.org/pdf/2410.07554.pdf' target='_blank'>https://arxiv.org/pdf/2410.07554.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenhai Liu, Junbo Wang, Yiming Wang, Weiming Wang, Cewu Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.07554">ForceMimic: Force-Centric Imitation Learning with Force-Motion Capture System for Contact-Rich Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In most contact-rich manipulation tasks, humans apply time-varying forces to the target object, compensating for inaccuracies in the vision-guided hand trajectory. However, current robot learning algorithms primarily focus on trajectory-based policy, with limited attention given to learning force-related skills. To address this limitation, we introduce ForceMimic, a force-centric robot learning system, providing a natural, force-aware and robot-free robotic demonstration collection system, along with a hybrid force-motion imitation learning algorithm for robust contact-rich manipulation. Using the proposed ForceCapture system, an operator can peel a zucchini in 5 minutes, while force-feedback teleoperation takes over 13 minutes and struggles with task completion. With the collected data, we propose HybridIL to train a force-centric imitation learning model, equipped with hybrid force-position control primitive to fit the predicted wrench-position parameters during robot execution. Experiments demonstrate that our approach enables the model to learn a more robust policy under the contact-rich task of vegetable peeling, increasing the success rates by 54.5% relatively compared to state-ofthe-art pure-vision-based imitation learning. Hardware, code, data and more results can be found on the project website at https://forcemimic.github.io.
<div id='section'>Paperid: <span id='pid'>1054, <a href='https://arxiv.org/pdf/2409.20527.pdf' target='_blank'>https://arxiv.org/pdf/2409.20527.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoyang Wang, Haoran Guo, He Ba, Zhengxiong Li, Lingfeng Tao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.20527">Bi-directional Momentum-based Haptic Feedback and Control System for In-Hand Dexterous Telemanipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In-hand dexterous telemanipulation requires not only precise remote motion control of the robot but also effective haptic feedback to the human operator to ensure stable and intuitive interactions between them. Most existing haptic devices for dexterous telemanipulation focus on force feedback and lack effective torque rendering, which is essential for tasks involving object rotation. While some torque feedback solutions in virtual reality applications-such as those based on geared motors or mechanically coupled actuators-have been explored, they often rely on bulky mechanical designs, limiting their use in portable or in-hand applications. In this paper, we propose a Bi-directional Momentum-based Haptic Feedback and Control (Bi-Hap) system that utilizes a palm-sized momentum-actuated mechanism to enable real-time haptic and torque feedback. The Bi-Hap system also integrates an Inertial Measurement Unit (IMU) to extract the human's manipulation command to establish a closed-loop learning-based telemanipulation framework. Furthermore, an error-adaptive feedback strategy is introduced to enhance operator perception and task performance in different error categories. Experimental evaluations demonstrate that Bi-Hap achieved feedback capability with low command following latency (Delay < 0.025 s) and highly accurate torque feedback (RMSE < 0.010 Nm).
<div id='section'>Paperid: <span id='pid'>1055, <a href='https://arxiv.org/pdf/2407.21339.pdf' target='_blank'>https://arxiv.org/pdf/2407.21339.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dang Van Trong, Hiroki Kotake, Sumitaka Honji, Takahiro Wada
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.21339">A Cooperation Control Framework Based on Admittance Control and Time-varying Passive Velocity Field Control for Human-Robot Co-carrying Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human-robot co-carrying tasks reveal their potential in both industrial and everyday applications by leveraging the strengths of both parties. Effective control of robots in these tasks requires managing the energy level in the closed-loop systems to prevent potential dangers while also minimizing motion errors to complete the shared tasks. The collaborative tasks pose numerous challenges due to varied human intentions in adapting to workspace characteristics, leading to human-robot conflicts. In this paper, we develop a cooperation control framework for human-robot co-carrying tasks constructed by utilizing reference generator and low-level controller to aim to achieve safe interaction and synchronized human-robot movement. Firstly, the human motion predictions are corrected in the event of prediction errors based on the conflicts measured by the interaction forces through admittance control, thereby mitigating conflict levels. Low-level controller using an energy-compensation passive velocity field control approach allows encoding the corrected motion to produce control torques for the robot. In this manner, the closed-loop robotic system is passive when the energy level exceeds the predetermined threshold, and otherwise. Furthermore, the proposed control approach ensures that the system's kinetic energy is compensated within a finite time interval. The passivity, stability, convergence rate of energy, and power flow regulation are analyzed from theoretical viewpoints. Human-in-the-loop experiments involving 18 participants have demonstrated that the proposed method significantly enhances task performance and reduces human workload, as evidenced by both objective metrics and subjective evaluations, with improvements confirmed by statistical tests (p < 0.05) relative to baseline methods.
<div id='section'>Paperid: <span id='pid'>1056, <a href='https://arxiv.org/pdf/2407.10547.pdf' target='_blank'>https://arxiv.org/pdf/2407.10547.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andrea Eirale, Matteo Leonetti, Marcello Chiaberge
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.10547">Learning Social Cost Functions for Human-Aware Path Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Achieving social acceptance is one of the main goals of Social Robotic Navigation. Despite this topic has received increasing interest in recent years, most of the research has focused on driving the robotic agent along obstacle-free trajectories, planning around estimates of future human motion to respect personal distances and optimize navigation. However, social interactions in everyday life are also dictated by norms that do not strictly depend on movement, such as when standing at the end of a queue rather than cutting it. In this paper, we propose a novel method to recognize common social scenarios and modify a traditional planner's cost function to adapt to them. This solution enables the robot to carry out different social navigation behaviors that would not arise otherwise, maintaining the robustness of traditional navigation. Our approach allows the robot to learn different social norms with a single learned model, rather than having different modules for each task. As a proof of concept, we consider the tasks of queuing and respect interaction spaces of groups of people talking to one another, but the method can be extended to other human activities that do not involve motion.
<div id='section'>Paperid: <span id='pid'>1057, <a href='https://arxiv.org/pdf/2406.14422.pdf' target='_blank'>https://arxiv.org/pdf/2406.14422.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingkun Wang, Xiaoguang Ren, Ruochun Jin, Minglong Li, Xiaochuan Zhang, Changqian Yu, Mingxu Wang, Wenjing Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.14422">FutureNet-LOF: Joint Trajectory Prediction and Lane Occupancy Field Prediction with Future Context Encoding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Most prior motion prediction endeavors in autonomous driving have inadequately encoded future scenarios, leading to predictions that may fail to accurately capture the diverse movements of agents (e.g., vehicles or pedestrians). To address this, we propose FutureNet, which explicitly integrates initially predicted trajectories into the future scenario and further encodes these future contexts to enhance subsequent forecasting. Additionally, most previous motion forecasting works have focused on predicting independent futures for each agent. However, safe and smooth autonomous driving requires accurately predicting the diverse future behaviors of numerous surrounding agents jointly in complex dynamic environments. Given that all agents occupy certain potential travel spaces and possess lane driving priority, we propose Lane Occupancy Field (LOF), a new representation with lane semantics for motion forecasting in autonomous driving. LOF can simultaneously capture the joint probability distribution of all road participants' future spatial-temporal positions. Due to the high compatibility between lane occupancy field prediction and trajectory prediction, we propose a novel network with future context encoding for the joint prediction of these two tasks. Our approach ranks 1st on two large-scale motion forecasting benchmarks: Argoverse 1 and Argoverse 2.
<div id='section'>Paperid: <span id='pid'>1058, <a href='https://arxiv.org/pdf/2405.16152.pdf' target='_blank'>https://arxiv.org/pdf/2405.16152.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiawei Fang, Haishan Song, Chengxu Zuo, Xiaoxia Gao, Xiaowei Chen, Shihui Guo, Yipeng Qin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.16152">SuDA: Support-based Domain Adaptation for Sim2Real Motion Capture with Flexible Sensors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Flexible sensors hold promise for human motion capture (MoCap), offering advantages such as wearability, privacy preservation, and minimal constraints on natural movement. However, existing flexible sensor-based MoCap methods rely on deep learning and necessitate large and diverse labeled datasets for training. These data typically need to be collected in MoCap studios with specialized equipment and substantial manual labor, making them difficult and expensive to obtain at scale. Thanks to the high-linearity of flexible sensors, we address this challenge by proposing a novel Sim2Real Mocap solution based on domain adaptation, eliminating the need for labeled data yet achieving comparable accuracy to supervised learning. Our solution relies on a novel Support-based Domain Adaptation method, namely SuDA, which aligns the supports of the predictive functions rather than the instance-dependent distributions between the source and target domains. Extensive experimental results demonstrate the effectiveness of our method andits superiority over state-of-the-art distribution-based domain adaptation methods in our task.
<div id='section'>Paperid: <span id='pid'>1059, <a href='https://arxiv.org/pdf/2405.07680.pdf' target='_blank'>https://arxiv.org/pdf/2405.07680.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ali Ismail-Fawaz, Maxime Devanne, Stefano Berretti, Jonathan Weber, Germain Forestier
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.07680">Establishing a Unified Evaluation Framework for Human Motion Generation: A Comparative Analysis of Metrics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The development of generative artificial intelligence for human motion generation has expanded rapidly, necessitating a unified evaluation framework. This paper presents a detailed review of eight evaluation metrics for human motion generation, highlighting their unique features and shortcomings. We propose standardized practices through a unified evaluation setup to facilitate consistent model comparisons. Additionally, we introduce a novel metric that assesses diversity in temporal distortion by analyzing warping diversity, thereby enhancing the evaluation of temporal data. We also conduct experimental analyses of three generative models using a publicly available dataset, offering insights into the interpretation of each metric in specific case scenarios. Our goal is to offer a clear, user-friendly evaluation framework for newcomers, complemented by publicly accessible code.
<div id='section'>Paperid: <span id='pid'>1060, <a href='https://arxiv.org/pdf/2404.11375.pdf' target='_blank'>https://arxiv.org/pdf/2404.11375.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinghan Wang, Zixi Kang, Yadong Mu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.11375">Text-controlled Motion Mamba: Text-Instructed Temporal Grounding of Human Motion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion understanding is a fundamental task with diverse practical applications, facilitated by the availability of large-scale motion capture datasets. Recent studies focus on text-motion tasks, such as text-based motion generation, editing and question answering. In this study, we introduce the novel task of text-based human motion grounding (THMG), aimed at precisely localizing temporal segments corresponding to given textual descriptions within untrimmed motion sequences. Capturing global temporal information is crucial for the THMG task. However, transformer-based models that rely on global temporal self-attention face challenges when handling long untrimmed sequences due to the quadratic computational cost. We address these challenges by proposing Text-controlled Motion Mamba (TM-Mamba), a unified model that integrates temporal global context, language query control, and spatial graph topology with only linear memory cost. The core of the model is a text-controlled selection mechanism which dynamically incorporates global temporal information based on text query. The model is further enhanced to be topology-aware through the integration of relational embeddings. For evaluation, we introduce BABEL-Grounding, the first text-motion dataset that provides detailed textual descriptions of human actions along with their corresponding temporal segments. Extensive evaluations demonstrate the effectiveness of TM-Mamba on BABEL-Grounding.
<div id='section'>Paperid: <span id='pid'>1061, <a href='https://arxiv.org/pdf/2404.10240.pdf' target='_blank'>https://arxiv.org/pdf/2404.10240.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fan Zhang, Jinfeng Chen, Yu Hu, Zhiqiang Gao, Ge Lv, Qin Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.10240">Disturbance Rejection-Guarded Learning for Vibration Suppression of Two-Inertia Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Model uncertainty presents significant challenges in vibration suppression of multi-inertia systems, as these systems often rely on inaccurate nominal mathematical models due to system identification errors or unmodeled dynamics. An observer, such as an extended state observer (ESO), can estimate the discrepancy between the inaccurate nominal model and the true model, thus improving control performance via disturbance rejection. The conventional observer design is memoryless in the sense that once its estimated disturbance is obtained and sent to the controller, the datum is discarded. In this research, we propose a seamless integration of ESO and machine learning. On one hand, the machine learning model attempts to model the disturbance. With the assistance of prior information about the disturbance, the observer is expected to achieve faster convergence in disturbance estimation. On the other hand, machine learning benefits from an additional assurance layer provided by the ESO, as any imperfections in the machine learning model can be compensated for by the ESO. We validated the effectiveness of this novel learning-for-control paradigm through simulation and physical tests on two-inertial motion control systems used for vibration studies.
<div id='section'>Paperid: <span id='pid'>1062, <a href='https://arxiv.org/pdf/2404.09499.pdf' target='_blank'>https://arxiv.org/pdf/2404.09499.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuaiying Hou, Hongyu Tao, Junheng Fang, Changqing Zou, Hujun Bao, Weiwei Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.09499">Learning Human Motion from Monocular Videos via Cross-Modal Manifold Alignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning 3D human motion from 2D inputs is a fundamental task in the realms of computer vision and computer graphics. Many previous methods grapple with this inherently ambiguous task by introducing motion priors into the learning process. However, these approaches face difficulties in defining the complete configurations of such priors or training a robust model. In this paper, we present the Video-to-Motion Generator (VTM), which leverages motion priors through cross-modal latent feature space alignment between 3D human motion and 2D inputs, namely videos and 2D keypoints. To reduce the complexity of modeling motion priors, we model the motion data separately for the upper and lower body parts. Additionally, we align the motion data with a scale-invariant virtual skeleton to mitigate the interference of human skeleton variations to the motion priors. Evaluated on AIST++, the VTM showcases state-of-the-art performance in reconstructing 3D human motion from monocular videos. Notably, our VTM exhibits the capabilities for generalization to unseen view angles and in-the-wild videos.
<div id='section'>Paperid: <span id='pid'>1063, <a href='https://arxiv.org/pdf/2404.05490.pdf' target='_blank'>https://arxiv.org/pdf/2404.05490.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Baiyi Li, Edmond S. L. Ho, Hubert P. H. Shum, He Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.05490">Two-Person Interaction Augmentation with Skeleton Priors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Close and continuous interaction with rich contacts is a crucial aspect of human activities (e.g. hugging, dancing) and of interest in many domains like activity recognition, motion prediction, character animation, etc. However, acquiring such skeletal motion is challenging. While direct motion capture is expensive and slow, motion editing/generation is also non-trivial, as complex contact patterns with topological and geometric constraints have to be retained. To this end, we propose a new deep learning method for two-body skeletal interaction motion augmentation, which can generate variations of contact-rich interactions with varying body sizes and proportions while retaining the key geometric/topological relations between two bodies. Our system can learn effectively from a relatively small amount of data and generalize to drastically different skeleton sizes. Through exhaustive evaluation and comparison, we show it can generate high-quality motions, has strong generalizability and outperforms traditional optimization-based methods and alternative deep learning solutions.
<div id='section'>Paperid: <span id='pid'>1064, <a href='https://arxiv.org/pdf/2404.04419.pdf' target='_blank'>https://arxiv.org/pdf/2404.04419.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ehsan Nasiri, Long Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.04419">Hybrid Force Motion Control with Estimated Surface Normal for Manufacturing Applications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper proposes a hybrid force-motion framework that utilizes real-time surface normal updates. The surface normal is estimated via a novel method that leverages force sensing measurements and velocity commands to compensate the friction bias. This approach is critical for robust execution of precision force-controlled tasks in manufacturing, such as thermoplastic tape replacement that traces surfaces or paths on a workpiece subject to uncertainties deviated from the model. We formulated the proposed method and implemented the framework in ROS2 environment. The approach was validated using kinematic simulations and a hardware platform. Specifically, we demonstrated the approach on a 7-DoF manipulator equipped with a force/torque sensor at the end-effector.
<div id='section'>Paperid: <span id='pid'>1065, <a href='https://arxiv.org/pdf/2403.14536.pdf' target='_blank'>https://arxiv.org/pdf/2403.14536.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Charlott Vallon, Mark Pustilnik, Alessandro Pinto, Francesco Borrelli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.14536">Learning Hierarchical Control Systems for Autonomous Systems with Energy Constraints</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper focuses on the design of hierarchical control architectures for autonomous systems with energy constraints. We focus on systems where energy storage limitations and slow recharge rates drastically affect the way the autonomous systems are operated. Using examples from space robotics and public transportation, we motivate the need for formally designed learning hierarchical control systems. We propose a learning control architecture which incorporates learning mechanisms at various levels of the control hierarchy to improve performance and resource utilization. The proposed hierarchical control scheme relies on high-level energy-aware task planning and assignment, complemented by a low-level predictive control mechanism responsible for the autonomous execution of tasks, including motion control and energy management. Simulation examples show the benefits and the limitations of the proposed architecture when learning is used to obtain a more energy-efficient task allocation.
<div id='section'>Paperid: <span id='pid'>1066, <a href='https://arxiv.org/pdf/2403.00691.pdf' target='_blank'>https://arxiv.org/pdf/2403.00691.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kangning Yin, Shihao Zou, Yuxuan Ge, Zheng Tian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.00691">Tri-Modal Motion Retrieval by Learning a Joint Embedding Space</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Information retrieval is an ever-evolving and crucial research domain. The substantial demand for high-quality human motion data especially in online acquirement has led to a surge in human motion research works. Prior works have mainly concentrated on dual-modality learning, such as text and motion tasks, but three-modality learning has been rarely explored. Intuitively, an extra introduced modality can enrich a model's application scenario, and more importantly, an adequate choice of the extra modality can also act as an intermediary and enhance the alignment between the other two disparate modalities. In this work, we introduce LAVIMO (LAnguage-VIdeo-MOtion alignment), a novel framework for three-modality learning integrating human-centric videos as an additional modality, thereby effectively bridging the gap between text and motion. Moreover, our approach leverages a specially designed attention mechanism to foster enhanced alignment and synergistic effects among text, video, and motion modalities. Empirically, our results on the HumanML3D and KIT-ML datasets show that LAVIMO achieves state-of-the-art performance in various motion-related cross-modal retrieval tasks, including text-to-motion, motion-to-text, video-to-motion and motion-to-video.
<div id='section'>Paperid: <span id='pid'>1067, <a href='https://arxiv.org/pdf/2402.02150.pdf' target='_blank'>https://arxiv.org/pdf/2402.02150.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Koyu Mizutani, Haruki Mitarai, Kakeru Miyazaki, Soichiro Kumano, Toshihiko Yamasaki
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.02150">Data-Driven Prediction of Seismic Intensity Distributions Featuring Hybrid Classification-Regression Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Earthquakes are among the most immediate and deadly natural disasters that humans face. Accurately forecasting the extent of earthquake damage and assessing potential risks can be instrumental in saving numerous lives. In this study, we developed linear regression models capable of predicting seismic intensity distributions based on earthquake parameters: location, depth, and magnitude. Because it is completely data-driven, it can predict intensity distributions without geographical information. The dataset comprises seismic intensity data from earthquakes that occurred in the vicinity of Japan between 1997 and 2020, specifically containing 1,857 instances of earthquakes with a magnitude of 5.0 or greater, sourced from the Japan Meteorological Agency. We trained both regression and classification models and combined them to take advantage of both to create a hybrid model. The proposed model outperformed commonly used Ground Motion Prediction Equations (GMPEs) in terms of the correlation coefficient, F1 score, and MCC. Furthermore, the proposed model can predict even abnormal seismic intensity distributions, a task at conventional GMPEs often struggle.
<div id='section'>Paperid: <span id='pid'>1068, <a href='https://arxiv.org/pdf/2401.12965.pdf' target='_blank'>https://arxiv.org/pdf/2401.12965.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi-Shiuan Tung, Matthew B. Luebbers, Alessandro Roncone, Bradley Hayes
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.12965">Workspace Optimization Techniques to Improve Prediction of Human Motion During Human-Robot Collaboration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding human intentions is critical for safe and effective human-robot collaboration. While state of the art methods for human goal prediction utilize learned models to account for the uncertainty of human motion data, that data is inherently stochastic and high variance, hindering those models' utility for interactions requiring coordination, including safety-critical or close-proximity tasks. Our key insight is that robot teammates can deliberately configure shared workspaces prior to interaction in order to reduce the variance in human motion, realizing classifier-agnostic improvements in goal prediction. In this work, we present an algorithmic approach for a robot to arrange physical objects and project "virtual obstacles" using augmented reality in shared human-robot workspaces, optimizing for human legibility over a given set of tasks. We compare our approach against other workspace arrangement strategies using two human-subjects studies, one in a virtual 2D navigation domain and the other in a live tabletop manipulation domain involving a robotic manipulator arm. We evaluate the accuracy of human motion prediction models learned from each condition, demonstrating that our workspace optimization technique with virtual obstacles leads to higher robot prediction accuracy using less training data.
<div id='section'>Paperid: <span id='pid'>1069, <a href='https://arxiv.org/pdf/2512.21133.pdf' target='_blank'>https://arxiv.org/pdf/2512.21133.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoyu Mo, Jintian Ge, Zifan Wang, Chen Lv, Karl Henrik Johansson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.21133">SparScene: Efficient Traffic Scene Representation via Sparse Graph Learning for Large-Scale Trajectory Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent trajectory generation is a core problem for autonomous driving and intelligent transportation systems. However, efficiently modeling the dynamic interactions between numerous road users and infrastructures in complex scenes remains an open problem. Existing methods typically employ distance-based or fully connected dense graph structures to capture interaction information, which not only introduces a large number of redundant edges but also requires complex and heavily parameterized networks for encoding, thereby resulting in low training and inference efficiency, limiting scalability to large and complex traffic scenes. To overcome the limitations of existing methods, we propose SparScene, a sparse graph learning framework designed for efficient and scalable traffic scene representation. Instead of relying on distance thresholds, SparScene leverages the lane graph topology to construct structure-aware sparse connections between agents and lanes, enabling efficient yet informative scene graph representation. SparScene adopts a lightweight graph encoder that efficiently aggregates agent-map and agent-agent interactions, yielding compact scene representations with substantially improved efficiency and scalability. On the motion prediction benchmark of the Waymo Open Motion Dataset (WOMD), SparScene achieves competitive performance with remarkable efficiency. It generates trajectories for more than 200 agents in a scene within 5 ms and scales to more than 5,000 agents and 17,000 lanes with merely 54 ms of inference time with a GPU memory of 2.9 GB, highlighting its superior scalability for large-scale traffic scenes.
<div id='section'>Paperid: <span id='pid'>1070, <a href='https://arxiv.org/pdf/2512.14095.pdf' target='_blank'>https://arxiv.org/pdf/2512.14095.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sisi Dai, Kai Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.14095">AnchorHOI: Zero-shot Generation of 4D Human-Object Interaction via Anchor-based Prior Distillation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite significant progress in text-driven 4D human-object interaction (HOI) generation with supervised methods, the scalability remains limited by the scarcity of large-scale 4D HOI datasets. To overcome this, recent approaches attempt zero-shot 4D HOI generation with pre-trained image diffusion models. However, interaction cues are minimally distilled during the generation process, restricting their applicability across diverse scenarios. In this paper, we propose AnchorHOI, a novel framework that thoroughly exploits hybrid priors by incorporating video diffusion models beyond image diffusion models, advancing 4D HOI generation. Nevertheless, directly optimizing high-dimensional 4D HOI with such priors remains challenging, particularly for human pose and compositional motion. To address this challenge, AnchorHOI introduces an anchor-based prior distillation strategy, which constructs interaction-aware anchors and then leverages them to guide generation in a tractable two-step process. Specifically, two tailored anchors are designed for 4D HOI generation: anchor Neural Radiance Fields (NeRFs) for expressive interaction composition, and anchor keypoints for realistic motion synthesis. Extensive experiments demonstrate that AnchorHOI outperforms previous methods with superior diversity and generalization.
<div id='section'>Paperid: <span id='pid'>1071, <a href='https://arxiv.org/pdf/2511.01463.pdf' target='_blank'>https://arxiv.org/pdf/2511.01463.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lei Hu, Yongjing Ye, Shihong Xia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.01463">HMVLM: Human Motion-Vision-Lanuage Model via MoE LoRA</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The expansion of instruction-tuning data has enabled foundation language models to exhibit improved instruction adherence and superior performance across diverse downstream tasks. Semantically-rich 3D human motion is being progressively integrated with these foundation models to enhance multimodal understanding and cross-modal generation capabilities. However, the modality gap between human motion and text raises unresolved concerns about catastrophic forgetting during this integration. In addition, developing autoregressive-compatible pose representations that preserve generalizability across heterogeneous downstream tasks remains a critical technical barrier. To address these issues, we propose the Human Motion-Vision-Language Model (HMVLM), a unified framework based on the Mixture of Expert Low-Rank Adaption(MoE LoRA) strategy. The framework leverages the gating network to dynamically allocate LoRA expert weights based on the input prompt, enabling synchronized fine-tuning of multiple tasks. To mitigate catastrophic forgetting during instruction-tuning, we introduce a novel zero expert that preserves the pre-trained parameters for general linguistic tasks. For pose representation, we implement body-part-specific tokenization by partitioning the human body into different joint groups, enhancing the spatial resolution of the representation. Experiments show that our method effectively alleviates knowledge forgetting during instruction-tuning and achieves remarkable performance across diverse human motion downstream tasks.
<div id='section'>Paperid: <span id='pid'>1072, <a href='https://arxiv.org/pdf/2510.06251.pdf' target='_blank'>https://arxiv.org/pdf/2510.06251.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ieva Bagdonaviciute, Vibhav Vineet
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.06251">Does Physics Knowledge Emerge in Frontier Models?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Leading Vision-Language Models (VLMs) show strong results in visual perception and general reasoning, but their ability to understand and predict physical dynamics remains unclear. We benchmark six frontier VLMs on three physical simulation datasets - CLEVRER, Physion, and Physion++ - where the evaluation tasks test whether a model can predict outcomes or hypothesize about alternative situations. To probe deeper, we design diagnostic subtests that isolate perception (objects, colors, occluders) from physics reasoning (motion prediction, spatial relations). Intuitively, stronger diagnostic performance should support higher evaluation accuracy. Yet our analysis reveals weak correlations: models that excel at perception or physics reasoning do not consistently perform better on predictive or counterfactual evaluation. This counterintuitive gap exposes a central limitation of current VLMs: perceptual and physics skills remain fragmented and fail to combine into causal understanding, underscoring the need for architectures that bind perception and reasoning more tightly.
<div id='section'>Paperid: <span id='pid'>1073, <a href='https://arxiv.org/pdf/2509.14010.pdf' target='_blank'>https://arxiv.org/pdf/2509.14010.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zong Chen, Shaoyang Li, Ben Liu, Min Li, Zhouping Yin, Yiqun Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14010">Whole-body Motion Control of an Omnidirectional Wheel-Legged Mobile Manipulator via Contact-Aware Dynamic Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Wheel-legged robots with integrated manipulators hold great promise for mobile manipulation in logistics, industrial automation, and human-robot collaboration. However, unified control of such systems remains challenging due to the redundancy in degrees of freedom, complex wheel-ground contact dynamics, and the need for seamless coordination between locomotion and manipulation. In this work, we present the design and whole-body motion control of an omnidirectional wheel-legged quadrupedal robot equipped with a dexterous manipulator. The proposed platform incorporates independently actuated steering modules and hub-driven wheels, enabling agile omnidirectional locomotion with high maneuverability in structured environments. To address the challenges of contact-rich interaction, we develop a contact-aware whole-body dynamic optimization framework that integrates point-contact modeling for manipulation with line-contact modeling for wheel-ground interactions. A warm-start strategy is introduced to accelerate online optimization, ensuring real-time feasibility for high-dimensional control. Furthermore, a unified kinematic model tailored for the robot's 4WIS-4WID actuation scheme eliminates the need for mode switching across different locomotion strategies, improving control consistency and robustness. Simulation and experimental results validate the effectiveness of the proposed framework, demonstrating agile terrain traversal, high-speed omnidirectional mobility, and precise manipulation under diverse scenarios, underscoring the system's potential for factory automation, urban logistics, and service robotics in semi-structured environments.
<div id='section'>Paperid: <span id='pid'>1074, <a href='https://arxiv.org/pdf/2509.12151.pdf' target='_blank'>https://arxiv.org/pdf/2509.12151.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zongyao Yi, Joachim Hertzberg, Martin Atzmueller
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.12151">Learning Contact Dynamics for Control with Action-conditioned Face Interaction Graph Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a learnable physics simulator that provides accurate motion and force-torque prediction of robot end effectors in contact-rich manipulation. The proposed model extends the state-of-the-art GNN-based simulator (FIGNet) with novel node and edge types, enabling action-conditional predictions for control and state estimation tasks. In simulation, the MPC agent using our model matches the performance of the same controller with the ground truth dynamics model in a challenging peg-in-hole task, while in the real-world experiment, our model achieves a 50% improvement in motion prediction accuracy and 3$\times$ increase in force-torque prediction precision over the baseline physics simulator. Source code and data are publicly available.
<div id='section'>Paperid: <span id='pid'>1075, <a href='https://arxiv.org/pdf/2509.11453.pdf' target='_blank'>https://arxiv.org/pdf/2509.11453.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>BaiChen Fan, Sifan Zhou, Jian Li, Shibo Zhao, Muqing Cao, Qin Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11453">Beyond Frame-wise Tracking: A Trajectory-based Paradigm for Efficient Point Cloud Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>LiDAR-based 3D single object tracking (3D SOT) is a critical task in robotics and autonomous systems. Existing methods typically follow frame-wise motion estimation or a sequence-based paradigm. However, the two-frame methods are efficient but lack long-term temporal context, making them vulnerable in sparse or occluded scenes, while sequence-based methods that process multiple point clouds gain robustness at a significant computational cost. To resolve this dilemma, we propose a novel trajectory-based paradigm and its instantiation, TrajTrack. TrajTrack is a lightweight framework that enhances a base two-frame tracker by implicitly learning motion continuity from historical bounding box trajectories alone-without requiring additional, costly point cloud inputs. It first generates a fast, explicit motion proposal and then uses an implicit motion modeling module to predict the future trajectory, which in turn refines and corrects the initial proposal. Extensive experiments on the large-scale NuScenes benchmark show that TrajTrack achieves new state-of-the-art performance, dramatically improving tracking precision by 4.48% over a strong baseline while running at 56 FPS. Besides, we also demonstrate the strong generalizability of TrajTrack across different base trackers. Video is available at https://www.bilibili.com/video/BV1ahYgzmEWP.
<div id='section'>Paperid: <span id='pid'>1076, <a href='https://arxiv.org/pdf/2509.06573.pdf' target='_blank'>https://arxiv.org/pdf/2509.06573.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jie Zhou, Linzi Qu, Miu-Ling Lam, Hongbo Fu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.06573">From Rigging to Waving: 3D-Guided Diffusion for Natural Animation of Hand-Drawn Characters</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Hand-drawn character animation is a vibrant field in computer graphics, presenting challenges in achieving geometric consistency while conveying expressive motion. Traditional skeletal animation methods maintain geometric consistency but struggle with complex non-rigid elements like flowing hair and skirts, leading to unnatural deformation. Conversely, video diffusion models synthesize realistic dynamics but often create geometric distortions in stylized drawings due to domain gaps. This work proposes a hybrid animation system that combines skeletal animation and video diffusion. Initially, coarse images are generated from characters retargeted with skeletal animations for geometric guidance. These images are then enhanced in texture and secondary dynamics using video diffusion priors, framing this enhancement as an inpainting task. A domain-adapted diffusion model refines user-masked regions needing improvement, especially for secondary dynamics. To enhance motion realism further, we introduce a Secondary Dynamics Injection (SDI) strategy in the denoising process, incorporating features from a pre-trained diffusion model enriched with human motion priors. Additionally, to tackle unnatural deformations from low-poly single-mesh character modeling, we present a Hair Layering Modeling (HLM) technique that uses segmentation maps to separate hair from the body, allowing for more natural animation of long-haired characters. Extensive experiments show that our system outperforms state-of-the-art methods in both quantitative and qualitative evaluations.
<div id='section'>Paperid: <span id='pid'>1077, <a href='https://arxiv.org/pdf/2509.04058.pdf' target='_blank'>https://arxiv.org/pdf/2509.04058.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lei Zhong, Yi Yang, Changjian Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.04058">SMooGPT: Stylized Motion Generation using Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Stylized motion generation is actively studied in computer graphics, especially benefiting from the rapid advances in diffusion models. The goal of this task is to produce a novel motion respecting both the motion content and the desired motion style, e.g., ``walking in a loop like a Monkey''. Existing research attempts to address this problem via motion style transfer or conditional motion generation. They typically embed the motion style into a latent space and guide the motion implicitly in a latent space as well. Despite the progress, their methods suffer from low interpretability and control, limited generalization to new styles, and fail to produce motions other than ``walking'' due to the strong bias in the public stylization dataset. In this paper, we propose to solve the stylized motion generation problem from a new perspective of reasoning-composition-generation, based on our observations: i) human motion can often be effectively described using natural language in a body-part centric manner, ii) LLMs exhibit a strong ability to understand and reason about human motion, and iii) human motion has an inherently compositional nature, facilitating the new motion content or style generation via effective recomposing. We thus propose utilizing body-part text space as an intermediate representation, and present SMooGPT, a fine-tuned LLM, acting as a reasoner, composer, and generator when generating the desired stylized motion. Our method executes in the body-part text space with much higher interpretability, enabling fine-grained motion control, effectively resolving potential conflicts between motion content and style, and generalizes well to new styles thanks to the open-vocabulary ability of LLMs. Comprehensive experiments and evaluations, and a user perceptual study, demonstrate the effectiveness of our approach, especially under the pure text-driven stylized motion generation.
<div id='section'>Paperid: <span id='pid'>1078, <a href='https://arxiv.org/pdf/2509.02983.pdf' target='_blank'>https://arxiv.org/pdf/2509.02983.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinghe Yang, Minh-Quan Le, Mingming Gong, Ye Pu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.02983">DUViN: Diffusion-Based Underwater Visual Navigation via Knowledge-Transferred Depth Features</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous underwater navigation remains a challenging problem due to limited sensing capabilities and the difficulty of constructing accurate maps in underwater environments. In this paper, we propose a Diffusion-based Underwater Visual Navigation policy via knowledge-transferred depth features, named DUViN, which enables vision-based end-to-end 4-DoF motion control for underwater vehicles in unknown environments. DUViN guides the vehicle to avoid obstacles and maintain a safe and perception awareness altitude relative to the terrain without relying on pre-built maps. To address the difficulty of collecting large-scale underwater navigation datasets, we propose a method that ensures robust generalization under domain shifts from in-air to underwater environments by leveraging depth features and introducing a novel model transfer strategy. Specifically, our training framework consists of two phases: we first train the diffusion-based visual navigation policy on in-air datasets using a pre-trained depth feature extractor. Secondly, we retrain the extractor on an underwater depth estimation task and integrate the adapted extractor into the trained navigation policy from the first step. Experiments in both simulated and real-world underwater environments demonstrate the effectiveness and generalization of our approach. The experimental videos are available at https://www.youtube.com/playlist?list=PLqt2s-RyCf1gfXJgFzKjmwIqYhrP4I-7Y.
<div id='section'>Paperid: <span id='pid'>1079, <a href='https://arxiv.org/pdf/2508.17173.pdf' target='_blank'>https://arxiv.org/pdf/2508.17173.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chao Ning, Han Wang, Longyan Li, Yang Shi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17173">Collaborative-Online-Learning-Enabled Distributionally Robust Motion Control for Multi-Robot Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper develops a novel COllaborative-Online-Learning (COOL)-enabled motion control framework for multi-robot systems to avoid collision amid randomly moving obstacles whose motion distributions are partially observable through decentralized data streams. To address the notable challenge of data acquisition due to occlusion, a COOL approach based on the Dirichlet process mixture model is proposed to efficiently extract motion distribution information by exchanging among robots selected learning structures. By leveraging the fine-grained local-moment information learned through COOL, a data-stream-driven ambiguity set for obstacle motion is constructed. We then introduce a novel ambiguity set propagation method, which theoretically admits the derivation of the ambiguity sets for obstacle positions over the entire prediction horizon by utilizing obstacle current positions and the ambiguity set for obstacle motion. Additionally, we develop a compression scheme with its safety guarantee to automatically adjust the complexity and granularity of the ambiguity set by aggregating basic ambiguity sets that are close in a measure space, thereby striking an attractive trade-off between control performance and computation time. Then the probabilistic collision-free trajectories are generated through distributionally robust optimization problems. The distributionally robust obstacle avoidance constraints based on the compressed ambiguity set are equivalently reformulated by deriving separating hyperplanes through tractable semi-definite programming. Finally, we establish the probabilistic collision avoidance guarantee and the long-term tracking performance guarantee for the proposed framework. The numerical simulations are used to demonstrate the efficacy and superiority of the proposed approach compared with state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>1080, <a href='https://arxiv.org/pdf/2508.01585.pdf' target='_blank'>https://arxiv.org/pdf/2508.01585.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hua Yu, Yaqing Hou, Xu Gui, Shanshan Feng, Dongsheng Zhou, Qiang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01585">A Spatio-temporal Continuous Network for Stochastic 3D Human Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Stochastic Human Motion Prediction (HMP) has received increasing attention due to its wide applications. Despite the rapid progress in generative fields, existing methods often face challenges in learning continuous temporal dynamics and predicting stochastic motion sequences. They tend to overlook the flexibility inherent in complex human motions and are prone to mode collapse. To alleviate these issues, we propose a novel method called STCN, for stochastic and continuous human motion prediction, which consists of two stages. Specifically, in the first stage, we propose a spatio-temporal continuous network to generate smoother human motion sequences. In addition, the anchor set is innovatively introduced into the stochastic HMP task to prevent mode collapse, which refers to the potential human motion patterns. In the second stage, STCN endeavors to acquire the Gaussian mixture distribution (GMM) of observed motion sequences with the aid of the anchor set. It also focuses on the probability associated with each anchor, and employs the strategy of sampling multiple sequences from each anchor to alleviate intra-class differences in human motions. Experimental results on two widely-used datasets (Human3.6M and HumanEva-I) demonstrate that our model obtains competitive performance on both diversity and accuracy.
<div id='section'>Paperid: <span id='pid'>1081, <a href='https://arxiv.org/pdf/2507.01491.pdf' target='_blank'>https://arxiv.org/pdf/2507.01491.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>S. Ali Hosseini, Fabian R. Quinten, Luke F. van Eijk, Dragan Kostic, S. Hassan HosseinNia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.01491">Frequency Domain Design of a Reset-Based Filter: An Add-On Nonlinear Filter for Industrial Motion Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study introduces a modified version of the Constant-in-Gain, Lead-in-Phase (CgLp) filter, which incorporates a feedthrough term in the First-Order Reset Element (FORE) to reduce the undesirable nonlinearities and achieve an almost constant gain across all frequencies. A backward calculation approach is proposed to derive the additional parameter introduced by the feedthrough term, enabling designers to easily tune the filter to generate the required phase. The paper also presents an add-on filter structure that can enhance the performance of an existing LTI controller without altering its robustness margins. A sensitivity improvement indicator is proposed to guide the tuning process, enabling designers to visualize the improvements in closed-loop performance. The proposed methodology is demonstrated through a case study of an industrial wire bonder machine, showcasing its effectiveness in addressing low-frequency vibrations and improving overall control performance.
<div id='section'>Paperid: <span id='pid'>1082, <a href='https://arxiv.org/pdf/2505.21146.pdf' target='_blank'>https://arxiv.org/pdf/2505.21146.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yang Zhao, Yan Zhang, Xubo Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.21146">IKMo: Image-Keyframed Motion Generation with Trajectory-Pose Conditioned Motion Diffusion Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing human motion generation methods with trajectory and pose inputs operate global processing on both modalities, leading to suboptimal outputs. In this paper, we propose IKMo, an image-keyframed motion generation method based on the diffusion model with trajectory and pose being decoupled. The trajectory and pose inputs go through a two-stage conditioning framework. In the first stage, the dedicated optimization module is applied to refine inputs. In the second stage, trajectory and pose are encoded via a Trajectory Encoder and a Pose Encoder in parallel. Then, motion with high spatial and semantic fidelity is guided by a motion ControlNet, which processes the fused trajectory and pose data. Experiment results based on HumanML3D and KIT-ML datasets demonstrate that the proposed method outperforms state-of-the-art on all metrics under trajectory-keyframe constraints. In addition, MLLM-based agents are implemented to pre-process model inputs. Given texts and keyframe images from users, the agents extract motion descriptions, keyframe poses, and trajectories as the optimized inputs into the motion generation model. We conducts a user study with 10 participants. The experiment results prove that the MLLM-based agents pre-processing makes generated motion more in line with users' expectation. We believe that the proposed method improves both the fidelity and controllability of motion generation by the diffusion model.
<div id='section'>Paperid: <span id='pid'>1083, <a href='https://arxiv.org/pdf/2505.09393.pdf' target='_blank'>https://arxiv.org/pdf/2505.09393.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Huakun Liu, Hiroki Ota, Xin Wei, Yutaro Hirao, Monica Perusquia-Hernandez, Hideaki Uchiyama, Kiyoshi Kiyokawa
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.09393">UMotion: Uncertainty-driven Human Motion Estimation from Inertial and Ultra-wideband Units</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Sparse wearable inertial measurement units (IMUs) have gained popularity for estimating 3D human motion. However, challenges such as pose ambiguity, data drift, and limited adaptability to diverse bodies persist. To address these issues, we propose UMotion, an uncertainty-driven, online fusing-all state estimation framework for 3D human shape and pose estimation, supported by six integrated, body-worn ultra-wideband (UWB) distance sensors with IMUs. UWB sensors measure inter-node distances to infer spatial relationships, aiding in resolving pose ambiguities and body shape variations when combined with anthropometric data. Unfortunately, IMUs are prone to drift, and UWB sensors are affected by body occlusions. Consequently, we develop a tightly coupled Unscented Kalman Filter (UKF) framework that fuses uncertainties from sensor data and estimated human motion based on individual body shape. The UKF iteratively refines IMU and UWB measurements by aligning them with uncertain human motion constraints in real-time, producing optimal estimates for each. Experiments on both synthetic and real-world datasets demonstrate the effectiveness of UMotion in stabilizing sensor data and the improvement over state of the art in pose accuracy.
<div id='section'>Paperid: <span id='pid'>1084, <a href='https://arxiv.org/pdf/2505.08235.pdf' target='_blank'>https://arxiv.org/pdf/2505.08235.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hanle Zheng, Xujie Han, Zegang Peng, Shangbin Zhang, Guangxun Du, Zhuo Zou, Xilin Wang, Jibin Wu, Hao Guo, Lei Deng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.08235">EventDiff: A Unified and Efficient Diffusion Model Framework for Event-based Video Frame Interpolation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video Frame Interpolation (VFI) is a fundamental yet challenging task in computer vision, particularly under conditions involving large motion, occlusion, and lighting variation. Recent advancements in event cameras have opened up new opportunities for addressing these challenges. While existing event-based VFI methods have succeeded in recovering large and complex motions by leveraging handcrafted intermediate representations such as optical flow, these designs often compromise high-fidelity image reconstruction under subtle motion scenarios due to their reliance on explicit motion modeling. Meanwhile, diffusion models provide a promising alternative for VFI by reconstructing frames through a denoising process, eliminating the need for explicit motion estimation or warping operations. In this work, we propose EventDiff, a unified and efficient event-based diffusion model framework for VFI. EventDiff features a novel Event-Frame Hybrid AutoEncoder (HAE) equipped with a lightweight Spatial-Temporal Cross Attention (STCA) module that effectively fuses dynamic event streams with static frames. Unlike previous event-based VFI methods, EventDiff performs interpolation directly in the latent space via a denoising diffusion process, making it more robust across diverse and challenging VFI scenarios. Through a two-stage training strategy that first pretrains the HAE and then jointly optimizes it with the diffusion model, our method achieves state-of-the-art performance across multiple synthetic and real-world event VFI datasets. The proposed method outperforms existing state-of-the-art event-based VFI methods by up to 1.98dB in PSNR on Vimeo90K-Triplet and shows superior performance in SNU-FILM tasks with multiple difficulty levels. Compared to the emerging diffusion-based VFI approach, our method achieves up to 5.72dB PSNR gain on Vimeo90K-Triplet and 4.24X faster inference.
<div id='section'>Paperid: <span id='pid'>1085, <a href='https://arxiv.org/pdf/2505.04961.pdf' target='_blank'>https://arxiv.org/pdf/2505.04961.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyu Zhang, Sergey Bashkirov, Dun Yang, Michael Taylor, Xue Bin Peng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.04961">ADD: Physics-Based Motion Imitation with Adversarial Differential Discriminators</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-objective optimization problems, which require the simultaneous optimization of multiple terms, are prevalent across numerous applications. Existing multi-objective optimization methods often rely on manually tuned aggregation functions to formulate a joint optimization target. The performance of such hand-tuned methods is heavily dependent on careful weight selection, a time-consuming and laborious process. These limitations also arise in the setting of reinforcement-learning-based motion tracking for physically simulated characters, where intricately crafted reward functions are typically used to achieve high-fidelity results. Such solutions not only require domain expertise and significant manual adjustment, but also limit the applicability of the resulting reward function across diverse skills. To bridge this gap, we present a novel adversarial multi-objective optimization technique that is broadly applicable to a range of multi-objective optimization problems, including motion tracking. The proposed adversarial differential discriminator receives a single positive sample, yet is still effective at guiding the optimization process. We demonstrate that our technique can enable characters to closely replicate a variety of acrobatic and agile behaviors, achieving comparable quality to state-of-the-art motion-tracking methods, without relying on manually tuned reward functions. Results are best visualized through https://youtu.be/rz8BYCE9E2w.
<div id='section'>Paperid: <span id='pid'>1086, <a href='https://arxiv.org/pdf/2503.19738.pdf' target='_blank'>https://arxiv.org/pdf/2503.19738.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yingqing Chen, Christos G. Cassandras
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.19738">Optimal Safe Sequencing and Motion Control for Mixed Traffic Roundabouts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper develops an Optimal Safe Sequencing (OSS) control framework for Connected and Automated Vehicles (CAVs) navigating a single-lane roundabout in mixed traffic, where both CAVs and Human-Driven Vehicles (HDVs) coexist. The framework jointly optimizes vehicle sequencing and motion control to minimize travel time, energy consumption, and discomfort while ensuring speed-dependent safety guarantees and adhering to velocity and acceleration constraints. This is achieved by integrating (a) a Safe Sequencing (SS) policy that ensures merging safety without requiring any knowledge of HDV behavior, and (b) a Model Predictive Control with Control Lyapunov Barrier Functions (MPC-CLBF) framework, which optimizes CAV motion control while mitigating infeasibility and myopic control issues common in the use of Control Barrier Functions (CBFs) to provide safety guarantees. Simulation results across various traffic demands, CAV penetration rates, and control parameters demonstrate the framework's effectiveness and stability.
<div id='section'>Paperid: <span id='pid'>1087, <a href='https://arxiv.org/pdf/2503.17695.pdf' target='_blank'>https://arxiv.org/pdf/2503.17695.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yikun Ma, Yiqing Li, Jiawei Wu, Xing Luo, Zhi Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.17695">MotionDiff: Training-free Zero-shot Interactive Motion Editing via Flow-assisted Multi-view Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generative models have made remarkable advancements and are capable of producing high-quality content. However, performing controllable editing with generative models remains challenging, due to their inherent uncertainty in outputs. This challenge is praticularly pronounced in motion editing, which involves the processing of spatial information. While some physics-based generative methods have attempted to implement motion editing, they typically operate on single-view images with simple motions, such as translation and dragging. These methods struggle to handle complex rotation and stretching motions and ensure multi-view consistency, often necessitating resource-intensive retraining. To address these challenges, we propose MotionDiff, a training-free zero-shot diffusion method that leverages optical flow for complex multi-view motion editing. Specifically, given a static scene, users can interactively select objects of interest to add motion priors. The proposed Point Kinematic Model (PKM) then estimates corresponding multi-view optical flows during the Multi-view Flow Estimation Stage (MFES). Subsequently, these optical flows are utilized to generate multi-view motion results through decoupled motion representation in the Multi-view Motion Diffusion Stage (MMDS). Extensive experiments demonstrate that MotionDiff outperforms other physics-based generative motion editing methods in achieving high-quality multi-view consistent motion results. Notably, MotionDiff does not require retraining, enabling users to conveniently adapt it for various down-stream tasks.
<div id='section'>Paperid: <span id='pid'>1088, <a href='https://arxiv.org/pdf/2503.17340.pdf' target='_blank'>https://arxiv.org/pdf/2503.17340.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Congyi Fan, Jian Guan, Xuanjia Zhao, Dongli Xu, Youtian Lin, Tong Ye, Pengming Feng, Haiwei Pan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.17340">Align Your Rhythm: Generating Highly Aligned Dance Poses with Gating-Enhanced Rhythm-Aware Feature Representation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automatically generating natural, diverse and rhythmic human dance movements driven by music is vital for virtual reality and film industries. However, generating dance that naturally follows music remains a challenge, as existing methods lack proper beat alignment and exhibit unnatural motion dynamics. In this paper, we propose Danceba, a novel framework that leverages gating mechanism to enhance rhythm-aware feature representation for music-driven dance generation, which achieves highly aligned dance poses with enhanced rhythmic sensitivity. Specifically, we introduce Phase-Based Rhythm Extraction (PRE) to precisely extract rhythmic information from musical phase data, capitalizing on the intrinsic periodicity and temporal structures of music. Additionally, we propose Temporal-Gated Causal Attention (TGCA) to focus on global rhythmic features, ensuring that dance movements closely follow the musical rhythm. We also introduce Parallel Mamba Motion Modeling (PMMM) architecture to separately model upper and lower body motions along with musical features, thereby improving the naturalness and diversity of generated dance movements. Extensive experiments confirm that Danceba outperforms state-of-the-art methods, achieving significantly better rhythmic alignment and motion diversity. Project page: https://danceba.github.io/ .
<div id='section'>Paperid: <span id='pid'>1089, <a href='https://arxiv.org/pdf/2502.14917.pdf' target='_blank'>https://arxiv.org/pdf/2502.14917.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rui Zhao, Qirui Yuan, Jinyu Li, Haofeng Hu, Yun Li, Chengyuan Zheng, Fei Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.14917">Sce2DriveX: A Generalized MLLM Framework for Scene-to-Drive Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>End-to-end autonomous driving, which directly maps raw sensor inputs to low-level vehicle controls, is an important part of Embodied AI. Despite successes in applying Multimodal Large Language Models (MLLMs) for high-level traffic scene semantic understanding, it remains challenging to effectively translate these conceptual semantics understandings into low-level motion control commands and achieve generalization and consensus in cross-scene driving. We introduce Sce2DriveX, a human-like driving chain-of-thought (CoT) reasoning MLLM framework. Sce2DriveX utilizes multimodal joint learning from local scene videos and global BEV maps to deeply understand long-range spatiotemporal relationships and road topology, enhancing its comprehensive perception and reasoning capabilities in 3D dynamic/static scenes and achieving driving generalization across scenes. Building on this, it reconstructs the implicit cognitive chain inherent in human driving, covering scene understanding, meta-action reasoning, behavior interpretation analysis, motion planning and control, thereby further bridging the gap between autonomous driving and human thought processes. To elevate model performance, we have developed the first extensive Visual Question Answering (VQA) driving instruction dataset tailored for 3D spatial understanding and long-axis task reasoning. Extensive experiments demonstrate that Sce2DriveX achieves state-of-the-art performance from scene understanding to end-to-end driving, as well as robust generalization on the CARLA Bench2Drive benchmark.
<div id='section'>Paperid: <span id='pid'>1090, <a href='https://arxiv.org/pdf/2502.14574.pdf' target='_blank'>https://arxiv.org/pdf/2502.14574.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinrui Zhang, Lu Xiong, Peizhi Zhang, Junpeng Huang, Yining Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.14574">Real-world Troublemaker: A 5G Cloud-controlled Track Testing Framework for Automated Driving Systems in Safety-critical Interaction Scenarios</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Track testing plays a critical role in the safety evaluation of autonomous driving systems (ADS), as it provides a real-world interaction environment. However, the inflexibility in motion control of object targets and the absence of intelligent interactive testing methods often result in pre-fixed and limited testing scenarios. To address these limitations, we propose a novel 5G cloud-controlled track testing framework, Real-world Troublemaker. This framework overcomes the rigidity of traditional pre-programmed control by leveraging 5G cloud-controlled object targets integrated with the Internet of Things (IoT) and vehicle teleoperation technologies. Unlike conventional testing methods that rely on pre-set conditions, we propose a dynamic game strategy based on a quadratic risk interaction utility function, facilitating intelligent interactions with the vehicle under test (VUT) and creating a more realistic and dynamic interaction environment. The proposed framework has been successfully implemented at the Tongji University Intelligent Connected Vehicle Evaluation Base. Field test results demonstrate that Troublemaker can perform dynamic interactive testing of ADS accurately and effectively. Compared to traditional methods, Troublemaker improves scenario reproduction accuracy by 65.2\%, increases the diversity of interaction strategies by approximately 9.2 times, and enhances exposure frequency of safety-critical scenarios by 3.5 times in unprotected left-turn scenarios.
<div id='section'>Paperid: <span id='pid'>1091, <a href='https://arxiv.org/pdf/2501.00317.pdf' target='_blank'>https://arxiv.org/pdf/2501.00317.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiexin Wang, Yiju Guo, Bing Su
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.00317">Spatio-Temporal Multi-Subgraph GCN for 3D Human Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion prediction (HMP) involves forecasting future human motion based on historical data. Graph Convolutional Networks (GCNs) have garnered widespread attention in this field for their proficiency in capturing relationships among joints in human motion. However, existing GCN-based methods tend to focus on either temporal-domain or spatial-domain features, or they combine spatio-temporal features without fully leveraging the complementarity and cross-dependency of these two features. In this paper, we propose the Spatial-Temporal Multi-Subgraph Graph Convolutional Network (STMS-GCN) to capture complex spatio-temporal dependencies in human motion. Specifically, we decouple the modeling of temporal and spatial dependencies, enabling cross-domain knowledge transfer at multiple scales through a spatio-temporal information consistency constraint mechanism. Besides, we utilize multiple subgraphs to extract richer motion information and enhance the learning associations of diverse subgraphs through a homogeneous information constraint mechanism. Extensive experiments on the standard HMP benchmarks demonstrate the superiority of our method.
<div id='section'>Paperid: <span id='pid'>1092, <a href='https://arxiv.org/pdf/2501.00315.pdf' target='_blank'>https://arxiv.org/pdf/2501.00315.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiexin Wang, Yiju Guo, Bing Su
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.00315">Temporal Dynamics Decoupling with Inverse Processing for Enhancing Human Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Exploring the bridge between historical and future motion behaviors remains a central challenge in human motion prediction. While most existing methods incorporate a reconstruction task as an auxiliary task into the decoder, thereby improving the modeling of spatio-temporal dependencies, they overlook the potential conflicts between reconstruction and prediction tasks. In this paper, we propose a novel approach: Temporal Decoupling Decoding with Inverse Processing (\textbf{$TD^2IP$}). Our method strategically separates reconstruction and prediction decoding processes, employing distinct decoders to decode the shared motion features into historical or future sequences. Additionally, inverse processing reverses motion information in the temporal dimension and reintroduces it into the model, leveraging the bidirectional temporal correlation of human motion behaviors. By alleviating the conflicts between reconstruction and prediction tasks and enhancing the association of historical and future information, \textbf{$TD^2IP$} fosters a deeper understanding of motion patterns. Extensive experiments demonstrate the adaptability of our method within existing methods.
<div id='section'>Paperid: <span id='pid'>1093, <a href='https://arxiv.org/pdf/2412.10350.pdf' target='_blank'>https://arxiv.org/pdf/2412.10350.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aykut Ä°Åleyen, Abhidnya Kadu, RenÃ© van de Molengraft, ÃmÃ¼r Arslan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.10350">Adaptive Dual-Headway Unicycle Pose Control and Motion Prediction for Optimal Sampling-Based Feedback Motion Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Safe, smooth, and optimal motion planning for nonholonomically constrained mobile robots and autonomous vehicles is essential for achieving reliable, seamless, and efficient autonomy in logistics, mobility, and service industries. In many such application settings, nonholonomic robots, like unicycles with restricted motion, require precise planning and control of both translational and orientational motion to approach specific locations in a designated orientation, such as for approaching changing, parking, and loading areas. In this paper, we introduce a new dual-headway unicycle pose control method by leveraging an adaptively placed headway point in front of the unicycle pose and a tailway point behind the goal pose. In summary, the unicycle robot continuously follows its headway point, which chases the tailway point of the goal pose and the asymptotic motion of the tailway point towards the goal position guides the unicycle robot to approach the goal location with the correct orientation. The simple and intuitive geometric construction of dual-headway unicycle pose control enables an explicit convex feedback motion prediction bound on the closed-loop unicycle motion trajectory for fast and accurate safety verification. We present an application of dual-headway unicycle control for optimal sampling-based motion planning around obstacles. In numerical simulations, we show that optimal unicycle motion planning using dual-headway translation and orientation distances significantly outperforms Euclidean translation and cosine orientation distances in generating smooth motion with minimal travel and turning effort.
<div id='section'>Paperid: <span id='pid'>1094, <a href='https://arxiv.org/pdf/2411.19786.pdf' target='_blank'>https://arxiv.org/pdf/2411.19786.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiming Wu, Wei Ji, Kecheng Zheng, Zicheng Wang, Dong Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.19786">MoTe: Learning Motion-Text Diffusion Model for Multiple Generation Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, human motion analysis has experienced great improvement due to inspiring generative models such as the denoising diffusion model and large language model. While the existing approaches mainly focus on generating motions with textual descriptions and overlook the reciprocal task. In this paper, we present~\textbf{MoTe}, a unified multi-modal model that could handle diverse tasks by learning the marginal, conditional, and joint distributions of motion and text simultaneously. MoTe enables us to handle the paired text-motion generation, motion captioning, and text-driven motion generation by simply modifying the input context. Specifically, MoTe is composed of three components: Motion Encoder-Decoder (MED), Text Encoder-Decoder (TED), and Moti-on-Text Diffusion Model (MTDM). In particular, MED and TED are trained for extracting latent embeddings, and subsequently reconstructing the motion sequences and textual descriptions from the extracted embeddings, respectively. MTDM, on the other hand, performs an iterative denoising process on the input context to handle diverse tasks. Experimental results on the benchmark datasets demonstrate the superior performance of our proposed method on text-to-motion generation and competitive performance on motion captioning.
<div id='section'>Paperid: <span id='pid'>1095, <a href='https://arxiv.org/pdf/2410.11404.pdf' target='_blank'>https://arxiv.org/pdf/2410.11404.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiawei Mo, Yixuan Chen, Rifen Lin, Yongkang Ni, Min Zeng, Xiping Hu, Min Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.11404">MoChat: Joints-Grouped Spatio-Temporal Grounding LLM for Multi-Turn Motion Comprehension and Description</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite continuous advancements in deep learning for understanding human motion, existing models often struggle to accurately identify action timing and specific body parts, typically supporting only single-round interaction. Such limitations in capturing fine-grained motion details reduce their effectiveness in motion understanding tasks. In this paper, we propose MoChat, a multimodal large language model capable of spatio-temporal grounding of human motion and understanding multi-turn dialogue context. To achieve these capabilities, we group the spatial information of each skeleton frame based on human anatomical structure and then apply them with Joints-Grouped Skeleton Encoder, whose outputs are combined with LLM embeddings to create spatio-aware and temporal-aware embeddings separately. Additionally, we develop a pipeline for extracting timestamps from skeleton sequences based on textual annotations, and construct multi-turn dialogues for spatially grounding. Finally, various task instructions are generated for jointly training. Experimental results demonstrate that MoChat achieves state-of-the-art performance across multiple metrics in motion understanding tasks, making it as the first model capable of fine-grained spatio-temporal grounding of human motion.
<div id='section'>Paperid: <span id='pid'>1096, <a href='https://arxiv.org/pdf/2410.03860.pdf' target='_blank'>https://arxiv.org/pdf/2410.03860.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Leo Bringer, Joey Wilson, Kira Barton, Maani Ghaffari
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.03860">MDMP: Multi-modal Diffusion for supervised Motion Predictions with uncertainty</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces a Multi-modal Diffusion model for Motion Prediction (MDMP) that integrates and synchronizes skeletal data and textual descriptions of actions to generate refined long-term motion predictions with quantifiable uncertainty. Existing methods for motion forecasting or motion generation rely solely on either prior motions or text prompts, facing limitations with precision or control, particularly over extended durations. The multi-modal nature of our approach enhances the contextual understanding of human motion, while our graph-based transformer framework effectively capture both spatial and temporal motion dynamics. As a result, our model consistently outperforms existing generative techniques in accurately predicting long-term motions. Additionally, by leveraging diffusion models' ability to capture different modes of prediction, we estimate uncertainty, significantly improving spatial awareness in human-robot interactions by incorporating zones of presence with varying confidence levels for each body joint.
<div id='section'>Paperid: <span id='pid'>1097, <a href='https://arxiv.org/pdf/2409.20554.pdf' target='_blank'>https://arxiv.org/pdf/2409.20554.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ameya Salvi, Pardha Sai Krishna Ala, Jonathon M. Smereka, Mark Brudnak, David Gorsich, Matthias Schmid, Venkat Krovi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.20554">Online identification of skidding modes with interactive multiple model estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Skid-steered wheel mobile robots (SSWMRs) operate in a variety of outdoor environments exhibiting motion behaviors dominated by the effects of complex wheel-ground interactions. Characterizing these interactions is crucial both from the immediate robot autonomy perspective (for motion prediction and control) as well as a long-term predictive maintenance and diagnostics perspective. An ideal solution entails capturing precise state measurements for decisions and controls, which is considerably difficult, especially in increasingly unstructured outdoor regimes of operations for these robots. In this milieu, a framework to identify pre-determined discrete modes of operation can considerably simplify the motion model identification process. To this end, we propose an interactive multiple model (IMM) based filtering framework to probabilistically identify predefined robot operation modes that could arise due to traversal in different terrains or loss of wheel traction.
<div id='section'>Paperid: <span id='pid'>1098, <a href='https://arxiv.org/pdf/2409.00014.pdf' target='_blank'>https://arxiv.org/pdf/2409.00014.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hua Yu, Yaqing Hou, Wenbin Pei, Qiang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.00014">DivDiff: A Conditional Diffusion Model for Diverse Human Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diverse human motion prediction (HMP) aims to predict multiple plausible future motions given an observed human motion sequence. It is a challenging task due to the diversity of potential human motions while ensuring an accurate description of future human motions. Current solutions are either low-diversity or limited in expressiveness. Recent denoising diffusion models (DDPM) hold potential generative capabilities in generative tasks. However, introducing DDPM directly into diverse HMP incurs some issues. Although DDPM can increase the diversity of the potential patterns of human motions, the predicted human motions become implausible over time because of the significant noise disturbances in the forward process of DDPM. This phenomenon leads to the predicted human motions being hard to control, seriously impacting the quality of predicted motions and restricting their practical applicability in real-world scenarios. To alleviate this, we propose a novel conditional diffusion-based generative model, called DivDiff, to predict more diverse and realistic human motions. Specifically, the DivDiff employs DDPM as our backbone and incorporates Discrete Cosine Transform (DCT) and transformer mechanisms to encode the observed human motion sequence as a condition to instruct the reverse process of DDPM. More importantly, we design a diversified reinforcement sampling function (DRSF) to enforce human skeletal constraints on the predicted human motions. DRSF utilizes the acquired information from human skeletal as prior knowledge, thereby reducing significant disturbances introduced during the forward process. Extensive results received in the experiments on two widely-used datasets (Human3.6M and HumanEva-I) demonstrate that our model obtains competitive performance on both diversity and accuracy.
<div id='section'>Paperid: <span id='pid'>1099, <a href='https://arxiv.org/pdf/2408.17168.pdf' target='_blank'>https://arxiv.org/pdf/2408.17168.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhen Fan, Peng Dai, Zhuo Su, Xu Gao, Zheng Lv, Jiarui Zhang, Tianyuan Du, Guidong Wang, Yang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.17168">EMHI: A Multimodal Egocentric Human Motion Dataset with HMD and Body-Worn IMUs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Egocentric human pose estimation (HPE) using wearable sensors is essential for VR/AR applications. Most methods rely solely on either egocentric-view images or sparse Inertial Measurement Unit (IMU) signals, leading to inaccuracies due to self-occlusion in images or the sparseness and drift of inertial sensors. Most importantly, the lack of real-world datasets containing both modalities is a major obstacle to progress in this field. To overcome the barrier, we propose EMHI, a multimodal \textbf{E}gocentric human \textbf{M}otion dataset with \textbf{H}ead-Mounted Display (HMD) and body-worn \textbf{I}MUs, with all data collected under the real VR product suite. Specifically, EMHI provides synchronized stereo images from downward-sloping cameras on the headset and IMU data from body-worn sensors, along with pose annotations in SMPL format. This dataset consists of 885 sequences captured by 58 subjects performing 39 actions, totaling about 28.5 hours of recording. We evaluate the annotations by comparing them with optical marker-based SMPL fitting results. To substantiate the reliability of our dataset, we introduce MEPoser, a new baseline method for multimodal egocentric HPE, which employs a multimodal fusion encoder, temporal feature encoder, and MLP-based regression heads. The experiments on EMHI show that MEPoser outperforms existing single-modal methods and demonstrates the value of our dataset in solving the problem of egocentric HPE. We believe the release of EMHI and the method could advance the research of egocentric HPE and expedite the practical implementation of this technology in VR/AR products.
<div id='section'>Paperid: <span id='pid'>1100, <a href='https://arxiv.org/pdf/2408.00712.pdf' target='_blank'>https://arxiv.org/pdf/2408.00712.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nikos Athanasiou, AlpÃ¡r Cseke, Markos Diomataris, Michael J. Black, GÃ¼l Varol
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.00712">MotionFix: Text-Driven 3D Human Motion Editing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The focus of this paper is on 3D motion editing. Given a 3D human motion and a textual description of the desired modification, our goal is to generate an edited motion as described by the text. The key challenges include the scarcity of training data and the need to design a model that accurately edits the source motion. In this paper, we address both challenges. We propose a methodology to semi-automatically collect a dataset of triplets comprising (i) a source motion, (ii) a target motion, and (iii) an edit text, introducing the new MotionFix dataset. Access to this data allows us to train a conditional diffusion model, TMED, that takes both the source motion and the edit text as input. We develop several baselines to evaluate our model, comparing it against models trained solely on text-motion pair datasets, and demonstrate the superior performance of our model trained on triplets. We also introduce new retrieval-based metrics for motion editing, establishing a benchmark on the evaluation set of MotionFix. Our results are promising, paving the way for further research in fine-grained motion generation. Code, models, and data are available at https://motionfix.is.tue.mpg.de/ .
<div id='section'>Paperid: <span id='pid'>1101, <a href='https://arxiv.org/pdf/2407.10127.pdf' target='_blank'>https://arxiv.org/pdf/2407.10127.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziqi Zhao, Peijia Xie, Max Q. -H. Meng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.10127">ODD: Omni Differential Drive for Simultaneous Reconfiguration and Omnidirectional Mobility of Wheeled Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Wheeled robots are highly efficient in human living environments. However, conventional wheeled designs, with their limited degrees of freedom and constraints in robot configuration, struggle to simultaneously achieve stability, passability, and agility due to varying footprint needs. This paper proposes a novel robot drive model inspired by human movements, termed as the Omni Differential Drive (ODD). The ODD model innovatively utilizes a lateral differential drive to adjust wheel spacing without adding additional actuators to the existing omnidirectional drive. This approach enables wheeled robots to achieve both simultaneous reconfiguration and omnidirectional mobility. To validate the feasibility of the ODD model, a functional prototype was developed, followed by comprehensive kinematic analyses. Control systems for self-balancing and motion control were designed and implemented. Experimental validations confirmed the feasibility of the ODD mechanism and the effectiveness of the control strategies. The results underline the potential of this innovative drive system to enhance the mobility and adaptability of robotic platforms.
<div id='section'>Paperid: <span id='pid'>1102, <a href='https://arxiv.org/pdf/2406.18537.pdf' target='_blank'>https://arxiv.org/pdf/2406.18537.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Keenon Werling, Janelle Kaneda, Alan Tan, Rishi Agarwal, Six Skov, Tom Van Wouwe, Scott Uhlrich, Nicholas Bianco, Carmichael Ong, Antoine Falisse, Shardul Sapkota, Aidan Chandra, Joshua Carter, Ezio Preatoni, Benjamin Fregly, Jennifer Hicks, Scott Delp, C. Karen Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.18537">AddBiomechanics Dataset: Capturing the Physics of Human Motion at Scale</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While reconstructing human poses in 3D from inexpensive sensors has advanced significantly in recent years, quantifying the dynamics of human motion, including the muscle-generated joint torques and external forces, remains a challenge. Prior attempts to estimate physics from reconstructed human poses have been hampered by a lack of datasets with high-quality pose and force data for a variety of movements. We present the AddBiomechanics Dataset 1.0, which includes physically accurate human dynamics of 273 human subjects, over 70 hours of motion and force plate data, totaling more than 24 million frames. To construct this dataset, novel analytical methods were required, which are also reported here. We propose a benchmark for estimating human dynamics from motion using this dataset, and present several baseline results. The AddBiomechanics Dataset is publicly available at https://addbiomechanics.org/download_data.html.
<div id='section'>Paperid: <span id='pid'>1103, <a href='https://arxiv.org/pdf/2406.00727.pdf' target='_blank'>https://arxiv.org/pdf/2406.00727.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Satoshi Yagi, Mitsunori Tada, Eiji Uchibe, Suguru Kanoga, Takamitsu Matsubara, Jun Morimoto
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.00727">Unsupervised Neural Motion Retargeting for Humanoid Teleoperation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study proposes an approach to human-to-humanoid teleoperation using GAN-based online motion retargeting, which obviates the need for the construction of pairwise datasets to identify the relationship between the human and the humanoid kinematics. Consequently, it can be anticipated that our proposed teleoperation system will reduce the complexity and setup requirements typically associated with humanoid controllers, thereby facilitating the development of more accessible and intuitive teleoperation systems for users without robotics knowledge. The experiments demonstrated the efficacy of the proposed method in retargeting a range of upper-body human motions to humanoid, including a body jab motion and a basketball shoot motion. Moreover, the human-in-the-loop teleoperation performance was evaluated by measuring the end-effector position errors between the human and the retargeted humanoid motions. The results demonstrated that the error was comparable to those of conventional motion retargeting methods that require pairwise motion datasets. Finally, a box pick-and-place task was conducted to demonstrate the usability of the developed humanoid teleoperation system.
<div id='section'>Paperid: <span id='pid'>1104, <a href='https://arxiv.org/pdf/2406.00636.pdf' target='_blank'>https://arxiv.org/pdf/2406.00636.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Taeryung Lee, Fabien Baradel, Thomas Lucas, Kyoung Mu Lee, Gregory Rogez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.00636">T2LM: Long-Term 3D Human Motion Generation from Multiple Sentences</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we address the challenging problem of long-term 3D human motion generation. Specifically, we aim to generate a long sequence of smoothly connected actions from a stream of multiple sentences (i.e., paragraph). Previous long-term motion generating approaches were mostly based on recurrent methods, using previously generated motion chunks as input for the next step. However, this approach has two drawbacks: 1) it relies on sequential datasets, which are expensive; 2) these methods yield unrealistic gaps between motions generated at each step. To address these issues, we introduce simple yet effective T2LM, a continuous long-term generation framework that can be trained without sequential data. T2LM comprises two components: a 1D-convolutional VQVAE, trained to compress motion to sequences of latent vectors, and a Transformer-based Text Encoder that predicts a latent sequence given an input text. At inference, a sequence of sentences is translated into a continuous stream of latent vectors. This is then decoded into a motion by the VQVAE decoder; the use of 1D convolutions with a local temporal receptive field avoids temporal inconsistencies between training and generated sequences. This simple constraint on the VQ-VAE allows it to be trained with short sequences only and produces smoother transitions. T2LM outperforms prior long-term generation models while overcoming the constraint of requiring sequential data; it is also competitive with SOTA single-action generation models.
<div id='section'>Paperid: <span id='pid'>1105, <a href='https://arxiv.org/pdf/2405.14626.pdf' target='_blank'>https://arxiv.org/pdf/2405.14626.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Laura Duarte, Pedro Neto
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.14626">Event-based dataset for the detection and classification of manufacturing assembly tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The featured dataset, the Event-based Dataset of Assembly Tasks (EDAT24), showcases a selection of manufacturing primitive tasks (idle, pick, place, and screw), which are basic actions performed by human operators in any manufacturing assembly. The data were captured using a DAVIS240C event camera, an asynchronous vision sensor that registers events when changes in light intensity value occur. Events are a lightweight data format for conveying visual information and are well-suited for real-time detection and analysis of human motion. Each manufacturing primitive has 100 recorded samples of DAVIS240C data, including events and greyscale frames, for a total of 400 samples. In the dataset, the user interacts with objects from the open-source CT-Benchmark in front of the static DAVIS event camera. All data are made available in raw form (.aedat) and in pre-processed form (.npy). Custom-built Python code is made available together with the dataset to aid researchers to add new manufacturing primitives or extend the dataset with more samples.
<div id='section'>Paperid: <span id='pid'>1106, <a href='https://arxiv.org/pdf/2405.06646.pdf' target='_blank'>https://arxiv.org/pdf/2405.06646.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lei Hu, Zihao Zhang, Yongjing Ye, Yiwen Xu, Shihong Xia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.06646">Diffusion-based Human Motion Style Transfer with Semantic Guidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D Human motion style transfer is a fundamental problem in computer graphic and animation processing. Existing AdaIN- based methods necessitate datasets with balanced style distribution and content/style labels to train the clustered latent space. However, we may encounter a single unseen style example in practical scenarios, but not in sufficient quantity to constitute a style cluster for AdaIN-based methods. Therefore, in this paper, we propose a novel two-stage framework for few-shot style transfer learning based on the diffusion model. Specifically, in the first stage, we pre-train a diffusion-based text-to-motion model as a generative prior so that it can cope with various content motion inputs. In the second stage, based on the single style example, we fine-tune the pre-trained diffusion model in a few-shot manner to make it capable of style transfer. The key idea is regarding the reverse process of diffusion as a motion-style translation process since the motion styles can be viewed as special motion variations. During the fine-tuning for style transfer, a simple yet effective semantic-guided style transfer loss coordinated with style example reconstruction loss is introduced to supervise the style transfer in CLIP semantic space. The qualitative and quantitative evaluations demonstrate that our method can achieve state-of-the-art performance and has practical applications.
<div id='section'>Paperid: <span id='pid'>1107, <a href='https://arxiv.org/pdf/2404.16705.pdf' target='_blank'>https://arxiv.org/pdf/2404.16705.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Diego Martinez-Baselga, Oscar de Groot, Luzia Knoedler, Luis Riazuelo, Javier Alonso-Mora, Luis Montano
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.16705">SHINE: Social Homology Identification for Navigation in Crowded Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Navigating mobile robots in social environments remains a challenging task due to the intricacies of human-robot interactions. Most of the motion planners designed for crowded and dynamic environments focus on choosing the best velocity to reach the goal while avoiding collisions, but do not explicitly consider the high-level navigation behavior (avoiding through the left or right side, letting others pass or passing before others, etc.). In this work, we present a novel motion planner that incorporates topology distinct paths representing diverse navigation strategies around humans. The planner selects the topology class that imitates human behavior the best using a deep neural network model trained on real-world human motion data, ensuring socially intelligent and contextually aware navigation. Our system refines the chosen path through an optimization-based local planner in real time, ensuring seamless adherence to desired social behaviors. In this way, we decouple perception and local planning from the decision-making process. We evaluate the prediction accuracy of the network with real-world data. In addition, we assess the navigation capabilities in both simulation and a real-world platform, comparing it with other state-of-the-art planners. We demonstrate that our planner exhibits socially desirable behaviors and shows a smooth and remarkable performance.
<div id='section'>Paperid: <span id='pid'>1108, <a href='https://arxiv.org/pdf/2404.16500.pdf' target='_blank'>https://arxiv.org/pdf/2404.16500.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Richard Schubert, Marvin Loba, Jasper SÃ¼nnemann, Torben Stolte, Markus Maurer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.16500">Conformal Prediction of Motion Control Performance for an Automated Vehicle in Presence of Actuator Degradations and Failures</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automated driving systems require monitoring mechanisms to ensure safe operation, especially if system components degrade or fail. Their runtime self-representation plays a key role as it provides a-priori knowledge about the system's capabilities and limitations. In this paper, we propose a data-driven approach for deriving such a self-representation model for the motion controller of an automated vehicle. A conformalized prediction model is learned and allows estimating how operational conditions as well as potential degradations and failures of the vehicle's actuators impact motion control performance. During runtime behavior generation, our predictor can provide a heuristic for determining the admissible action space.
<div id='section'>Paperid: <span id='pid'>1109, <a href='https://arxiv.org/pdf/2403.17916.pdf' target='_blank'>https://arxiv.org/pdf/2403.17916.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zehao Wang, Yuping Wang, Zhuoyuan Wu, Hengbo Ma, Zhaowei Li, Hang Qiu, Jiachen Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.17916">CMP: Cooperative Motion Prediction with Multi-Agent Communication</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The confluence of the advancement of Autonomous Vehicles (AVs) and the maturity of Vehicle-to-Everything (V2X) communication has enabled the capability of cooperative connected and automated vehicles (CAVs). Building on top of cooperative perception, this paper explores the feasibility and effectiveness of cooperative motion prediction. Our method, CMP, takes LiDAR signals as model input to enhance tracking and prediction capabilities. Unlike previous work that focuses separately on either cooperative perception or motion prediction, our framework, to the best of our knowledge, is the first to address the unified problem where CAVs share information in both perception and prediction modules. Incorporated into our design is the unique capability to tolerate realistic V2X transmission delays, while dealing with bulky perception representations. We also propose a prediction aggregation module, which unifies the predictions obtained by different CAVs and generates the final prediction. Through extensive experiments and ablation studies on the OPV2V and V2V4Real datasets, we demonstrate the effectiveness of our method in cooperative perception, tracking, and motion prediction. In particular, CMP reduces the average prediction error by 12.3% compared with the strongest baseline. Our work marks a significant step forward in the cooperative capabilities of CAVs, showcasing enhanced performance in complex scenarios. More details can be found on the project website: https://cmp-cooperative-prediction.github.io.
<div id='section'>Paperid: <span id='pid'>1110, <a href='https://arxiv.org/pdf/2403.16374.pdf' target='_blank'>https://arxiv.org/pdf/2403.16374.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yinke Dong, Haifeng Yuan, Hongkun Liu, Wei Jing, Fangzhen Li, Hongmin Liu, Bin Fan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.16374">ProIn: Learning to Predict Trajectory Based on Progressive Interactions for Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate motion prediction of pedestrians, cyclists, and other surrounding vehicles (all called agents) is very important for autonomous driving. Most existing works capture map information through an one-stage interaction with map by vector-based attention, to provide map constraints for social interaction and multi-modal differentiation. However, these methods have to encode all required map rules into the focal agent's feature, so as to retain all possible intentions' paths while at the meantime to adapt to potential social interaction. In this work, a progressive interaction network is proposed to enable the agent's feature to progressively focus on relevant maps, in order to better learn agents' feature representation capturing the relevant map constraints. The network progressively encode the complex influence of map constraints into the agent's feature through graph convolutions at the following three stages: after historical trajectory encoder, after social interaction, and after multi-modal differentiation. In addition, a weight allocation mechanism is proposed for multi-modal training, so that each mode can obtain learning opportunities from a single-mode ground truth. Experiments have validated the superiority of progressive interactions to the existing one-stage interaction, and demonstrate the effectiveness of each component. Encouraging results were obtained in the challenging benchmarks.
<div id='section'>Paperid: <span id='pid'>1111, <a href='https://arxiv.org/pdf/2403.15891.pdf' target='_blank'>https://arxiv.org/pdf/2403.15891.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiangbei Yue, Baiyi Li, Julien PettrÃ©, Armin Seyfried, He Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.15891">Human Motion Prediction under Unexpected Perturbation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We investigate a new task in human motion prediction, which is predicting motions under unexpected physical perturbation potentially involving multiple people. Compared with existing research, this task involves predicting less controlled, unpremeditated and pure reactive motions in response to external impact and how such motions can propagate through people. It brings new challenges such as data scarcity and predicting complex interactions. To this end, we propose a new method capitalizing differential physics and deep neural networks, leading to an explicit Latent Differential Physics (LDP) model. Through experiments, we demonstrate that LDP has high data efficiency, outstanding prediction accuracy, strong generalizability and good explainability. Since there is no similar research, a comprehensive comparison with 11 adapted baselines from several relevant domains is conducted, showing LDP outperforming existing research both quantitatively and qualitatively, improving prediction accuracy by as much as 70%, and demonstrating significantly stronger generalization.
<div id='section'>Paperid: <span id='pid'>1112, <a href='https://arxiv.org/pdf/2403.09923.pdf' target='_blank'>https://arxiv.org/pdf/2403.09923.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yingqing Chen, Christos G. Cassandras, Kaiyuan Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.09923">Optimal Sequencing and Motion Control in a Roundabout with Safety Guarantees</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper develops a controller for Connected and Automated Vehicles (CAVs) traversing a single-lane roundabout. The controller simultaneously determines the optimal sequence and associated optimal motion control jointly minimizing travel time and energy consumption while providing speed-dependent safety guarantees, as well as satisfying velocity and acceleration constraints. This is achieved by integrating (a) Model Predictive Control (MPC) to enable receding horizon optimization with (b) Control Lyapunov-Barrier Functions (CLBFs) to guarantee convergence to a safe set in finite time, thus providing an extended stability region compared to the use of classic Control Barrier Functions (CBFs). The proposed MPC-CLBF framework addresses both infeasibility and myopic control issues commonly encountered when controlling CAVs over multiple interconnected control zones in a traffic network, which has been a limitation of prior work on CAVs going through roundabouts, while still providing safety guarantees. Simulations under varying traffic demands demonstrate the controller's effectiveness and stability.
<div id='section'>Paperid: <span id='pid'>1113, <a href='https://arxiv.org/pdf/2403.03460.pdf' target='_blank'>https://arxiv.org/pdf/2403.03460.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xunjie Chen, Aditya Anikode, Jingang Yi, Tao Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.03460">Foot Shape-Dependent Resistive Force Model for Bipedal Walkers on Granular Terrains</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Legged robots have demonstrated high efficiency and effectiveness in unstructured and dynamic environments. However, it is still challenging for legged robots to achieve rapid and efficient locomotion on deformable, yielding substrates, such as granular terrains. We present an enhanced resistive force model for bipedal walkers on soft granular terrains by introducing effective intrusion depth correction. The enhanced force model captures fundamental kinetic results considering the robot foot shape, walking gait speed variation, and energy expense. The model is validated by extensive foot intrusion experiments with a bipedal robot. The results confirm the model accuracy on the given type of granular terrains. The model can be further integrated with the motion control of bipedal robotic walkers.
<div id='section'>Paperid: <span id='pid'>1114, <a href='https://arxiv.org/pdf/2403.03105.pdf' target='_blank'>https://arxiv.org/pdf/2403.03105.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chunchu Zhu, Xunjie Chen, Jingang Yi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.03105">Biomechanical Comparison of Human Walking Locomotion on Solid Ground and Sand</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current studies on human locomotion focus mainly on solid ground walking conditions. In this paper, we present a biomechanic comparison of human walking locomotion on solid ground and sand. A novel dataset containing 3-dimensional motion and biomechanical data from 20 able-bodied adults for locomotion on solid ground and sand is collected. We present the data collection methods and report the sensor data along with the kinematic and kinetic profiles of joint biomechanics. A comprehensive analysis of human gait and joint stiffness profiles is presented. The kinematic and kinetic analysis reveals that human walking locomotion on sand shows different ground reaction forces and joint torque profiles, compared with those patterns from walking on solid ground. These gait differences reflect that humans adopt motion control strategies for yielding terrain conditions such as sand. The dataset also provides a source of locomotion data for researchers to study human activity recognition and assistive devices for walking on different terrains.
<div id='section'>Paperid: <span id='pid'>1115, <a href='https://arxiv.org/pdf/2402.07087.pdf' target='_blank'>https://arxiv.org/pdf/2402.07087.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nate Gillman, Michael Freeman, Daksh Aggarwal, Chia-Hong Hsu, Calvin Luo, Yonglong Tian, Chen Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.07087">Self-Correcting Self-Consuming Loops for Generative Model Training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As synthetic data becomes higher quality and proliferates on the internet, machine learning models are increasingly trained on a mix of human- and machine-generated data. Despite the successful stories of using synthetic data for representation learning, using synthetic data for generative model training creates "self-consuming loops" which may lead to training instability or even collapse, unless certain conditions are met. Our paper aims to stabilize self-consuming generative model training. Our theoretical results demonstrate that by introducing an idealized correction function, which maps a data point to be more likely under the true data distribution, self-consuming loops can be made exponentially more stable. We then propose self-correction functions, which rely on expert knowledge (e.g. the laws of physics programmed in a simulator), and aim to approximate the idealized corrector automatically and at scale. We empirically validate the effectiveness of self-correcting self-consuming loops on the challenging human motion synthesis task, and observe that it successfully avoids model collapse, even when the ratio of synthetic data to real data is as high as 100%.
<div id='section'>Paperid: <span id='pid'>1116, <a href='https://arxiv.org/pdf/2402.04061.pdf' target='_blank'>https://arxiv.org/pdf/2402.04061.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jumman Hossain, Abu-Zaher Faridee, Nirmalya Roy, Jade Freeman, Timothy Gregory, Theron T. Trout
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.04061">TopoNav: Topological Navigation for Efficient Exploration in Sparse Reward Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous robots exploring unknown environments face a significant challenge: navigating effectively without prior maps and with limited external feedback. This challenge intensifies in sparse reward environments, where traditional exploration techniques often fail. In this paper, we present TopoNav, a novel topological navigation framework that integrates active mapping, hierarchical reinforcement learning, and intrinsic motivation to enable efficient goal-oriented exploration and navigation in sparse-reward settings. TopoNav dynamically constructs a topological map of the environment, capturing key locations and pathways. A two-level hierarchical policy architecture, comprising a high-level graph traversal policy and low-level motion control policies, enables effective navigation and obstacle avoidance while maintaining focus on the overall goal. Additionally, TopoNav incorporates intrinsic motivation to guide exploration toward relevant regions and frontier nodes in the topological map, addressing the challenges of sparse extrinsic rewards. We evaluate TopoNav both in the simulated and real-world off-road environments using a Clearpath Jackal robot, across three challenging navigation scenarios: goal-reaching, feature-based navigation, and navigation in complex terrains. We observe an increase in exploration coverage by 7- 20%, in success rates by 9-19%, and reductions in navigation times by 15-36% across various scenarios, compared to state-of-the-art methods
<div id='section'>Paperid: <span id='pid'>1117, <a href='https://arxiv.org/pdf/2402.03703.pdf' target='_blank'>https://arxiv.org/pdf/2402.03703.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhirong Luan, Yujun Lai, Rundong Huang, Yan Yan, Jingwei Wang, Jizhou Lu, Badong Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.03703">Hierarchical Large Language Models in Cloud Edge End Architecture for Heterogeneous Robot Cluster Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite their powerful semantic understanding and code generation capabilities, Large Language Models (LLMs) still face challenges when dealing with complex tasks. Multi agent strategy generation and motion control are highly complex domains that inherently require experts from multiple fields to collaborate. To enhance multi agent strategy generation and motion control, we propose an innovative architecture that employs the concept of a cloud edge end hierarchical structure. By leveraging multiple large language models with distinct areas of expertise, we can efficiently generate strategies and perform task decomposition. Introducing the cosine similarity approach,aligning task decomposition instructions with robot task sequences at the vector level, we can identify subtasks with incomplete task decomposition and iterate on them multiple times to ultimately generate executable machine task sequences.The robot is guided through these task sequences to complete tasks of higher complexity. With this architecture, we implement the process of natural language control of robots to perform complex tasks, and successfully address the challenge of multi agent execution of open tasks in open scenarios and the problem of task decomposition.
<div id='section'>Paperid: <span id='pid'>1118, <a href='https://arxiv.org/pdf/2512.23635.pdf' target='_blank'>https://arxiv.org/pdf/2512.23635.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoyu Li, Peidong Li, Xian Wu, Long Shi, Dedong Liu, Yitao Wu, Jiajia Fu, Dixiao Cui, Lijun Zhao, Lining Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.23635">Rethinking the Spatio-Temporal Alignment of End-to-End 3D Perception</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Spatio-temporal alignment is crucial for temporal modeling of end-to-end (E2E) perception in autonomous driving (AD), providing valuable structural and textural prior information. Existing methods typically rely on the attention mechanism to align objects across frames, simplifying the motion model with a unified explicit physical model (constant velocity, etc.). These approaches prefer semantic features for implicit alignment, challenging the importance of explicit motion modeling in the traditional perception paradigm. However, variations in motion states and object features across categories and frames render this alignment suboptimal. To address this, we propose HAT, a spatio-temporal alignment module that allows each object to adaptively decode the optimal alignment proposal from multiple hypotheses without direct supervision. Specifically, HAT first utilizes multiple explicit motion models to generate spatial anchors and motion-aware feature proposals for historical instances. It then performs multi-hypothesis decoding by incorporating semantic and motion cues embedded in cached object queries, ultimately providing the optimal alignment proposal for the target frame. On nuScenes, HAT consistently improves 3D temporal detectors and trackers across diverse baselines. It achieves state-of-the-art tracking results with 46.0% AMOTA on the test set when paired with the DETR3D detector. In an object-centric E2E AD method, HAT enhances perception accuracy (+1.3% mAP, +3.1% AMOTA) and reduces the collision rate by 32%. When semantics are corrupted (nuScenes-C), the enhancement of motion modeling by HAT enables more robust perception and planning in the E2E AD.
<div id='section'>Paperid: <span id='pid'>1119, <a href='https://arxiv.org/pdf/2512.21237.pdf' target='_blank'>https://arxiv.org/pdf/2512.21237.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bowen Dang, Lin Wu, Xiaohang Yang, Zheng Yuan, Zhixiang Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.21237">SegMo: Segment-aligned Text to 3D Human Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating 3D human motions from textual descriptions is an important research problem with broad applications in video games, virtual reality, and augmented reality. Recent methods align the textual description with human motion at the sequence level, neglecting the internal semantic structure of modalities. However, both motion descriptions and motion sequences can be naturally decomposed into smaller and semantically coherent segments, which can serve as atomic alignment units to achieve finer-grained correspondence. Motivated by this, we propose SegMo, a novel Segment-aligned text-conditioned human Motion generation framework to achieve fine-grained text-motion alignment. Our framework consists of three modules: (1) Text Segment Extraction, which decomposes complex textual descriptions into temporally ordered phrases, each representing a simple atomic action; (2) Motion Segment Extraction, which partitions complete motion sequences into corresponding motion segments; and (3) Fine-grained Text-Motion Alignment, which aligns text and motion segments with contrastive learning. Extensive experiments demonstrate that SegMo improves the strong baseline on two widely used datasets, achieving an improved TOP 1 score of 0.553 on the HumanML3D test set. Moreover, thanks to the learned shared embedding space for text and motion segments, SegMo can also be applied to retrieval-style tasks such as motion grounding and motion-to-text retrieval.
<div id='section'>Paperid: <span id='pid'>1120, <a href='https://arxiv.org/pdf/2512.19909.pdf' target='_blank'>https://arxiv.org/pdf/2512.19909.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maxime Lacour, Pu Ren, Rie Nakata, Nori Nakata, Michael Mahoney
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.19909">Modeling Non-Ergodic Path Effects Using Conditional Generative Model for Fourier Amplitude Spectra</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent developments in non-ergodic ground-motion models (GMMs) explicitly model systematic spatial variations in source, site, and path effects, reducing standard deviation to 30-40% of ergodic models and enabling more accurate site-specific seismic hazard analysis. Current non-ergodic GMMs rely on Gaussian Process (GP) methods with prescribed correlation functions and thus have computational limitations for large-scale predictions. This study proposes a deep-learning approach called Conditional Generative Modeling for Fourier Amplitude Spectra (CGM-FAS) as an alternative to GP-based methods for modeling non-ergodic path effects in Fourier Amplitude Spectra (FAS). CGM-FAS uses a Conditional Variational Autoencoder architecture to learn spatial patterns and interfrequency correlation directly from data by using geographical coordinates of earthquakes and stations as conditional variables. Using San Francisco Bay Area earthquake data, we compare CGM-FAS against a recent GP-based GMM for the region and demonstrate consistent predictions of non-ergodic path effects. Additionally, CGM-FAS offers advantages compared to GP-based approaches in learning spatial patterns without prescribed correlation functions, capturing interfrequency correlations, and enabling rapid predictions, generating maps for 10,000 sites across 1,000 frequencies within 10 seconds using a few GB of memory. CGM-FAS hyperparameters can be tuned to ensure generated path effects exhibit variability consistent with the GP-based empirical GMM. This work demonstrates a promising direction for efficient non-ergodic ground-motion prediction across multiple frequencies and large spatial domains.
<div id='section'>Paperid: <span id='pid'>1121, <a href='https://arxiv.org/pdf/2512.16705.pdf' target='_blank'>https://arxiv.org/pdf/2512.16705.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>David Müller, Espen Knoop, Dario Mylonopoulos, Agon Serifi, Michael A. Hopkins, Ruben Grandia, Moritz Bächer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.16705">Olaf: Bringing an Animated Character to Life in the Physical World</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Animated characters often move in non-physical ways and have proportions that are far from a typical walking robot. This provides an ideal platform for innovation in both mechanical design and stylized motion control. In this paper, we bring Olaf to life in the physical world, relying on reinforcement learning guided by animation references for control. To create the illusion of Olaf's feet moving along his body, we hide two asymmetric legs under a soft foam skirt. To fit actuators inside the character, we use spherical and planar linkages in the arms, mouth, and eyes. Because the walk cycle results in harsh contact sounds, we introduce additional rewards that noticeably reduce impact noise. The large head, driven by small actuators in the character's slim neck, creates a risk of overheating, amplified by the costume. To keep actuators from overheating, we feed temperature values as additional inputs to policies, introducing new rewards to keep them within bounds. We validate the efficacy of our modeling in simulation and on hardware, demonstrating an unmatched level of believability for a costumed robotic character.
<div id='section'>Paperid: <span id='pid'>1122, <a href='https://arxiv.org/pdf/2512.01803.pdf' target='_blank'>https://arxiv.org/pdf/2512.01803.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xavier Thomas, Youngsun Lim, Ananya Srinivasan, Audrey Zheng, Deepti Ghadiyaram
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.01803">Generative Action Tell-Tales: Assessing Human Motion in Synthesized Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite rapid advances in video generative models, robust metrics for evaluating visual and temporal correctness of complex human actions remain elusive. Critically, existing pure-vision encoders and Multimodal Large Language Models (MLLMs) are strongly appearance-biased, lack temporal understanding, and thus struggle to discern intricate motion dynamics and anatomical implausibilities in generated videos. We tackle this gap by introducing a novel evaluation metric derived from a learned latent space of real-world human actions. Our method first captures the nuances, constraints, and temporal smoothness of real-world motion by fusing appearance-agnostic human skeletal geometry features with appearance-based features. We posit that this combined feature space provides a robust representation of action plausibility. Given a generated video, our metric quantifies its action quality by measuring the distance between its underlying representations and this learned real-world action distribution. For rigorous validation, we develop a new multi-faceted benchmark specifically designed to probe temporally challenging aspects of human action fidelity. Through extensive experiments, we show that our metric achieves substantial improvement of more than 68% compared to existing state-of-the-art methods on our benchmark, performs competitively on established external benchmarks, and has a stronger correlation with human perception. Our in-depth analysis reveals critical limitations in current video generative models and establishes a new standard for advanced research in video generation.
<div id='section'>Paperid: <span id='pid'>1123, <a href='https://arxiv.org/pdf/2511.18209.pdf' target='_blank'>https://arxiv.org/pdf/2511.18209.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi-Yang Zhang, Tengjiao Sun, Pengcheng Fang, Deng-Bao Wang, Xiaohao Cai, Min-Ling Zhang, Hansung Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.18209">MotionDuet: Dual-Conditioned 3D Human Motion Generation with Video-Regularized Text Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D Human motion generation is pivotal across film, animation, gaming, and embodied intelligence. Traditional 3D motion synthesis relies on costly motion capture, while recent work shows that 2D videos provide rich, temporally coherent observations of human behavior. Existing approaches, however, either map high-level text descriptions to motion or rely solely on video conditioning, leaving a gap between generated dynamics and real-world motion statistics. We introduce MotionDuet, a multimodal framework that aligns motion generation with the distribution of video-derived representations. In this dual-conditioning paradigm, video cues extracted from a pretrained model (e.g., VideoMAE) ground low-level motion dynamics, while textual prompts provide semantic intent. To bridge the distribution gap across modalities, we propose Dual-stream Unified Encoding and Transformation (DUET) and a Distribution-Aware Structural Harmonization (DASH) loss. DUET fuses video-informed cues into the motion latent space via unified encoding and dynamic attention, while DASH aligns motion trajectories with both distributional and structural statistics of video features. An auto-guidance mechanism further balances textual and visual signals by leveraging a weakened copy of the model, enhancing controllability without sacrificing diversity. Extensive experiments demonstrate that MotionDuet generates realistic and controllable human motions, surpassing strong state-of-the-art baselines.
<div id='section'>Paperid: <span id='pid'>1124, <a href='https://arxiv.org/pdf/2511.04679.pdf' target='_blank'>https://arxiv.org/pdf/2511.04679.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qingzhou Lu, Yao Feng, Baiyu Shi, Michael Piseno, Zhenan Bao, C. Karen Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.04679">GentleHumanoid: Learning Upper-body Compliance for Contact-rich Human and Object Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humanoid robots are expected to operate in human-centered environments where safe and natural physical interaction is essential. However, most recent reinforcement learning (RL) policies emphasize rigid tracking and suppress external forces. Existing impedance-augmented approaches are typically restricted to base or end-effector control and focus on resisting extreme forces rather than enabling compliance. We introduce GentleHumanoid, a framework that integrates impedance control into a whole-body motion tracking policy to achieve upper-body compliance. At its core is a unified spring-based formulation that models both resistive contacts (restoring forces when pressing against surfaces) and guiding contacts (pushes or pulls sampled from human motion data). This formulation ensures kinematically consistent forces across the shoulder, elbow, and wrist, while exposing the policy to diverse interaction scenarios. Safety is further supported through task-adjustable force thresholds. We evaluate our approach in both simulation and on the Unitree G1 humanoid across tasks requiring different levels of compliance, including gentle hugging, sit-to-stand assistance, and safe object manipulation. Compared to baselines, our policy consistently reduces peak contact forces while maintaining task success, resulting in smoother and more natural interactions. These results highlight a step toward humanoid robots that can safely and effectively collaborate with humans and handle objects in real-world environments.
<div id='section'>Paperid: <span id='pid'>1125, <a href='https://arxiv.org/pdf/2510.19364.pdf' target='_blank'>https://arxiv.org/pdf/2510.19364.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Golnaz Raja, Ruslan Agishev, Miloš Prágr, Joni Pajarinen, Karel Zimmermann, Arun Kumar Singh, Reza Ghabcheloo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.19364">ProTerrain: Probabilistic Physics-Informed Rough Terrain World Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Uncertainty-aware robot motion prediction is crucial for downstream traversability estimation and safe autonomous navigation in unstructured, off-road environments, where terrain is heterogeneous and perceptual uncertainty is high. Most existing methods assume deterministic or spatially independent terrain uncertainties, ignoring the inherent local correlations of 3D spatial data and often producing unreliable predictions. In this work, we introduce an efficient probabilistic framework that explicitly models spatially correlated aleatoric uncertainty over terrain parameters as a probabilistic world model and propagates this uncertainty through a differentiable physics engine for probabilistic trajectory forecasting. By leveraging structured convolutional operators, our approach provides high-resolution multivariate predictions at manageable computational cost. Experimental evaluation on a publicly available dataset shows significantly improved uncertainty estimation and trajectory prediction accuracy over aleatoric uncertainty estimation baselines.
<div id='section'>Paperid: <span id='pid'>1126, <a href='https://arxiv.org/pdf/2510.16709.pdf' target='_blank'>https://arxiv.org/pdf/2510.16709.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Liu Haojie, Gao Suixiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.16709">HumanCM: One Step Human Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present HumanCM, a one-step human motion prediction framework built upon consistency models. Instead of relying on multi-step denoising as in diffusion-based methods, HumanCM performs efficient single-step generation by learning a self-consistent mapping between noisy and clean motion states. The framework adopts a Transformer-based spatiotemporal architecture with temporal embeddings to model long-range dependencies and preserve motion coherence. Experiments on Human3.6M and HumanEva-I demonstrate that HumanCM achieves comparable or superior accuracy to state-of-the-art diffusion models while reducing inference steps by up to two orders of magnitude.
<div id='section'>Paperid: <span id='pid'>1127, <a href='https://arxiv.org/pdf/2510.04233.pdf' target='_blank'>https://arxiv.org/pdf/2510.04233.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kai Yang, Yuqi Huang, Junheng Tao, Wanyu Wang, Qitian Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04233">Physics-Inspired All-Pair Interaction Learning for 3D Dynamics Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modeling 3D dynamics is a fundamental problem in multi-body systems across scientific and engineering domains and has important practical implications in trajectory prediction and simulation. While recent GNN-based approaches have achieved strong performance by enforcing geometric symmetries, encoding high-order features or incorporating neural-ODE mechanics, they typically depend on explicitly observed structures and inherently fail to capture the unobserved interactions that are crucial to complex physical behaviors and dynamics mechanism. In this paper, we propose PAINET, a principled SE(3)-equivariant neural architecture for learning all-pair interactions in multi-body systems. The model comprises: (1) a novel physics-inspired attention network derived from the minimization trajectory of an energy function, and (2) a parallel decoder that preserves equivariance while enabling efficient inference. Empirical results on diverse real-world benchmarks, including human motion capture, molecular dynamics, and large-scale protein simulations, show that PAINET consistently outperforms recently proposed models, yielding 4.7% to 41.5% error reductions in 3D dynamics prediction with comparable computation costs in terms of time and memory.
<div id='section'>Paperid: <span id='pid'>1128, <a href='https://arxiv.org/pdf/2509.08534.pdf' target='_blank'>https://arxiv.org/pdf/2509.08534.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shubham Singh, Anoop Jain
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.08534">Phase-Coordinated Multi-Agent Circular Formation Control with Non-Concentric Boundary Constraints</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper addresses the problem of collective circular motion control for unicycle agents, with the objective of achieving phase coordination of their velocity vectors while ensuring that their trajectories remain confined within a prescribed non-concentric circular boundary. To accommodate such nonuniform motion constraints, we build upon our earlier work and extend the use of Mobius transformation to a multi-agent framework. The Mobius transformation maps two nonconcentric circles to concentric ones, thereby converting spatially nonuniform constraints into uniform ones in the transformed plane. Leveraging this property, we introduce the notion of a phase-shifted order parameter, along with the associated concepts of Mobius phase-shift coupled synchronization and balancing, which characterize the phase-coordinated patterns studied in this paper. We establish an equivalence between the unicycle dynamics in the original and transformed planes under the Mobius transformation and its inverse, and show that synchronization is preserved across both planes, whereas balancing is generally not. Distributed control laws are then designed in the transformed plane using barrier Lyapunov functions, under the assumption of an undirected and connected communication topology among agents. These controllers are subsequently mapped back to the original plane to obtain the linear acceleration and turn-rate control inputs applied to the actual agents. Both simulations and experimental results are provided to illustrate the proposed framework.
<div id='section'>Paperid: <span id='pid'>1129, <a href='https://arxiv.org/pdf/2509.04600.pdf' target='_blank'>https://arxiv.org/pdf/2509.04600.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qijun Ying, Zhongyuan Hu, Rui Zhang, Ronghui Li, Yu Lu, Zijiao Zeng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.04600">WATCH: World-aware Allied Trajectory and pose reconstruction for Camera and Human</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Global human motion reconstruction from in-the-wild monocular videos is increasingly demanded across VR, graphics, and robotics applications, yet requires accurate mapping of human poses from camera to world coordinates-a task challenged by depth ambiguity, motion ambiguity, and the entanglement between camera and human movements. While human-motion-centric approaches excel in preserving motion details and physical plausibility, they suffer from two critical limitations: insufficient exploitation of camera orientation information and ineffective integration of camera translation cues. We present WATCH (World-aware Allied Trajectory and pose reconstruction for Camera and Human), a unified framework addressing both challenges. Our approach introduces an analytical heading angle decomposition technique that offers superior efficiency and extensibility compared to existing geometric methods. Additionally, we design a camera trajectory integration mechanism inspired by world models, providing an effective pathway for leveraging camera translation information beyond naive hard-decoding approaches. Through experiments on in-the-wild benchmarks, WATCH achieves state-of-the-art performance in end-to-end trajectory reconstruction. Our work demonstrates the effectiveness of jointly modeling camera-human motion relationships and offers new insights for addressing the long-standing challenge of camera translation integration in global human motion reconstruction. The code will be available publicly.
<div id='section'>Paperid: <span id='pid'>1130, <a href='https://arxiv.org/pdf/2508.12176.pdf' target='_blank'>https://arxiv.org/pdf/2508.12176.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiwei Zheng, Dongyin Hu, Mingmin Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.12176">Scalable RF Simulation in Generative 4D Worlds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Radio Frequency (RF) sensing has emerged as a powerful, privacy-preserving alternative to vision-based methods for indoor perception tasks. However, collecting high-quality RF data in dynamic and diverse indoor environments remains a major challenge. To address this, we introduce WaveVerse, a prompt-based, scalable framework that simulates realistic RF signals from generated indoor scenes with human motions. WaveVerse introduces a language-guided 4D world generator, which includes a state-aware causal transformer for human motion generation conditioned on spatial constraints and texts, and a phase-coherent ray tracing simulator that enables the simulation of accurate and coherent RF signals. Experiments demonstrate the effectiveness of our approach in conditioned human motion generation and highlight how phase coherence is applied to beamforming and respiration monitoring. We further present two case studies in ML-based high-resolution imaging and human activity recognition, demonstrating that WaveVerse not only enables data generation for RF imaging for the first time, but also consistently achieves performance gain in both data-limited and data-adequate scenarios.
<div id='section'>Paperid: <span id='pid'>1131, <a href='https://arxiv.org/pdf/2508.07146.pdf' target='_blank'>https://arxiv.org/pdf/2508.07146.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Liu, Zhijie Liu, Xiao Ren, You-Fu Li, He Kong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.07146">Intention-Aware Diffusion Model for Pedestrian Trajectory Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Predicting pedestrian motion trajectories is critical for the path planning and motion control of autonomous vehicles. Recent diffusion-based models have shown promising results in capturing the inherent stochasticity of pedestrian behavior for trajectory prediction. However, the absence of explicit semantic modelling of pedestrian intent in many diffusion-based methods may result in misinterpreted behaviors and reduced prediction accuracy. To address the above challenges, we propose a diffusion-based pedestrian trajectory prediction framework that incorporates both short-term and long-term motion intentions. Short-term intent is modelled using a residual polar representation, which decouples direction and magnitude to capture fine-grained local motion patterns. Long-term intent is estimated through a learnable, token-based endpoint predictor that generates multiple candidate goals with associated probabilities, enabling multimodal and context-aware intention modelling. Furthermore, we enhance the diffusion process by incorporating adaptive guidance and a residual noise predictor that dynamically refines denoising accuracy. The proposed framework is evaluated on the widely used ETH, UCY, and SDD benchmarks, demonstrating competitive results against state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>1132, <a href='https://arxiv.org/pdf/2508.04966.pdf' target='_blank'>https://arxiv.org/pdf/2508.04966.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifan Zhou, Beizhen Zhao, Pengcheng Wu, Hao Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.04966">Laplacian Analysis Meets Dynamics Modelling: Gaussian Splatting for 4D Reconstruction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While 3D Gaussian Splatting (3DGS) excels in static scene modeling, its extension to dynamic scenes introduces significant challenges. Existing dynamic 3DGS methods suffer from either over-smoothing due to low-rank decomposition or feature collision from high-dimensional grid sampling. This is because of the inherent spectral conflicts between preserving motion details and maintaining deformation consistency at different frequency. To address these challenges, we propose a novel dynamic 3DGS framework with hybrid explicit-implicit functions. Our approach contains three key innovations: a spectral-aware Laplacian encoding architecture which merges Hash encoding and Laplacian-based module for flexible frequency motion control, an enhanced Gaussian dynamics attribute that compensates for photometric distortions caused by geometric deformation, and an adaptive Gaussian split strategy guided by KDTree-based primitive control to efficiently query and optimize dynamic areas. Through extensive experiments, our method demonstrates state-of-the-art performance in reconstructing complex dynamic scenes, achieving better reconstruction fidelity.
<div id='section'>Paperid: <span id='pid'>1133, <a href='https://arxiv.org/pdf/2508.04229.pdf' target='_blank'>https://arxiv.org/pdf/2508.04229.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Liu, Zhijie Liu, Xiao Ren, You-Fu Li, He Kong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.04229">Intention Enhanced Diffusion Model for Multimodal Pedestrian Trajectory Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Predicting pedestrian motion trajectories is critical for path planning and motion control of autonomous vehicles. However, accurately forecasting crowd trajectories remains a challenging task due to the inherently multimodal and uncertain nature of human motion. Recent diffusion-based models have shown promising results in capturing the stochasticity of pedestrian behavior for trajectory prediction. However, few diffusion-based approaches explicitly incorporate the underlying motion intentions of pedestrians, which can limit the interpretability and precision of prediction models. In this work, we propose a diffusion-based multimodal trajectory prediction model that incorporates pedestrians' motion intentions into the prediction framework. The motion intentions are decomposed into lateral and longitudinal components, and a pedestrian intention recognition module is introduced to enable the model to effectively capture these intentions. Furthermore, we adopt an efficient guidance mechanism that facilitates the generation of interpretable trajectories. The proposed framework is evaluated on two widely used human trajectory prediction benchmarks, ETH and UCY, on which it is compared against state-of-the-art methods. The experimental results demonstrate that our method achieves competitive performance.
<div id='section'>Paperid: <span id='pid'>1134, <a href='https://arxiv.org/pdf/2507.15649.pdf' target='_blank'>https://arxiv.org/pdf/2507.15649.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haocheng Xu, Haodong Zhang, Zhenghan Chen, Rong Xiong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.15649">EMP: Executable Motion Prior for Humanoid Robot Standing Upper-body Motion Imitation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To support humanoid robots in performing manipulation tasks, it is essential to study stable standing while accommodating upper-body motions. However, the limited controllable range of humanoid robots in a standing position affects the stability of the entire body. Thus we introduce a reinforcement learning based framework for humanoid robots to imitate human upper-body motions while maintaining overall stability. Our approach begins with designing a retargeting network that generates a large-scale upper-body motion dataset for training the reinforcement learning (RL) policy, which enables the humanoid robot to track upper-body motion targets, employing domain randomization for enhanced robustness. To avoid exceeding the robot's execution capability and ensure safety and stability, we propose an Executable Motion Prior (EMP) module, which adjusts the input target movements based on the robot's current state. This adjustment improves standing stability while minimizing changes to motion amplitude. We evaluate our framework through simulation and real-world tests, demonstrating its practical applicability.
<div id='section'>Paperid: <span id='pid'>1135, <a href='https://arxiv.org/pdf/2507.09704.pdf' target='_blank'>https://arxiv.org/pdf/2507.09704.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaotang Zhang, Ziyi Chang, Qianhui Men, Hubert Shum
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.09704">Real-time and Controllable Reactive Motion Synthesis via Intention Guidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a real-time method for reactive motion synthesis based on the known trajectory of input character, predicting instant reactions using only historical, user-controlled motions. Our method handles the uncertainty of future movements by introducing an intention predictor, which forecasts key joint intentions to make pose prediction more deterministic from the historical interaction. The intention is later encoded into the latent space of its reactive motion, matched with a codebook which represents mappings between input and output. It samples a categorical distribution for pose generation and strengthens model robustness through adversarial training. Unlike previous offline approaches, the system can recursively generate intentions and reactive motions using feedback from earlier steps, enabling real-time, long-term realistic interactive synthesis. Both quantitative and qualitative experiments show our approach outperforms other matching-based motion synthesis approaches, delivering superior stability and generalizability. In our method, user can also actively influence the outcome by controlling the moving directions, creating a personalized interaction path that deviates from predefined trajectories.
<div id='section'>Paperid: <span id='pid'>1136, <a href='https://arxiv.org/pdf/2507.07734.pdf' target='_blank'>https://arxiv.org/pdf/2507.07734.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Michael Neumeier, Jules Lecomte, Nils Kazinski, Soubarna Banik, Bing Li, Axel von Arnim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07734">EEvAct: Early Event-Based Action Recognition with High-Rate Two-Stream Spiking Neural Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recognizing human activities early is crucial for the safety and responsiveness of human-robot and human-machine interfaces. Due to their high temporal resolution and low latency, event-based vision sensors are a perfect match for this early recognition demand. However, most existing processing approaches accumulate events to low-rate frames or space-time voxels which limits the early prediction capabilities. In contrast, spiking neural networks (SNNs) can process the events at a high-rate for early predictions, but most works still fall short on final accuracy. In this work, we introduce a high-rate two-stream SNN which closes this gap by outperforming previous work by 2% in final accuracy on the large-scale THU EACT-50 dataset. We benchmark the SNNs within a novel early event-based recognition framework by reporting Top-1 and Top-5 recognition scores for growing observation time. Finally, we exemplify the impact of these methods on a real-world task of early action triggering for human motion capture in sports.
<div id='section'>Paperid: <span id='pid'>1137, <a href='https://arxiv.org/pdf/2507.02857.pdf' target='_blank'>https://arxiv.org/pdf/2507.02857.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziye Li, Hao Luo, Xincheng Shuai, Henghui Ding
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02857">AnyI2V: Animating Any Conditional Image with Motion Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in video generation, particularly in diffusion models, have driven notable progress in text-to-video (T2V) and image-to-video (I2V) synthesis. However, challenges remain in effectively integrating dynamic motion signals and flexible spatial constraints. Existing T2V methods typically rely on text prompts, which inherently lack precise control over the spatial layout of generated content. In contrast, I2V methods are limited by their dependence on real images, which restricts the editability of the synthesized content. Although some methods incorporate ControlNet to introduce image-based conditioning, they often lack explicit motion control and require computationally expensive training. To address these limitations, we propose AnyI2V, a training-free framework that animates any conditional images with user-defined motion trajectories. AnyI2V supports a broader range of modalities as the conditional image, including data types such as meshes and point clouds that are not supported by ControlNet, enabling more flexible and versatile video generation. Additionally, it supports mixed conditional inputs and enables style transfer and editing via LoRA and text prompts. Extensive experiments demonstrate that the proposed AnyI2V achieves superior performance and provides a new perspective in spatial- and motion-controlled video generation. Code is available at https://henghuiding.com/AnyI2V/.
<div id='section'>Paperid: <span id='pid'>1138, <a href='https://arxiv.org/pdf/2507.01737.pdf' target='_blank'>https://arxiv.org/pdf/2507.01737.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lin Wu, Zhixiang Chen, Jianglin Lan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.01737">HOI-Dyn: Learning Interaction Dynamics for Human-Object Motion Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating realistic 3D human-object interactions (HOIs) remains a challenging task due to the difficulty of modeling detailed interaction dynamics. Existing methods treat human and object motions independently, resulting in physically implausible and causally inconsistent behaviors. In this work, we present HOI-Dyn, a novel framework that formulates HOI generation as a driver-responder system, where human actions drive object responses. At the core of our method is a lightweight transformer-based interaction dynamics model that explicitly predicts how objects should react to human motion. To further enforce consistency, we introduce a residual-based dynamics loss that mitigates the impact of dynamics prediction errors and prevents misleading optimization signals. The dynamics model is used only during training, preserving inference efficiency. Through extensive qualitative and quantitative experiments, we demonstrate that our approach not only enhances the quality of HOI generation but also establishes a feasible metric for evaluating the quality of generated interactions.
<div id='section'>Paperid: <span id='pid'>1139, <a href='https://arxiv.org/pdf/2506.21632.pdf' target='_blank'>https://arxiv.org/pdf/2506.21632.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Da Li, Donggang Jia, Markus Hadwiger, Ivan Viola
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.21632">SkinningGS: Editable Dynamic Human Scene Reconstruction Using Gaussian Splatting Based on a Skinning Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reconstructing an interactive human avatar and the background from a monocular video of a dynamic human scene is highly challenging. In this work we adopt a strategy of point cloud decoupling and joint optimization to achieve the decoupled reconstruction of backgrounds and human bodies while preserving the interactivity of human motion. We introduce a position texture to subdivide the Skinned Multi-Person Linear (SMPL) body model's surface and grow the human point cloud. To capture fine details of human dynamics and deformations, we incorporate a convolutional neural network structure to predict human body point cloud features based on texture. This strategy makes our approach free of hyperparameter tuning for densification and efficiently represents human points with half the point cloud of HUGS. This approach ensures high-quality human reconstruction and reduces GPU resource consumption during training. As a result, our method surpasses the previous state-of-the-art HUGS in reconstruction metrics while maintaining the ability to generalize to novel poses and views. Furthermore, our technique achieves real-time rendering at over 100 FPS, $\sim$6$\times$ the HUGS speed using only Linear Blend Skinning (LBS) weights for human transformation. Additionally, this work demonstrates that this framework can be extended to animal scene reconstruction when an accurately-posed model of an animal is available.
<div id='section'>Paperid: <span id='pid'>1140, <a href='https://arxiv.org/pdf/2506.08851.pdf' target='_blank'>https://arxiv.org/pdf/2506.08851.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sepehr Samavi, Garvish Bhutani, Florian Shkurti, Angela P. Schoellig
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.08851">Deploying SICNav in the Field: Safe and Interactive Crowd Navigation using MPC and Bilevel Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Safe and efficient navigation in crowded environments remains a critical challenge for robots that provide a variety of service tasks such as food delivery or autonomous wheelchair mobility. Classical robot crowd navigation methods decouple human motion prediction from robot motion planning, which neglects the closed-loop interactions between humans and robots. This lack of a model for human reactions to the robot plan (e.g. moving out of the way) can cause the robot to get stuck. Our proposed Safe and Interactive Crowd Navigation (SICNav) method is a bilevel Model Predictive Control (MPC) framework that combines prediction and planning into one optimization problem, explicitly modeling interactions among agents. In this paper, we present a systems overview of the crowd navigation platform we use to deploy SICNav in previously unseen indoor and outdoor environments. We provide a preliminary analysis of the system's operation over the course of nearly 7 km of autonomous navigation over two hours in both indoor and outdoor environments.
<div id='section'>Paperid: <span id='pid'>1141, <a href='https://arxiv.org/pdf/2506.05952.pdf' target='_blank'>https://arxiv.org/pdf/2506.05952.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongjie Fu, Tengjiao Sun, Pengcheng Fang, Xiaohao Cai, Hansung Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.05952">MOGO: Residual Quantized Hierarchical Causal Transformer for High-Quality and Real-Time 3D Human Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in transformer-based text-to-motion generation have led to impressive progress in synthesizing high-quality human motion. Nevertheless, jointly achieving high fidelity, streaming capability, real-time responsiveness, and scalability remains a fundamental challenge. In this paper, we propose MOGO (Motion Generation with One-pass), a novel autoregressive framework tailored for efficient and real-time 3D motion generation. MOGO comprises two key components: (1) MoSA-VQ, a motion scale-adaptive residual vector quantization module that hierarchically discretizes motion sequences with learnable scaling to produce compact yet expressive representations; and (2) RQHC-Transformer, a residual quantized hierarchical causal transformer that generates multi-layer motion tokens in a single forward pass, significantly reducing inference latency. To enhance semantic fidelity, we further introduce a text condition alignment mechanism that improves motion decoding under textual control. Extensive experiments on benchmark datasets including HumanML3D, KIT-ML, and CMP demonstrate that MOGO achieves competitive or superior generation quality compared to state-of-the-art transformer-based methods, while offering substantial improvements in real-time performance, streaming generation, and generalization under zero-shot settings.
<div id='section'>Paperid: <span id='pid'>1142, <a href='https://arxiv.org/pdf/2506.03191.pdf' target='_blank'>https://arxiv.org/pdf/2506.03191.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Muhammad Islam, Tao Huang, Euijoon Ahn, Usman Naseem
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.03191">Multimodal Generative AI with Autoregressive LLMs for Human Motion Understanding and Generation: A Way Forward</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents an in-depth survey on the use of multimodal Generative Artificial Intelligence (GenAI) and autoregressive Large Language Models (LLMs) for human motion understanding and generation, offering insights into emerging methods, architectures, and their potential to advance realistic and versatile motion synthesis. Focusing exclusively on text and motion modalities, this research investigates how textual descriptions can guide the generation of complex, human-like motion sequences. The paper explores various generative approaches, including autoregressive models, diffusion models, Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), and transformer-based models, by analyzing their strengths and limitations in terms of motion quality, computational efficiency, and adaptability. It highlights recent advances in text-conditioned motion generation, where textual inputs are used to control and refine motion outputs with greater precision. The integration of LLMs further enhances these models by enabling semantic alignment between instructions and motion, improving coherence and contextual relevance. This systematic survey underscores the transformative potential of text-to-motion GenAI and LLM architectures in applications such as healthcare, humanoids, gaming, animation, and assistive technologies, while addressing ongoing challenges in generating efficient and realistic human motion.
<div id='section'>Paperid: <span id='pid'>1143, <a href='https://arxiv.org/pdf/2505.21161.pdf' target='_blank'>https://arxiv.org/pdf/2505.21161.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Leon Tolksdorf, Arturo Tejada, Christian Birkner, Nathan van de Wouw
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.21161">Collision Probability Estimation for Optimization-based Vehicular Motion Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Many motion planning algorithms for automated driving require estimating the probability of collision (POC) to account for uncertainties in the measurement and estimation of the motion of road users. Common POC estimation techniques often utilize sampling-based methods that suffer from computational inefficiency and a non-deterministic estimation, i.e., each estimation result for the same inputs is slightly different. In contrast, optimization-based motion planning algorithms require computationally efficient POC estimation, ideally using deterministic estimation, such that typical optimization algorithms for motion planning retain feasibility. Estimating the POC analytically, however, is challenging because it depends on understanding the collision conditions (e.g., vehicle's shape) and characterizing the uncertainty in motion prediction. In this paper, we propose an approach in which we estimate the POC between two vehicles by over-approximating their shapes by a multi-circular shape approximation. The position and heading of the predicted vehicle are modelled as random variables, contrasting with the literature, where the heading angle is often neglected. We guarantee that the provided POC is an over-approximation, which is essential in providing safety guarantees, and present a computationally efficient algorithm for computing the POC estimate for Gaussian uncertainty in the position and heading. This algorithm is then used in a path-following stochastic model predictive controller (SMPC) for motion planning. With the proposed algorithm, the SMPC generates reproducible trajectories while the controller retains its feasibility in the presented test cases and demonstrates the ability to handle varying levels of uncertainty.
<div id='section'>Paperid: <span id='pid'>1144, <a href='https://arxiv.org/pdf/2505.19530.pdf' target='_blank'>https://arxiv.org/pdf/2505.19530.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Amartya Purushottam, Jack Yan, Christopher Yu, Joao Ramos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.19530">Heavy lifting tasks via haptic teleoperation of a wheeled humanoid</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humanoid robots can support human workers in physically demanding environments by performing tasks that require whole-body coordination, such as lifting and transporting heavy objects.These tasks, which we refer to as Dynamic Mobile Manipulation (DMM), require the simultaneous control of locomotion, manipulation, and posture under dynamic interaction forces. This paper presents a teleoperation framework for DMM on a height-adjustable wheeled humanoid robot for carrying heavy payloads. A Human-Machine Interface (HMI) enables whole-body motion retargeting from the human pilot to the robot by capturing the motion of the human and applying haptic feedback. The pilot uses body motion to regulate robot posture and locomotion, while arm movements guide manipulation.Real time haptic feedback delivers end effector wrenches and balance related cues, closing the loop between human perception and robot environment interaction. We evaluate the different telelocomotion mappings that offer varying levels of balance assistance, allowing the pilot to either manually or automatically regulate the robot's lean in response to payload-induced disturbances. The system is validated in experiments involving dynamic lifting of barbells and boxes up to 2.5 kg (21% of robot mass), demonstrating coordinated whole-body control, height variation, and disturbance handling under pilot guidance. Video demo can be found at: https://youtu.be/jF270_bG1h8?feature=shared
<div id='section'>Paperid: <span id='pid'>1145, <a href='https://arxiv.org/pdf/2505.12774.pdf' target='_blank'>https://arxiv.org/pdf/2505.12774.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zichen Geng, Zeeshan Hayder, Wei Liu, Ajmal Mian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.12774">UniHM: Universal Human Motion Generation with Object Interactions in Indoor Scenes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion synthesis in complex scenes presents a fundamental challenge, extending beyond conventional Text-to-Motion tasks by requiring the integration of diverse modalities such as static environments, movable objects, natural language prompts, and spatial waypoints. Existing language-conditioned motion models often struggle with scene-aware motion generation due to limitations in motion tokenization, which leads to information loss and fails to capture the continuous, context-dependent nature of 3D human movement. To address these issues, we propose UniHM, a unified motion language model that leverages diffusion-based generation for synthesizing scene-aware human motion. UniHM is the first framework to support both Text-to-Motion and Text-to-Human-Object Interaction (HOI) in complex 3D scenes. Our approach introduces three key contributions: (1) a mixed-motion representation that fuses continuous 6DoF motion with discrete local motion tokens to improve motion realism; (2) a novel Look-Up-Free Quantization VAE (LFQ-VAE) that surpasses traditional VQ-VAEs in both reconstruction accuracy and generative performance; and (3) an enriched version of the Lingo dataset augmented with HumanML3D annotations, providing stronger supervision for scene-specific motion learning. Experimental results demonstrate that UniHM achieves comparative performance on the OMOMO benchmark for text-to-HOI synthesis and yields competitive results on HumanML3D for general text-conditioned motion generation.
<div id='section'>Paperid: <span id='pid'>1146, <a href='https://arxiv.org/pdf/2505.08293.pdf' target='_blank'>https://arxiv.org/pdf/2505.08293.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhizhuo Yin, Yuk Hang Tsui, Pan Hui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.08293">M3G: Multi-Granular Gesture Generator for Audio-Driven Full-Body Human Motion Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating full-body human gestures encompassing face, body, hands, and global movements from audio is a valuable yet challenging task in virtual avatar creation. Previous systems focused on tokenizing the human gestures framewisely and predicting the tokens of each frame from the input audio. However, one observation is that the number of frames required for a complete expressive human gesture, defined as granularity, varies among different human gesture patterns. Existing systems fail to model these gesture patterns due to the fixed granularity of their gesture tokens. To solve this problem, we propose a novel framework named Multi-Granular Gesture Generator (M3G) for audio-driven holistic gesture generation. In M3G, we propose a novel Multi-Granular VQ-VAE (MGVQ-VAE) to tokenize motion patterns and reconstruct motion sequences from different temporal granularities. Subsequently, we proposed a multi-granular token predictor that extracts multi-granular information from audio and predicts the corresponding motion tokens. Then M3G reconstructs the human gestures from the predicted tokens using the MGVQ-VAE. Both objective and subjective experiments demonstrate that our proposed M3G framework outperforms the state-of-the-art methods in terms of generating natural and expressive full-body human gestures.
<div id='section'>Paperid: <span id='pid'>1147, <a href='https://arxiv.org/pdf/2505.07301.pdf' target='_blank'>https://arxiv.org/pdf/2505.07301.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Katsuki Shimbo, Hiromu Taketsugu, Norimichi Ukita
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.07301">Human Motion Prediction via Test-domain-aware Adaptation with Easily-available Human Motions Estimated from Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In 3D Human Motion Prediction (HMP), conventional methods train HMP models with expensive motion capture data. However, the data collection cost of such motion capture data limits the data diversity, which leads to poor generalizability to unseen motions or subjects. To address this issue, this paper proposes to enhance HMP with additional learning using estimated poses from easily available videos. The 2D poses estimated from the monocular videos are carefully transformed into motion capture-style 3D motions through our pipeline. By additional learning with the obtained motions, the HMP model is adapted to the test domain. The experimental results demonstrate the quantitative and qualitative impact of our method.
<div id='section'>Paperid: <span id='pid'>1148, <a href='https://arxiv.org/pdf/2505.04596.pdf' target='_blank'>https://arxiv.org/pdf/2505.04596.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohammad Merati, David CastaÃ±Ã³n
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.04596">Dynamic Network Flow Optimization for Task Scheduling in PTZ Camera Surveillance Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a novel approach for optimizing the scheduling and control of Pan-Tilt-Zoom (PTZ) cameras in dynamic surveillance environments. The proposed method integrates Kalman filters for motion prediction with a dynamic network flow model to enhance real-time video capture efficiency. By assigning Kalman filters to tracked objects, the system predicts future locations, enabling precise scheduling of camera tasks. This prediction-driven approach is formulated as a network flow optimization, ensuring scalability and adaptability to various surveillance scenarios. To further reduce redundant monitoring, we also incorporate group-tracking nodes, allowing multiple objects to be captured within a single camera focus when appropriate. In addition, a value-based system is introduced to prioritize camera actions, focusing on the timely capture of critical events. By adjusting the decay rates of these values over time, the system ensures prompt responses to tasks with imminent deadlines. Extensive simulations demonstrate that this approach improves coverage, reduces average wait times, and minimizes missed events compared to traditional master-slave camera systems. Overall, our method significantly enhances the efficiency, scalability, and effectiveness of surveillance systems, particularly in dynamic and crowded environments.
<div id='section'>Paperid: <span id='pid'>1149, <a href='https://arxiv.org/pdf/2504.16722.pdf' target='_blank'>https://arxiv.org/pdf/2504.16722.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yingjie Xi, Jian Jun Zhang, Xiaosong Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.16722">PMG: Progressive Motion Generation via Sparse Anchor Postures Curriculum Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In computer animation, game design, and human-computer interaction, synthesizing human motion that aligns with user intent remains a significant challenge. Existing methods have notable limitations: textual approaches offer high-level semantic guidance but struggle to describe complex actions accurately; trajectory-based techniques provide intuitive global motion direction yet often fall short in generating precise or customized character movements; and anchor poses-guided methods are typically confined to synthesize only simple motion patterns. To generate more controllable and precise human motions, we propose \textbf{ProMoGen (Progressive Motion Generation)}, a novel framework that integrates trajectory guidance with sparse anchor motion control. Global trajectories ensure consistency in spatial direction and displacement, while sparse anchor motions only deliver precise action guidance without displacement. This decoupling enables independent refinement of both aspects, resulting in a more controllable, high-fidelity, and sophisticated motion synthesis. ProMoGen supports both dual and single control paradigms within a unified training process. Moreover, we recognize that direct learning from sparse motions is inherently unstable, we introduce \textbf{SAP-CL (Sparse Anchor Posture Curriculum Learning)}, a curriculum learning strategy that progressively adjusts the number of anchors used for guidance, thereby enabling more precise and stable convergence. Extensive experiments demonstrate that ProMoGen excels in synthesizing vivid and diverse motions guided by predefined trajectory and arbitrary anchor frames. Our approach seamlessly integrates personalized motion with structured guidance, significantly outperforming state-of-the-art methods across multiple control scenarios.
<div id='section'>Paperid: <span id='pid'>1150, <a href='https://arxiv.org/pdf/2504.08206.pdf' target='_blank'>https://arxiv.org/pdf/2504.08206.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lansu Dai, Burak Kantarci
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.08206">Advancing Autonomous Vehicle Safety: A Combined Fault Tree Analysis and Bayesian Network Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper integrates Fault Tree Analysis (FTA) and Bayesian Networks (BN) to assess collision risk and establish Automotive Safety Integrity Level (ASIL) B failure rate targets for critical autonomous vehicle (AV) components. The FTA-BN integration combines the systematic decomposition of failure events provided by FTA with the probabilistic reasoning capabilities of BN, which allow for dynamic updates in failure probabilities, enhancing the adaptability of risk assessment. A fault tree is constructed based on AV subsystem architecture, with collision as the top event, and failure rates are assigned while ensuring the total remains within 100 FIT. Bayesian inference is applied to update posterior probabilities, and the results indicate that perception system failures (46.06 FIT) are the most significant contributor, particularly failures to detect existing objects (PF5) and misclassification (PF6). Mitigation strategies are proposed for sensors, perception, decision-making, and motion control to reduce the collision risk. The FTA-BN integration approach provides dynamic risk quantification, offering system designers refined failure rate targets to improve AV safety.
<div id='section'>Paperid: <span id='pid'>1151, <a href='https://arxiv.org/pdf/2503.12628.pdf' target='_blank'>https://arxiv.org/pdf/2503.12628.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ritik Batra, Narjes Pourjafarian, Samantha Chang, Margaret Tsai, Jacob Revelo, Cindy Hsin-Liu Kao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.12628">texTENG: Fabricating Wearable Textile-Based Triboelectric Nanogenerators</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, there has been a surge of interest in sustainable energy sources, particularly for wearable computing. Triboelectric nanogenerators (TENGs) have shown promise in converting human motion into electric power. Textile-based TENGs, valued for their flexibility and breathability, offer an ideal form factor for wearables. However, uptake in maker communities has been slow due to commercially unavailable materials, complex fabrication processes, and structures incompatible with human motion. This paper introduces texTENG, a textile-based framework simplifying the fabrication of power harvesting and self-powered sensing applications. By leveraging accessible materials and familiar tools, texTENG bridges the gap between advanced TENG research and wearable applications. We explore a design menu for creating multidimensional TENG structures using braiding, weaving, and knitting. Technical evaluations and example applications highlight the performance and feasibility of these designs, offering DIY-friendly pathways for fabricating textile-based TENGs and promoting sustainable prototyping practices within the HCI and maker communities.
<div id='section'>Paperid: <span id='pid'>1152, <a href='https://arxiv.org/pdf/2503.00371.pdf' target='_blank'>https://arxiv.org/pdf/2503.00371.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuehao Gao, Yang Yang, Shaoyi Du, Guo-Jun Qi, Junwei Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.00371">Jointly Understand Your Command and Intention:Reciprocal Co-Evolution between Scene-Aware 3D Human Motion Synthesis and Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As two intimate reciprocal tasks, scene-aware human motion synthesis and analysis require a joint understanding between multiple modalities, including 3D body motions, 3D scenes, and textual descriptions. In this paper, we integrate these two paired processes into a Co-Evolving Synthesis-Analysis (CESA) pipeline and mutually benefit their learning. Specifically, scene-aware text-to-human synthesis generates diverse indoor motion samples from the same textual description to enrich human-scene interaction intra-class diversity, thus significantly benefiting training a robust human motion analysis system. Reciprocally, human motion analysis would enforce semantic scrutiny on each synthesized motion sample to ensure its semantic consistency with the given textual description, thus improving realistic motion synthesis. Considering that real-world indoor human motions are goal-oriented and path-guided, we propose a cascaded generation strategy that factorizes text-driven scene-specific human motion generation into three stages: goal inferring, path planning, and pose synthesizing. Coupling CESA with this powerful cascaded motion synthesis model, we jointly improve realistic human motion synthesis and robust human motion analysis in 3D scenes.
<div id='section'>Paperid: <span id='pid'>1153, <a href='https://arxiv.org/pdf/2502.16908.pdf' target='_blank'>https://arxiv.org/pdf/2502.16908.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jaehyung Kim, Jiho Kim, Dongryung Lee, Yujin Jang, Beomjoon Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.16908">A low-cost and lightweight 6 DoF bimanual arm for dynamic and contact-rich manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dynamic and contact-rich object manipulation, such as striking, snatching, or hammering, remains challenging for robotic systems due to hardware limitations. Most existing robots are constrained by high-inertia design, limited compliance, and reliance on expensive torque sensors. To address this, we introduce ARMADA (Affordable Robot for Manipulation and Dynamic Actions), a 6 degrees-of-freedom bimanual robot designed for dynamic manipulation research. ARMADA combines low-inertia, back-drivable actuators with a lightweight design, using readily available components and 3D-printed links for ease of assembly in research labs. The entire system, including both arms, is built for just $6,100. Each arm achieves speeds up to 6.16m/s, almost twice that of most collaborative robots, with a comparable payload of 2.5kg. We demonstrate ARMADA can perform dynamic manipulation like snatching, hammering, and bimanual throwing in real-world environments. We also showcase its effectiveness in reinforcement learning (RL) by training a non-prehensile manipulation policy in simulation and transferring it zero-shot to the real world, as well as human motion shadowing for dynamic bimanual object throwing. ARMADA is fully open-sourced with detailed assembly instructions, CAD models, URDFs, simulation, and learning codes. We highly recommend viewing the supplementary video at https://sites.google.com/view/im2-humanoid-arm.
<div id='section'>Paperid: <span id='pid'>1154, <a href='https://arxiv.org/pdf/2502.02063.pdf' target='_blank'>https://arxiv.org/pdf/2502.02063.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Che-Jui Chang, Qingze Tony Liu, Honglu Zhou, Vladimir Pavlovic, Mubbasir Kapadia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.02063">CASIM: Composite Aware Semantic Injection for Text to Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in generative modeling and tokenization have driven significant progress in text-to-motion generation, leading to enhanced quality and realism in generated motions. However, effectively leveraging textual information for conditional motion generation remains an open challenge. We observe that current approaches, primarily relying on fixed-length text embeddings (e.g., CLIP) for global semantic injection, struggle to capture the composite nature of human motion, resulting in suboptimal motion quality and controllability. To address this limitation, we propose the Composite Aware Semantic Injection Mechanism (CASIM), comprising a composite-aware semantic encoder and a text-motion aligner that learns the dynamic correspondence between text and motion tokens. Notably, CASIM is model and representation-agnostic, readily integrating with both autoregressive and diffusion-based methods. Experiments on HumanML3D and KIT benchmarks demonstrate that CASIM consistently improves motion quality, text-motion alignment, and retrieval scores across state-of-the-art methods. Qualitative analyses further highlight the superiority of our composite-aware approach over fixed-length semantic injection, enabling precise motion control from text prompts and stronger generalization to unseen text inputs.
<div id='section'>Paperid: <span id='pid'>1155, <a href='https://arxiv.org/pdf/2501.19083.pdf' target='_blank'>https://arxiv.org/pdf/2501.19083.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lei Jiang, Ye Wei, Hao Ni
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.19083">MotionPCM: Real-Time Motion Synthesis with Phased Consistency Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion models have become a popular choice for human motion synthesis due to their powerful generative capabilities. However, their high computational complexity and large sampling steps pose challenges for real-time applications. Fortunately, the Consistency Model (CM) provides a solution to greatly reduce the number of sampling steps from hundreds to a few, typically fewer than four, significantly accelerating the synthesis of diffusion models. However, applying CM to text-conditioned human motion synthesis in latent space yields unsatisfactory generation results. In this paper, we introduce \textbf{MotionPCM}, a phased consistency model-based approach designed to improve the quality and efficiency for real-time motion synthesis in latent space. Experimental results on the HumanML3D dataset show that our model achieves real-time inference at over 30 frames per second in a single sampling step while outperforming the previous state-of-the-art with a 38.9\% improvement in FID. The code will be available for reproduction.
<div id='section'>Paperid: <span id='pid'>1156, <a href='https://arxiv.org/pdf/2412.17333.pdf' target='_blank'>https://arxiv.org/pdf/2412.17333.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jaeheun Jung, Jaehyuk Lee, Changhae Jung, Hanyoung Kim, Bosung Jung, Donghun Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.17333">Broadband Ground Motion Synthesis by Diffusion Model with Minimal Condition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Shock waves caused by earthquakes can be devastating. Generating realistic earthquake-caused ground motion waveforms help reducing losses in lives and properties, yet generative models for the task tend to generate subpar waveforms. We present High-fidelity Earthquake Groundmotion Generation System (HEGGS) and demonstrate its superior performance using earthquakes from North American, East Asian, and European regions. HEGGS exploits the intrinsic characteristics of earthquake dataset and learns the waveforms using an end-to-end differentiable generator containing conditional latent diffusion model and hi-fidelity waveform construction model. We show the learning efficiency of HEGGS by training it on a single GPU machine and validate its performance using earthquake databases from North America, East Asia, and Europe, using diverse criteria from waveform generation tasks and seismology. Once trained, HEGGS can generate three dimensional E-N-Z seismic waveforms with accurate P/S phase arrivals, envelope correlation, signal-to-noise ratio, GMPE analysis, frequency content analysis, and section plot analysis.
<div id='section'>Paperid: <span id='pid'>1157, <a href='https://arxiv.org/pdf/2412.05277.pdf' target='_blank'>https://arxiv.org/pdf/2412.05277.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hee Jae Kim, Kathakoli Sengupta, Masaki Kuribayashi, Hernisa Kacorri, Eshed Ohn-Bar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.05277">Text to Blind Motion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>People who are blind perceive the world differently than those who are sighted, which can result in distinct motion characteristics. For instance, when crossing at an intersection, blind individuals may have different patterns of movement, such as veering more from a straight path or using touch-based exploration around curbs and obstacles. These behaviors may appear less predictable to motion models embedded in technologies such as autonomous vehicles. Yet, the ability of 3D motion models to capture such behavior has not been previously studied, as existing datasets for 3D human motion currently lack diversity and are biased toward people who are sighted. In this work, we introduce BlindWays, the first multimodal motion benchmark for pedestrians who are blind. We collect 3D motion data using wearable sensors with 11 blind participants navigating eight different routes in a real-world urban setting. Additionally, we provide rich textual descriptions that capture the distinctive movement characteristics of blind pedestrians and their interactions with both the navigation aid (e.g., a white cane or a guide dog) and the environment. We benchmark state-of-the-art 3D human prediction models, finding poor performance with off-the-shelf and pre-training-based methods for our novel task. To contribute toward safer and more reliable systems that can seamlessly reason over diverse human movements in their environments, our text-and-motion benchmark is available at https://blindways.github.io.
<div id='section'>Paperid: <span id='pid'>1158, <a href='https://arxiv.org/pdf/2411.19459.pdf' target='_blank'>https://arxiv.org/pdf/2411.19459.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuhang Zhang, Yuan Zhou, Zeyu Liu, Yuxuan Cai, Qiuyue Wang, Aidong Men, Huan Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.19459">Fleximo: Towards Flexible Text-to-Human Motion Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current methods for generating human motion videos rely on extracting pose sequences from reference videos, which restricts flexibility and control. Additionally, due to the limitations of pose detection techniques, the extracted pose sequences can sometimes be inaccurate, leading to low-quality video outputs. We introduce a novel task aimed at generating human motion videos solely from reference images and natural language. This approach offers greater flexibility and ease of use, as text is more accessible than the desired guidance videos. However, training an end-to-end model for this task requires millions of high-quality text and human motion video pairs, which are challenging to obtain. To address this, we propose a new framework called Fleximo, which leverages large-scale pre-trained text-to-3D motion models. This approach is not straightforward, as the text-generated skeletons may not consistently match the scale of the reference image and may lack detailed information. To overcome these challenges, we introduce an anchor point based rescale method and design a skeleton adapter to fill in missing details and bridge the gap between text-to-motion and motion-to-video generation. We also propose a video refinement process to further enhance video quality. A large language model (LLM) is employed to decompose natural language into discrete motion sequences, enabling the generation of motion videos of any desired length. To assess the performance of Fleximo, we introduce a new benchmark called MotionBench, which includes 400 videos across 20 identities and 20 motions. We also propose a new metric, MotionScore, to evaluate the accuracy of motion following. Both qualitative and quantitative results demonstrate that our method outperforms existing text-conditioned image-to-video generation methods. All code and model weights will be made publicly available.
<div id='section'>Paperid: <span id='pid'>1159, <a href='https://arxiv.org/pdf/2411.17917.pdf' target='_blank'>https://arxiv.org/pdf/2411.17917.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Boqi Li, Haojie Zhu, Henry X. Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.17917">DECODE: Domain-aware Continual Domain Expansion for Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion prediction is critical for autonomous vehicles to effectively navigate complex environments and accurately anticipate the behaviors of other traffic participants. As autonomous driving continues to evolve, the need to assimilate new and varied driving scenarios necessitates frequent model updates through retraining. To address these demands, we introduce DECODE, a novel continual learning framework that begins with a pre-trained generalized model and incrementally develops specialized models for distinct domains. Unlike existing continual learning approaches that attempt to develop a unified model capable of generalizing across diverse scenarios, DECODE uniquely balances specialization with generalization, dynamically adjusting to real-time demands. The proposed framework leverages a hypernetwork to generate model parameters, significantly reducing storage requirements, and incorporates a normalizing flow mechanism for real-time model selection based on likelihood estimation. Furthermore, DECODE merges outputs from the most relevant specialized and generalized models using deep Bayesian uncertainty estimation techniques. This integration ensures optimal performance in familiar conditions while maintaining robustness in unfamiliar scenarios. Extensive evaluations confirm the effectiveness of the framework, achieving a notably low forgetting rate of 0.044 and an average minADE of 0.584 m, significantly surpassing traditional learning strategies and demonstrating adaptability across a wide range of driving conditions.
<div id='section'>Paperid: <span id='pid'>1160, <a href='https://arxiv.org/pdf/2411.17335.pdf' target='_blank'>https://arxiv.org/pdf/2411.17335.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zeyu Ling, Bo Han, Shiyang Li, Jikang Cheng, Hongdeng Shen, Changqing Zou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.17335">VersatileMotion: A Unified Framework for Motion Synthesis and Comprehension</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models (LLMs) are, by design, inherently capable of multi-task learning: through a unified next-token prediction paradigm, they can naturally address a wide variety of downstream tasks. Prior work in the motion domain has demonstrated some generality by adapting LLMs via a Motion Tokenizer coupled with an autoregressive Transformer to generate and understand human motion. However, this generality remains limited in scope and yields only modest performance gains. We introduce VersatileMotion, a unified multimodal motion LLM that combines a novel motion tokenizer, integrating VQ-VAE with flow matching, and an autoregressive transformer backbone to seamlessly support at least nine distinct motion-related tasks. VersatileMotion is the first method to handle single-agent and multi-agent motions in a single framework and enable cross-modal conversion between motion, text, music, and speech, achieving state-of-the-art performance on seven of these tasks. Each sequence in MotionHub may include one or more of the following annotations: natural-language captions, music or audio clips, speech transcripts, and multi-agent interaction data. To facilitate evaluation, we define and release benchmark splits covering nine core tasks. Extensive experiments demonstrate the superior performance, versatility, and potential of VersatileMotion as a foundational model for future understanding and generation of motion.
<div id='section'>Paperid: <span id='pid'>1161, <a href='https://arxiv.org/pdf/2410.08931.pdf' target='_blank'>https://arxiv.org/pdf/2410.08931.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Clayton Leite, Yu Xiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.08931">Enhancing Motion Variation in Text-to-Motion Models via Pose and Video Conditioned Editing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-to-motion models that generate sequences of human poses from textual descriptions are garnering significant attention. However, due to data scarcity, the range of motions these models can produce is still limited. For instance, current text-to-motion models cannot generate a motion of kicking a football with the instep of the foot, since the training data only includes martial arts kicks. We propose a novel method that uses short video clips or images as conditions to modify existing basic motions. In this approach, the model's understanding of a kick serves as the prior, while the video or image of a football kick acts as the posterior, enabling the generation of the desired motion. By incorporating these additional modalities as conditions, our method can create motions not present in the training set, overcoming the limitations of text-motion datasets. A user study with 26 participants demonstrated that our approach produces unseen motions with realism comparable to commonly represented motions in text-motion datasets (e.g., HumanML3D), such as walking, running, squatting, and kicking.
<div id='section'>Paperid: <span id='pid'>1162, <a href='https://arxiv.org/pdf/2410.05737.pdf' target='_blank'>https://arxiv.org/pdf/2410.05737.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ashish Kumar, Laxmidhar Behera
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.05737">Thrust Microstepping via Acceleration Feedback in Quadrotor Control for Aerial Grasping of Dynamic Payload</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we propose an end-to-end Thrust Microstepping and Decoupled Control (TMDC) of quadrotors. TMDC focuses on precise off-centered aerial grasping of payloads dynamically, which are attached rigidly to the UAV body via a gripper contrary to the swinging payload. The dynamic payload grasping quickly changes UAV's mass, inertia etc, causing instability while performing a grasping operation in-air. We identify that to handle unknown payload grasping, the role of thrust controller is crucial. Hence, we focus on thrust control without involving system parameters such as mass etc. TMDC is based on our novel Thrust Microstepping via Acceleration Feedback (TMAF) thrust controller and Decoupled Motion Control (DMC). TMAF precisely estimates the desired thrust even at smaller loop rates while DMC decouples the horizontal and vertical motion to counteract disturbances in the case of dynamic payloads. We prove the controller's efficacy via exhaustive experiments in practically interesting and adverse real-world cases, such as fully onboard state estimation without any positioning sensor, narrow and indoor flying workspaces with intense wind turbulence, heavy payloads, non-uniform loop rates, etc. Our TMDC outperforms recent direct acceleration feedback thrust controller (DA) and geometric tracking control (GT) in flying stably for aerial grasping and achieves RMSE below 0.04m in contrast to 0.15m of DA and 0.16m of GT.
<div id='section'>Paperid: <span id='pid'>1163, <a href='https://arxiv.org/pdf/2410.02510.pdf' target='_blank'>https://arxiv.org/pdf/2410.02510.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>James Gao, Jacob Lee, Yuting Zhou, Yunze Hu, Chang Liu, Pingping Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.02510">SwarmCVT: Centroidal Voronoi Tessellation-Based Path Planning for Very-Large-Scale Robotics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Swarm robotics, or very large-scale robotics (VLSR), has many meaningful applications for complicated tasks. However, the complexity of motion control and energy costs stack up quickly as the number of robots increases. In addressing this problem, our previous studies have formulated various methods employing macroscopic and microscopic approaches. These methods enable microscopic robots to adhere to a reference Gaussian mixture model (GMM) distribution observed at the macroscopic scale. As a result, optimizing the macroscopic level will result in an optimal overall result. However, all these methods require systematic and global generation of Gaussian components (GCs) within obstacle-free areas to construct the GMM trajectories. This work utilizes centroidal Voronoi tessellation to generate GCs methodically. Consequently, it demonstrates performance improvement while also ensuring consistency and reliability.
<div id='section'>Paperid: <span id='pid'>1164, <a href='https://arxiv.org/pdf/2409.11920.pdf' target='_blank'>https://arxiv.org/pdf/2409.11920.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lorenzo Mandelli, Stefano Berretti
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.11920">Generation of Complex 3D Human Motion by Temporal and Spatial Composition of Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we address the challenge of generating realistic 3D human motions for action classes that were never seen during the training phase. Our approach involves decomposing complex actions into simpler movements, specifically those observed during training, by leveraging the knowledge of human motion contained in GPTs models. These simpler movements are then combined into a single, realistic animation using the properties of diffusion models. Our claim is that this decomposition and subsequent recombination of simple movements can synthesize an animation that accurately represents the complex input action. This method operates during the inference phase and can be integrated with any pre-trained diffusion model, enabling the synthesis of motion classes not present in the training data. We evaluate our method by dividing two benchmark human motion datasets into basic and complex actions, and then compare its performance against the state-of-the-art.
<div id='section'>Paperid: <span id='pid'>1165, <a href='https://arxiv.org/pdf/2409.06791.pdf' target='_blank'>https://arxiv.org/pdf/2409.06791.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Michael Adewole, Oluwaseyi Giwa, Favour Nerrise, Martins Osifeko, Ajibola Oyedeji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.06791">Human Motion Synthesis_ A Diffusion Approach for Motion Stitching and In-Betweening</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion generation is an important area of research in many fields. In this work, we tackle the problem of motion stitching and in-betweening. Current methods either require manual efforts, or are incapable of handling longer sequences. To address these challenges, we propose a diffusion model with a transformer-based denoiser to generate realistic human motion. Our method demonstrated strong performance in generating in-betweening sequences, transforming a variable number of input poses into smooth and realistic motion sequences consisting of 75 frames at 15 fps, resulting in a total duration of 5 seconds. We present the performance evaluation of our method using quantitative metrics such as Frechet Inception Distance (FID), Diversity, and Multimodality, along with visual assessments of the generated outputs.
<div id='section'>Paperid: <span id='pid'>1166, <a href='https://arxiv.org/pdf/2407.10481.pdf' target='_blank'>https://arxiv.org/pdf/2407.10481.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jordan Juravsky, Yunrong Guo, Sanja Fidler, Xue Bin Peng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.10481">SuperPADL: Scaling Language-Directed Physics-Based Control with Progressive Supervised Distillation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Physically-simulated models for human motion can generate high-quality responsive character animations, often in real-time. Natural language serves as a flexible interface for controlling these models, allowing expert and non-expert users to quickly create and edit their animations. Many recent physics-based animation methods, including those that use text interfaces, train control policies using reinforcement learning (RL). However, scaling these methods beyond several hundred motions has remained challenging. Meanwhile, kinematic animation models are able to successfully learn from thousands of diverse motions by leveraging supervised learning methods. Inspired by these successes, in this work we introduce SuperPADL, a scalable framework for physics-based text-to-motion that leverages both RL and supervised learning to train controllers on thousands of diverse motion clips. SuperPADL is trained in stages using progressive distillation, starting with a large number of specialized experts using RL. These experts are then iteratively distilled into larger, more robust policies using a combination of reinforcement learning and supervised learning. Our final SuperPADL controller is trained on a dataset containing over 5000 skills and runs in real time on a consumer GPU. Moreover, our policy can naturally transition between skills, allowing for users to interactively craft multi-stage animations. We experimentally demonstrate that SuperPADL significantly outperforms RL-based baselines at this large data scale.
<div id='section'>Paperid: <span id='pid'>1167, <a href='https://arxiv.org/pdf/2406.02767.pdf' target='_blank'>https://arxiv.org/pdf/2406.02767.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kathrin Donandt, Dirk SÃ¶ffker
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.02767">Spatial and social situation-aware transformer-based trajectory prediction of autonomous systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous transportation systems such as road vehicles or vessels require the consideration of the static and dynamic environment to dislocate without collision. Anticipating the behavior of an agent in a given situation is required to adequately react to it in time. Developing deep learning-based models has become the dominant approach to motion prediction recently. The social environment is often considered through a CNN-LSTM-based sub-module processing a $\textit{social tensor}$ that includes information of the past trajectory of surrounding agents. For the proposed transformer-based trajectory prediction model, an alternative, computationally more efficient social tensor definition and processing is suggested. It considers the interdependencies between target and surrounding agents at each time step directly instead of relying on information of last hidden LSTM states of individually processed agents. A transformer-based sub-module, the Social Tensor Transformer, is integrated into the overall prediction model. It is responsible for enriching the target agent's dislocation features with social interaction information obtained from the social tensor. For the awareness of spatial limitations, dislocation features are defined in relation to the navigable area. This replaces additional, computationally expensive map processing sub-modules. An ablation study shows, that for longer prediction horizons, the deviation of the predicted trajectory from the ground truth is lower compared to a spatially and socially agnostic model. Even if the performance gain from a spatial-only to a spatial and social context-sensitive model is small in terms of common error measures, by visualizing the results it can be shown that the proposed model in fact is able to predict reactions to surrounding agents and explicitely allows an interpretable behavior.
<div id='section'>Paperid: <span id='pid'>1168, <a href='https://arxiv.org/pdf/2405.18700.pdf' target='_blank'>https://arxiv.org/pdf/2405.18700.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuehao Gao, Yang Yang, Yang Wu, Shaoyi Du, Guo-Jun Qi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.18700">Multi-Condition Latent Diffusion Network for Scene-Aware Neural Human Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Inferring 3D human motion is fundamental in many applications, including understanding human activity and analyzing one's intention. While many fruitful efforts have been made to human motion prediction, most approaches focus on pose-driven prediction and inferring human motion in isolation from the contextual environment, thus leaving the body location movement in the scene behind. However, real-world human movements are goal-directed and highly influenced by the spatial layout of their surrounding scenes. In this paper, instead of planning future human motion in a 'dark' room, we propose a Multi-Condition Latent Diffusion network (MCLD) that reformulates the human motion prediction task as a multi-condition joint inference problem based on the given historical 3D body motion and the current 3D scene contexts. Specifically, instead of directly modeling joint distribution over the raw motion sequences, MCLD performs a conditional diffusion process within the latent embedding space, characterizing the cross-modal mapping from the past body movement and current scene context condition embeddings to the future human motion embedding. Extensive experiments on large-scale human motion prediction datasets demonstrate that our MCLD achieves significant improvements over the state-of-the-art methods on both realistic and diverse predictions.
<div id='section'>Paperid: <span id='pid'>1169, <a href='https://arxiv.org/pdf/2405.16909.pdf' target='_blank'>https://arxiv.org/pdf/2405.16909.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>LÃ©ore Bensabath, Mathis Petrovich, GÃ¼l Varol
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.16909">A Cross-Dataset Study for Text-based 3D Human Motion Retrieval</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We provide results of our study on text-based 3D human motion retrieval and particularly focus on cross-dataset generalization. Due to practical reasons such as dataset-specific human body representations, existing works typically benchmarkby training and testing on partitions from the same dataset. Here, we employ a unified SMPL body format for all datasets, which allows us to perform training on one dataset, testing on the other, as well as training on a combination of datasets. Our results suggest that there exist dataset biases in standard text-motion benchmarks such as HumanML3D, KIT Motion-Language, and BABEL. We show that text augmentations help close the domain gap to some extent, but the gap remains. We further provide the first zero-shot action recognition results on BABEL, without using categorical action labels during training, opening up a new avenue for future research.
<div id='section'>Paperid: <span id='pid'>1170, <a href='https://arxiv.org/pdf/2405.06989.pdf' target='_blank'>https://arxiv.org/pdf/2405.06989.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shubham Singh, Anoop Jain
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.06989">Mobius Transformation-Based Circular Motion Control for Unicycle Robots in Nonconcentric Circular Geofences</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Nonuniform motion constraints are ubiquitous in robotic applications. Geofencing control is one such paradigm where the motion of a robot must be constrained within a predefined boundary. This paper addresses the problem of stabilizing a unicycle robot around a desired circular orbit while confining its motion within a nonconcentric external circular boundary. Our solution approach relies on the concept of the so-called Mobius transformation that, under certain practical conditions, maps two nonconcentric circles to a pair of concentric circles, and hence, results in uniform spatial motion constraints. The choice of such a Mobius transformation is governed by the roots of a quadratic equation in the post-design analysis that decides how the regions enclosed by the two circles are mapped onto the two planes. We show that the problem can be formulated either as a trajectory-constraining problem or an obstacle-avoidance problem in the transformed plane, depending on these roots. Exploiting the idea of the barrier Lyapunov function, we propose a unique control law that solves both these contrasting problems in the transformed plane and renders a solution to the original problem in the actual plane. By relating parameters of two planes under Mobius transformation and its inverse map, we further establish a connection between the control laws in two planes and determine the control law to be applied in the actual plane. Simulation and experimental results are provided to illustrate the key theoretical developments.
<div id='section'>Paperid: <span id='pid'>1171, <a href='https://arxiv.org/pdf/2405.06290.pdf' target='_blank'>https://arxiv.org/pdf/2405.06290.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jin Dai, Zejiang Wang, Yebin Wang, Rien Quirynen, Stefano Di Cairano
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.06290">Path Planning and Motion Control for Accurate Positioning of Car-like Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper investigates the planning and control for accurate positioning of car-like robots. We propose a solution that integrates two modules: a motion planner, facilitated by the rapidly-exploring random tree algorithm and continuous-curvature (CC) steering technique, generates a CC trajectory as a reference; and a nonlinear model predictive controller (NMPC) regulates the robot to accurately track the reference trajectory. Based on the $Î¼$-tangency conditions in prior art, we derive explicit existence conditions and develop associated computation methods for a special class of CC paths which not only admit the same driving patterns as Reeds-Shepp paths but also consist of cusp-free clothoid turns. Afterwards, we create an autonomous vehicle parking scenario where the NMPC endeavors to follow the reference trajectory. Feasibility and computational efficiency of the CC steering are validated by numerical simulation. CarSim-Simulink joint simulations statistically verify that with exactly same NMPC, the closed-loop system with CC trajectories as references substantially outperforms the case where Reeds-Shepp trajectories are used as references.
<div id='section'>Paperid: <span id='pid'>1172, <a href='https://arxiv.org/pdf/2405.04804.pdf' target='_blank'>https://arxiv.org/pdf/2405.04804.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yin Li, Rajalakshmi Nandakumar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.04804">WixUp: A General Data Augmentation Framework for Wireless Perception in Tracking of Humans</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in wireless perception technologies, including mmWave, WiFi, and acoustics, have expanded their application in human motion tracking and health monitoring. They are promising alternatives to traditional camera-based perception systems, thanks to their efficacy under diverse conditions or occlusions, and enhanced privacy. However, the integration of deep learning within this field introduces new challenges such as the need for extensive training data and poor model generalization, especially with sparse and noisy wireless point clouds. As a remedy, data augmentation is one solution well-explored in other deep learning fields, but they are not directly applicable to the unique characteristics of wireless signals. This motivates us to propose a custom data augmentation framework, WixUp, tailored for wireless perception. Moreover, we aim to make it a general framework supporting various datasets, model architectures, sensing modalities, and tasks; while previous wireless data augmentation or generative simulations do not exhibit this generalizability, only limited to certain use cases. More specifically, WixUp can reverse-transform lossy coordinates into dense range profiles using Gaussian mixture and probability tricks, making it capable of in-depth data diversity enhancement; and its mixing-based method enables unsupervised domain adaptation via self-training, allowing training of the model with no labels from new users or environments in practice. In summary, our extensive evaluation experiments show that WixUp provides consistent performance improvement across various scenarios and outperforms the baselines.
<div id='section'>Paperid: <span id='pid'>1173, <a href='https://arxiv.org/pdf/2405.02911.pdf' target='_blank'>https://arxiv.org/pdf/2405.02911.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenyu Lou, Qiongjie Cui, Haofan Wang, Xu Tang, Hong Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.02911">Multimodal Sense-Informed Prediction of 3D Human Motions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Predicting future human pose is a fundamental application for machine intelligence, which drives robots to plan their behavior and paths ahead of time to seamlessly accomplish human-robot collaboration in real-world 3D scenarios. Despite encouraging results, existing approaches rarely consider the effects of the external scene on the motion sequence, leading to pronounced artifacts and physical implausibilities in the predictions. To address this limitation, this work introduces a novel multi-modal sense-informed motion prediction approach, which conditions high-fidelity generation on two modal information: external 3D scene, and internal human gaze, and is able to recognize their salience for future human activity. Furthermore, the gaze information is regarded as the human intention, and combined with both motion and scene features, we construct a ternary intention-aware attention to supervise the generation to match where the human wants to reach. Meanwhile, we introduce semantic coherence-aware attention to explicitly distinguish the salient point clouds and the underlying ones, to ensure a reasonable interaction of the generated sequence with the 3D scene. On two real-world benchmarks, the proposed method achieves state-of-the-art performance both in 3D human pose and trajectory prediction.
<div id='section'>Paperid: <span id='pid'>1174, <a href='https://arxiv.org/pdf/2403.15959.pdf' target='_blank'>https://arxiv.org/pdf/2403.15959.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Justin Lidard, Hang Pham, Ariel Bachman, Bryan Boateng, Anirudha Majumdar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.15959">Risk-Calibrated Human-Robot Interaction via Set-Valued Intent Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tasks where robots must anticipate human intent, such as navigating around a cluttered home or sorting everyday items, are challenging because they exhibit a wide range of valid actions that lead to similar outcomes. Moreover, zero-shot cooperation between human-robot partners is an especially challenging problem because it requires the robot to infer and adapt on the fly to a latent human intent, which could vary significantly from human to human. Recently, deep learned motion prediction models have shown promising results in predicting human intent but are prone to being confidently incorrect. In this work, we present Risk-Calibrated Interactive Planning (RCIP), which is a framework for measuring and calibrating risk associated with uncertain action selection in human-robot cooperation, with the fundamental idea that the robot should ask for human clarification when the risk associated with the uncertainty in the human's intent cannot be controlled. RCIP builds on the theory of set-valued risk calibration to provide a finite-sample statistical guarantee on the cumulative loss incurred by the robot while minimizing the cost of human clarification in complex multi-step settings. Our main insight is to frame the risk control problem as a sequence-level multi-hypothesis testing problem, allowing efficient calibration using a low-dimensional parameter that controls a pre-trained risk-aware policy. Experiments across a variety of simulated and real-world environments demonstrate RCIP's ability to predict and adapt to a diverse set of dynamic human intents.
<div id='section'>Paperid: <span id='pid'>1175, <a href='https://arxiv.org/pdf/2403.05878.pdf' target='_blank'>https://arxiv.org/pdf/2403.05878.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yorick Broens, Hans Butler, Roland TÃ³th
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.05878">Frequency Domain Auto-tuning of Structured LPV Controllers for High-Precision Motion Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion systems are a vital part of many industrial processes. However, meeting the increasingly stringent demands of these systems, especially concerning precision and throughput, requires novel control design methods that can go beyond the capabilities of traditional solutions. Traditional control methods often struggle with the complexity and position-dependent effects inherent in modern motion systems, leading to compromises in performance and a laborious task of controller design. This paper addresses these challenges by introducing a novel structured feedback control auto-tuning approach for multiple-input multiple-output (MIMO) motion systems. By leveraging frequency response function (FRF) estimates and the linear-parameter-varying (LPV) control framework, the proposed approach automates the controller design, while providing local stability and performance guarantees. Key innovations include norm-based magnitude optimization of the sensitivity functions, an automated stability check through a novel extended factorized Nyquist criterion, a modular structured MIMO LPV controller parameterization, and a controller discretization approach which preserves the continuous-time (CT) controller parameterization. The proposed approach is validated through experiments using a state-of-the-art moving-magnet planar actuator prototype.
<div id='section'>Paperid: <span id='pid'>1176, <a href='https://arxiv.org/pdf/2402.17339.pdf' target='_blank'>https://arxiv.org/pdf/2402.17339.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Xiang, Haoteng Yin, He Wang, Xiaogang Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.17339">SocialCVAE: Predicting Pedestrian Trajectory via Interaction Conditioned Latents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pedestrian trajectory prediction is the key technology in many applications for providing insights into human behavior and anticipating human future motions. Most existing empirical models are explicitly formulated by observed human behaviors using explicable mathematical terms with a deterministic nature, while recent work has focused on developing hybrid models combined with learning-based techniques for powerful expressiveness while maintaining explainability. However, the deterministic nature of the learned steering behaviors from the empirical models limits the models' practical performance. To address this issue, this work proposes the social conditional variational autoencoder (SocialCVAE) for predicting pedestrian trajectories, which employs a CVAE to explore behavioral uncertainty in human motion decisions. SocialCVAE learns socially reasonable motion randomness by utilizing a socially explainable interaction energy map as the CVAE's condition, which illustrates the future occupancy of each pedestrian's local neighborhood area. The energy map is generated using an energy-based interaction model, which anticipates the energy cost (i.e., repulsion intensity) of pedestrians' interactions with neighbors. Experimental results on two public benchmarks including 25 scenes demonstrate that SocialCVAE significantly improves prediction accuracy compared with the state-of-the-art methods, with up to 16.85% improvement in Average Displacement Error (ADE) and 69.18% improvement in Final Displacement Error (FDE).
<div id='section'>Paperid: <span id='pid'>1177, <a href='https://arxiv.org/pdf/2402.09459.pdf' target='_blank'>https://arxiv.org/pdf/2402.09459.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Javier GonzÃ¡lez-Alonso, David Oviedo-Pastor, HÃ©ctor J. Aguado, Francisco J. DÃ­az-Pernas, David GonzÃ¡lez-Ortega, Mario MartÃ­nez-Zarzuela
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.09459">Custom IMU-Based Wearable System for Robust 2.4 GHz Wireless Human Body Parts Orientation Tracking and 3D Movement Visualization on an Avatar</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent studies confirm the applicability of Inertial Measurement Unit (IMU)-based systems for human motion analysis. Notwithstanding, high-end IMU-based commercial solutions are yet too expensive and complex to democratize their use among a wide range of potential users. Less featured entry-level commercial solutions are being introduced in the market, trying to fill this gap, but still present some limitations that need to be overcome. At the same time, there is a growing number of scientific papers using not commercial, but custom do-it-yourself IMU-based systems in medical and sports applications. Even though these solutions can help to popularize the use of this technology, they have more limited features and the description on how to design and build them from scratch is yet too scarce in the literature. The aim of this work is two-fold: (1) Proving the feasibility of building an affordable custom solution aimed at simultaneous multiple body parts orientation tracking; while providing a detailed bottom-up description of the required hardware, tools, and mathematical operations to estimate and represent 3D movement in real-time. (2) Showing how the introduction of a custom 2.4 GHz communication protocol including a channel hopping strategy can address some of the current communication limitations of entry-level commercial solutions. The proposed system can be used for wireless real-time human body parts orientation tracking with up to 10 custom sensors, at least at 50 Hz. In addition, it provides a more reliable motion data acquisition in Bluetooth and Wi-Fi crowded environments, where the use of entry-level commercial solutions might be unfeasible. This system can be used as a groundwork for developing affordable human motion analysis solutions that do not require an accurate kinematic analysis.
<div id='section'>Paperid: <span id='pid'>1178, <a href='https://arxiv.org/pdf/2402.09442.pdf' target='_blank'>https://arxiv.org/pdf/2402.09442.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weixiang Wan, Wenjian Sun, Qiang Zeng, Linying Pan, Jingyu Xu, Bo Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.09442">Progress in artificial intelligence applications based on the combination of self-driven sensors and deep learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the era of Internet of Things, how to develop a smart sensor system with sustainable power supply, easy deployment and flexible use has become a difficult problem to be solved. The traditional power supply has problems such as frequent replacement or charging when in use, which limits the development of wearable devices. The contact-to-separate friction nanogenerator (TENG) was prepared by using polychotomy thy lene (PTFE) and aluminum (AI) foils. Human motion energy was collected by human body arrangement, and human motion posture was monitored according to the changes of output electrical signals. In 2012, Academician Wang Zhong lin and his team invented the triboelectric nanogenerator (TENG), which uses Maxwell displacement current as a driving force to directly convert mechanical stimuli into electrical signals, so it can be used as a self-driven sensor. Teng-based sensors have the advantages of simple structure and high instantaneous power density, which provides an important means for building intelligent sensor systems. At the same time, machine learning, as a technology with low cost, short development cycle, strong data processing ability and prediction ability, has a significant effect on the processing of a large number of electrical signals generated by TENG, and the combination with TENG sensors will promote the rapid development of intelligent sensor networks in the future. Therefore, this paper is based on the intelligent sound monitoring and recognition system of TENG, which has good sound recognition capability, and aims to evaluate the feasibility of the sound perception module architecture in ubiquitous sensor networks.
<div id='section'>Paperid: <span id='pid'>1179, <a href='https://arxiv.org/pdf/2402.04356.pdf' target='_blank'>https://arxiv.org/pdf/2402.04356.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Canyu Zhang, Youbao Tang, Ning Zhang, Ruei-Sung Lin, Mei Han, Jing Xiao, Song Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.04356">Bidirectional Autoregressive Diffusion Model for Dance Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dance serves as a powerful medium for expressing human emotions, but the lifelike generation of dance is still a considerable challenge. Recently, diffusion models have showcased remarkable generative abilities across various domains. They hold promise for human motion generation due to their adaptable many-to-many nature. Nonetheless, current diffusion-based motion generation models often create entire motion sequences directly and unidirectionally, lacking focus on the motion with local and bidirectional enhancement. When choreographing high-quality dance movements, people need to take into account not only the musical context but also the nearby music-aligned dance motions. To authentically capture human behavior, we propose a Bidirectional Autoregressive Diffusion Model (BADM) for music-to-dance generation, where a bidirectional encoder is built to enforce that the generated dance is harmonious in both the forward and backward directions. To make the generated dance motion smoother, a local information decoder is built for local motion enhancement. The proposed framework is able to generate new motions based on the input conditions and nearby motions, which foresees individual motion slices iteratively and consolidates all predictions. To further refine the synchronicity between the generated dance and the beat, the beat information is incorporated as an input to generate better music-aligned dance movements. Experimental results demonstrate that the proposed model achieves state-of-the-art performance compared to existing unidirectional approaches on the prominent benchmark for music-to-dance generation.
<div id='section'>Paperid: <span id='pid'>1180, <a href='https://arxiv.org/pdf/2402.03559.pdf' target='_blank'>https://arxiv.org/pdf/2402.03559.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jacob K Christopher, Stephen Baek, Ferdinando Fioretto
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.03559">Constrained Synthesis with Projected Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces an approach to endow generative diffusion processes the ability to satisfy and certify compliance with constraints and physical principles. The proposed method recast the traditional sampling process of generative diffusion models as a constrained optimization problem, steering the generated data distribution to remain within a specified region to ensure adherence to the given constraints. These capabilities are validated on applications featuring both convex and challenging, non-convex, constraints as well as ordinary differential equations, in domains spanning from synthesizing new materials with precise morphometric properties, generating physics-informed motion, optimizing paths in planning scenarios, and human motion synthesis.
<div id='section'>Paperid: <span id='pid'>1181, <a href='https://arxiv.org/pdf/2402.02904.pdf' target='_blank'>https://arxiv.org/pdf/2402.02904.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Yu, Zebin Huang, Qingbo Liu, Ignacio Carlucho, Mustafa Suphi Erden
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.02904">Replication of Impedance Identification Experiments on a Reinforcement-Learning-Controlled Digital Twin of Human Elbows</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study presents a pioneering effort to replicate human neuromechanical experiments within a virtual environment utilising a digital human model. By employing MyoSuite, a state-of-the-art human motion simulation platform enhanced by Reinforcement Learning (RL), multiple types of impedance identification experiments of human elbow were replicated on a musculoskeletal model. We compared the elbow movement controlled by an RL agent with the motion of an actual human elbow in terms of the impedance identified in torque-perturbation experiments. The findings reveal that the RL agent exhibits higher elbow impedance to stabilise the target elbow motion under perturbation than a human does, likely due to its shorter reaction time and superior sensory capabilities. This study serves as a preliminary exploration into the potential of virtual environment simulations for neuromechanical research, offering an initial yet promising alternative to conventional experimental approaches. An RL-controlled digital twin with complete musculoskeletal models of the human body is expected to be useful in designing experiments and validating rehabilitation theory before experiments on real human subjects.
<div id='section'>Paperid: <span id='pid'>1182, <a href='https://arxiv.org/pdf/2402.01566.pdf' target='_blank'>https://arxiv.org/pdf/2402.01566.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiawei Wang, Yuchen Zhang, Jiaxin Zou, Yan Zeng, Guoqiang Wei, Liping Yuan, Hang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.01566">Boximator: Generating Rich and Controllable Motions for Video Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating rich and controllable motion is a pivotal challenge in video synthesis. We propose Boximator, a new approach for fine-grained motion control. Boximator introduces two constraint types: hard box and soft box. Users select objects in the conditional frame using hard boxes and then use either type of boxes to roughly or rigorously define the object's position, shape, or motion path in future frames. Boximator functions as a plug-in for existing video diffusion models. Its training process preserves the base model's knowledge by freezing the original weights and training only the control module. To address training challenges, we introduce a novel self-tracking technique that greatly simplifies the learning of box-object correlations. Empirically, Boximator achieves state-of-the-art video quality (FVD) scores, improving on two base models, and further enhanced after incorporating box constraints. Its robust motion controllability is validated by drastic increases in the bounding box alignment metric. Human evaluation also shows that users favor Boximator generation results over the base model.
<div id='section'>Paperid: <span id='pid'>1183, <a href='https://arxiv.org/pdf/2402.01049.pdf' target='_blank'>https://arxiv.org/pdf/2402.01049.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zikang Leng, Amitrajit Bhattacharjee, Hrudhai Rajasekhar, Lizhe Zhang, Elizabeth Bruda, Hyeokhyen Kwon, Thomas PlÃ¶tz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.01049">IMUGPT 2.0: Language-Based Cross Modality Transfer for Sensor-Based Human Activity Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>One of the primary challenges in the field of human activity recognition (HAR) is the lack of large labeled datasets. This hinders the development of robust and generalizable models. Recently, cross modality transfer approaches have been explored that can alleviate the problem of data scarcity. These approaches convert existing datasets from a source modality, such as video, to a target modality (IMU). With the emergence of generative AI models such as large language models (LLMs) and text-driven motion synthesis models, language has become a promising source data modality as well as shown in proof of concepts such as IMUGPT. In this work, we conduct a large-scale evaluation of language-based cross modality transfer to determine their effectiveness for HAR. Based on this study, we introduce two new extensions for IMUGPT that enhance its use for practical HAR application scenarios: a motion filter capable of filtering out irrelevant motion sequences to ensure the relevance of the generated virtual IMU data, and a set of metrics that measure the diversity of the generated data facilitating the determination of when to stop generating virtual IMU data for both effective and efficient processing. We demonstrate that our diversity metrics can reduce the effort needed for the generation of virtual IMU data by at least 50%, which open up IMUGPT for practical use cases beyond a mere proof of concept.
<div id='section'>Paperid: <span id='pid'>1184, <a href='https://arxiv.org/pdf/2402.00663.pdf' target='_blank'>https://arxiv.org/pdf/2402.00663.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Raul Fernandez-Fernandez, Bartek Åukawski, Juan G. Victores, Claudio Pacchierotti
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.00663">Transferring human emotions to robot motions using Neural Policy Style Transfer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Neural Style Transfer (NST) was originally proposed to use feature extraction capabilities of Neural Networks as a way to perform Style Transfer with images. Pre-trained image classification architectures were selected for feature extraction, leading to new images showing the same content as the original but with a different style. In robotics, Style Transfer can be employed to transfer human motion styles to robot motions. The challenge lies in the lack of pre-trained classification architectures for robot motions that could be used for feature extraction. Neural Policy Style Transfer TD3 (NPST3) is proposed for the transfer of human motion styles to robot motions. This framework allows the same robot motion to be executed in different human-centered motion styles, such as in an angry, happy, calm, or sad fashion. The Twin Delayed Deep Deterministic Policy Gradient (TD3) network is introduced for the generation of control policies. An autoencoder network is in charge of feature extraction for the Style Transfer step. The Style Transfer step can be performed both offline and online: offline for the autonomous executions of human-style robot motions, and online for adapting at runtime the style of e.g., a teleoperated robot. The framework is tested using two different robotic platforms: a robotic manipulator designed for telemanipulation tasks, and a humanoid robot designed for social interaction. The proposed approach was evaluated for both platforms, performing a total of 147 questionnaires asking human subjects to recognize the human motion style transferred to the robot motion for a predefined set of actions.
<div id='section'>Paperid: <span id='pid'>1185, <a href='https://arxiv.org/pdf/2401.08559.pdf' target='_blank'>https://arxiv.org/pdf/2401.08559.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mathis Petrovich, Or Litany, Umar Iqbal, Michael J. Black, GÃ¼l Varol, Xue Bin Peng, Davis Rempe
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.08559">Multi-Track Timeline Control for Text-Driven 3D Human Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in generative modeling have led to promising progress on synthesizing 3D human motion from text, with methods that can generate character animations from short prompts and specified durations. However, using a single text prompt as input lacks the fine-grained control needed by animators, such as composing multiple actions and defining precise durations for parts of the motion. To address this, we introduce the new problem of timeline control for text-driven motion synthesis, which provides an intuitive, yet fine-grained, input interface for users. Instead of a single prompt, users can specify a multi-track timeline of multiple prompts organized in temporal intervals that may overlap. This enables specifying the exact timings of each action and composing multiple actions in sequence or at overlapping intervals. To generate composite animations from a multi-track timeline, we propose a new test-time denoising method. This method can be integrated with any pre-trained motion diffusion model to synthesize realistic motions that accurately reflect the timeline. At every step of denoising, our method processes each timeline interval (text prompt) individually, subsequently aggregating the predictions with consideration for the specific body parts engaged in each action. Experimental comparisons and ablations validate that our method produces realistic motions that respect the semantics and timing of given text prompts. Our code and models are publicly available at https://mathis.petrovich.fr/stmc.
<div id='section'>Paperid: <span id='pid'>1186, <a href='https://arxiv.org/pdf/2401.04872.pdf' target='_blank'>https://arxiv.org/pdf/2401.04872.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Liu, Yuexin Zhang, Kunming Li, Yongliang Qiao, Stewart Worrall, You-Fu Li, He Kong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.04872">Knowledge-aware Graph Transformer for Pedestrian Trajectory Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Predicting pedestrian motion trajectories is crucial for path planning and motion control of autonomous vehicles. Accurately forecasting crowd trajectories is challenging due to the uncertain nature of human motions in different environments. For training, recent deep learning-based prediction approaches mainly utilize information like trajectory history and interactions between pedestrians, among others. This can limit the prediction performance across various scenarios since the discrepancies between training datasets have not been properly incorporated. To overcome this limitation, this paper proposes a graph transformer structure to improve prediction performance, capturing the differences between the various sites and scenarios contained in the datasets. In particular, a self-attention mechanism and a domain adaption module have been designed to improve the generalization ability of the model. Moreover, an additional metric considering cross-dataset sequences is introduced for training and performance evaluation purposes. The proposed framework is validated and compared against existing methods using popular public datasets, i.e., ETH and UCY. Experimental results demonstrate the improved performance of our proposed scheme.
<div id='section'>Paperid: <span id='pid'>1187, <a href='https://arxiv.org/pdf/2512.20451.pdf' target='_blank'>https://arxiv.org/pdf/2512.20451.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anh Dao, Manh Tran, Yufei Zhang, Xiaoming Liu, Zijun Cui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.20451">Beyond Motion Pattern: An Empirical Study of Physical Forces for Human Motion Understanding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion understanding has advanced rapidly through vision-based progress in recognition, tracking, and captioning. However, most existing methods overlook physical cues such as joint actuation forces that are fundamental in biomechanics. This gap motivates our study: if and when do physically inferred forces enhance motion understanding? By incorporating forces into established motion understanding pipelines, we systematically evaluate their impact across baseline models on 3 major tasks: gait recognition, action recognition, and fine-grained video captioning. Across 8 benchmarks, incorporating forces yields consistent performance gains; for example, on CASIA-B, Rank-1 gait recognition accuracy improved from 89.52% to 90.39% (+0.87), with larger gain observed under challenging conditions: +2.7% when wearing a coat and +3.0% at the side view. On Gait3D, performance also increases from 46.0% to 47.3% (+1.3). In action recognition, CTR-GCN achieved +2.00% on Penn Action, while high-exertion classes like punching/slapping improved by +6.96%. Even in video captioning, Qwen2.5-VL's ROUGE-L score rose from 0.310 to 0.339 (+0.029), indicating that physics-inferred forces enhance temporal grounding and semantic richness. These results demonstrate that force cues can substantially complement visual and kinematic features under dynamic, occluded, or appearance-varying conditions.
<div id='section'>Paperid: <span id='pid'>1188, <a href='https://arxiv.org/pdf/2512.16019.pdf' target='_blank'>https://arxiv.org/pdf/2512.16019.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiping Zhang, Nathan Tsoi, Mofeed Nagib, Hao-Tien Lewis Chiang, Marynel Vázquez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.16019">Few-Shot Inference of Human Perceptions of Robot Performance in Social Navigation Scenarios</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding how humans evaluate robot behavior during human-robot interactions is crucial for developing socially aware robots that behave according to human expectations. While the traditional approach to capturing these evaluations is to conduct a user study, recent work has proposed utilizing machine learning instead. However, existing data-driven methods require large amounts of labeled data, which limits their use in practice. To address this gap, we propose leveraging the few-shot learning capabilities of Large Language Models (LLMs) to improve how well a robot can predict a user's perception of its performance, and study this idea experimentally in social navigation tasks. To this end, we extend the SEAN TOGETHER dataset with additional real-world human-robot navigation episodes and participant feedback. Using this augmented dataset, we evaluate the ability of several LLMs to predict human perceptions of robot performance from a small number of in-context examples, based on observed spatio-temporal cues of the robot and surrounding human motion. Our results demonstrate that LLMs can match or exceed the performance of traditional supervised learning models while requiring an order of magnitude fewer labeled instances. We further show that prediction performance can improve with more in-context examples, confirming the scalability of our approach. Additionally, we investigate what kind of sensor-based information an LLM relies on to make these inferences by conducting an ablation study on the input features considered for performance prediction. Finally, we explore the novel application of personalized examples for in-context learning, i.e., drawn from the same user being evaluated, finding that they further enhance prediction accuracy. This work paves the path to improving robot behavior in a scalable manner through user-centered feedback.
<div id='section'>Paperid: <span id='pid'>1189, <a href='https://arxiv.org/pdf/2512.12717.pdf' target='_blank'>https://arxiv.org/pdf/2512.12717.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mattia Catellani, Marta Gabbi, Lorenzo Sabattini
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.12717">HMPCC: Human-Aware Model Predictive Coverage Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We address the problem of coordinating a team of robots to cover an unknown environment while ensuring safe operation and avoiding collisions with non-cooperative agents. Traditional coverage strategies often rely on simplified assumptions, such as known or convex environments and static density functions, and struggle to adapt to real-world scenarios, especially when humans are involved. In this work, we propose a human-aware coverage framework based on Model Predictive Control (MPC), namely HMPCC, where human motion predictions are integrated into the planning process. By anticipating human trajectories within the MPC horizon, robots can proactively coordinate their actions %avoid redundant exploration, and adapt to dynamic conditions. The environment is modeled as a Gaussian Mixture Model (GMM), representing regions of interest. Team members operate in a fully decentralized manner, without relying on explicit communication, an essential feature in hostile or communication-limited scenarios. Our results show that human trajectory forecasting enables more efficient and adaptive coverage, improving coordination between human and robotic agents.
<div id='section'>Paperid: <span id='pid'>1190, <a href='https://arxiv.org/pdf/2511.14237.pdf' target='_blank'>https://arxiv.org/pdf/2511.14237.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Juncheng Hu, Zijian Zhang, Zeyu Wang, Guoyu Wang, Yingji Li, Kedi Lyu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.14237">Breaking the Passive Learning Trap: An Active Perception Strategy for Human Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Forecasting 3D human motion is an important embodiment of fine-grained understanding and cognition of human behavior by artificial agents. Current approaches excessively rely on implicit network modeling of spatiotemporal relationships and motion characteristics, falling into the passive learning trap that results in redundant and monotonous 3D coordinate information acquisition while lacking actively guided explicit learning mechanisms. To overcome these issues, we propose an Active Perceptual Strategy (APS) for human motion prediction, leveraging quotient space representations to explicitly encode motion properties while introducing auxiliary learning objectives to strengthen spatio-temporal modeling. Specifically, we first design a data perception module that projects poses into the quotient space, decoupling motion geometry from coordinate redundancy. By jointly encoding tangent vectors and Grassmann projections, this module simultaneously achieves geometric dimension reduction, semantic decoupling, and dynamic constraint enforcement for effective motion pose characterization. Furthermore, we introduce a network perception module that actively learns spatio-temporal dependencies through restorative learning. This module deliberately masks specific joints or injects noise to construct auxiliary supervision signals. A dedicated auxiliary learning network is designed to actively adapt and learn from perturbed information. Notably, APS is model agnostic and can be integrated with different prediction models to enhance active perceptual. The experimental results demonstrate that our method achieves the new state-of-the-art, outperforming existing methods by large margins: 16.3% on H3.6M, 13.9% on CMU Mocap, and 10.1% on 3DPW.
<div id='section'>Paperid: <span id='pid'>1191, <a href='https://arxiv.org/pdf/2511.08912.pdf' target='_blank'>https://arxiv.org/pdf/2511.08912.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinyu Zhang, Lijun Han, Feng Jian, Lingxi Zhang, Hesheng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.08912">A Shared Control Framework for Mobile Robots with Planning-Level Intention Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In mobile robot shared control, effectively understanding human motion intention is critical for seamless human-robot collaboration. This paper presents a novel shared control framework featuring planning-level intention prediction. A path replanning algorithm is designed to adjust the robot's desired trajectory according to inferred human intentions. To represent future motion intentions, we introduce the concept of an intention domain, which serves as a constraint for path replanning. The intention-domain prediction and path replanning problems are jointly formulated as a Markov Decision Process and solved through deep reinforcement learning. In addition, a Voronoi-based human trajectory generation algorithm is developed, allowing the model to be trained entirely in simulation without human participation or demonstration data. Extensive simulations and real-world user studies demonstrate that the proposed method significantly reduces operator workload and enhances safety, without compromising task efficiency compared with existing assistive teleoperation approaches.
<div id='section'>Paperid: <span id='pid'>1192, <a href='https://arxiv.org/pdf/2510.18267.pdf' target='_blank'>https://arxiv.org/pdf/2510.18267.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiang Zhang, Suping Wu, Sheng Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.18267">Latent-Info and Low-Dimensional Learning for Human Mesh Recovery and Parallel Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing 3D human mesh recovery methods often fail to fully exploit the latent information (e.g., human motion, shape alignment), leading to issues with limb misalignment and insufficient local details in the reconstructed human mesh (especially in complex scenes). Furthermore, the performance improvement gained by modelling mesh vertices and pose node interactions using attention mechanisms comes at a high computational cost. To address these issues, we propose a two-stage network for human mesh recovery based on latent information and low dimensional learning. Specifically, the first stage of the network fully excavates global (e.g., the overall shape alignment) and local (e.g., textures, detail) information from the low and high-frequency components of image features and aggregates this information into a hybrid latent frequency domain feature. This strategy effectively extracts latent information. Subsequently, utilizing extracted hybrid latent frequency domain features collaborates to enhance 2D poses to 3D learning. In the second stage, with the assistance of hybrid latent features, we model the interaction learning between the rough 3D human mesh template and the 3D pose, optimizing the pose and shape of the human mesh. Unlike existing mesh pose interaction methods, we design a low-dimensional mesh pose interaction method through dimensionality reduction and parallel optimization that significantly reduces computational costs without sacrificing reconstruction accuracy. Extensive experimental results on large publicly available datasets indicate superiority compared to the most state-of-the-art.
<div id='section'>Paperid: <span id='pid'>1193, <a href='https://arxiv.org/pdf/2510.09445.pdf' target='_blank'>https://arxiv.org/pdf/2510.09445.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ashkan Sebghati, S. Hassan HosseinNia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.09445">Robust reset control design for piezo-actuated nano-positioner in presence of hysteresis nonlinearity</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, a robust nonlinear control scheme is designed for the motion control of a class of piezo-actuated nano-positioning systems using frequency-domain analysis. The hysteresis, the nonlinearity in the piezoelectric material, degrades the precision in tracking references with high frequency contents and different travel ranges. The hysteresis compensation by the inverse model, as the state-of-the-art solution, is not reliable alone. Therefore, a control framework with robustness against the remaining nonlinearity is needed. It is shown that there is an unavoidable limitation in robust linear control design to improve the performance. A robust control methodology based on a complex-order element is established to relax the limitation. Then, a constant-in-gain-lead-in-phase (CgLp) reset controller is utilized to realize the complex-order control. The control design is based on the sinusoidal input describing function (SIDF) and the higher-order SIDF (HOSIDF) tools. A constrained optimization problem is provided to tune the control parameters. The achieved improvements by the CgLp control is validated by the simulation.
<div id='section'>Paperid: <span id='pid'>1194, <a href='https://arxiv.org/pdf/2510.05957.pdf' target='_blank'>https://arxiv.org/pdf/2510.05957.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vaughn Gzenda, Robin Chhabra
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.05957">Learning to Crawl: Latent Model-Based Reinforcement Learning for Soft Robotic Adaptive Locomotion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Soft robotic crawlers are mobile robots that utilize soft body deformability and compliance to achieve locomotion through surface contact. Designing control strategies for such systems is challenging due to model inaccuracies, sensor noise, and the need to discover locomotor gaits. In this work, we present a model-based reinforcement learning (MB-RL) framework in which latent dynamics inferred from onboard sensors serve as a predictive model that guides an actor-critic algorithm to optimize locomotor policies. We evaluate the framework on a minimal crawler model in simulation using inertial measurement units and time-of-flight sensors as observations. The learned latent dynamics enable short-horizon motion prediction while the actor-critic discovers effective locomotor policies. This approach highlights the potential of latent-dynamics MB-RL for enabling embodied soft robotic adaptive locomotion based solely on noisy sensor feedback.
<div id='section'>Paperid: <span id='pid'>1195, <a href='https://arxiv.org/pdf/2510.03496.pdf' target='_blank'>https://arxiv.org/pdf/2510.03496.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vadivelan Murugesan, Rajasundaram Mathiazhagan, Sanjana Joshi, Aliasghar Arab
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03496">Digital-Twin Evaluation for Proactive Human-Robot Collision Avoidance via Prediction-Guided A-RRT*</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human-robot collaboration requires precise prediction of human motion over extended horizons to enable proactive collision avoidance. Unlike existing planners that rely solely on kinodynamic models, we present a prediction-driven safe planning framework that leverages granular, joint-by-joint human motion forecasting validated in a physics-based digital twin. A capsule-based artificial potential field (APF) converts these granular predictions into collision risk metrics, triggering an Adaptive RRT* (A-RRT*) planner when thresholds are exceeded. The depth camera is used to extract 3D skeletal poses and a convolutional neural network-bidirectional long short-term memory (CNN-BiLSTM) model to predict individual joint trajectories ahead of time. A digital twin model integrates real-time human posture prediction placed in front of a simulated robot to evaluate motions and physical contacts. The proposed method enables validation of planned trajectories ahead of time and bridging potential latency gaps in updating planned trajectories in real-time. In 50 trials, our method achieved 100% proactive avoidance with > 250 mm clearance and sub-2 s replanning, demonstrating superior precision and reliability compared to existing kinematic-only planners through the integration of predictive human modeling with digital twin validation.
<div id='section'>Paperid: <span id='pid'>1196, <a href='https://arxiv.org/pdf/2509.20927.pdf' target='_blank'>https://arxiv.org/pdf/2509.20927.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Akihisa Watanabe, Jiawei Ren, Li Siyao, Yichen Peng, Erwin Wu, Edgar Simo-Serra
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20927">SimDiff: Simulator-constrained Diffusion Model for Physically Plausible Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating physically plausible human motion is crucial for applications such as character animation and virtual reality. Existing approaches often incorporate a simulator-based motion projection layer to the diffusion process to enforce physical plausibility. However, such methods are computationally expensive due to the sequential nature of the simulator, which prevents parallelization. We show that simulator-based motion projection can be interpreted as a form of guidance, either classifier-based or classifier-free, within the diffusion process. Building on this insight, we propose SimDiff, a Simulator-constrained Diffusion Model that integrates environment parameters (e.g., gravity, wind) directly into the denoising process. By conditioning on these parameters, SimDiff generates physically plausible motions efficiently, without repeated simulator calls at inference, and also provides fine-grained control over different physical coefficients. Moreover, SimDiff successfully generalizes to unseen combinations of environmental parameters, demonstrating compositional generalization.
<div id='section'>Paperid: <span id='pid'>1197, <a href='https://arxiv.org/pdf/2509.07990.pdf' target='_blank'>https://arxiv.org/pdf/2509.07990.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Charan Gajjala Chenchu, Kinam Kim, Gao Lu, Zia Ud Din
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.07990">Signals vs. Videos: Advancing Motion Intention Recognition for Human-Robot Collaboration in Construction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human-robot collaboration (HRC) in the construction industry depends on precise and prompt recognition of human motion intentions and actions by robots to maximize safety and workflow efficiency. There is a research gap in comparing data modalities, specifically signals and videos, for motion intention recognition. To address this, the study leverages deep learning to assess two different modalities in recognizing workers' motion intention at the early stage of movement in drywall installation tasks. The Convolutional Neural Network - Long Short-Term Memory (CNN-LSTM) model utilizing surface electromyography (sEMG) data achieved an accuracy of around 87% with an average time of 0.04 seconds to perform prediction on a sample input. Meanwhile, the pre-trained Video Swin Transformer combined with transfer learning harnessed video sequences as input to recognize motion intention and attained an accuracy of 94% but with a longer average time of 0.15 seconds for a similar prediction. This study emphasizes the unique strengths and trade-offs of both data formats, directing their systematic deployments to enhance HRC in real-world construction projects.
<div id='section'>Paperid: <span id='pid'>1198, <a href='https://arxiv.org/pdf/2509.04984.pdf' target='_blank'>https://arxiv.org/pdf/2509.04984.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Koji Matsuno, Chien Chern Cheah
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.04984">Lyapunov-Based Deep Learning Control for Robots with Unknown Jacobian</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning, with its exceptional learning capabilities and flexibility, has been widely applied in various applications. However, its black-box nature poses a significant challenge in real-time robotic applications, particularly in robot control, where trustworthiness and robustness are critical in ensuring safety. In robot motion control, it is essential to analyze and ensure system stability, necessitating the establishment of methodologies that address this need. This paper aims to develop a theoretical framework for end-to-end deep learning control that can be integrated into existing robot control theories. The proposed control algorithm leverages a modular learning approach to update the weights of all layers in real time, ensuring system stability based on Lyapunov-like analysis. Experimental results on industrial robots are presented to illustrate the performance of the proposed deep learning controller. The proposed method offers an effective solution to the black-box problem in deep learning, demonstrating the possibility of deploying real-time deep learning strategies for robot kinematic control in a stable manner. This achievement provides a critical foundation for future advancements in deep learning based real-time robotic applications.
<div id='section'>Paperid: <span id='pid'>1199, <a href='https://arxiv.org/pdf/2509.03238.pdf' target='_blank'>https://arxiv.org/pdf/2509.03238.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Martin Goubej, Lauria Clarke, Martin HrabaÄka, David Tolar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03238">Vibration Damping in Underactuated Cable-suspended Artwork -- Flying Belt Motion Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a comprehensive refurbishment of the interactive robotic art installation Standards and Double Standards by Rafael Lozano-Hemmer. The installation features an array of belts suspended from the ceiling, each actuated by stepper motors and dynamically oriented by a vision-based tracking system that follows the movements of exhibition visitors. The original system was limited by oscillatory dynamics, resulting in torsional and pendulum-like vibrations that constrained rotational speed and reduced interactive responsiveness. To address these challenges, the refurbishment involved significant upgrades to both hardware and motion control algorithms. A detailed mathematical model of the flying belt system was developed to accurately capture its dynamic behavior, providing a foundation for advanced control design. An input shaping method, formulated as a convex optimization problem, was implemented to effectively suppress vibrations, enabling smoother and faster belt movements. Experimental results demonstrate substantial improvements in system performance and audience interaction. This work exemplifies the integration of robotics, control engineering, and interactive art, offering new solutions to technical challenges in real-time motion control and vibration damping for large-scale kinetic installations.
<div id='section'>Paperid: <span id='pid'>1200, <a href='https://arxiv.org/pdf/2508.19731.pdf' target='_blank'>https://arxiv.org/pdf/2508.19731.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maryam Kazemi Eskeri, Ville Kyrki, Dominik Baumann, Tomasz Piotr Kucner
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19731">Efficient Human-Aware Task Allocation for Multi-Robot Systems in Shared Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-robot systems are increasingly deployed in applications, such as intralogistics or autonomous delivery, where multiple robots collaborate to complete tasks efficiently. One of the key factors enabling their efficient cooperation is Multi-Robot Task Allocation (MRTA). Algorithms solving this problem optimize task distribution among robots to minimize the overall execution time. In shared environments, apart from the relative distance between the robots and the tasks, the execution time is also significantly impacted by the delay caused by navigating around moving people. However, most existing MRTA approaches are dynamics-agnostic, relying on static maps and neglecting human motion patterns, leading to inefficiencies and delays. In this paper, we introduce \acrfull{method name}. This method leverages Maps of Dynamics (MoDs), spatio-temporal queryable models designed to capture historical human movement patterns, to estimate the impact of humans on the task execution time during deployment. \acrshort{method name} utilizes a stochastic cost function that includes MoDs. Experimental results show that integrating MoDs enhances task allocation performance, resulting in reduced mission completion times by up to $26\%$ compared to the dynamics-agnostic method and up to $19\%$ compared to the baseline. This work underscores the importance of considering human dynamics in MRTA within shared environments and presents an efficient framework for deploying multi-robot systems in environments populated by humans.
<div id='section'>Paperid: <span id='pid'>1201, <a href='https://arxiv.org/pdf/2508.19595.pdf' target='_blank'>https://arxiv.org/pdf/2508.19595.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maryam Kazemi Eskeri, Thomas Wiedemann, Ville Kyrki, Dominik Baumann, Tomasz Piotr Kucner
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19595">A Lightweight Crowd Model for Robot Social Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robots operating in human-populated environments must navigate safely and efficiently while minimizing social disruption. Achieving this requires estimating crowd movement to avoid congested areas in real-time. Traditional microscopic models struggle to scale in dense crowds due to high computational cost, while existing macroscopic crowd prediction models tend to be either overly simplistic or computationally intensive. In this work, we propose a lightweight, real-time macroscopic crowd prediction model tailored for human motion, which balances prediction accuracy and computational efficiency. Our approach simplifies both spatial and temporal processing based on the inherent characteristics of pedestrian flow, enabling robust generalization without the overhead of complex architectures. We demonstrate a 3.6 times reduction in inference time, while improving prediction accuracy by 3.1 %. Integrated into a socially aware planning framework, the model enables efficient and socially compliant robot navigation in dynamic environments. This work highlights that efficient human crowd modeling enables robots to navigate dense environments without costly computations.
<div id='section'>Paperid: <span id='pid'>1202, <a href='https://arxiv.org/pdf/2507.17445.pdf' target='_blank'>https://arxiv.org/pdf/2507.17445.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haichuan Li, Changda Tian, Panos Trahanias, Tomi Westerlund
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.17445">IndoorBEV: Joint Detection and Footprint Completion of Objects via Mask-based Prediction in Indoor Scenarios for Bird's-Eye View Perception</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Detecting diverse objects within complex indoor 3D point clouds presents significant challenges for robotic perception, particularly with varied object shapes, clutter, and the co-existence of static and dynamic elements where traditional bounding box methods falter. To address these limitations, we propose IndoorBEV, a novel mask-based Bird's-Eye View (BEV) method for indoor mobile robots.
  In a BEV method, a 3D scene is projected into a 2D BEV grid which handles naturally occlusions and provides a consistent top-down view aiding to distinguish static obstacles from dynamic agents. The obtained 2D BEV results is directly usable to downstream robotic tasks like navigation, motion prediction, and planning. Our architecture utilizes an axis compact encoder and a window-based backbone to extract rich spatial features from this BEV map. A query-based decoder head then employs learned object queries to concurrently predict object classes and instance masks in the BEV space. This mask-centric formulation effectively captures the footprint of both static and dynamic objects regardless of their shape, offering a robust alternative to bounding box regression. We demonstrate the effectiveness of IndoorBEV on a custom indoor dataset featuring diverse object classes including static objects
  and dynamic elements like robots and miscellaneous items, showcasing its potential for robust indoor scene understanding.
<div id='section'>Paperid: <span id='pid'>1203, <a href='https://arxiv.org/pdf/2507.07805.pdf' target='_blank'>https://arxiv.org/pdf/2507.07805.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kim P. Wabersich, Felix Berkel, Felix Gruber, Sven Reimann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07805">Set-Based Control Barrier Functions and Safety Filters</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>High performance and formal safety guarantees are common requirements for industrial control applications. Control barrier function (CBF) methods provide a systematic approach to the modularization of safety and performance. However, the design of such CBFs can be challenging, which limits their applicability to large-scale or data-driven systems. This paper introduces the concept of a set-based CBF for linear systems with convex constraints. By leveraging control invariant sets from reachability analysis and predictive control, the set-based CBF is defined implicitly through the minimal scaling of such a set to contain the current system state. This approach enables the development of implicit, data-driven, and high-dimensional CBF representations. The paper demonstrates the design of a safety filter using set-based CBFs, which is suitable for real-time implementations and learning-based approximations to reduce online computational demands. The effectiveness of the method is illustrated through comprehensive simulations on a high-dimensional mass-spring-damper system and a motion control task, and it is validated experimentally using an electric drive application with short sampling times, highlighting its practical benefits for safety-critical control.
<div id='section'>Paperid: <span id='pid'>1204, <a href='https://arxiv.org/pdf/2506.10240.pdf' target='_blank'>https://arxiv.org/pdf/2506.10240.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rongfei Li, Francis Assadian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.10240">Innovative Adaptive Imaged Based Visual Servoing Control of 6 DoFs Industrial Robot Manipulators</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image-based visual servoing (IBVS) methods have been well developed and used in many applications, especially in pose (position and orientation) alignment. However, most research papers focused on developing control solutions when 3D point features can be detected inside the field of view. This work proposes an innovative feedforward-feedback adaptive control algorithm structure with the Youla Parameterization method. A designed feature estimation loop ensures stable and fast motion control when point features are outside the field of view. As 3D point features move inside the field of view, the IBVS feedback loop preserves the precision of the pose at the end of the control period. Also, an adaptive controller is developed in the feedback loop to stabilize the system in the entire range of operations. The nonlinear camera and robot manipulator model is linearized and decoupled online by an adaptive algorithm. The adaptive controller is then computed based on the linearized model evaluated at current linearized point. The proposed solution is robust and easy to implement in different industrial robotic systems. Various scenarios are used in simulations to validate the effectiveness and robust performance of the proposed controller.
<div id='section'>Paperid: <span id='pid'>1205, <a href='https://arxiv.org/pdf/2505.20920.pdf' target='_blank'>https://arxiv.org/pdf/2505.20920.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qihang Fang, Chengcheng Tang, Bugra Tekin, Shugao Ma, Yanchao Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.20920">HuMoCon: Concept Discovery for Human Motion Understanding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present HuMoCon, a novel motion-video understanding framework designed for advanced human behavior analysis. The core of our method is a human motion concept discovery framework that efficiently trains multi-modal encoders to extract semantically meaningful and generalizable features. HuMoCon addresses key challenges in motion concept discovery for understanding and reasoning, including the lack of explicit multi-modality feature alignment and the loss of high-frequency information in masked autoencoding frameworks. Our approach integrates a feature alignment strategy that leverages video for contextual understanding and motion for fine-grained interaction modeling, further with a velocity reconstruction mechanism to enhance high-frequency feature expression and mitigate temporal over-smoothing. Comprehensive experiments on standard benchmarks demonstrate that HuMoCon enables effective motion concept discovery and significantly outperforms state-of-the-art methods in training large models for human motion understanding. We will open-source the associated code with our paper.
<div id='section'>Paperid: <span id='pid'>1206, <a href='https://arxiv.org/pdf/2505.13054.pdf' target='_blank'>https://arxiv.org/pdf/2505.13054.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Max Grobbel, Daniel FlÃ¶gel, Philipp Rigoll, SÃ¶ren Hohmann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.13054">Disentangling Coordiante Frames for Task Specific Motion Retargeting in Teleoperation using Shared Control and VR Controllers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Task performance in terms of task completion time in teleoperation is still far behind compared to humans conducting tasks directly. One large identified impact on this is the human capability to perform transformations and alignments, which is directly influenced by the point of view and the motion retargeting strategy. In modern teleoperation systems, motion retargeting is usually implemented through a one time calibration or switching modes. Complex tasks, like concatenated screwing, might be difficult, because the operator has to align (e.g. mirror) rotational and translational input commands. Recent research has shown, that the separation of translation and rotation leads to increased task performance. This work proposes a formal motion retargeting method, which separates translational and rotational input commands. This method is then included in a optimal control based trajectory planner and shown to work on a UR5e manipulator.
<div id='section'>Paperid: <span id='pid'>1207, <a href='https://arxiv.org/pdf/2505.01752.pdf' target='_blank'>https://arxiv.org/pdf/2505.01752.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Longze Zheng, Qinghe Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.01752">NMPCB: A Lightweight and Safety-Critical Motion Control Framework for Ackermann Mobile Robot</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In multi-obstacle environments, real-time performance and safety in robot motion control have long been challenging issues, as conventional methods often struggle to balance the two. In this paper, we propose a novel motion control framework composed of a Neural network-based path planner and a Model Predictive Control (MPC) controller based on control Barrier function (NMPCB) . The planner predicts the next target point through a lightweight neural network and generates a reference trajectory for the controller. In the design of the controller, we introduce the dual problem of control barrier function (CBF) as the obstacle avoidance constraint, enabling it to ensure robot motion safety while significantly reducing computation time. The controller directly outputs control commands to the robot by tracking the reference trajectory. This framework achieves a balance between real-time performance and safety. We validate the feasibility of the framework through numerical simulations and real-world experiments.
<div id='section'>Paperid: <span id='pid'>1208, <a href='https://arxiv.org/pdf/2503.16801.pdf' target='_blank'>https://arxiv.org/pdf/2503.16801.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zichen Geng, Zeeshan Hayder, Wei Liu, Ajmal Saeed Mian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.16801">Auto-Regressive Diffusion for Generating 3D Human-Object Interactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-driven Human-Object Interaction (Text-to-HOI) generation is an emerging field with applications in animation, video games, virtual reality, and robotics. A key challenge in HOI generation is maintaining interaction consistency in long sequences. Existing Text-to-Motion-based approaches, such as discrete motion tokenization, cannot be directly applied to HOI generation due to limited data in this domain and the complexity of the modality. To address the problem of interaction consistency in long sequences, we propose an autoregressive diffusion model (ARDHOI) that predicts the next continuous token. Specifically, we introduce a Contrastive Variational Autoencoder (cVAE) to learn a physically plausible space of continuous HOI tokens, thereby ensuring that generated human-object motions are realistic and natural. For generating sequences autoregressively, we develop a Mamba-based context encoder to capture and maintain consistent sequential actions. Additionally, we implement an MLP-based denoiser to generate the subsequent token conditioned on the encoded context. Our model has been evaluated on the OMOMO and BEHAVE datasets, where it outperforms existing state-of-the-art methods in terms of both performance and inference speed. This makes ARDHOI a robust and efficient solution for text-driven HOI tasks
<div id='section'>Paperid: <span id='pid'>1209, <a href='https://arxiv.org/pdf/2503.15225.pdf' target='_blank'>https://arxiv.org/pdf/2503.15225.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Angelo Di Porzio, Marco Coraggio
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.15225">A Personalized Data-Driven Generative Model of Human Motion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The deployment of autonomous virtual avatars (in extended reality) and robots in human group activities - such as rehabilitation therapy, sports, and manufacturing - is expected to increase as these technologies become more pervasive. Designing cognitive architectures and control strategies to drive these agents requires realistic models of human motion. However, existing models only provide simplified descriptions of human motor behavior. In this work, we propose a fully data-driven approach, based on Long Short-Term Memory neural networks, to generate original motion that captures the unique characteristics of specific individuals. We validate the architecture using real data of scalar oscillatory motion. Extensive analyses show that our model effectively replicates the velocity distribution and amplitude envelopes of the individual it was trained on, remaining different from other individuals, and outperforming state-of-the-art models in terms of similarity to human data.
<div id='section'>Paperid: <span id='pid'>1210, <a href='https://arxiv.org/pdf/2503.01857.pdf' target='_blank'>https://arxiv.org/pdf/2503.01857.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi Yang, Xiao Li, Xuchen Wang, Mei Liu, Junwei Yin, Weibing Li, Richard M. Voyles, Xin Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.01857">A strictly predefined-time convergent and anti-noise fractional-order zeroing neural network for solving time-variant quadratic programming in kinematic robot control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper proposes a strictly predefined-time convergent and anti-noise fractional-order zeroing neural network (SPTC-AN-FOZNN) model, meticulously designed for addressing time-variant quadratic programming (TVQP) problems. This model marks the first variable-gain ZNN to collectively manifest strictly predefined-time convergence and noise resilience, specifically tailored for kinematic motion control of robots. The SPTC-AN-FOZNN advances traditional ZNNs by incorporating a conformable fractional derivative in accordance with the Leibniz rule, a compliance not commonly achieved by other fractional derivative definitions. It also features a novel activation function designed to ensure favorable convergence independent of the model's order. When compared to five recently published recurrent neural networks (RNNs), the SPTC-AN-FOZNN, configured with $0<Î±\leq 1$, exhibits superior positional accuracy and robustness against additive noises for TVQP applications. Extensive empirical evaluations, including simulations with two types of robotic manipulators and experiments with a Flexiv Rizon robot, have validated the SPTC-AN-FOZNN's effectiveness in precise tracking and computational efficiency, establishing its utility for robust kinematic control.
<div id='section'>Paperid: <span id='pid'>1211, <a href='https://arxiv.org/pdf/2502.10585.pdf' target='_blank'>https://arxiv.org/pdf/2502.10585.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anshul Nayak, Azim Eskandarian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.10585">Prediction uncertainty-aware planning using deep ensembles and trajectory optimisation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion is stochastic and ensuring safe robot navigation in a pedestrian-rich environment requires proactive decision-making. Past research relied on incorporating deterministic future states of surrounding pedestrians which can be overconfident leading to unsafe robot behaviour. The current paper proposes a predictive uncertainty-aware planner that integrates neural network based probabilistic trajectory prediction into planning. Our method uses a deep ensemble based network for probabilistic forecasting of surrounding humans and integrates the predictive uncertainty as constraints into the planner. We compare numerous constraint satisfaction methods on the planner and evaluated its performance on real world pedestrian datasets. Further, offline robot navigation was carried out on out-of-distribution pedestrian trajectories inside a narrow corridor
<div id='section'>Paperid: <span id='pid'>1212, <a href='https://arxiv.org/pdf/2412.10458.pdf' target='_blank'>https://arxiv.org/pdf/2412.10458.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiayi Zhao, Dongdong Weng, Qiuxin Du, Zeyu Tian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.10458">Motion Generation Review: Exploring Deep Learning for Lifelike Animation with Manifold</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion generation involves creating natural sequences of human body poses, widely used in gaming, virtual reality, and human-computer interaction. It aims to produce lifelike virtual characters with realistic movements, enhancing virtual agents and immersive experiences. While previous work has focused on motion generation based on signals like movement, music, text, or scene background, the complexity of human motion and its relationships with these signals often results in unsatisfactory outputs. Manifold learning offers a solution by reducing data dimensionality and capturing subspaces of effective motion. In this review, we present a comprehensive overview of manifold applications in human motion generation, one of the first in this domain. We explore methods for extracting manifolds from unstructured data, their application in motion generation, and discuss their advantages and future directions. This survey aims to provide a broad perspective on the field and stimulate new approaches to ongoing challenges.
<div id='section'>Paperid: <span id='pid'>1213, <a href='https://arxiv.org/pdf/2412.01930.pdf' target='_blank'>https://arxiv.org/pdf/2412.01930.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anirudh S Chakravarthy, Shuai Kyle Zheng, Xin Huang, Sachithra Hemachandra, Xiao Zhang, Yuning Chai, Zhao Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.01930">PROFIT: A Specialized Optimizer for Deep Fine Tuning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fine-tuning pre-trained models has become invaluable in computer vision and robotics. Recent fine-tuning approaches focus on improving efficiency rather than accuracy by using a mixture of smaller learning rates or frozen backbones. To return the spotlight to model accuracy, we present PROFIT (Proximally Restricted Optimizer For Iterative Training), one of the first optimizers specifically designed for incrementally fine-tuning converged models on new tasks or datasets. Unlike traditional optimizers such as SGD or Adam, which make minimal assumptions due to random initialization, PROFIT leverages the structure of a converged model to regularize the optimization process, leading to improved results. By employing a simple temporal gradient orthogonalization process, PROFIT outperforms traditional fine-tuning methods across various tasks: image classification, representation learning, and large-scale motion prediction. Moreover, PROFIT is encapsulated within the optimizer logic, making it easily integrated into any training pipeline with minimal engineering effort. A new class of fine-tuning optimizers like PROFIT can drive advancements as fine-tuning and incremental training become increasingly prevalent, reducing reliance on costly model training from scratch.
<div id='section'>Paperid: <span id='pid'>1214, <a href='https://arxiv.org/pdf/2412.00526.pdf' target='_blank'>https://arxiv.org/pdf/2412.00526.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Matyas Bohacek, Hany Farid
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.00526">Human Action CLIPs: Detecting AI-generated Human Motion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>AI-generated video generation continues its journey through the uncanny valley to produce content that is increasingly perceptually indistinguishable from reality. To better protect individuals, organizations, and societies from its malicious applications, we describe an effective and robust technique for distinguishing real from AI-generated human motion using multi-modal semantic embeddings. Our method is robust to the types of laundering that typically confound more low- to mid-level approaches, including resolution and compression attacks. This method is evaluated against DeepAction, a custom-built, open-sourced dataset of video clips with human actions generated by seven text-to-video AI models and matching real footage. The dataset is available under an academic license at https://www.huggingface.co/datasets/faridlab/deepaction_v1.
<div id='section'>Paperid: <span id='pid'>1215, <a href='https://arxiv.org/pdf/2411.10582.pdf' target='_blank'>https://arxiv.org/pdf/2411.10582.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jaewoo Heo, Kuan-Chieh Wang, Karen Liu, Serena Yeung-Levy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.10582">Motion Diffusion-Guided 3D Global HMR from a Dynamic Camera</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion capture technologies have transformed numerous fields, from the film and gaming industries to sports science and healthcare, by providing a tool to capture and analyze human movement in great detail. The holy grail in the topic of monocular global human mesh and motion reconstruction (GHMR) is to achieve accuracy on par with traditional multi-view capture on any monocular videos captured with a dynamic camera, in-the-wild. This is a challenging task as the monocular input has inherent depth ambiguity, and the moving camera adds additional complexity as the rendered human motion is now a product of both human and camera movement. Not accounting for this confusion, existing GHMR methods often output motions that are unrealistic, e.g. unaccounted root translation of the human causes foot sliding. We present DiffOpt, a novel 3D global HMR method using Diffusion Optimization. Our key insight is that recent advances in human motion generation, such as the motion diffusion model (MDM), contain a strong prior of coherent human motion. The core of our method is to optimize the initial motion reconstruction using the MDM prior. This step can lead to more globally coherent human motion. Our optimization jointly optimizes the motion prior loss and reprojection loss to correctly disentangle the human and camera motions. We validate DiffOpt with video sequences from the Electromagnetic Database of Global 3D Human Pose and Shape in the Wild (EMDB) and Egobody, and demonstrate superior global human motion recovery capability over other state-of-the-art global HMR methods most prominently in long video settings.
<div id='section'>Paperid: <span id='pid'>1216, <a href='https://arxiv.org/pdf/2411.08409.pdf' target='_blank'>https://arxiv.org/pdf/2411.08409.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Franz Franco Gallo, Hui-Yin Wu, Lucile Sassatelli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.08409">DiVR: incorporating context from diverse VR scenes for human trajectory prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Virtual environments provide a rich and controlled setting for collecting detailed data on human behavior, offering unique opportunities for predicting human trajectories in dynamic scenes. However, most existing approaches have overlooked the potential of these environments, focusing instead on static contexts without considering userspecific factors. Employing the CREATTIVE3D dataset, our work models trajectories recorded in virtual reality (VR) scenes for diverse situations including road-crossing tasks with user interactions and simulated visual impairments. We propose Diverse Context VR Human Motion Prediction (DiVR), a cross-modal transformer based on the Perceiver architecture that integrates both static and dynamic scene context using a heterogeneous graph convolution network. We conduct extensive experiments comparing DiVR against existing architectures including MLP, LSTM, and transformers with gaze and point cloud context. Additionally, we also stress test our model's generalizability across different users, tasks, and scenes. Results show that DiVR achieves higher accuracy and adaptability compared to other models and to static graphs. This work highlights the advantages of using VR datasets for context-aware human trajectory modeling, with potential applications in enhancing user experiences in the metaverse. Our source code is publicly available at https://gitlab.inria.fr/ffrancog/creattive3d-divr-model.
<div id='section'>Paperid: <span id='pid'>1217, <a href='https://arxiv.org/pdf/2411.06459.pdf' target='_blank'>https://arxiv.org/pdf/2411.06459.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nian Liu, Libin Liu, Zilong Zhang, Zi Wang, Hongzhao Xie, Tengyu Liu, Xinyi Tong, Yaodong Yang, Zhaofeng He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.06459">Learning Uniformly Distributed Embedding Clusters of Stylistic Skills for Physically Simulated Characters</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning natural and diverse behaviors from human motion datasets remains challenging in physics-based character control. Existing conditional adversarial models often suffer from tight and biased embedding distributions where embeddings from the same motion are closely grouped in a small area and shorter motions occupy even less space. Our empirical observations indicate this limits the representational capacity and diversity under each skill. An ideal latent space should be maximally packed by all motion's embedding clusters. In this paper, we propose a skill-conditioned controller that learns diverse skills with expressive variations. Our approach leverages the Neural Collapse phenomenon, a natural outcome of the classification-based encoder, to uniformly distributed cluster centers. We additionally propose a novel Embedding Expansion technique to form stylistic embedding clusters for diverse skills that are uniformly distributed on a hypersphere, maximizing the representational area occupied by each skill and minimizing unmapped regions. This maximally packed and uniformly distributed embedding space ensures that embeddings within the same cluster generate behaviors conforming to the characteristics of the corresponding motion clips, yet exhibiting noticeable variations within each cluster. Compared to existing methods, our controller not only generates high-quality, diverse motions covering the entire dataset but also achieves superior controllability, motion coverage, and diversity under each skill. Both qualitative and quantitative results confirm these traits, enabling our controller to be applied to a wide range of downstream tasks and serving as a cornerstone for diverse applications.
<div id='section'>Paperid: <span id='pid'>1218, <a href='https://arxiv.org/pdf/2410.13847.pdf' target='_blank'>https://arxiv.org/pdf/2410.13847.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ariel Slepyan, Dian Li, Hongjun Cai, Ryan McGovern, Aidan Aug, Sriramana Sankar, Trac Tran, Nitish Thakor
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.13847">Adaptive Compressive Tactile Subsampling: Enabling High Spatiotemporal Resolution in Scalable Robotic Skin</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robots require full-body, high-resolution tactile sensing to operate safely in unstructured environments, enabling reflexive responses and closed-loop control. However, the pixel counts needed for dense, large-area coverage limit readout rates of most tactile arrays to <100 Hz, hindering their use in high-speed tasks. We present Adaptive Compressive Tactile Subsampling (ACTS), a scalable and data-driven method that greatly enhances traditional tactile matrices by leveraging adaptive sensor sampling and sparse recovery. By adaptively allocating measurements to informative regions, ACTS is especially effective for spatially sparse signals common in real-world interactions. Tested on a 1024-pixel tactile sensor array (32x32), ACTS achieved frame rates up to 1,000 Hz, an 18X improvement over conventional raster scanning, with minimal reconstruction error. For the first time, ACTS enables wearable, large-area, high-density tactile sensing systems that can deliver high-speed results. We demonstrate rapid object classification within 20 ms of contact, high-speed projectile detection, ricochet angle estimation, and soft deformation tracking, in tactile and robotics applications, all using flexible, high-density tactile arrays. These include high-resolution tactile gloves, pressure insoles, and full-body configurations covering robotic arms and human-sized mannequins. We further showcase tactile-based closed-loop control by guiding a metallic ball to trace letters using tactile feedback and by executing tactile-only whole-hand reflexes on a fully sensorized LEAP hand to stabilize grasps, prevent slip, and avoid sharp objects, validating ACTS for real-time interaction and motion control. ACTS transforms standard, low-cost, and robust tactile sensors into high-speed systems enabling scalable, responsive, and adaptive tactile perception for robots and wearables operating in dynamic environments.
<div id='section'>Paperid: <span id='pid'>1219, <a href='https://arxiv.org/pdf/2410.09505.pdf' target='_blank'>https://arxiv.org/pdf/2410.09505.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoran Wang, Yaoru Sun, Zeshen Tang, Haibo Shi, Chenyuan Jiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.09505">HG2P: Hippocampus-inspired High-reward Graph and Model-Free Q-Gradient Penalty for Path Planning and Motion Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Goal-conditioned hierarchical reinforcement learning (HRL) decomposes complex reaching tasks into a sequence of simple subgoal-conditioned tasks, showing significant promise for addressing long-horizon planning in large-scale environments. This paper bridges the goal-conditioned HRL based on graph-based planning to brain mechanisms, proposing a hippocampus-striatum-like dual-controller hypothesis. Inspired by the brain mechanisms of organisms (i.e., the high-reward preferences observed in hippocampal replay) and instance-based theory, we propose a high-return sampling strategy for constructing memory graphs, improving sample efficiency. Additionally, we derive a model-free lower-level Q-function gradient penalty to resolve the model dependency issues present in prior work, improving the generalization of Lipschitz constraints in applications. Finally, we integrate these two extensions, High-reward Graph and model-free Gradient Penalty (HG2P), into the state-of-the-art framework ACLG, proposing a novel goal-conditioned HRL framework, HG2P+ACLG. Experimentally, the results demonstrate that our method outperforms state-of-the-art goal-conditioned HRL algorithms on a variety of long-horizon navigation tasks and robotic manipulation tasks.
<div id='section'>Paperid: <span id='pid'>1220, <a href='https://arxiv.org/pdf/2410.07543.pdf' target='_blank'>https://arxiv.org/pdf/2410.07543.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weicheng Gao, Xiaodong Qu, Xiaopeng Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.07543">Generalization Ability Analysis of Through-the-Wall Radar Human Activity Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Through-the-Wall radar (TWR) human activity recognition (HAR) is a technology that uses low-frequency ultra-wideband (UWB) signal to detect and analyze indoor human motion. However, the high dependence of existing end-to-end recognition models on the distribution of TWR training data makes it difficult to achieve good generalization across different indoor testers. In this regard, the generalization ability of TWR HAR is analyzed in this paper. In detail, an end-to-end linear neural network method for TWR HAR and its generalization error bound are first discussed. Second, a micro-Doppler corner representation method and the change of the generalization error before and after dimension reduction are presented. The appropriateness of the theoretical generalization errors is proved through numerical simulations and experiments. The results demonstrate that feature dimension reduction is effective in allowing recognition models to generalize across different indoor testers.
<div id='section'>Paperid: <span id='pid'>1221, <a href='https://arxiv.org/pdf/2409.19638.pdf' target='_blank'>https://arxiv.org/pdf/2409.19638.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chaohui Xu, Si Wang, Chip-Hong Chang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.19638">BadHMP: Backdoor Attack against Human Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Precise future human motion prediction over sub-second horizons from past observations is crucial for various safety-critical applications. To date, only a few studies have examined the vulnerability of skeleton-based neural networks to evasion and backdoor attacks. In this paper, we propose BadHMP, a novel backdoor attack that targets specifically human motion prediction tasks. Our approach involves generating poisoned training samples by embedding a localized backdoor trigger in one limb of the skeleton, causing selected joints to follow predefined motion in historical time steps. Subsequently, the future sequences are globally modified that all the joints move following the target trajectories. Our carefully designed backdoor triggers and targets guarantee the smoothness and naturalness of the poisoned samples, making them stealthy enough to evade detection by the model trainer while keeping the poisoned model unobtrusive in terms of prediction fidelity to untainted sequences. The target sequences can be successfully activated by the designed input sequences even with a low poisoned sample injection ratio. Experimental results on two datasets (Human3.6M and CMU-Mocap) and two network architectures (LTD and HRI) demonstrate the high-fidelity, effectiveness, and stealthiness of BadHMP. Robustness of our attack against fine-tuning defense is also verified.
<div id='section'>Paperid: <span id='pid'>1222, <a href='https://arxiv.org/pdf/2408.12077.pdf' target='_blank'>https://arxiv.org/pdf/2408.12077.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaopeng Yang, Weicheng Gao, Xiaodong Qu, Zeyu Ma, Hao Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.12077">Through-the-Wall Radar Human Activity Micro-Doppler Signature Representation Method Based on Joint Boulic-Sinusoidal Pendulum Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the help of micro-Doppler signature, ultra-wideband (UWB) through-the-wall radar (TWR) enables the reconstruction of range and velocity information of limb nodes to accurately identify indoor human activities. However, existing methods are usually trained and validated directly using range-time maps (RTM) and Doppler-time maps (DTM), which have high feature redundancy and poor generalization ability. In order to solve this problem, this paper proposes a human activity micro-Doppler signature representation method based on joint Boulic-sinusoidal pendulum motion model. In detail, this paper presents a simplified joint Boulic-sinusoidal pendulum human motion model by taking head, torso, both hands and feet into consideration improved from Boulic-Thalmann kinematic model. The paper also calculates the minimum number of key points needed to describe the Doppler and micro-Doppler information sufficiently. Both numerical simulations and experiments are conducted to verify the effectiveness. The results demonstrate that the proposed number of key points of micro-Doppler signature can precisely represent the indoor human limb node motion characteristics, and substantially improve the generalization capability of the existing methods for different testers.
<div id='section'>Paperid: <span id='pid'>1223, <a href='https://arxiv.org/pdf/2406.19798.pdf' target='_blank'>https://arxiv.org/pdf/2406.19798.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vinicius Trentin, Juan Medina-Lee, Antonio ArtuÃ±edo, Jorge Villagra
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.19798">Integrating occlusion awareness in urban motion prediction for enhanced autonomous vehicle navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion prediction is a key factor towards the full deployment of autonomous vehicles. It is fundamental in order to ensure safety while navigating through highly interactive and complex scenarios. Lack of visibility due to an obstructed view or sensor range poses a great safety issue for autonomous vehicles. The inclusion of occlusion in interaction-aware approaches is not very well explored in the literature. In this work, the MultIAMP framework, which produces multimodal probabilistic outputs from the integration of a Dynamic Bayesian Network and Markov chains, is extended to tackle occlusions. The framework is evaluated with a state-of-the-art motion planner in two realistic use cases.
<div id='section'>Paperid: <span id='pid'>1224, <a href='https://arxiv.org/pdf/2406.16638.pdf' target='_blank'>https://arxiv.org/pdf/2406.16638.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohammad Belal, Taimur Hassan, Abdelfatah Ahmed, Ahmad Aljarah, Nael Alsheikh, Irfan Hussain
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.16638">Feature Fusion for Human Activity Recognition using Parameter-Optimized Multi-Stage Graph Convolutional Network and Transformer Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human activity recognition (HAR) is a crucial area of research that involves understanding human movements using computer and machine vision technology. Deep learning has emerged as a powerful tool for this task, with models such as Convolutional Neural Networks (CNNs) and Transformers being employed to capture various aspects of human motion. One of the key contributions of this work is the demonstration of the effectiveness of feature fusion in improving HAR accuracy by capturing spatial and temporal features, which has important implications for the development of more accurate and robust activity recognition systems. The study uses sensory data from HuGaDB, PKU-MMD, LARa, and TUG datasets. Two model, the PO-MS-GCN and a Transformer were trained and evaluated, with PO-MS-GCN outperforming state-of-the-art models. HuGaDB and TUG achieved high accuracies and f1-scores, while LARa and PKU-MMD had lower scores. Feature fusion improved results across datasets.
<div id='section'>Paperid: <span id='pid'>1225, <a href='https://arxiv.org/pdf/2406.01952.pdf' target='_blank'>https://arxiv.org/pdf/2406.01952.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ricardo B. Grando, Raul Steinmetz, Victor A. Kich, Alisson H. Kolling, Pablo M. Furik, Junior C. de Jesus, Bruna V. Guterres, Daniel T. Gamarra, Rodrigo S. Guerra, Paulo L. J. Drews-Jr
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.01952">Improving Generalization in Aerial and Terrestrial Mobile Robots Control Through Delayed Policy Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep Reinforcement Learning (DRL) has emerged as a promising approach to enhancing motion control and decision-making through a wide range of robotic applications. While prior research has demonstrated the efficacy of DRL algorithms in facilitating autonomous mapless navigation for aerial and terrestrial mobile robots, these methods often grapple with poor generalization when faced with unknown tasks and environments. This paper explores the impact of the Delayed Policy Updates (DPU) technique on fostering generalization to new situations, and bolstering the overall performance of agents. Our analysis of DPU in aerial and terrestrial mobile robots reveals that this technique significantly curtails the lack of generalization and accelerates the learning process for agents, enhancing their efficiency across diverse tasks and unknown scenarios.
<div id='section'>Paperid: <span id='pid'>1226, <a href='https://arxiv.org/pdf/2405.12460.pdf' target='_blank'>https://arxiv.org/pdf/2405.12460.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianan Li, Tao Huang, Qingxu Zhu, Tien-Tsin Wong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.12460">Physics-based Scene Layout Generation from Human Motion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Creating scenes for captured motions that achieve realistic human-scene interaction is crucial for 3D animation in movies or video games. As character motion is often captured in a blue-screened studio without real furniture or objects in place, there may be a discrepancy between the planned motion and the captured one. This gives rise to the need for automatic scene layout generation to relieve the burdens of selecting and positioning furniture and objects. Previous approaches cannot avoid artifacts like penetration and floating due to the lack of physical constraints. Furthermore, some heavily rely on specific data to learn the contact affordances, restricting the generalization ability to different motions. In this work, we present a physics-based approach that simultaneously optimizes a scene layout generator and simulates a moving human in a physics simulator. To attain plausible and realistic interaction motions, our method explicitly introduces physical constraints. To automatically recover and generate the scene layout, we minimize the motion tracking errors to identify the objects that can afford interaction. We use reinforcement learning to perform a dual-optimization of both the character motion imitation controller and the scene layout generator. To facilitate the optimization, we reshape the tracking rewards and devise pose prior guidance obtained from our estimated pseudo-contact labels. We evaluate our method using motions from SAMP and PROX, and demonstrate physically plausible scene layout reconstruction compared with the previous kinematics-based method.
<div id='section'>Paperid: <span id='pid'>1227, <a href='https://arxiv.org/pdf/2404.12886.pdf' target='_blank'>https://arxiv.org/pdf/2404.12886.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zeyu Ling, Bo Han, Yongkang Wongkan, Han Lin, Mohan Kankanhalli, Weidong Geng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.12886">MCM: Multi-condition Motion Synthesis Framework</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Conditional human motion synthesis (HMS) aims to generate human motion sequences that conform to specific conditions. Text and audio represent the two predominant modalities employed as HMS control conditions. While existing research has primarily focused on single conditions, the multi-condition human motion synthesis remains underexplored. In this study, we propose a multi-condition HMS framework, termed MCM, based on a dual-branch structure composed of a main branch and a control branch. This framework effectively extends the applicability of the diffusion model, which is initially predicated solely on textual conditions, to auditory conditions. This extension encompasses both music-to-dance and co-speech HMS while preserving the intrinsic quality of motion and the capabilities for semantic association inherent in the original model. Furthermore, we propose the implementation of a Transformer-based diffusion model, designated as MWNet, as the main branch. This model adeptly apprehends the spatial intricacies and inter-joint correlations inherent in motion sequences, facilitated by the integration of multi-wise self-attention modules. Extensive experiments show that our method achieves competitive results in single-condition and multi-condition HMS tasks.
<div id='section'>Paperid: <span id='pid'>1228, <a href='https://arxiv.org/pdf/2404.05578.pdf' target='_blank'>https://arxiv.org/pdf/2404.05578.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mahsa Ehsanpour, Ian Reid, Hamid Rezatofighi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.05578">Social-MAE: Social Masked Autoencoder for Multi-person Motion Representation Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>For a complete comprehension of multi-person scenes, it is essential to go beyond basic tasks like detection and tracking. Higher-level tasks, such as understanding the interactions and social activities among individuals, are also crucial. Progress towards models that can fully understand scenes involving multiple people is hindered by a lack of sufficient annotated data for such high-level tasks. To address this challenge, we introduce Social-MAE, a simple yet effective transformer-based masked autoencoder framework for multi-person human motion data. The framework uses masked modeling to pre-train the encoder to reconstruct masked human joint trajectories, enabling it to learn generalizable and data efficient representations of motion in human crowded scenes. Social-MAE comprises a transformer as the MAE encoder and a lighter-weight transformer as the MAE decoder which operates on multi-person joints' trajectory in the frequency domain. After the reconstruction task, the MAE decoder is replaced with a task-specific decoder and the model is fine-tuned end-to-end for a variety of high-level social tasks. Our proposed model combined with our pre-training approach achieves the state-of-the-art results on various high-level social tasks, including multi-person pose forecasting, social grouping, and social action understanding. These improvements are demonstrated across four popular multi-person datasets encompassing both human 2D and 3D body pose.
<div id='section'>Paperid: <span id='pid'>1229, <a href='https://arxiv.org/pdf/2404.05404.pdf' target='_blank'>https://arxiv.org/pdf/2404.05404.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Meng Yuan, Ye Wang, Chris Manzie, Zhezhuang Xu, Tianyou Chai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.05404">Contouring Error Bounded Control for Biaxial Switched Linear Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Biaxial motion control systems are used extensively in manufacturing and printing industries. To improve throughput and reduce machine cost, lightweight materials are being proposed in structural components but may result in higher flexibility in the machine links. This flexibility is often position dependent and compromises precision of the end effector of the machine. To address the need for improved contouring accuracy in industrial machines with position-dependent structural flexibility, this paper introduces a novel contouring error-bounded control algorithm for biaxial switched linear systems. The proposed algorithm utilizes model predictive control to guarantee the satisfaction of state, input, and contouring error constraints for any admissible mode switching. In this paper, the switching signal remains unknown to the controller, although information about the minimum time the system is expected to stay in a specific mode is considered to be available. The proposed algorithm has the property of recursive feasibility and ensures the stability of the closed-loop system. The effectiveness of the proposed method is demonstrated by applying it to a high-fidelity simulation of a dual-drive industrial laser machine. The results show that the contouring error is successfully bounded within the given tolerance.
<div id='section'>Paperid: <span id='pid'>1230, <a href='https://arxiv.org/pdf/2404.04430.pdf' target='_blank'>https://arxiv.org/pdf/2404.04430.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yufei Zhang, Jeffrey O. Kephart, Zijun Cui, Qiang Ji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.04430">PhysPT: Physics-aware Pretrained Transformer for Estimating Human Dynamics from Monocular Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While current methods have shown promising progress on estimating 3D human motion from monocular videos, their motion estimates are often physically unrealistic because they mainly consider kinematics. In this paper, we introduce Physics-aware Pretrained Transformer (PhysPT), which improves kinematics-based motion estimates and infers motion forces. PhysPT exploits a Transformer encoder-decoder backbone to effectively learn human dynamics in a self-supervised manner. Moreover, it incorporates physics principles governing human motion. Specifically, we build a physics-based body representation and contact force model. We leverage them to impose novel physics-inspired training losses (i.e., force loss, contact loss, and Euler-Lagrange loss), enabling PhysPT to capture physical properties of the human body and the forces it experiences. Experiments demonstrate that, once trained, PhysPT can be directly applied to kinematics-based estimates to significantly enhance their physical plausibility and generate favourable motion forces. Furthermore, we show that these physically meaningful quantities translate into improved accuracy of an important downstream task: human action recognition.
<div id='section'>Paperid: <span id='pid'>1231, <a href='https://arxiv.org/pdf/2403.11304.pdf' target='_blank'>https://arxiv.org/pdf/2403.11304.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Steffen Hagedorn, Marcel Milich, Alexandru P. Condurache
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.11304">Pioneering SE(2)-Equivariant Trajectory Planning for Automated Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Planning the trajectory of the controlled ego vehicle is a key challenge in automated driving. As for human drivers, predicting the motions of surrounding vehicles is important to plan the own actions. Recent motion prediction methods utilize equivariant neural networks to exploit geometric symmetries in the scene. However, no existing method combines motion prediction and trajectory planning in a joint step while guaranteeing equivariance under roto-translations of the input space. We address this gap by proposing a lightweight equivariant planning model that generates multi-modal joint predictions for all vehicles and selects one mode as the ego plan. The equivariant network design improves sample efficiency, guarantees output stability, and reduces model parameters. We further propose equivariant route attraction to guide the ego vehicle along a high-level route provided by an off-the-shelf GPS navigation system. This module creates a momentum from embedded vehicle positions toward the route in latent space while keeping the equivariance property. Route attraction enables goal-oriented behavior without forcing the vehicle to stick to the exact route. We conduct experiments on the challenging nuScenes dataset to investigate the capability of our planner. The results show that the planned trajectory is stable under roto-translations of the input scene which demonstrates the equivariance of our model. Despite using only a small split of the dataset for training, our method improves L2 distance at 3 s by 20.6 % and surpasses the state of the art.
<div id='section'>Paperid: <span id='pid'>1232, <a href='https://arxiv.org/pdf/2403.02075.pdf' target='_blank'>https://arxiv.org/pdf/2403.02075.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weiyi Lv, Yuhang Huang, Ning Zhang, Ruei-Sung Lin, Mei Han, Dan Zeng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.02075">DiffMOT: A Real-time Diffusion-based Multiple Object Tracker with Non-linear Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In Multiple Object Tracking, objects often exhibit non-linear motion of acceleration and deceleration, with irregular direction changes. Tacking-by-detection (TBD) trackers with Kalman Filter motion prediction work well in pedestrian-dominant scenarios but fall short in complex situations when multiple objects perform non-linear and diverse motion simultaneously. To tackle the complex non-linear motion, we propose a real-time diffusion-based MOT approach named DiffMOT. Specifically, for the motion predictor component, we propose a novel Decoupled Diffusion-based Motion Predictor (D$^2$MP). It models the entire distribution of various motion presented by the data as a whole. It also predicts an individual object's motion conditioning on an individual's historical motion information. Furthermore, it optimizes the diffusion process with much fewer sampling steps. As a MOT tracker, the DiffMOT is real-time at 22.7FPS, and also outperforms the state-of-the-art on DanceTrack and SportsMOT datasets with $62.3\%$ and $76.2\%$ in HOTA metrics, respectively. To the best of our knowledge, DiffMOT is the first to introduce a diffusion probabilistic model into the MOT to tackle non-linear motion prediction.
<div id='section'>Paperid: <span id='pid'>1233, <a href='https://arxiv.org/pdf/2402.14227.pdf' target='_blank'>https://arxiv.org/pdf/2402.14227.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pauline Bourigault, Dongpo Xu, Danilo P. Mandic
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.14227">Quaternion recurrent neural network with real-time recurrent learning and maximum correntropy criterion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We develop a robust quaternion recurrent neural network (QRNN) for real-time processing of 3D and 4D data with outliers. This is achieved by combining the real-time recurrent learning (RTRL) algorithm and the maximum correntropy criterion (MCC) as a loss function. While both the mean square error and maximum correntropy criterion are viable cost functions, it is shown that the non-quadratic maximum correntropy loss function is less sensitive to outliers, making it suitable for applications with multidimensional noisy or uncertain data. Both algorithms are derived based on the novel generalised HR (GHR) calculus, which allows for the differentiation of real functions of quaternion variables and offers the product and chain rules, thus enabling elegant and compact derivations. Simulation results in the context of motion prediction of chest internal markers for lung cancer radiotherapy, which includes regular and irregular breathing sequences, support the analysis.
<div id='section'>Paperid: <span id='pid'>1234, <a href='https://arxiv.org/pdf/2402.12676.pdf' target='_blank'>https://arxiv.org/pdf/2402.12676.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nikolaos Smyrnakis, Tasos Karakostas, R. James Cotton
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.12676">Advancing Monocular Video-Based Gait Analysis Using Motion Imitation with Physics-Based Simulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Gait analysis from videos obtained from a smartphone would open up many clinical opportunities for detecting and quantifying gait impairments. However, existing approaches for estimating gait parameters from videos can produce physically implausible results. To overcome this, we train a policy using reinforcement learning to control a physics simulation of human movement to replicate the movement seen in video. This forces the inferred movements to be physically plausible, while improving the accuracy of the inferred step length and walking velocity.
<div id='section'>Paperid: <span id='pid'>1235, <a href='https://arxiv.org/pdf/2401.14879.pdf' target='_blank'>https://arxiv.org/pdf/2401.14879.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Max Bastian Mertens, Jona Ruof, Jan Strohbeck, Michael Buchholz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.14879">Fast Long-Term Multi-Scenario Prediction for Maneuver Planning at Unsignalized Intersections</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion prediction for intelligent vehicles typically focuses on estimating the most probable future evolutions of a traffic scenario. Estimating the gap acceptance, i.e., whether a vehicle merges or crosses before another vehicle with the right of way, is often handled implicitly in the prediction. However, an infrastructure-based maneuver planning can assign artificial priorities between cooperative vehicles, so it needs to evaluate many more potential scenarios. Additionally, the prediction horizon has to be long enough to assess the impact of a maneuver. We, therefore, present a novel long-term prediction approach handling the gap acceptance estimation and the velocity prediction in two separate stages. Thereby, the behavior of regular vehicles as well as priority assignments of cooperative vehicles can be considered. We train both stages on real-world traffic observations to achieve realistic prediction results. Our method has a competitive accuracy and is fast enough to predict a multitude of scenarios in a short time, making it suitable to be used in a maneuver planning framework.
<div id='section'>Paperid: <span id='pid'>1236, <a href='https://arxiv.org/pdf/2401.05412.pdf' target='_blank'>https://arxiv.org/pdf/2401.05412.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xueyuan Yang, Chao Yao, Xiaojuan Ban
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.05412">Spatial-Related Sensors Matters: 3D Human Motion Reconstruction Assisted with Textual Semantics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Leveraging wearable devices for motion reconstruction has emerged as an economical and viable technique. Certain methodologies employ sparse Inertial Measurement Units (IMUs) on the human body and harness data-driven strategies to model human poses. However, the reconstruction of motion based solely on sparse IMUs data is inherently fraught with ambiguity, a consequence of numerous identical IMU readings corresponding to different poses. In this paper, we explore the spatial importance of multiple sensors, supervised by text that describes specific actions. Specifically, uncertainty is introduced to derive weighted features for each IMU. We also design a Hierarchical Temporal Transformer (HTT) and apply contrastive learning to achieve precise temporal and feature alignment of sensor data with textual semantics. Experimental results demonstrate our proposed approach achieves significant improvements in multiple metrics compared to existing methods. Notably, with textual supervision, our method not only differentiates between ambiguous actions such as sitting and standing but also produces more precise and natural motion.
<div id='section'>Paperid: <span id='pid'>1237, <a href='https://arxiv.org/pdf/2512.22464.pdf' target='_blank'>https://arxiv.org/pdf/2512.22464.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sukhyun Jeong, Yong-Hoon Choi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.22464">Pose-Guided Residual Refinement for Interpretable Text-to-Motion Generation and Editing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-based 3D motion generation aims to automatically synthesize diverse motions from natural-language descriptions to extend user creativity, whereas motion editing modifies an existing motion sequence in response to text while preserving its overall structure. Pose-code-based frameworks such as CoMo map quantifiable pose attributes into discrete pose codes that support interpretable motion control, but their frame-wise representation struggles to capture subtle temporal dynamics and high-frequency details, often degrading reconstruction fidelity and local controllability. To address this limitation, we introduce pose-guided residual refinement for motion (PGR$^2$M), a hybrid representation that augments interpretable pose codes with residual codes learned via residual vector quantization (RVQ). A pose-guided RVQ tokenizer decomposes motion into pose latents that encode coarse global structure and residual latents that model fine-grained temporal variations. Residual dropout further discourages over-reliance on residuals, preserving the semantic alignment and editability of the pose codes. On top of this tokenizer, a base Transformer autoregressively predicts pose codes from text, and a refine Transformer predicts residual codes conditioned on text, pose codes, and quantization stage. Experiments on HumanML3D and KIT-ML show that PGR$^2$M improves Fréchet inception distance and reconstruction metrics for both generation and editing compared with CoMo and recent diffusion- and tokenization-based baselines, while user studies confirm that it enables intuitive, structure-preserving motion edits.
<div id='section'>Paperid: <span id='pid'>1238, <a href='https://arxiv.org/pdf/2512.12664.pdf' target='_blank'>https://arxiv.org/pdf/2512.12664.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sreehari Rajan, Kunal Bhosikar, Charu Sharma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.12664">InteracTalker: Prompt-Based Human-Object Interaction with Co-Speech Gesture Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating realistic human motions that naturally respond to both spoken language and physical objects is crucial for interactive digital experiences. Current methods, however, address speech-driven gestures or object interactions independently, limiting real-world applicability due to a lack of integrated, comprehensive datasets. To overcome this, we introduce InteracTalker, a novel framework that seamlessly integrates prompt-based object-aware interactions with co-speech gesture generation. We achieve this by employing a multi-stage training process to learn a unified motion, speech, and prompt embedding space. To support this, we curate a rich human-object interaction dataset, formed by augmenting an existing text-to-motion dataset with detailed object interaction annotations. Our framework utilizes a Generalized Motion Adaptation Module that enables independent training, adapting to the corresponding motion condition, which is then dynamically combined during inference. To address the imbalance between heterogeneous conditioning signals, we propose an adaptive fusion strategy, which dynamically reweights the conditioning signals during diffusion sampling. InteracTalker successfully unifies these previously separate tasks, outperforming prior methods in both co-speech gesture generation and object-interaction synthesis, outperforming gesture-focused diffusion methods, yielding highly realistic, object-aware full-body motions with enhanced realism, flexibility, and control.
<div id='section'>Paperid: <span id='pid'>1239, <a href='https://arxiv.org/pdf/2511.16966.pdf' target='_blank'>https://arxiv.org/pdf/2511.16966.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiheng Bian, Zechen Li, Lanqing Yang, Hao Pan, Yezhou Wang, Longyuan Ge, Jeffery Wu, Ruiheng Liu, Yongjian Fu, Yichao chen, Guangtao xue
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.16966">One Walk is All You Need: Data-Efficient 3D RF Scene Reconstruction with Human Movements</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reconstructing 3D Radiance Field (RF) scenes through opaque obstacles is a long-standing goal, yet it is fundamentally constrained by a laborious data acquisition process requiring thousands of static measurements, which treats human motion as noise to be filtered. This work introduces a new paradigm with a core objective: to perform fast, data-efficient, and high-fidelity RF reconstruction of occluded 3D static scenes, using only a single, brief human walk. We argue that this unstructured motion is not noise, but is in fact an information-rich signal available for reconstruction. To achieve this, we design a factorization framework based on composite 3D Gaussian Splatting (3DGS) that learns to model the dynamic effects of human motion from the persistent static scene geometry within a raw RF stream. Trained on just a single 60-second casual walk, our model reconstructs the full static scene with a Structural Similarity Index (SSIM) of 0.96, remarkably outperforming heavily-sampled state-of-the-art (SOTA) by 12%. By transforming the human movements into its valuable signals, our method eliminates the data acquisition bottleneck and paves the way for on-the-fly 3D RF mapping of unseen environments.
<div id='section'>Paperid: <span id='pid'>1240, <a href='https://arxiv.org/pdf/2511.11368.pdf' target='_blank'>https://arxiv.org/pdf/2511.11368.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sheng Liu, Yuanzhi Liang, Sidan Du
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.11368">Free3D: 3D Human Motion Emerges from Single-View 2D Supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent 3D human motion generation models demonstrate remarkable reconstruction accuracy yet struggle to generalize beyond training distributions. This limitation arises partly from the use of precise 3D supervision, which encourages models to fit fixed coordinate patterns instead of learning the essential 3D structure and motion semantic cues required for robust generalization.To overcome this limitation, we propose Free3D, a framework that synthesizes realistic 3D motions without any 3D motion annotations. Free3D introduces a Motion-Lifting Residual Quantized VAE (ML-RQ) that maps 2D motion sequences into 3D-consistent latent spaces, and a suite of 3D-free regularization objectives enforcing view consistency, orientation coherence, and physical plausibility. Trained entirely on 2D motion data, Free3D generates diverse, temporally coherent, and semantically aligned 3D motions, achieving performance comparable to or even surpassing fully 3D-supervised counterparts. These results suggest that relaxing explicit 3D supervision encourages stronger structural reasoning and generalization, offering a scalable and data-efficient paradigm for 3D motion generation.
<div id='section'>Paperid: <span id='pid'>1241, <a href='https://arxiv.org/pdf/2511.09735.pdf' target='_blank'>https://arxiv.org/pdf/2511.09735.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ahmed Alia, Mohcine Chraibi, Armin Seyfried
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.09735">Social LSTM with Dynamic Occupancy Modeling for Realistic Pedestrian Trajectory Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In dynamic and crowded environments, realistic pedestrian trajectory prediction remains a challenging task due to the complex nature of human motion and the mutual influences among individuals. Deep learning models have recently achieved promising results by implicitly learning such patterns from 2D trajectory data. However, most approaches treat pedestrians as point entities, ignoring the physical space that each person occupies. To address these limitations, this paper proposes a novel deep learning model that enhances the Social LSTM with a new Dynamic Occupied Space loss function. This loss function guides Social LSTM in learning to avoid realistic collisions without increasing displacement error across different crowd densities, ranging from low to high, in both homogeneous and heterogeneous density settings. Such a function achieves this by combining the average displacement error with a new collision penalty that is sensitive to scene density and individual spatial occupancy. For efficient training and evaluation, five datasets were generated from real pedestrian trajectories recorded during the Festival of Lights in Lyon 2022. Four datasets represent homogeneous crowd conditions -- low, medium, high, and very high density -- while the fifth corresponds to a heterogeneous density distribution. The experimental findings indicate that the proposed model not only lowers collision rates but also enhances displacement prediction accuracy in each dataset. Specifically, the model achieves up to a 31% reduction in the collision rate and reduces the average displacement error and the final displacement error by 5% and 6%, respectively, on average across all datasets compared to the baseline. Moreover, the proposed model consistently outperforms several state-of-the-art deep learning models across most test sets.
<div id='section'>Paperid: <span id='pid'>1242, <a href='https://arxiv.org/pdf/2510.24994.pdf' target='_blank'>https://arxiv.org/pdf/2510.24994.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Matsive Ali, Blake Gassen, Sen Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.24994">Defect Mitigation for Robot Arm-based Additive Manufacturing Utilizing Intelligent Control and IOT</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents an integrated robotic fused deposition modeling additive manufacturing system featuring closed-loop thermal control and intelligent in-situ defect correction using a 6-degree of freedom robotic arm and an Oak-D camera. The robot arm end effector was modified to mount an E3D hotend thermally regulated by an IoT microcontroller, enabling precise temperature control through real-time feedback. Filament extrusion system was synchronized with robotic motion, coordinated via ROS2, ensuring consistent deposition along complex trajectories. A vision system based on OpenCV detects layer-wise defects position, commanding autonomous re-extrusion at identified sites. Experimental validation demonstrated successful defect mitigation in printing operations. The integrated system effectively addresses challenges real-time quality assurance. Inverse kinematics were used for motion planning, while homography transformations corrected camera perspectives for accurate defect localization. The intelligent system successfully mitigated surface anomalies without interrupting the print process. By combining real-time thermal regulation, motion control, and intelligent defect detection & correction, this architecture establishes a scalable and adaptive robotic additive manufacturing framework suitable for aerospace, biomedical, and industrial applications.
<div id='section'>Paperid: <span id='pid'>1243, <a href='https://arxiv.org/pdf/2510.22949.pdf' target='_blank'>https://arxiv.org/pdf/2510.22949.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Benedictus C. G. Cinun, Tua A. Tamba, Immanuel R. Santjoko, Xiaofeng Wang, Michael A. Gunarso, Bin Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.22949">End-to-End Design and Validation of a Low-Cost Stewart Platform with Nonlinear Estimation and Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents the complete design, control, and experimental validation of a low-cost Stewart platform prototype developed as an affordable yet capable robotic testbed for research and education. The platform combines off the shelf components with 3D printed and custom fabricated parts to deliver full six degrees of freedom motions using six linear actuators connecting a moving platform to a fixed base. The system software integrates dynamic modeling, data acquisition, and real time control within a unified framework. A robust trajectory tracking controller based on feedback linearization, augmented with an LQR scheme, compensates for the platform's nonlinear dynamics to achieve precise motion control. In parallel, an Extended Kalman Filter fuses IMU and actuator encoder feedback to provide accurate and reliable state estimation under sensor noise and external disturbances. Unlike prior efforts that emphasize only isolated aspects such as modeling or control, this work delivers a complete hardware-software platform validated through both simulation and experiments on static and dynamic trajectories. Results demonstrate effective trajectory tracking and real-time state estimation, highlighting the platform's potential as a cost effective and versatile tool for advanced research and educational applications.
<div id='section'>Paperid: <span id='pid'>1244, <a href='https://arxiv.org/pdf/2510.22789.pdf' target='_blank'>https://arxiv.org/pdf/2510.22789.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Abhijeet M. Kulkarni, Ioannis Poulakakis, Guoquan Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.22789">Learning Neural Observer-Predictor Models for Limb-level Sampling-based Locomotion Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate full-body motion prediction is essential for the safe, autonomous navigation of legged robots, enabling critical capabilities like limb-level collision checking in cluttered environments. Simplified kinematic models often fail to capture the complex, closed-loop dynamics of the robot and its low-level controller, limiting their predictions to simple planar motion. To address this, we present a learning-based observer-predictor framework that accurately predicts this motion. Our method features a neural observer with provable UUB guarantees that provides a reliable latent state estimate from a history of proprioceptive measurements. This stable estimate initializes a computationally efficient predictor, designed for the rapid, parallel evaluation of thousands of potential trajectories required by modern sampling-based planners. We validated the system by integrating our neural predictor into an MPPI-based planner on a Vision 60 quadruped. Hardware experiments successfully demonstrated effective, limb-aware motion planning in a challenging, narrow passage and over small objects, highlighting our system's ability to provide a robust foundation for high-performance, collision-aware planning on dynamic robotic platforms.
<div id='section'>Paperid: <span id='pid'>1245, <a href='https://arxiv.org/pdf/2510.22199.pdf' target='_blank'>https://arxiv.org/pdf/2510.22199.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kunal Bhosikar, Siddharth Katageri, Vivek Madhavaram, Kai Han, Charu Sharma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.22199">MOGRAS: Human Motion with Grasping in 3D Scenes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating realistic full-body motion interacting with objects is critical for applications in robotics, virtual reality, and human-computer interaction. While existing methods can generate full-body motion within 3D scenes, they often lack the fidelity for fine-grained tasks like object grasping. Conversely, methods that generate precise grasping motions typically ignore the surrounding 3D scene. This gap, generating full-body grasping motions that are physically plausible within a 3D scene, remains a significant challenge. To address this, we introduce MOGRAS (Human MOtion with GRAsping in 3D Scenes), a large-scale dataset that bridges this gap. MOGRAS provides pre-grasping full-body walking motions and final grasping poses within richly annotated 3D indoor scenes. We leverage MOGRAS to benchmark existing full-body grasping methods and demonstrate their limitations in scene-aware generation. Furthermore, we propose a simple yet effective method to adapt existing approaches to work seamlessly within 3D scenes. Through extensive quantitative and qualitative experiments, we validate the effectiveness of our dataset and highlight the significant improvements our proposed method achieves, paving the way for more realistic human-scene interactions.
<div id='section'>Paperid: <span id='pid'>1246, <a href='https://arxiv.org/pdf/2510.12537.pdf' target='_blank'>https://arxiv.org/pdf/2510.12537.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>David Björkstrand, Tiesheng Wang, Lars Bretzner, Josephine Sullivan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.12537">Unconditional Human Motion and Shape Generation via Balanced Score-Based Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent work has explored a range of model families for human motion generation, including Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs), and diffusion-based models. Despite their differences, many methods rely on over-parameterized input features and auxiliary losses to improve empirical results. These strategies should not be strictly necessary for diffusion models to match the human motion distribution. We show that on par with state-of-the-art results in unconditional human motion generation are achievable with a score-based diffusion model using only careful feature-space normalization and analytically derived weightings for the standard L2 score-matching loss, while generating both motion and shape directly, thereby avoiding slow post hoc shape recovery from joints. We build the method step by step, with a clear theoretical motivation for each component, and provide targeted ablations demonstrating the effectiveness of each proposed addition in isolation.
<div id='section'>Paperid: <span id='pid'>1247, <a href='https://arxiv.org/pdf/2508.14561.pdf' target='_blank'>https://arxiv.org/pdf/2508.14561.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sukhyun Jeong, Hong-Gi Shin, Yong-Hoon Choi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.14561">Making Pose Representations More Expressive and Disentangled via Residual Vector Quantization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent progress in text-to-motion has advanced both 3D human motion generation and text-based motion control. Controllable motion generation (CoMo), which enables intuitive control, typically relies on pose code representations, but discrete pose codes alone cannot capture fine-grained motion details, limiting expressiveness. To overcome this, we propose a method that augments pose code-based latent representations with continuous motion features using residual vector quantization (RVQ). This design preserves the interpretability and manipulability of pose codes while effectively capturing subtle motion characteristics such as high-frequency details. Experiments on the HumanML3D dataset show that our model reduces Frechet inception distance (FID) from 0.041 to 0.015 and improves Top-1 R-Precision from 0.508 to 0.510. Qualitative analysis of pairwise direction similarity between pose codes further confirms the model's controllability for motion editing.
<div id='section'>Paperid: <span id='pid'>1248, <a href='https://arxiv.org/pdf/2508.14309.pdf' target='_blank'>https://arxiv.org/pdf/2508.14309.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaohai Hu, Jason Laks, Guoxiao Guo, Xu Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.14309">Iterative Youla-Kucera Loop Shaping For Precision Motion Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a numerically robust approach to multi-band disturbance rejection using an iterative Youla-Kucera parameterization technique. The proposed method offers precise control over shaping the frequency response of a feedback loop while maintaining numerical stability through a systematic design process. By implementing an iterative approach, we overcome a critical numerical issue in rejecting vibrations with multiple frequency bands. Meanwhile, our proposed modification of the all-stabilizing Youla-Kucera architecture enables intuitive design while respecting fundamental performance trade-offs and minimizing undesired waterbed amplifications. Numerical validation on a hard disk drive servo system demonstrates significant performance improvements, enabling enhanced positioning precision for increased storage density. The design methodology extends beyond storage systems to various high-precision control applications where multi-band disturbance rejection is critical.
<div id='section'>Paperid: <span id='pid'>1249, <a href='https://arxiv.org/pdf/2508.01894.pdf' target='_blank'>https://arxiv.org/pdf/2508.01894.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haozhe Zhou, Riku Arakawa, Yuvraj Agarwal, Mayank Goel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01894">IMUCoCo: Enabling Flexible On-Body IMU Placement for Human Pose Estimation and Activity Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>IMUs are regularly used to sense human motion, recognize activities, and estimate full-body pose. Users are typically required to place sensors in predefined locations that are often dictated by common wearable form factors and the machine learning model's training process. Consequently, despite the increasing number of everyday devices equipped with IMUs, the limited adaptability has seriously constrained the user experience to only using a few well-explored device placements (e.g., wrist and ears). In this paper, we rethink IMU-based motion sensing by acknowledging that signals can be captured from any point on the human body. We introduce IMU over Continuous Coordinates (IMUCoCo), a novel framework that maps signals from a variable number of IMUs placed on the body surface into a unified feature space based on their spatial coordinates. These features can be plugged into downstream models for pose estimation and activity recognition. Our evaluations demonstrate that IMUCoCo supports accurate pose estimation in a wide range of typical and atypical sensor placements. Overall, IMUCoCo supports significantly more flexible use of IMUs for motion sensing than the state-of-the-art, allowing users to place their sensors-laden devices according to their needs and preferences. The framework also supports the ability to change device locations depending on the context and suggests placement depending on the use case.
<div id='section'>Paperid: <span id='pid'>1250, <a href='https://arxiv.org/pdf/2507.15194.pdf' target='_blank'>https://arxiv.org/pdf/2507.15194.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yilin Lyu, Fan Yang, Xiaoyue Liu, Zichen Jiang, Joshua Dillon, Debbie Zhao, Martyn Nash, Charlene Mauger, Alistair Young, Ching-Hui Sia, Mark YY Chan, Lei Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.15194">Personalized 3D Myocardial Infarct Geometry Reconstruction from Cine MRI with Explicit Cardiac Motion Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate representation of myocardial infarct geometry is crucial for patient-specific cardiac modeling in MI patients. While Late gadolinium enhancement (LGE) MRI is the clinical gold standard for infarct detection, it requires contrast agents, introducing side effects and patient discomfort. Moreover, infarct reconstruction from LGE often relies on sparsely sampled 2D slices, limiting spatial resolution and accuracy. In this work, we propose a novel framework for automatically reconstructing high-fidelity 3D myocardial infarct geometry from 2D clinically standard cine MRI, eliminating the need for contrast agents. Specifically, we first reconstruct the 4D biventricular mesh from multi-view cine MRIs via an automatic deep shape fitting model, biv-me. Then, we design a infarction reconstruction model, CMotion2Infarct-Net, to explicitly utilize the motion patterns within this dynamic geometry to localize infarct regions. Evaluated on 205 cine MRI scans from 126 MI patients, our method shows reasonable agreement with manual delineation. This study demonstrates the feasibility of contrast-free, cardiac motion-driven 3D infarct reconstruction, paving the way for efficient digital twin of MI.
<div id='section'>Paperid: <span id='pid'>1251, <a href='https://arxiv.org/pdf/2507.01308.pdf' target='_blank'>https://arxiv.org/pdf/2507.01308.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Muhammad Atta ur Rahman, Dooseop Choi, KyoungWook Min
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.01308">LANet: A Lane Boundaries-Aware Approach For Robust Trajectory Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate motion forecasting is critical for safe and efficient autonomous driving, enabling vehicles to predict future trajectories and make informed decisions in complex traffic scenarios. Most of the current designs of motion prediction models are based on the major representation of lane centerlines, which limits their capability to capture critical road environments and traffic rules and constraints. In this work, we propose an enhanced motion forecasting model informed by multiple vector map elements, including lane boundaries and road edges, that facilitates a richer and more complete representation of driving environments. An effective feature fusion strategy is developed to merge information in different vector map components, where the model learns holistic information on road structures and their interactions with agents. Since encoding more information about the road environment increases memory usage and is computationally expensive, we developed an effective pruning mechanism that filters the most relevant map connections to the target agent, ensuring computational efficiency while maintaining essential spatial and semantic relationships for accurate trajectory prediction. Overcoming the limitations of lane centerline-based models, our method provides a more informative and efficient representation of the driving environment and advances the state of the art for autonomous vehicle motion forecasting. We verify our approach with extensive experiments on the Argoverse 2 motion forecasting dataset, where our method maintains competitiveness on AV2 while achieving improved performance.
  Index Terms-Autonomous driving, trajectory prediction, vector map elements, road topology, connection pruning, Argoverse 2.
<div id='section'>Paperid: <span id='pid'>1252, <a href='https://arxiv.org/pdf/2506.11419.pdf' target='_blank'>https://arxiv.org/pdf/2506.11419.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bin Sun, Boao Zhang, Jiayi Lu, Xinjie Feng, Jiachen Shang, Rui Cao, Mengchao Zheng, Chuanye Wang, Shichun Yang, Yaoguang Cao, Ziying Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.11419">FocalAD: Local Motion Planning for End-to-End Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In end-to-end autonomous driving,the motion prediction plays a pivotal role in ego-vehicle planning. However, existing methods often rely on globally aggregated motion features, ignoring the fact that planning decisions are primarily influenced by a small number of locally interacting agents. Failing to attend to these critical local interactions can obscure potential risks and undermine planning reliability. In this work, we propose FocalAD, a novel end-to-end autonomous driving framework that focuses on critical local neighbors and refines planning by enhancing local motion representations. Specifically, FocalAD comprises two core modules: the Ego-Local-Agents Interactor (ELAI) and the Focal-Local-Agents Loss (FLA Loss). ELAI conducts a graph-based ego-centric interaction representation that captures motion dynamics with local neighbors to enhance both ego planning and agent motion queries. FLA Loss increases the weights of decision-critical neighboring agents, guiding the model to prioritize those more relevant to planning. Extensive experiments show that FocalAD outperforms existing state-of-the-art methods on the open-loop nuScenes datasets and closed-loop Bench2Drive benchmark. Notably, on the robustness-focused Adv-nuScenes dataset, FocalAD achieves even greater improvements, reducing the average colilision rate by 41.9% compared to DiffusionDrive and by 15.6% compared to SparseDrive.
<div id='section'>Paperid: <span id='pid'>1253, <a href='https://arxiv.org/pdf/2506.07076.pdf' target='_blank'>https://arxiv.org/pdf/2506.07076.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinyi Wu, Haohong Wang, Aggelos K. Katsaggelos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.07076">Harmony-Aware Music-driven Motion Synthesis with Perceptual Constraint on UGC Datasets</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the popularity of video-based user-generated content (UGC) on social media, harmony, as dictated by human perceptual principles, is critical in assessing the rhythmic consistency of audio-visual UGCs for better user engagement. In this work, we propose a novel harmony-aware GAN framework, following a specifically designed harmony evaluation strategy to enhance rhythmic synchronization in the automatic music-to-motion synthesis using a UGC dance dataset. This harmony strategy utilizes refined cross-modal beat detection to capture closely correlated audio and visual rhythms in an audio-visual pair. To mimic human attention mechanism, we introduce saliency-based beat weighting and interval-driven beat alignment, which ensures accurate harmony score estimation consistent with human perception. Building on this strategy, our model, employing efficient encoder-decoder and depth-lifting designs, is adversarially trained based on categorized musical meter segments to generate realistic and rhythmic 3D human motions. We further incorporate our harmony evaluation strategy as a weakly supervised perceptual constraint to flexibly guide the synchronized audio-visual rhythms during the generation process. Experimental results show that our proposed model significantly outperforms other leading music-to-motion methods in rhythmic harmony, both quantitatively and qualitatively, even with limited UGC training data. Live samples 15 can be watched at: https://youtu.be/tWwz7yq4aUs
<div id='section'>Paperid: <span id='pid'>1254, <a href='https://arxiv.org/pdf/2506.06318.pdf' target='_blank'>https://arxiv.org/pdf/2506.06318.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Feiyang Pan, Shenghe Zheng, Chunyan Yin, Guangbin Dou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.06318">MoE-Gyro: Self-Supervised Over-Range Reconstruction and Denoising for MEMS Gyroscopes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>MEMS gyroscopes play a critical role in inertial navigation and motion control applications but typically suffer from a fundamental trade-off between measurement range and noise performance. Existing hardware-based solutions aimed at mitigating this issue introduce additional complexity, cost, and scalability challenges. Deep-learning methods primarily focus on noise reduction and typically require precisely aligned ground-truth signals, making them difficult to deploy in practical scenarios and leaving the fundamental trade-off unresolved. To address these challenges, we introduce Mixture of Experts for MEMS Gyroscopes (MoE-Gyro), a novel self-supervised framework specifically designed for simultaneous over-range signal reconstruction and noise suppression. MoE-Gyro employs two experts: an Over-Range Reconstruction Expert (ORE), featuring a Gaussian-Decay Attention mechanism for reconstructing saturated segments; and a Denoise Expert (DE), utilizing dual-branch complementary masking combined with FFT-guided augmentation for robust noise reduction. A lightweight gating module dynamically routes input segments to the appropriate expert. Furthermore, existing evaluation lack a comprehensive standard for assessing multi-dimensional signal enhancement. To bridge this gap, we introduce IMU Signal Enhancement Benchmark (ISEBench), an open-source benchmarking platform comprising the GyroPeak-100 dataset and a unified evaluation of IMU signal enhancement methods. We evaluate MoE-Gyro using our proposed ISEBench, demonstrating that our framework significantly extends the measurable range from 450 deg/s to 1500 deg/s, reduces Bias Instability by 98.4%, and achieves state-of-the-art performance, effectively addressing the long-standing trade-off in inertial sensing.
<div id='section'>Paperid: <span id='pid'>1255, <a href='https://arxiv.org/pdf/2505.23465.pdf' target='_blank'>https://arxiv.org/pdf/2505.23465.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zi-An Wang, Shihao Zou, Shiyao Yu, Mingyuan Zhang, Chao Dong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.23465">Semantics-Aware Human Motion Generation from Audio Instructions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in interactive technologies have highlighted the prominence of audio signals for semantic encoding. This paper explores a new task, where audio signals are used as conditioning inputs to generate motions that align with the semantics of the audio. Unlike text-based interactions, audio provides a more natural and intuitive communication method. However, existing methods typically focus on matching motions with music or speech rhythms, which often results in a weak connection between the semantics of the audio and generated motions. We propose an end-to-end framework using a masked generative transformer, enhanced by a memory-retrieval attention module to handle sparse and lengthy audio inputs. Additionally, we enrich existing datasets by converting descriptions into conversational style and generating corresponding audio with varied speaker identities. Experiments demonstrate the effectiveness and efficiency of the proposed framework, demonstrating that audio instructions can convey semantics similar to text while providing more practical and user-friendly interactions.
<div id='section'>Paperid: <span id='pid'>1256, <a href='https://arxiv.org/pdf/2505.02668.pdf' target='_blank'>https://arxiv.org/pdf/2505.02668.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Antonio Grotta, Francesco De Lellis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.02668">Online Phase Estimation of Human Oscillatory Motions using Deep Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurately estimating the phase of oscillatory systems is essential for analyzing cyclic activities such as repetitive gestures in human motion. In this work we introduce a learning-based approach for online phase estimation in three-dimensional motion trajectories, using a Long Short- Term Memory (LSTM) network. A calibration procedure is applied to standardize trajectory position and orientation, ensuring invariance to spatial variations. The proposed model is evaluated on motion capture data and further tested in a dynamical system, where the estimated phase is used as input to a reinforcement learning (RL)-based control to assess its impact on the synchronization of a network of Kuramoto oscillators.
<div id='section'>Paperid: <span id='pid'>1257, <a href='https://arxiv.org/pdf/2504.06176.pdf' target='_blank'>https://arxiv.org/pdf/2504.06176.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ian Groves, Andrew Campbell, James Fernandes, Diego RamÃ­rez RodrÃ­guez, Paul Murray, Massimiliano Vasile, Victoria Nockles
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.06176">A Self-Supervised Framework for Space Object Behaviour Characterisation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Foundation Models, pre-trained on large unlabelled datasets before task-specific fine-tuning, are increasingly being applied to specialised domains. Recent examples include ClimaX for climate and Clay for satellite Earth observation, but a Foundation Model for Space Object Behavioural Analysis has not yet been developed. As orbital populations grow, automated methods for characterising space object behaviour are crucial for space safety. We present a Space Safety and Sustainability Foundation Model focusing on space object behavioural analysis using light curves (LCs). We implemented a Perceiver-Variational Autoencoder (VAE) architecture, pre-trained with self-supervised reconstruction and masked reconstruction on 227,000 LCs from the MMT-9 observatory. The VAE enables anomaly detection, motion prediction, and LC generation. We fine-tuned the model for anomaly detection & motion prediction using two independent LC simulators (CASSANDRA and GRIAL respectively), using CAD models of boxwing, Sentinel-3, SMOS, and Starlink platforms. Our pre-trained model achieved a reconstruction error of 0.01%, identifying potentially anomalous light curves through reconstruction difficulty. After fine-tuning, the model scored 88% and 82% accuracy, with 0.90 and 0.95 ROC AUC scores respectively in both anomaly detection and motion mode prediction (sun-pointing, spin, etc.). Analysis of high-confidence anomaly predictions on real data revealed distinct patterns including characteristic object profiles and satellite glinting. Here, we demonstrate how self-supervised learning can simultaneously enable anomaly detection, motion prediction, and synthetic data generation from rich representations learned in pre-training. Our work therefore supports space safety and sustainability through automated monitoring and simulation capabilities.
<div id='section'>Paperid: <span id='pid'>1258, <a href='https://arxiv.org/pdf/2503.24272.pdf' target='_blank'>https://arxiv.org/pdf/2503.24272.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yizhou Huang, Yihua Cheng, Kezhi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.24272">Learning Velocity and Acceleration: Self-Supervised Motion Consistency for Pedestrian Trajectory Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding human motion is crucial for accurate pedestrian trajectory prediction. Conventional methods typically rely on supervised learning, where ground-truth labels are directly optimized against predicted trajectories. This amplifies the limitations caused by long-tailed data distributions, making it difficult for the model to capture abnormal behaviors. In this work, we propose a self-supervised pedestrian trajectory prediction framework that explicitly models position, velocity, and acceleration. We leverage velocity and acceleration information to enhance position prediction through feature injection and a self-supervised motion consistency mechanism. Our model hierarchically injects velocity features into the position stream. Acceleration features are injected into the velocity stream. This enables the model to predict position, velocity, and acceleration jointly. From the predicted position, we compute corresponding pseudo velocity and acceleration, allowing the model to learn from data-generated pseudo labels and thus achieve self-supervised learning. We further design a motion consistency evaluation strategy grounded in physical principles; it selects the most reasonable predicted motion trend by comparing it with historical dynamics and uses this trend to guide and constrain trajectory generation. We conduct experiments on the ETH-UCY and Stanford Drone datasets, demonstrating that our method achieves state-of-the-art performance on both datasets.
<div id='section'>Paperid: <span id='pid'>1259, <a href='https://arxiv.org/pdf/2503.21775.pdf' target='_blank'>https://arxiv.org/pdf/2503.21775.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyu Guo, Young Yoon Lee, Joseph Liu, Yizhak Ben-Shabat, Victor Zordan, Mubbasir Kapadia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.21775">StyleMotif: Multi-Modal Motion Stylization using Style-Content Cross Fusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present StyleMotif, a novel Stylized Motion Latent Diffusion model, generating motion conditioned on both content and style from multiple modalities. Unlike existing approaches that either focus on generating diverse motion content or transferring style from sequences, StyleMotif seamlessly synthesizes motion across a wide range of content while incorporating stylistic cues from multi-modal inputs, including motion, text, image, video, and audio. To achieve this, we introduce a style-content cross fusion mechanism and align a style encoder with a pre-trained multi-modal model, ensuring that the generated motion accurately captures the reference style while preserving realism. Extensive experiments demonstrate that our framework surpasses existing methods in stylized motion generation and exhibits emergent capabilities for multi-modal motion stylization, enabling more nuanced motion synthesis. Source code and pre-trained models will be released upon acceptance. Project Page: https://stylemotif.github.io
<div id='section'>Paperid: <span id='pid'>1260, <a href='https://arxiv.org/pdf/2503.14040.pdf' target='_blank'>https://arxiv.org/pdf/2503.14040.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Binjie Liu, Lina Liu, Sanyi Zhang, Songen Gu, Yihao Zhi, Tianyi Zhu, Lei Yang, Long Ye
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.14040">MAG: Multi-Modal Aligned Autoregressive Co-Speech Gesture Generation without Vector Quantization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work focuses on full-body co-speech gesture generation. Existing methods typically employ an autoregressive model accompanied by vector-quantized tokens for gesture generation, which results in information loss and compromises the realism of the generated gestures. To address this, inspired by the natural continuity of real-world human motion, we propose MAG, a novel multi-modal aligned framework for high-quality and diverse co-speech gesture synthesis without relying on discrete tokenization. Specifically, (1) we introduce a motion-text-audio-aligned variational autoencoder (MTA-VAE), which leverages pre-trained WavCaps' text and audio embeddings to enhance both semantic and rhythmic alignment with motion, ultimately producing more realistic gestures. (2) Building on this, we propose a multimodal masked autoregressive model (MMAG) that enables autoregressive modeling in continuous motion embeddings through diffusion without vector quantization. To further ensure multi-modal consistency, MMAG incorporates a hybrid granularity audio-text fusion block, which serves as conditioning for diffusion process. Extensive experiments on two benchmark datasets demonstrate that MAG achieves stateof-the-art performance both quantitatively and qualitatively, producing highly realistic and diverse co-speech gestures.The code will be released to facilitate future research.
<div id='section'>Paperid: <span id='pid'>1261, <a href='https://arxiv.org/pdf/2503.13025.pdf' target='_blank'>https://arxiv.org/pdf/2503.13025.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>ChangHee Yang, Hyeonseop Song, Seokhun Choi, Seungwoo Lee, Jaechul Kim, Hoseok Do
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.13025">PoseSyn: Synthesizing Diverse 3D Pose Data from In-the-Wild 2D Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite considerable efforts to enhance the generalization of 3D pose estimators without costly 3D annotations, existing data augmentation methods struggle in real world scenarios with diverse human appearances and complex poses. We propose PoseSyn, a novel data synthesis framework that transforms abundant in the wild 2D pose dataset into diverse 3D pose image pairs. PoseSyn comprises two key components: Error Extraction Module (EEM), which identifies challenging poses from the 2D pose datasets, and Motion Synthesis Module (MSM), which synthesizes motion sequences around the challenging poses. Then, by generating realistic 3D training data via a human animation model aligned with challenging poses and appearances PoseSyn boosts the accuracy of various 3D pose estimators by up to 14% across real world benchmarks including various backgrounds and occlusions, challenging poses, and multi view scenarios. Extensive experiments further confirm that PoseSyn is a scalable and effective approach for improving generalization without relying on expensive 3D annotations, regardless of the pose estimator's model size or design.
<div id='section'>Paperid: <span id='pid'>1262, <a href='https://arxiv.org/pdf/2503.10898.pdf' target='_blank'>https://arxiv.org/pdf/2503.10898.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yizhou Huang, Yihua Cheng, Kezhi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.10898">Trajectory Mamba: Efficient Attention-Mamba Forecasting Model Based on Selective SSM</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion prediction is crucial for autonomous driving, as it enables accurate forecasting of future vehicle trajectories based on historical inputs. This paper introduces Trajectory Mamba, a novel efficient trajectory prediction framework based on the selective state-space model (SSM). Conventional attention-based models face the challenge of computational costs that grow quadratically with the number of targets, hindering their application in highly dynamic environments. In response, we leverage the SSM to redesign the self-attention mechanism in the encoder-decoder architecture, thereby achieving linear time complexity. To address the potential reduction in prediction accuracy resulting from modifications to the attention mechanism, we propose a joint polyline encoding strategy to better capture the associations between static and dynamic contexts, ultimately enhancing prediction accuracy. Additionally, to balance prediction accuracy and inference speed, we adopted the decoder that differs entirely from the encoder. Through cross-state space attention, all target agents share the scene context, allowing the SSM to interact with the shared scene representation during decoding, thus inferring different trajectories over the next prediction steps. Our model achieves state-of-the-art results in terms of inference speed and parameter efficiency on both the Argoverse 1 and Argoverse 2 datasets. It demonstrates a four-fold reduction in FLOPs compared to existing methods and reduces parameter count by over 40% while surpassing the performance of the vast majority of previous methods. These findings validate the effectiveness of Trajectory Mamba in trajectory prediction tasks.
<div id='section'>Paperid: <span id='pid'>1263, <a href='https://arxiv.org/pdf/2503.05092.pdf' target='_blank'>https://arxiv.org/pdf/2503.05092.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Adam Labiosa, Josiah P. Hanna
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.05092">Multi-Robot Collaboration through Reinforcement Learning and Abstract Simulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Teams of people coordinate to perform complex tasks by forming abstract mental models of world and agent dynamics. The use of abstract models contrasts with much recent work in robot learning that uses a high-fidelity simulator and reinforcement learning (RL) to obtain policies for physical robots. Motivated by this difference, we investigate the extent to which so-called abstract simulators can be used for multi-agent reinforcement learning (MARL) and the resulting policies successfully deployed on teams of physical robots. An abstract simulator models the robot's target task at a high-level of abstraction and discards many details of the world that could impact optimal decision-making. Policies are trained in an abstract simulator then transferred to the physical robot by making use of separately-obtained low-level perception and motion control modules. We identify three key categories of modifications to the abstract simulator that enable policy transfer to physical robots: simulation fidelity enhancements, training optimizations and simulation stochasticity. We then run an empirical study with extensive ablations to determine the value of each modification category for enabling policy transfer in cooperative robot soccer tasks. We also compare the performance of policies produced by our method with a well-tuned non-learning-based behavior architecture from the annual RoboCup competition and find that our approach leads to a similar level of performance. Broadly we show that MARL can be use to train cooperative physical robot behaviors using highly abstract models of the world.
<div id='section'>Paperid: <span id='pid'>1264, <a href='https://arxiv.org/pdf/2503.00389.pdf' target='_blank'>https://arxiv.org/pdf/2503.00389.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuto Shibata, Yusuke Oumi, Go Irie, Akisato Kimura, Yoshimitsu Aoki, Mariko Isogawa
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.00389">BGM2Pose: Active 3D Human Pose Estimation with Non-Stationary Sounds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose BGM2Pose, a non-invasive 3D human pose estimation method using arbitrary music (e.g., background music) as active sensing signals. Unlike existing approaches that significantly limit practicality by employing intrusive chirp signals within the audible range, our method utilizes natural music that causes minimal discomfort to humans. Estimating human poses from standard music presents significant challenges. In contrast to sound sources specifically designed for measurement, regular music varies in both volume and pitch. These dynamic changes in signals caused by music are inevitably mixed with alterations in the sound field resulting from human motion, making it hard to extract reliable cues for pose estimation. To address these challenges, BGM2Pose introduces a Contrastive Pose Extraction Module that employs contrastive learning and hard negative sampling to eliminate musical components from the recorded data, isolating the pose information. Additionally, we propose a Frequency-wise Attention Module that enables the model to focus on subtle acoustic variations attributable to human movement by dynamically computing attention across frequency bands. Experiments suggest that our method outperforms the existing methods, demonstrating substantial potential for real-world applications. Our datasets and code will be made publicly available.
<div id='section'>Paperid: <span id='pid'>1265, <a href='https://arxiv.org/pdf/2502.17372.pdf' target='_blank'>https://arxiv.org/pdf/2502.17372.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Stella DumenÄiÄ, Luka LanÄa, Karlo Jakac, Stefan IviÄ
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.17372">Experimental validation of UAV search and detection system in real wilderness environment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Search and rescue (SAR) missions require reliable search methods to locate survivors, especially in challenging or inaccessible environments. This is why introducing unmanned aerial vehicles (UAVs) can be of great help to enhance the efficiency of SAR missions while simultaneously increasing the safety of everyone involved in the mission. Motivated by this, we design and experiment with autonomous UAV search for humans in a Mediterranean karst environment. The UAVs are directed using Heat equation-driven area coverage (HEDAC) ergodic control method according to known probability density and detection function. The implemented sensing framework consists of a probabilistic search model, motion control system, and computer vision object detection. It enables calculation of the probability of the target being detected in the SAR mission, and this paper focuses on experimental validation of proposed probabilistic framework and UAV control. The uniform probability density to ensure the even probability of finding the targets in the desired search area is achieved by assigning suitably thought-out tasks to 78 volunteers. The detection model is based on YOLO and trained with a previously collected ortho-photo image database. The experimental search is carefully planned and conducted, while as many parameters as possible are recorded. The thorough analysis consists of the motion control system, object detection, and the search validation. The assessment of the detection and search performance provides strong indication that the designed detection model in the UAV control algorithm is aligned with real-world results.
<div id='section'>Paperid: <span id='pid'>1266, <a href='https://arxiv.org/pdf/2502.10429.pdf' target='_blank'>https://arxiv.org/pdf/2502.10429.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhang Minghao, Yang Xiaojun, Wang Zhihe, Wang Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.10429">Real Time Control of Tandem-Wing Experimental Platform Using Concerto Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces the CRL2RT algorithm, an advanced reinforcement learning method aimed at improving the real-time control performance of the Direct-Drive Tandem-Wing Experimental Platform (DDTWEP). Inspired by dragonfly flight, DDTWEP's tandem wing structure causes nonlinear and unsteady aerodynamic interactions, leading to complex load behaviors during pitch, roll, and yaw maneuvers. These complexities challenge stable motion control at high frequencies (2000 Hz). To overcome these issues, we developed the CRL2RT algorithm, which combines classical control elements with reinforcement learning-based controllers using a time-interleaved architecture and a rule-based policy composer. This integration ensures finite-time convergence and single-life adaptability. Experimental results under various conditions, including different flapping frequencies and yaw disturbances, show that CRL2RT achieves a control frequency surpassing 2500 Hz on standard CPUs. Additionally, when integrated with classical controllers like PID, Adaptive PID, and Model Reference Adaptive Control (MRAC), CRL2RT enhances tracking performance by 18.3% to 60.7%. These findings demonstrate CRL2RT's broad applicability and superior performance in complex real-time control scenarios, validating its effectiveness in overcoming existing control strategy limitations and advancing robust, efficient real-time control for biomimetic aerial vehicles.
<div id='section'>Paperid: <span id='pid'>1267, <a href='https://arxiv.org/pdf/2501.16778.pdf' target='_blank'>https://arxiv.org/pdf/2501.16778.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Arvin Tashakori, Arash Tashakori, Gongbo Yang, Z. Jane Wang, Peyman Servati
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.16778">FlexMotion: Lightweight, Physics-Aware, and Controllable Human Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Lightweight, controllable, and physically plausible human motion synthesis is crucial for animation, virtual reality, robotics, and human-computer interaction applications. Existing methods often compromise between computational efficiency, physical realism, or spatial controllability. We propose FlexMotion, a novel framework that leverages a computationally lightweight diffusion model operating in the latent space, eliminating the need for physics simulators and enabling fast and efficient training. FlexMotion employs a multimodal pre-trained Transformer encoder-decoder, integrating joint locations, contact forces, joint actuations and muscle activations to ensure the physical plausibility of the generated motions. FlexMotion also introduces a plug-and-play module, which adds spatial controllability over a range of motion parameters (e.g., joint locations, joint actuations, contact forces, and muscle activations). Our framework achieves realistic motion generation with improved efficiency and control, setting a new benchmark for human motion synthesis. We evaluate FlexMotion on extended datasets and demonstrate its superior performance in terms of realism, physical plausibility, and controllability.
<div id='section'>Paperid: <span id='pid'>1268, <a href='https://arxiv.org/pdf/2501.08609.pdf' target='_blank'>https://arxiv.org/pdf/2501.08609.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaleab A. Kinfu, Carolina Pacheco, Alice D. Sperry, Deana Crocetti, Bahar TunÃ§genÃ§, Stewart H. Mostofsky, RenÃ© Vidal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.08609">Computerized Assessment of Motor Imitation for Distinguishing Autism in Video (CAMI-2DNet)</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motor imitation impairments are commonly reported in individuals with autism spectrum conditions (ASCs), suggesting that motor imitation could be used as a phenotype for addressing autism heterogeneity. Traditional methods for assessing motor imitation are subjective, labor-intensive, and require extensive human training. Modern Computerized Assessment of Motor Imitation (CAMI) methods, such as CAMI-3D for motion capture data and CAMI-2D for video data, are less subjective. However, they rely on labor-intensive data normalization and cleaning techniques, and human annotations for algorithm training. To address these challenges, we propose CAMI-2DNet, a scalable and interpretable deep learning-based approach to motor imitation assessment in video data, which eliminates the need for data normalization, cleaning and annotation. CAMI-2DNet uses an encoder-decoder architecture to map a video to a motion encoding that is disentangled from nuisance factors such as body shape and camera views. To learn a disentangled representation, we employ synthetic data generated by motion retargeting of virtual characters through the reshuffling of motion, body shape, and camera views, as well as real participant data. To automatically assess how well an individual imitates an actor, we compute a similarity score between their motion encodings, and use it to discriminate individuals with ASCs from neurotypical (NT) individuals. Our comparative analysis demonstrates that CAMI-2DNet has a strong correlation with human scores while outperforming CAMI-2D in discriminating ASC vs NT children. Moreover, CAMI-2DNet performs comparably to CAMI-3D while offering greater practicality by operating directly on video data and without the need for ad-hoc data normalization and human annotations.
<div id='section'>Paperid: <span id='pid'>1269, <a href='https://arxiv.org/pdf/2501.01449.pdf' target='_blank'>https://arxiv.org/pdf/2501.01449.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Avinash Amballa, Gayathri Akkinapalli, Vinitra Muralikrishnan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.01449">LS-GAN: Human Motion Synthesis with Latent-space GANs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion synthesis conditioned on textual input has gained significant attention in recent years due to its potential applications in various domains such as gaming, film production, and virtual reality. Conditioned Motion synthesis takes a text input and outputs a 3D motion corresponding to the text. While previous works have explored motion synthesis using raw motion data and latent space representations with diffusion models, these approaches often suffer from high training and inference times. In this paper, we introduce a novel framework that utilizes Generative Adversarial Networks (GANs) in the latent space to enable faster training and inference while achieving results comparable to those of the state-of-the-art diffusion methods. We perform experiments on the HumanML3D, HumanAct12 benchmarks and demonstrate that a remarkably simple GAN in the latent space achieves a FID of 0.482 with more than 91% in FLOPs reduction compared to latent diffusion model. Our work opens up new possibilities for efficient and high-quality motion synthesis using latent space GANs.
<div id='section'>Paperid: <span id='pid'>1270, <a href='https://arxiv.org/pdf/2412.07922.pdf' target='_blank'>https://arxiv.org/pdf/2412.07922.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinyue Hu, Wei Ye, Jiaxiang Tang, Eman Ramadan, Zhi-Li Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.07922">Robust Multiple Description Neural Video Codec with Masked Transformer for Dynamic and Noisy Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multiple Description Coding (MDC) is a promising error-resilient source coding method that is particularly suitable for dynamic networks with multiple (yet noisy and unreliable) paths. However, conventional MDC video codecs suffer from cumbersome architectures, poor scalability, limited loss resilience, and lower compression efficiency. As a result, MDC has never been widely adopted. Inspired by the potential of neural video codecs, this paper rethinks MDC design. We propose a novel MDC video codec, NeuralMDC, demonstrating how bidirectional transformers trained for masked token prediction can vastly simplify the design of MDC video codec. To compress a video, NeuralMDC starts by tokenizing each frame into its latent representation and then splits the latent tokens to create multiple descriptions containing correlated information. Instead of using motion prediction and warping operations, NeuralMDC trains a bidirectional masked transformer to model the spatial-temporal dependencies of latent representations and predict the distribution of the current representation based on the past. The predicted distribution is used to independently entropy code each description and infer any potentially lost tokens. Extensive experiments demonstrate NeuralMDC achieves state-of-the-art loss resilience with minimal sacrifices in compression efficiency, significantly outperforming the best existing residual-coding-based error-resilient neural video codec.
<div id='section'>Paperid: <span id='pid'>1271, <a href='https://arxiv.org/pdf/2412.05560.pdf' target='_blank'>https://arxiv.org/pdf/2412.05560.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenqing Wang, Yun Fu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.05560">Text-to-3D Gaussian Splatting with Physics-Grounded Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-to-3D generation is a valuable technology in virtual reality and digital content creation. While recent works have pushed the boundaries of text-to-3D generation, producing high-fidelity 3D objects with inefficient prompts and simulating their physics-grounded motion accurately still remain unsolved challenges. To address these challenges, we present an innovative framework that utilizes the Large Language Model (LLM)-refined prompts and diffusion priors-guided Gaussian Splatting (GS) for generating 3D models with accurate appearances and geometric structures. We also incorporate a continuum mechanics-based deformation map and color regularization to synthesize vivid physics-grounded motion for the generated 3D Gaussians, adhering to the conservation of mass and momentum. By integrating text-to-3D generation with physics-grounded motion synthesis, our framework renders photo-realistic 3D objects that exhibit physics-aware motion, accurately reflecting the behaviors of the objects under various forces and constraints across different materials. Extensive experiments demonstrate that our approach achieves high-quality 3D generations with realistic physics-grounded motion.
<div id='section'>Paperid: <span id='pid'>1272, <a href='https://arxiv.org/pdf/2412.05460.pdf' target='_blank'>https://arxiv.org/pdf/2412.05460.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qihang Fang, Chengcheng Tang, Bugra Tekin, Yanchao Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.05460">CigTime: Corrective Instruction Generation Through Inverse Motion Editing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in models linking natural language with human motions have shown significant promise in motion generation and editing based on instructional text. Motivated by applications in sports coaching and motor skill learning, we investigate the inverse problem: generating corrective instructional text, leveraging motion editing and generation models. We introduce a novel approach that, given a user's current motion (source) and the desired motion (target), generates text instructions to guide the user towards achieving the target motion. We leverage large language models to generate corrective texts and utilize existing motion generation and editing frameworks to compile datasets of triplets (source motion, target motion, and corrective text). Using this data, we propose a new motion-language model for generating corrective instructions. We present both qualitative and quantitative results across a diverse range of applications that largely improve upon baselines. Our approach demonstrates its effectiveness in instructional scenarios, offering text-based guidance to correct and enhance user performance.
<div id='section'>Paperid: <span id='pid'>1273, <a href='https://arxiv.org/pdf/2412.00112.pdf' target='_blank'>https://arxiv.org/pdf/2412.00112.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Seong-Eun Hong, Soobin Lim, Juyeong Hwang, Minwook Chang, Hyeongyeop Kang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.00112">BiPO: Bidirectional Partial Occlusion Network for Text-to-Motion Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating natural and expressive human motions from textual descriptions is challenging due to the complexity of coordinating full-body dynamics and capturing nuanced motion patterns over extended sequences that accurately reflect the given text. To address this, we introduce BiPO, Bidirectional Partial Occlusion Network for Text-to-Motion Synthesis, a novel model that enhances text-to-motion synthesis by integrating part-based generation with a bidirectional autoregressive architecture. This integration allows BiPO to consider both past and future contexts during generation while enhancing detailed control over individual body parts without requiring ground-truth motion length. To relax the interdependency among body parts caused by the integration, we devise the Partial Occlusion technique, which probabilistically occludes the certain motion part information during training. In our comprehensive experiments, BiPO achieves state-of-the-art performance on the HumanML3D dataset, outperforming recent methods such as ParCo, MoMask, and BAMM in terms of FID scores and overall motion quality. Notably, BiPO excels not only in the text-to-motion generation task but also in motion editing tasks that synthesize motion based on partially generated motion sequences and textual descriptions. These results reveal the BiPO's effectiveness in advancing text-to-motion synthesis and its potential for practical applications.
<div id='section'>Paperid: <span id='pid'>1274, <a href='https://arxiv.org/pdf/2411.18281.pdf' target='_blank'>https://arxiv.org/pdf/2411.18281.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haopeng Fang, Di Qiu, Binjie Mao, Pengfei Yan, He Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.18281">MotionCharacter: Identity-Preserving and Motion Controllable Human Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in personalized Text-to-Video (T2V) generation highlight the importance of integrating character-specific identities and actions. However, previous T2V models struggle with identity consistency and controllable motion dynamics, mainly due to limited fine-grained facial and action-based textual prompts, and datasets that overlook key human attributes and actions. To address these challenges, we propose MotionCharacter, an efficient and high-fidelity human video generation framework designed for identity preservation and fine-grained motion control. We introduce an ID-preserving module to maintain identity fidelity while allowing flexible attribute modifications, and further integrate ID-consistency and region-aware loss mechanisms, significantly enhancing identity consistency and detail fidelity. Additionally, our approach incorporates a motion control module that prioritizes action-related text while maintaining subject consistency, along with a dataset, Human-Motion, which utilizes large language models to generate detailed motion descriptions. For simplify user control during inference, we parameterize motion intensity through a single coefficient, allowing for easy adjustments. Extensive experiments highlight the effectiveness of MotionCharacter, demonstrating significant improvements in ID-preserving, high-quality video generation.
<div id='section'>Paperid: <span id='pid'>1275, <a href='https://arxiv.org/pdf/2411.13327.pdf' target='_blank'>https://arxiv.org/pdf/2411.13327.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kilian Freitag, Yiannis Karayiannidis, Jan Zbinden, Rita Laezza
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.13327">Fine-tuning Myoelectric Control through Reinforcement Learning in a Game Environment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Objective: Enhancing the reliability of myoelectric controllers that decode motor intent is a pressing challenge in the field of bionic prosthetics. State-of-the-art research has mostly focused on Supervised Learning (SL) techniques to tackle this problem. However, obtaining high-quality labeled data that accurately represents muscle activity during daily usage remains difficult. We investigate the potential of Reinforcement Learning (RL) to further improve the decoding of human motion intent by incorporating usage-based data. Methods: The starting point of our method is a SL control policy, pretrained on a static recording of electromyographic (EMG) ground truth data. We then apply RL to fine-tune the pretrained classifier with dynamic EMG data obtained during interaction with a game environment developed for this work. We conducted real-time experiments to evaluate our approach and achieved significant improvements in human-in-the-loop performance. Results: The method effectively predicts simultaneous finger movements, leading to a two-fold increase in decoding accuracy during gameplay and a 39\% improvement in a separate motion test. Conclusion: By employing RL and incorporating usage-based EMG data during fine-tuning, our method achieves significant improvements in accuracy and robustness. Significance: These results showcase the potential of RL for enhancing the reliability of myoelectric controllers, of particular importance for advanced bionic limbs. See our project page for visual demonstrations: https://sites.google.com/view/bionic-limb-rl
<div id='section'>Paperid: <span id='pid'>1276, <a href='https://arxiv.org/pdf/2411.07261.pdf' target='_blank'>https://arxiv.org/pdf/2411.07261.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Arthur Candalot, James Hurrell, Malik Manel Hashim, Brigid Hickey, Mickael Laine, Kazuya Yoshida
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.07261">Sinkage Study in Granular Material for Space Exploration Legged Robot Gripper</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Wheeled rovers have been the primary choice for lunar exploration due to their speed and efficiency. However, deeper areas, such as lunar caves and craters, require the mobility of legged robots. To do so, appropriate end effectors must be designed to enable climbing and walking on the granular surface of the Moon. This paper investigates the behavior of an underactuated soft gripper on deformable granular material when a legged robot is walking in soft soil. A modular test bench and a simulation model were developed to observe the gripper sinkage behavior under load. The gripper uses tendon-driven fingers to match its target shape and grasp on the target surface using multiple micro-spines. The sinkage of the gripper in silica sand was measured by comparing the axial displacement of the gripper with the nominal load of the robot mass. Multiple experiments were performed to observe the sinkage of the gripper over a range of slope angles. A simulation model accounting for the degrees of compliance of the gripper fingers was created using Altair MotionSolve software and coupled to Altair EDEM to compute the gripper interaction with particles utilizing the discrete element method. After validation of the model, complementary simulations using Lunar gravity and a regolith particle model were performed. The results show that a satisfactory gripper model with accurate freedom of motion can be created in simulation using the Altair simulation packages and expected sinkage under load in a particle-filled environment can be estimated using this model. By computing the sinkage of the end effector of legged robots, the results can be directly integrated into the motion control algorithm and improve the accuracy of mobility in a granular material environment.
<div id='section'>Paperid: <span id='pid'>1277, <a href='https://arxiv.org/pdf/2410.15554.pdf' target='_blank'>https://arxiv.org/pdf/2410.15554.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhang Minghao, Song Bifeng, Yang Xiaojun, Wang Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.15554">A Plug-and-Play Fully On-the-Job Real-Time Reinforcement Learning Algorithm for a Direct-Drive Tandem-Wing Experiment Platforms Under Multiple Random Operating Conditions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The nonlinear and unstable aerodynamic interference generated by the tandem wings of such biomimetic systems poses substantial challenges for motion control, especially under multiple random operating conditions. To address these challenges, the Concerto Reinforcement Learning Extension (CRL2E) algorithm has been developed. This plug-and-play, fully on-the-job, real-time reinforcement learning algorithm incorporates a novel Physics-Inspired Rule-Based Policy Composer Strategy with a Perturbation Module alongside a lightweight network optimized for real-time control. To validate the performance and the rationality of the module design, experiments were conducted under six challenging operating conditions, comparing seven different algorithms. The results demonstrate that the CRL2E algorithm achieves safe and stable training within the first 500 steps, improving tracking accuracy by 14 to 66 times compared to the Soft Actor-Critic, Proximal Policy Optimization, and Twin Delayed Deep Deterministic Policy Gradient algorithms. Additionally, CRL2E significantly enhances performance under various random operating conditions, with improvements in tracking accuracy ranging from 8.3% to 60.4% compared to the Concerto Reinforcement Learning (CRL) algorithm. The convergence speed of CRL2E is 36.11% to 57.64% faster than the CRL algorithm with only the Composer Perturbation and 43.52% to 65.85% faster than the CRL algorithm when both the Composer Perturbation and Time-Interleaved Capability Perturbation are introduced, especially in conditions where the standard CRL struggles to converge. Hardware tests indicate that the optimized lightweight network structure excels in weight loading and average inference time, meeting real-time control requirements.
<div id='section'>Paperid: <span id='pid'>1278, <a href='https://arxiv.org/pdf/2410.01628.pdf' target='_blank'>https://arxiv.org/pdf/2410.01628.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aron Distelzweig, Andreas Look, Eitan Kosman, Faris JanjoÅ¡, JÃ¶rg Wagner, Abhinav Valada
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.01628">Stochasticity in Motion: An Information-Theoretic Approach to Trajectory Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In autonomous driving, accurate motion prediction is crucial for safe and efficient motion planning. To ensure safety, planners require reliable uncertainty estimates of the predicted behavior of surrounding agents, yet this aspect has received limited attention. In particular, decomposing uncertainty into its aleatoric and epistemic components is essential for distinguishing between inherent environmental randomness and model uncertainty, thereby enabling more robust and informed decision-making. This paper addresses the challenge of uncertainty modeling in trajectory prediction with a holistic approach that emphasizes uncertainty quantification, decomposition, and the impact of model composition. Our method, grounded in information theory, provides a theoretically principled way to measure uncertainty and decompose it into aleatoric and epistemic components. Unlike prior work, our approach is compatible with state-of-the-art motion predictors, allowing for broader applicability. We demonstrate its utility by conducting extensive experiments on the nuScenes dataset, which shows how different architectures and configurations influence uncertainty quantification and model robustness.
<div id='section'>Paperid: <span id='pid'>1279, <a href='https://arxiv.org/pdf/2409.15564.pdf' target='_blank'>https://arxiv.org/pdf/2409.15564.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xingrui Gu, Chuyi Jiang, Erte Wang, Qiang Cui, Leimin Tian, Lianlong Wu, Siyang Song, Chuang Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.15564">CauSkelNet: Causal Representation Learning for Human Behaviour Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Traditional machine learning methods for movement recognition often struggle with limited model interpretability and a lack of insight into human movement dynamics. This study introduces a novel representation learning framework based on causal inference to address these challenges. Our two-stage approach combines the Peter-Clark (PC) algorithm and Kullback-Leibler (KL) divergence to identify and quantify causal relationships between human joints. By capturing joint interactions, the proposed causal Graph Convolutional Network (GCN) produces interpretable and robust representations. Experimental results on the EmoPain dataset demonstrate that the causal GCN outperforms traditional GCNs in accuracy, F1 score, and recall, particularly in detecting protective behaviors. This work contributes to advancing human motion analysis and lays a foundation for adaptive and intelligent healthcare solutions.
<div id='section'>Paperid: <span id='pid'>1280, <a href='https://arxiv.org/pdf/2409.00449.pdf' target='_blank'>https://arxiv.org/pdf/2409.00449.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Longyun Liao, Rong Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.00449">ActionPose: Pretraining 3D Human Pose Estimation with the Dark Knowledge of Action</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>2D-to-3D human pose lifting is an ill-posed problem due to depth ambiguity and occlusion. Existing methods relying on spatial and temporal consistency alone are insufficient to resolve these problems because they lack semantic information of the motions. To overcome this, we propose ActionPose, a framework that leverages action knowledge by aligning motion embeddings with text embeddings of fine-grained action labels. ActionPose operates in two stages: pretraining and fine-tuning. In the pretraining stage, the model learns to recognize actions and reconstruct 3D poses from masked and noisy 2D poses. During the fine-tuning stage, the model is further refined using real-world 3D human pose estimation datasets without action labels. Additionally, our framework incorporates masked body parts and masked time windows in motion modeling to mitigate the effects of ambiguous boundaries between actions in both temporal and spatial domains. Experiments demonstrate the effectiveness of ActionPose, achieving state-of-the-art performance in 3D pose estimation on public datasets, including Human3.6M and MPI-INF-3DHP. Specifically, ActionPose achieves an MPJPE of 36.7mm on Human3.6M with detected 2D poses as input and 15.5mm on MPI-INF-3DHP with ground-truth 2D poses as input.
<div id='section'>Paperid: <span id='pid'>1281, <a href='https://arxiv.org/pdf/2408.09409.pdf' target='_blank'>https://arxiv.org/pdf/2408.09409.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chen Long-fei, Subramanian Ramamoorthy, Robert B Fisher
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.09409">OPPH: A Vision-Based Operator for Measuring Body Movements for Personal Healthcare</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-based motion estimation methods show promise in accurately and unobtrusively estimating human body motion for healthcare purposes. However, these methods are not specifically designed for healthcare purposes and face challenges in real-world applications. Human pose estimation methods often lack the accuracy needed for detecting fine-grained, subtle body movements, while optical flow-based methods struggle with poor lighting conditions and unseen real-world data. These issues result in human body motion estimation errors, particularly during critical medical situations where the body is motionless, such as during unconsciousness. To address these challenges and improve the accuracy of human body motion estimation for healthcare purposes, we propose the OPPH operator designed to enhance current vision-based motion estimation methods. This operator, which considers human body movement and noise properties, functions as a multi-stage filter. Results tested on two real-world and one synthetic human motion dataset demonstrate that the operator effectively removes real-world noise, significantly enhances the detection of motionless states, maintains the accuracy of estimating active body movements, and maintains long-term body movement trends. This method could be beneficial for analyzing both critical medical events and chronic medical conditions.
<div id='section'>Paperid: <span id='pid'>1282, <a href='https://arxiv.org/pdf/2408.09178.pdf' target='_blank'>https://arxiv.org/pdf/2408.09178.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Changcheng Xiao, Qiong Cao, Zhigang Luo, Long Lan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.09178">MambaTrack: A Simple Baseline for Multiple Object Tracking with State Space Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tracking by detection has been the prevailing paradigm in the field of Multi-object Tracking (MOT). These methods typically rely on the Kalman Filter to estimate the future locations of objects, assuming linear object motion. However, they fall short when tracking objects exhibiting nonlinear and diverse motion in scenarios like dancing and sports. In addition, there has been limited focus on utilizing learning-based motion predictors in MOT. To address these challenges, we resort to exploring data-driven motion prediction methods. Inspired by the great expectation of state space models (SSMs), such as Mamba, in long-term sequence modeling with near-linear complexity, we introduce a Mamba-based motion model named Mamba moTion Predictor (MTP). MTP is designed to model the complex motion patterns of objects like dancers and athletes. Specifically, MTP takes the spatial-temporal location dynamics of objects as input, captures the motion pattern using a bi-Mamba encoding layer, and predicts the next motion. In real-world scenarios, objects may be missed due to occlusion or motion blur, leading to premature termination of their trajectories. To tackle this challenge, we further expand the application of MTP. We employ it in an autoregressive way to compensate for missing observations by utilizing its own predictions as inputs, thereby contributing to more consistent trajectories. Our proposed tracker, MambaTrack, demonstrates advanced performance on benchmarks such as Dancetrack and SportsMOT, which are characterized by complex motion and severe occlusion.
<div id='section'>Paperid: <span id='pid'>1283, <a href='https://arxiv.org/pdf/2407.15675.pdf' target='_blank'>https://arxiv.org/pdf/2407.15675.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rabbia Asghar, Wenqian Liu, Lukas Rummelhard, Anne Spalanzani, Christian Laugier
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.15675">Flow-guided Motion Prediction with Semantics and Dynamic Occupancy Grid Maps</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate prediction of driving scenes is essential for road safety and autonomous driving. Occupancy Grid Maps (OGMs) are commonly employed for scene prediction due to their structured spatial representation, flexibility across sensor modalities and integration of uncertainty. Recent studies have successfully combined OGMs with deep learning methods to predict the evolution of scene and learn complex behaviours. These methods, however, do not consider prediction of flow or velocity vectors in the scene. In this work, we propose a novel multi-task framework that leverages dynamic OGMs and semantic information to predict both future vehicle semantic grids and the future flow of the scene. This incorporation of semantic flow not only offers intermediate scene features but also enables the generation of warped semantic grids. Evaluation on the real-world NuScenes dataset demonstrates improved prediction capabilities and enhanced ability of the model to retain dynamic vehicles within the scene.
<div id='section'>Paperid: <span id='pid'>1284, <a href='https://arxiv.org/pdf/2407.15408.pdf' target='_blank'>https://arxiv.org/pdf/2407.15408.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kent Fujiwara, Mikihiro Tanaka, Qing Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.15408">Chronologically Accurate Retrieval for Temporal Grounding of Motion-Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the release of large-scale motion datasets with textual annotations, the task of establishing a robust latent space for language and 3D human motion has recently witnessed a surge of interest. Methods have been proposed to convert human motion and texts into features to achieve accurate correspondence between them. Despite these efforts to align language and motion representations, we claim that the temporal element is often overlooked, especially for compound actions, resulting in chronological inaccuracies. To shed light on the temporal alignment in motion-language latent spaces, we propose Chronologically Accurate Retrieval (CAR) to evaluate the chronological understanding of the models. We decompose textual descriptions into events, and prepare negative text samples by shuffling the order of events in compound action descriptions. We then design a simple task for motion-language models to retrieve the more likely text from the ground truth and its chronologically shuffled version. CAR reveals many cases where current motion-language models fail to distinguish the event chronology of human motion, despite their impressive performance in terms of conventional evaluation metrics. To achieve better temporal alignment between text and motion, we further propose to use these texts with shuffled sequence of events as negative samples during training to reinforce the motion-language models. We conduct experiments on text-motion retrieval and text-to-motion generation using the reinforced motion-language models, which demonstrate improved performance over conventional approaches, indicating the necessity to consider temporal elements in motion-language alignment.
<div id='section'>Paperid: <span id='pid'>1285, <a href='https://arxiv.org/pdf/2407.05376.pdf' target='_blank'>https://arxiv.org/pdf/2407.05376.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiayu Guo, Mingyue Feng, Pengfei Zhu, Chengjun Li, Jian Pu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.05376">Rethinking Closed-loop Planning Framework for Imitation-based Model Integrating Prediction and Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, the integration of prediction and planning through neural networks has received substantial attention. Despite extensive studies on it, there is a noticeable gap in understanding the operation of such models within a closed-loop planning setting. To bridge this gap, we propose a novel closed-loop planning framework compatible with neural networks engaged in joint prediction and planning. The framework contains two running modes, namely planning and safety monitoring, wherein the neural network performs Motion Prediction and Planning (MPP) and Conditional Motion Prediction (CMP) correspondingly without altering architecture. We evaluate the efficacy of our framework using the nuPlan dataset and its simulator, conducting closed-loop experiments across diverse scenarios. The results demonstrate that the proposed framework ensures the feasibility and local stability of the planning process while maintaining safety with CMP safety monitoring. Compared to other learning-based methods, our approach achieves substantial improvement.
<div id='section'>Paperid: <span id='pid'>1286, <a href='https://arxiv.org/pdf/2406.18159.pdf' target='_blank'>https://arxiv.org/pdf/2406.18159.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaolin Hong, Hongwei Yi, Fazhi He, Qiong Cao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.18159">Human-Aware 3D Scene Generation with Spatially-constrained Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating 3D scenes from human motion sequences supports numerous applications, including virtual reality and architectural design. However, previous auto-regression-based human-aware 3D scene generation methods have struggled to accurately capture the joint distribution of multiple objects and input humans, often resulting in overlapping object generation in the same space. To address this limitation, we explore the potential of diffusion models that simultaneously consider all input humans and the floor plan to generate plausible 3D scenes. Our approach not only satisfies all input human interactions but also adheres to spatial constraints with the floor plan. Furthermore, we introduce two spatial collision guidance mechanisms: human-object collision avoidance and object-room boundary constraints. These mechanisms help avoid generating scenes that conflict with human motions while respecting layout constraints. To enhance the diversity and accuracy of human-guided scene generation, we have developed an automated pipeline that improves the variety and plausibility of human-object interactions in the existing 3D FRONT HUMAN dataset. Extensive experiments on both synthetic and real-world datasets demonstrate that our framework can generate more natural and plausible 3D scenes with precise human-scene interactions, while significantly reducing human-object collisions compared to previous state-of-the-art methods. Our code and data will be made publicly available upon publication of this work.
<div id='section'>Paperid: <span id='pid'>1287, <a href='https://arxiv.org/pdf/2406.00960.pdf' target='_blank'>https://arxiv.org/pdf/2406.00960.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Takara E. Truong, Michael Piseno, Zhaoming Xie, C. Karen Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.00960">PDP: Physics-Based Character Animation via Diffusion Policy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating diverse and realistic human motion that can physically interact with an environment remains a challenging research area in character animation. Meanwhile, diffusion-based methods, as proposed by the robotics community, have demonstrated the ability to capture highly diverse and multi-modal skills. However, naively training a diffusion policy often results in unstable motions for high-frequency, under-actuated control tasks like bipedal locomotion due to rapidly accumulating compounding errors, pushing the agent away from optimal training trajectories. The key idea lies in using RL policies not just for providing optimal trajectories but for providing corrective actions in sub-optimal states, giving the policy a chance to correct for errors caused by environmental stimulus, model errors, or numerical errors in simulation. Our method, Physics-Based Character Animation via Diffusion Policy (PDP), combines reinforcement learning (RL) and behavior cloning (BC) to create a robust diffusion policy for physics-based character animation. We demonstrate PDP on perturbation recovery, universal motion tracking, and physics-based text-to-motion synthesis.
<div id='section'>Paperid: <span id='pid'>1288, <a href='https://arxiv.org/pdf/2406.00211.pdf' target='_blank'>https://arxiv.org/pdf/2406.00211.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yufei Huang, Yulin Li, Andrea Matta, Mohsen Jafari
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.00211">Navigating Autonomous Vehicle on Unmarked Roads with Diffusion-Based Motion Prediction and Active Inference</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a novel approach to improving autonomous vehicle control in environments lacking clear road markings by integrating a diffusion-based motion predictor within an Active Inference Framework (AIF). Using a simulated parking lot environment as a parallel to unmarked roads, we develop and test our model to predict and guide vehicle movements effectively. The diffusion-based motion predictor forecasts vehicle actions by leveraging probabilistic dynamics, while AIF aids in decision-making under uncertainty. Unlike traditional methods such as Model Predictive Control (MPC) and Reinforcement Learning (RL), our approach reduces computational demands and requires less extensive training, enhancing navigation safety and efficiency. Our results demonstrate the model's capability to navigate complex scenarios, marking significant progress in autonomous driving technology.
<div id='section'>Paperid: <span id='pid'>1289, <a href='https://arxiv.org/pdf/2405.18438.pdf' target='_blank'>https://arxiv.org/pdf/2405.18438.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>ZoltÃ¡n Ã. Milacski, Koichiro Niinuma, Ryosuke Kawamura, Fernando de la Torre, LÃ¡szlÃ³ A. Jeni
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.18438">GHOST: Grounded Human Motion Generation with Open Vocabulary Scene-and-Text Contexts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The connection between our 3D surroundings and the descriptive language that characterizes them would be well-suited for localizing and generating human motion in context but for one problem. The complexity introduced by multiple modalities makes capturing this connection challenging with a fixed set of descriptors. Specifically, closed vocabulary scene encoders, which require learning text-scene associations from scratch, have been favored in the literature, often resulting in inaccurate motion grounding. In this paper, we propose a method that integrates an open vocabulary scene encoder into the architecture, establishing a robust connection between text and scene. Our two-step approach starts with pretraining the scene encoder through knowledge distillation from an existing open vocabulary semantic image segmentation model, ensuring a shared text-scene feature space. Subsequently, the scene encoder is fine-tuned for conditional motion generation, incorporating two novel regularization losses that regress the category and size of the goal object. Our methodology achieves up to a 30% reduction in the goal object distance metric compared to the prior state-of-the-art baseline model on the HUMANISE dataset. This improvement is demonstrated through evaluations conducted using three implementations of our framework and a perceptual study. Additionally, our method is designed to seamlessly accommodate future 2D segmentation methods that provide per-pixel text-aligned features for distillation.
<div id='section'>Paperid: <span id='pid'>1290, <a href='https://arxiv.org/pdf/2405.12408.pdf' target='_blank'>https://arxiv.org/pdf/2405.12408.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinhao Liu, Jun Yang, Jianliang Mao, Tianqi Zhu, Qihang Xie, Yimeng Li, Xiangyu Wang, Shihua Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.12408">Flexible Active Safety Motion Control for Robotic Obstacle Avoidance: A CBF-Guided MPC Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A flexible active safety motion (FASM) control approach is proposed for the avoidance of dynamic obstacles and the reference tracking in robot manipulators. The distinctive feature of the proposed method lies in its utilization of control barrier functions (CBF) to design flexible CBF-guided safety criteria (CBFSC) with dynamically optimized decay rates, thereby offering flexibility and active safety for robot manipulators in dynamic environments. First, discrete-time CBFs are employed to formulate the novel flexible CBFSC with dynamic decay rates for robot manipulators. Following that, the model predictive control (MPC) philosophy is applied, integrating flexible CBFSC as safety constraints into the receding-horizon optimization problem. Significantly, the decay rates of the designed CBFSC are incorporated as decision variables in the optimization problem, facilitating the dynamic enhancement of flexibility during the obstacle avoidance process. In particular, a novel cost function that integrates a penalty term is designed to dynamically adjust the safety margins of the CBFSC. Finally, experiments are conducted in various scenarios using a Universal Robots 5 (UR5) manipulator to validate the effectiveness of the proposed approach.
<div id='section'>Paperid: <span id='pid'>1291, <a href='https://arxiv.org/pdf/2405.10134.pdf' target='_blank'>https://arxiv.org/pdf/2405.10134.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tobias Demmler, Andreas Tamke, Thao Dang, Karsten Haug, Lars Mikelsons
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.10134">Towards Consistent and Explainable Motion Prediction using Heterogeneous Graph Attention</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In autonomous driving, accurately interpreting the movements of other road users and leveraging this knowledge to forecast future trajectories is crucial. This is typically achieved through the integration of map data and tracked trajectories of various agents. Numerous methodologies combine this information into a singular embedding for each agent, which is then utilized to predict future behavior. However, these approaches have a notable drawback in that they may lose exact location information during the encoding process. The encoding still includes general map information. However, the generation of valid and consistent trajectories is not guaranteed. This can cause the predicted trajectories to stray from the actual lanes. This paper introduces a new refinement module designed to project the predicted trajectories back onto the actual map, rectifying these discrepancies and leading towards more consistent predictions. This versatile module can be readily incorporated into a wide range of architectures. Additionally, we propose a novel scene encoder that handles all relations between agents and their environment in a single unified heterogeneous graph attention network. By analyzing the attention values on the different edges in this graph, we can gain unique insights into the neural network's inner workings leading towards a more explainable prediction.
<div id='section'>Paperid: <span id='pid'>1292, <a href='https://arxiv.org/pdf/2405.04771.pdf' target='_blank'>https://arxiv.org/pdf/2405.04771.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qing Yu, Mikihiro Tanaka, Kent Fujiwara
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.04771">Exploring Vision Transformers for 3D Human Motion-Language Models with Motion Patches</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To build a cross-modal latent space between 3D human motion and language, acquiring large-scale and high-quality human motion data is crucial. However, unlike the abundance of image data, the scarcity of motion data has limited the performance of existing motion-language models. To counter this, we introduce "motion patches", a new representation of motion sequences, and propose using Vision Transformers (ViT) as motion encoders via transfer learning, aiming to extract useful knowledge from the image domain and apply it to the motion domain. These motion patches, created by dividing and sorting skeleton joints based on body parts in motion sequences, are robust to varying skeleton structures, and can be regarded as color image patches in ViT. We find that transfer learning with pre-trained weights of ViT obtained through training with 2D image data can boost the performance of motion analysis, presenting a promising direction for addressing the issue of limited motion data. Our extensive experiments show that the proposed motion patches, used jointly with ViT, achieve state-of-the-art performance in the benchmarks of text-to-motion retrieval, and other novel challenging tasks, such as cross-skeleton recognition, zero-shot motion classification, and human interaction recognition, which are currently impeded by the lack of data.
<div id='section'>Paperid: <span id='pid'>1293, <a href='https://arxiv.org/pdf/2404.13915.pdf' target='_blank'>https://arxiv.org/pdf/2404.13915.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiyuan Lu, Muhammad Hanif, Takumi Shimizu, Takeshi Hatanaka
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.13915">Angle-Aware Coverage with Camera Rotational Motion Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a novel control strategy for drone networks to improve the quality of 3D structures reconstructed from aerial images by drones. Unlike the existing coverage control strategies for this purpose, our proposed approach simultaneously controls both the camera orientation and drone translational motion, enabling more comprehensive perspectives and enhancing the map's overall quality. Subsequently, we present a novel problem formulation, including a new performance function to evaluate the drone positions and camera orientations. We then design a QP-based controller with a control barrier-like function for a constraint on the decay rate of the objective function. The present problem formulation poses a new challenge, requiring significantly greater computational efforts than the case involving only translational motion control. We approach this issue technologically, namely by introducing JAX, utilizing just-in-time (JIT) compilation and Graphical Processing Unit (GPU) acceleration. We finally conduct extensive verifications through simulation in ROS (Robot Operating System) and show the real-time feasibility of the controller and the superiority of the present controller to the conventional method.
<div id='section'>Paperid: <span id='pid'>1294, <a href='https://arxiv.org/pdf/2404.03584.pdf' target='_blank'>https://arxiv.org/pdf/2404.03584.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pengxiang Ding, Jianqin Yin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.03584">Towards more realistic human motion prediction with attention to motion coordination</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Joint relation modeling is a curial component in human motion prediction. Most existing methods rely on skeletal-based graphs to build the joint relations, where local interactive relations between joint pairs are well learned. However, the motion coordination, a global joint relation reflecting the simultaneous cooperation of all joints, is usually weakened because it is learned from part to whole progressively and asynchronously. Thus, the final predicted motions usually appear unrealistic. To tackle this issue, we learn a medium, called coordination attractor (CA), from the spatiotemporal features of motion to characterize the global motion features, which is subsequently used to build new relative joint relations. Through the CA, all joints are related simultaneously, and thus the motion coordination of all joints can be better learned. Based on this, we further propose a novel joint relation modeling module, Comprehensive Joint Relation Extractor (CJRE), to combine this motion coordination with the local interactions between joint pairs in a unified manner. Additionally, we also present a Multi-timescale Dynamics Extractor (MTDE) to extract enriched dynamics from the raw position information for effective prediction. Extensive experiments show that the proposed framework outperforms state-of-the-art methods in both short- and long-term predictions on H3.6M, CMU-Mocap, and 3DPW.
<div id='section'>Paperid: <span id='pid'>1295, <a href='https://arxiv.org/pdf/2403.18695.pdf' target='_blank'>https://arxiv.org/pdf/2403.18695.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Luyao Zhang, George Pantazis, Shaohang Han, Sergio Grammatico
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.18695">An Efficient Risk-aware Branch MPC for Automated Driving that is Robust to Uncertain Vehicle Behaviors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>One of the critical challenges in automated driving is ensuring safety of automated vehicles despite the unknown behavior of the other vehicles. Although motion prediction modules are able to generate a probability distribution associated with various behavior modes, their probabilistic estimates are often inaccurate, thus leading to a possibly unsafe trajectory. To overcome this challenge, we propose a risk-aware motion planning framework that appropriately accounts for the ambiguity in the estimated probability distribution. We formulate the risk-aware motion planning problem as a min-max optimization problem and develop an efficient iterative method by incorporating a regularization term in the probability update step. Via extensive numerical studies, we validate the convergence of our method and demonstrate its advantages compared to the state-of-the-art approaches.
<div id='section'>Paperid: <span id='pid'>1296, <a href='https://arxiv.org/pdf/2403.08363.pdf' target='_blank'>https://arxiv.org/pdf/2403.08363.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Karthikeya Puttur Venkatraj, Wo Meijer, Monica PerusquÃ­a-HernÃ¡ndez, Gijs Huisman, Abdallah El Ali
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.08363">ShareYourReality: Investigating Haptic Feedback and Agency in Virtual Avatar Co-embodiment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Virtual co-embodiment enables two users to share a single avatar in Virtual Reality (VR). During such experiences, the illusion of shared motion control can break during joint-action activities, highlighting the need for position-aware feedback mechanisms. Drawing on the perceptual crossing paradigm, we explore how haptics can enable non-verbal coordination between co-embodied participants. In a within-subjects study (20 participant pairs), we examined the effects of vibrotactile haptic feedback (None, Present) and avatar control distribution (25-75%, 50-50%, 75-25%) across two VR reaching tasks (Targeted, Free-choice) on participants Sense of Agency (SoA), co-presence, body ownership, and motion synchrony. We found (a) lower SoA in the free-choice with haptics than without, (b) higher SoA during the shared targeted task, (c) co-presence and body ownership were significantly higher in the free-choice task, (d) players hand motions synchronized more in the targeted task. We provide cautionary considerations when including haptic feedback mechanisms for avatar co-embodiment experiences.
<div id='section'>Paperid: <span id='pid'>1297, <a href='https://arxiv.org/pdf/2403.05478.pdf' target='_blank'>https://arxiv.org/pdf/2403.05478.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mengsha Hu, Jinzhou Li, Runxiang Jin, Chao Shi, Lei Xu, Rui Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.05478">HGIC: A Hand Gesture Based Interactive Control System for Efficient and Scalable Multi-UAV Operations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As technological advancements continue to expand the capabilities of multi unmanned-aerial-vehicle systems (mUAV), human operators face challenges in scalability and efficiency due to the complex cognitive load and operations associated with motion adjustments and team coordination. Such cognitive demands limit the feasible size of mUAV teams and necessitate extensive operator training, impeding broader adoption. This paper developed a Hand Gesture Based Interactive Control (HGIC), a novel interface system that utilize computer vision techniques to intuitively translate hand gestures into modular commands for robot teaming. Through learning control models, these commands enable efficient and scalable mUAV motion control and adjustments. HGIC eliminates the need for specialized hardware and offers two key benefits: 1) Minimal training requirements through natural gestures; and 2) Enhanced scalability and efficiency via adaptable commands. By reducing the cognitive burden on operators, HGIC opens the door for more effective large-scale mUAV applications in complex, dynamic, and uncertain scenarios. HGIC will be open-sourced after the paper being published online for the research community, aiming to drive forward innovations in human-mUAV interactions.
<div id='section'>Paperid: <span id='pid'>1298, <a href='https://arxiv.org/pdf/2403.03681.pdf' target='_blank'>https://arxiv.org/pdf/2403.03681.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chuanyu Luo, Nuo Cheng, Ren Zhong, Haipeng Jiang, Wenyu Chen, Aoli Wang, Pu Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.03681">3D Object Visibility Prediction in Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapid advancement of hardware and software technologies, research in autonomous driving has seen significant growth. The prevailing framework for multi-sensor autonomous driving encompasses sensor installation, perception, path planning, decision-making, and motion control. At the perception phase, a common approach involves utilizing neural networks to infer 3D bounding box (Bbox) attributes from raw sensor data, including classification, size, and orientation. In this paper, we present a novel attribute and its corresponding algorithm: 3D object visibility. By incorporating multi-task learning, the introduction of this attribute, visibility, negligibly affects the model's effectiveness and efficiency. Our proposal of this attribute and its computational strategy aims to expand the capabilities for downstream tasks, thereby enhancing the safety and reliability of real-time autonomous driving in real-world scenarios.
<div id='section'>Paperid: <span id='pid'>1299, <a href='https://arxiv.org/pdf/2402.17790.pdf' target='_blank'>https://arxiv.org/pdf/2402.17790.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Niklas Kueper, Su Kyoung Kim, Elsa Andrea Kirchner
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.17790">EEG classifier cross-task transfer to avoid training sessions in robot-assisted rehabilitation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Background: For an individualized support of patients during rehabilitation, learning of individual machine learning models from the human electroencephalogram (EEG) is required. Our approach allows labeled training data to be recorded without the need for a specific training session. For this, the planned exoskeleton-assisted rehabilitation enables bilateral mirror therapy, in which movement intentions can be inferred from the activity of the unaffected arm. During this therapy, labeled EEG data can be collected to enable movement predictions of only the affected arm of a patient. Methods: A study was conducted with 8 healthy subjects and the performance of the classifier transfer approach was evaluated. Each subject performed 3 runs of 40 self-intended unilateral and bilateral reaching movements toward a target while EEG data was recorded from 64 channels. A support vector machine (SVM) classifier was trained under both movement conditions to make predictions for the same type of movement. Furthermore, the classifier was evaluated to predict unilateral movements by only beeing trained on the data of the bilateral movement condition. Results: The results show that the performance of the classifier trained on selected EEG channels evoked by bilateral movement intentions is not significantly reduced compared to a classifier trained directly on EEG data including unilateral movement intentions. Moreover, the results show that our approach also works with only 8 or even 4 channels. Conclusion: It was shown that the proposed classifier transfer approach enables motion prediction without explicit collection of training data. Since the approach can be applied even with a small number of EEG channels, this speaks for the feasibility of the approach in real therapy sessions with patients and motivates further investigations with stroke patients.
<div id='section'>Paperid: <span id='pid'>1300, <a href='https://arxiv.org/pdf/2402.05115.pdf' target='_blank'>https://arxiv.org/pdf/2402.05115.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Louis Annabi, Ziqi Ma, Sao Mai Nguyen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.05115">Unsupervised Motion Retargeting for Human-Robot Imitation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This early-stage research work aims to improve online human-robot imitation by translating sequences of joint positions from the domain of human motions to a domain of motions achievable by a given robot, thus constrained by its embodiment. Leveraging the generalization capabilities of deep learning methods, we address this problem by proposing an encoder-decoder neural network model performing domain-to-domain translation. In order to train such a model, one could use pairs of associated robot and human motions. Though, such paired data is extremely rare in practice, and tedious to collect. Therefore, we turn towards deep learning methods for unpaired domain-to-domain translation, that we adapt in order to perform human-robot imitation.
<div id='section'>Paperid: <span id='pid'>1301, <a href='https://arxiv.org/pdf/2402.04820.pdf' target='_blank'>https://arxiv.org/pdf/2402.04820.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Arjun S. Lakshmipathy, Jessica K. Hodgins, Nancy S. Pollard
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.04820">Kinematic Motion Retargeting for Contact-Rich Anthropomorphic Manipulations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Hand motion capture data is now relatively easy to obtain, even for complicated grasps; however this data is of limited use without the ability to retarget it onto the hands of a specific character or robot. The target hand may differ dramatically in geometry, number of degrees of freedom (DOFs), or number of fingers. We present a simple, but effective framework capable of kinematically retargeting multiple human hand-object manipulations from a publicly available dataset to a wide assortment of kinematically and morphologically diverse target hands through the exploitation of contact areas. We do so by formulating the retarget operation as a non-isometric shape matching problem and use a combination of both surface contact and marker data to progressively estimate, refine, and fit the final target hand trajectory using inverse kinematics (IK). Foundational to our framework is the introduction of a novel shape matching process, which we show enables predictable and robust transfer of contact data over full manipulations while providing an intuitive means for artists to specify correspondences with relatively few inputs. We validate our framework through thirty demonstrations across five different hand shapes and six motions of different objects. We additionally compare our method against existing hand retargeting approaches. Finally, we demonstrate our method enabling novel capabilities such as object substitution and the ability to visualize the impact of design choices over full trajectories.
<div id='section'>Paperid: <span id='pid'>1302, <a href='https://arxiv.org/pdf/2401.13414.pdf' target='_blank'>https://arxiv.org/pdf/2401.13414.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xingyu Song, Zhan Li, Shi Chen, Kazuyuki Demachi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.13414">GTAutoAct: An Automatic Datasets Generation Framework Based on Game Engine Redevelopment for Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current datasets for action recognition tasks face limitations stemming from traditional collection and generation methods, including the constrained range of action classes, absence of multi-viewpoint recordings, limited diversity, poor video quality, and labor-intensive manually collection. To address these challenges, we introduce GTAutoAct, a innovative dataset generation framework leveraging game engine technology to facilitate advancements in action recognition. GTAutoAct excels in automatically creating large-scale, well-annotated datasets with extensive action classes and superior video quality. Our framework's distinctive contributions encompass: (1) it innovatively transforms readily available coordinate-based 3D human motion into rotation-orientated representation with enhanced suitability in multiple viewpoints; (2) it employs dynamic segmentation and interpolation of rotation sequences to create smooth and realistic animations of action; (3) it offers extensively customizable animation scenes; (4) it implements an autonomous video capture and processing pipeline, featuring a randomly navigating camera, with auto-trimming and labeling functionalities. Experimental results underscore the framework's robustness and highlights its potential to significantly improve action recognition model training.
<div id='section'>Paperid: <span id='pid'>1303, <a href='https://arxiv.org/pdf/2401.02899.pdf' target='_blank'>https://arxiv.org/pdf/2401.02899.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Luka LanÄa, Karlo Jakac, Stefan IviÄ
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.02899">Model predictive altitude and velocity control in ergodic potential field directed multi-UAV search</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This research addresses the challenge of executing multi-UAV survey missions over diverse terrains characterized by varying elevations. The approach integrates advanced two-dimensional ergodic search technique with model predictive control of UAV altitude and velocity. Optimization of altitude and velocity is performed along anticipated UAV ground routes, considering multiple objectives and constraints. This yields a flight regimen tailored to the terrain, as well as the motion and sensing characteristics of the UAVs. The proposed UAV motion control strategy is assessed through simulations of realistic search missions and actual terrain models. Results demonstrate the successful integration of model predictive altitude and velocity control with a two-dimensional potential field-guided ergodic search. Adjusting UAV altitudes to near-ideal levels facilitates the utilization of sensing ranges, thereby enhancing the effectiveness of the search. Furthermore, the control algorithm is capable of real-time computation, encouraging its practical application in real-world scenarios.
<div id='section'>Paperid: <span id='pid'>1304, <a href='https://arxiv.org/pdf/2512.22249.pdf' target='_blank'>https://arxiv.org/pdf/2512.22249.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zheng Xing, Weibing Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.22249">Temporal Visual Semantics-Induced Human Motion Understanding with Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unsupervised human motion segmentation (HMS) can be effectively achieved using subspace clustering techniques. However, traditional methods overlook the role of temporal semantic exploration in HMS. This paper explores the use of temporal vision semantics (TVS) derived from human motion sequences, leveraging the image-to-text capabilities of a large language model (LLM) to enhance subspace clustering performance. The core idea is to extract textual motion information from consecutive frames via LLM and incorporate this learned information into the subspace clustering framework. The primary challenge lies in learning TVS from human motion sequences using LLM and integrating this information into subspace clustering. To address this, we determine whether consecutive frames depict the same motion by querying the LLM and subsequently learn temporal neighboring information based on its response. We then develop a TVS-integrated subspace clustering approach, incorporating subspace embedding with a temporal regularizer that induces each frame to share similar subspace embeddings with its temporal neighbors. Additionally, segmentation is performed based on subspace embedding with a temporal constraint that induces the grouping of each frame with its temporal neighbors. We also introduce a feedback-enabled framework that continuously optimizes subspace embedding based on the segmentation output. Experimental results demonstrate that the proposed method outperforms existing state-of-the-art approaches on four benchmark human motion datasets.
<div id='section'>Paperid: <span id='pid'>1305, <a href='https://arxiv.org/pdf/2512.17215.pdf' target='_blank'>https://arxiv.org/pdf/2512.17215.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yan Gao, Jiliang Wang, Minghan Wang, Xiaohua Chen, Demin Chen, Zhiyong Ren, Tian-Yun Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.17215">Research on Dead Reckoning Algorithm for Self-Propelled Pipeline Robots in Three-Dimensional Complex Pipelines</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the field of gas pipeline location, existing pipeline location methods mostly rely on pipeline location instruments. However, when faced with complex and curved pipeline scenarios, these methods often fail due to problems such as cable entanglement and insufficient equipment flexibility. To address this pain point, we designed a self-propelled pipeline robot. This robot can autonomously complete the location work of complex and curved pipelines in complex pipe networks without external dragging. In terms of pipeline mapping technology, traditional visual mapping and laser mapping methods are easily affected by lighting conditions and insufficient features in the confined space of pipelines, resulting in mapping drift and divergence problems. In contrast, the pipeline location method that integrates inertial navigation and wheel odometers is less affected by pipeline environmental factors. Based on this, this paper proposes a pipeline robot location method based on extended Kalman filtering (EKF). Firstly, the body attitude angle is initially obtained through an inertial measurement unit (IMU). Then, the extended Kalman filtering algorithm is used to improve the accuracy of attitude angle estimation. Finally, high-precision pipeline location is achieved by combining wheel odometers. During the testing phase, the roll wheels of the pipeline robot needed to fit tightly against the pipe wall to reduce slippage. However, excessive tightness would reduce the flexibility of motion control due to excessive friction. Therefore, a balance needed to be struck between the robot's motion capability and positioning accuracy. Experiments were conducted using the self-propelled pipeline robot in a rectangular loop pipeline, and the results verified the effectiveness of the proposed dead reckoning algorithm.
<div id='section'>Paperid: <span id='pid'>1306, <a href='https://arxiv.org/pdf/2512.17212.pdf' target='_blank'>https://arxiv.org/pdf/2512.17212.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yan Gao, Jiliang Wang, Ming Cheng, Tianyun Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.17212">Design and Research of a Self-Propelled Pipeline Robot Based on Force Analysis and Dynamic Simulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In pipeline inspection, traditional tethered inspection robots are severely constrained by cable length and weight, which greatly limit their travel range and accessibility. To address these issues, this paper proposes a self-propelled pipeline robot design based on force analysis and dynamic simulation, with a specific focus on solving core challenges including vertical climbing failure and poor passability in T-branch pipes. Adopting a wheeled configuration and modular design, the robot prioritizes the core demand of body motion control. Specifically, 3D modeling of the robot was first completed using SolidWorks. Subsequently, the model was imported into ADAMS for dynamic simulation, which provided a basis for optimizing the drive module and motion control strategy.To verify the robot's dynamic performance, an experimental platform with acrylic pipes was constructed. Through adjusting its body posture to surmount obstacles and select directions, the robot has demonstrated its ability to stably traverse various complex pipeline scenarios. Notably, this work offers a technical feasibility reference for the application of pipeline robots in the inspection of medium and low-pressure urban gas pipelines.
<div id='section'>Paperid: <span id='pid'>1307, <a href='https://arxiv.org/pdf/2512.13056.pdf' target='_blank'>https://arxiv.org/pdf/2512.13056.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chen Huang, Ronghui Hou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.13056">The Optimal Control Algorithm of Connected and Automated Vehicles at Roundabouts with Communication Delay</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Connected and automated vehicles (CAVs) rely on wireless communication to exchange state information for distributed control, making communication delays a critical factor that can affect vehicle motion and degrade control performance, particularly in high-speed scenarios. To address these challenges in the complex environment of roundabout intersections, this paper proposes a roundabout control algorithm, which takes into account the uncertainty of interactive information caused by time delays. First, to maintain the required distance between the current vehicle and its preceding and following vehicles, conflicting vehicles are identified based on the time-to-collision (TTC) in the conflict zone. To fully consider communication performance, a vehicle motion model incorporating time delays is established. According to the distributed model predictive control (DMPC) mechanism, the vehicle motion control that satisfies the roundabout constraints is determined. Second, by scheduling the sequence of vehicles entering the roundabout, a multiscale optimization objective is developed by integrating vehicle motion indicators and roundabout system indicators. Traffic density and travel time are embedded into the optimization problem to guide vehicles to enter the roundabout safely and stably. Through a variety of simulation experiments, the effectiveness of the proposed control algorithm is verified by comparing its performance with that of multiple control algorithms under different autonomous vehicle penetration rates and heavy traffic load scenarios.
<div id='section'>Paperid: <span id='pid'>1308, <a href='https://arxiv.org/pdf/2512.00355.pdf' target='_blank'>https://arxiv.org/pdf/2512.00355.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junqiao Fan, Pengfei Liu, Haocong Rao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.00355">SMamDiff: Spatial Mamba for Stochastic Human Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With intelligent room-side sensing and service robots widely deployed, human motion prediction (HMP) is essential for safe, proactive assistance. However, many existing HMP methods either produce a single, deterministic forecast that ignores uncertainty or rely on probabilistic models that sacrifice kinematic plausibility. Diffusion models improve the accuracy-diversity trade-off but often depend on multi-stage pipelines that are costly for edge deployment. This work focuses on how to ensure spatial-temporal coherence within a single-stage diffusion model for HMP. We introduce SMamDiff, a Spatial Mamba-based Diffusion model with two novel designs: (i) a residual-DCT motion encoding that subtracts the last observed pose before a temporal DCT, reducing the first DC component ($f=0$) dominance and highlighting informative higher-frequency cues so the model learns how joints move rather than where they are; and (ii) a stickman-drawing spatial-mamba module that processes joints in an ordered, joint-by-joint manner, making later joints condition on earlier ones to induce long-range, cross-joint dependencies. On Human3.6M and HumanEva, these coherence mechanisms deliver state-of-the-art results among single-stage probabilistic HMP methods while using less latency and memory than multi-stage diffusion baselines.
<div id='section'>Paperid: <span id='pid'>1309, <a href='https://arxiv.org/pdf/2511.20431.pdf' target='_blank'>https://arxiv.org/pdf/2511.20431.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dohun Lim, Minji Kim, Jaewoon Lim, Sungchan Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.20431">BRIC: Bridging Kinematic Plans and Physical Control at Test Time</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose BRIC, a novel test-time adaptation (TTA) framework that enables long-term human motion generation by resolving execution discrepancies between diffusion-based kinematic motion planners and reinforcement learning-based physics controllers. While diffusion models can generate diverse and expressive motions conditioned on text and scene context, they often produce physically implausible outputs, leading to execution drift during simulation. To address this, BRIC dynamically adapts the physics controller to noisy motion plans at test time, while preserving pre-trained skills via a loss function that mitigates catastrophic forgetting. In addition, BRIC introduces a lightweight test-time guidance mechanism that steers the diffusion model in the signal space without updating its parameters. By combining both adaptation strategies, BRIC ensures consistent and physically plausible long-term executions across diverse environments in an effective and efficient manner. We validate the effectiveness of BRIC on a variety of long-term tasks, including motion composition, obstacle avoidance, and human-scene interaction, achieving state-of-the-art performance across all tasks.
<div id='section'>Paperid: <span id='pid'>1310, <a href='https://arxiv.org/pdf/2511.19109.pdf' target='_blank'>https://arxiv.org/pdf/2511.19109.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohan Ramesh, Mark Azer, Fabian B. Flohr
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.19109">HABIT: Human Action Benchmark for Interactive Traffic in CARLA</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current autonomous driving (AD) simulations are critically limited by their inadequate representation of realistic and diverse human behavior, which is essential for ensuring safety and reliability. Existing benchmarks often simplify pedestrian interactions, failing to capture complex, dynamic intentions and varied responses critical for robust system deployment. To overcome this, we introduce HABIT (Human Action Benchmark for Interactive Traffic), a high-fidelity simulation benchmark. HABIT integrates real-world human motion, sourced from mocap and videos, into CARLA (Car Learning to Act, a full autonomous driving simulator) via a modular, extensible, and physically consistent motion retargeting pipeline. From an initial pool of approximately 30,000 retargeted motions, we curate 4,730 traffic-compatible pedestrian motions, standardized in SMPL format for physically consistent trajectories. HABIT seamlessly integrates with CARLA's Leaderboard, enabling automated scenario generation and rigorous agent evaluation. Our safety metrics, including Abbreviated Injury Scale (AIS) and False Positive Braking Rate (FPBR), reveal critical failure modes in state-of-the-art AD agents missed by prior evaluations. Evaluating three state-of-the-art autonomous driving agents, InterFuser, TransFuser, and BEVDriver, demonstrates how HABIT exposes planner weaknesses that remain hidden in scripted simulations. Despite achieving close or equal to zero collisions per kilometer on the CARLA Leaderboard, the autonomous agents perform notably worse on HABIT, with up to 7.43 collisions/km and a 12.94% AIS 3+ injury risk, and they brake unnecessarily in up to 33% of cases. All components are publicly released to support reproducible, pedestrian-aware AI research.
<div id='section'>Paperid: <span id='pid'>1311, <a href='https://arxiv.org/pdf/2511.17727.pdf' target='_blank'>https://arxiv.org/pdf/2511.17727.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Victor Li, Naveenraj Kamalakannan, Avinash Parnandi, Heidi Schambra, Carlos Fernandez-Granda
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.17727">The Potential and Limitations of Vision-Language Models for Human Motion Understanding: A Case Study in Data-Driven Stroke Rehabilitation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-language models (VLMs) have demonstrated remarkable performance across a wide range of computer-vision tasks, sparking interest in their potential for digital health applications. Here, we apply VLMs to two fundamental challenges in data-driven stroke rehabilitation: automatic quantification of rehabilitation dose and impairment from videos. We formulate these problems as motion-identification tasks, which can be addressed using VLMs. We evaluate our proposed framework on a cohort of 29 healthy controls and 51 stroke survivors. Our results show that current VLMs lack the fine-grained motion understanding required for precise quantification: dose estimates are comparable to a baseline that excludes visual information, and impairment scores cannot be reliably predicted. Nevertheless, several findings suggest future promise. With optimized prompting and post-processing, VLMs can classify high-level activities from a few frames, detect motion and grasp with moderate accuracy, and approximate dose counts within 25% of ground truth for mildly impaired and healthy participants, all without task-specific training or finetuning. These results highlight both the current limitations and emerging opportunities of VLMs for data-driven stroke rehabilitation and broader clinical video analysis.
<div id='section'>Paperid: <span id='pid'>1312, <a href='https://arxiv.org/pdf/2511.13988.pdf' target='_blank'>https://arxiv.org/pdf/2511.13988.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bokyung Jang, Eunho Jung, Yoonsang Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.13988">B2F: End-to-End Body-to-Face Motion Generation with Style Reference</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion naturally integrates body movements and facial expressions, forming a unified perception. If a virtual character's facial expression does not align well with its body movements, it may weaken the perception of the character as a cohesive whole. Motivated by this, we propose B2F, a model that generates facial motions aligned with body movements. B2F takes a facial style reference as input, generating facial animations that reflect the provided style while maintaining consistency with the associated body motion. To achieve this, B2F learns a disentangled representation of content and style, using alignment and consistency-based objectives. We represent style using discrete latent codes learned via the Gumbel-Softmax trick, enabling diverse expression generation with a structured latent representation. B2F outputs facial motion in the FLAME format, making it compatible with SMPL-X characters, and supports ARKit-style avatars through a dedicated conversion module. Our evaluations show that B2F generates expressive and engaging facial animations that synchronize with body movements and style intent, while mitigating perceptual dissonance from mismatched cues, and generalizing across diverse characters and styles.
<div id='section'>Paperid: <span id='pid'>1313, <a href='https://arxiv.org/pdf/2511.11850.pdf' target='_blank'>https://arxiv.org/pdf/2511.11850.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ali Mashhadireza, Ali Sadighi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.11850">Neural Network-Augmented Iterative Learning Control for Friction Compensation of Motion Control Systems with Varying Disturbances</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper proposes a robust control strategy that integrates Iterative Learning Control (ILC) with a simple lateral neural network to enhance the trajectory tracking performance of a linear Lorentz force actuator under friction and model uncertainties. The ILC compensates for nonlinear friction effects, while the neural network estimates the nonlinear ILC effort for varying reference commands. By dynamically adjusting the ILC effort, the method adapts to time-varying friction, reduces errors at reference changes, and accelerates convergence. Compared to previous approaches using complex neural networks, this method simplifies online training and implementation, making it practical for real-time applications. Experimental results confirm its effectiveness in achieving precise tracking across multiple tasks with different reference trajectories.
<div id='section'>Paperid: <span id='pid'>1314, <a href='https://arxiv.org/pdf/2511.04976.pdf' target='_blank'>https://arxiv.org/pdf/2511.04976.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xin Nie, Zhiyuan Cheng, Yuan Zhang, Chao Ji, Jiajia Wu, Yuhan Zhang, Jia Pan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.04976">iFlyBot-VLM Technical Report</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce iFlyBot-VLM, a general-purpose Vision-Language Model (VLM) used to improve the domain of Embodied Intelligence. The central objective of iFlyBot-VLM is to bridge the cross-modal semantic gap between high-dimensional environmental perception and low-level robotic motion control. To this end, the model abstracts complex visual and spatial information into a body-agnostic and transferable Operational Language, thereby enabling seamless perception-action closed-loop coordination across diverse robotic platforms. The architecture of iFlyBot-VLM is systematically designed to realize four key functional capabilities essential for embodied intelligence: 1) Spatial Understanding and Metric Reasoning; 2) Interactive Target Grounding; 3) Action Abstraction and Control Parameter Generation; 4) Task Planning and Skill Sequencing. We envision iFlyBot-VLM as a scalable and generalizable foundation model for embodied AI, facilitating the progression from specialized task-oriented systems toward generalist, cognitively capable agents. We conducted evaluations on 10 current mainstream embodied intelligence-related VLM benchmark datasets, such as Blink and Where2Place, and achieved optimal performance while preserving the model's general capabilities. We will publicly release both the training data and model weights to foster further research and development in the field of Embodied Intelligence.
<div id='section'>Paperid: <span id='pid'>1315, <a href='https://arxiv.org/pdf/2511.03676.pdf' target='_blank'>https://arxiv.org/pdf/2511.03676.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Taito Tashiro, Tomoko Yonezawa, Hirotake Yamazoe
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.03676">Unconscious and Intentional Human Motion Cues for Expressive Robot-Arm Motion Design</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study investigates how human motion cues can be used to design expressive robot-arm movements. Using the imperfect-information game Geister, we analyzed two types of human piece-moving motions: natural gameplay (unconscious tendencies) and instructed expressions (intentional cues). Based on these findings, we created phase-specific robot motions by varying movement speed and stop duration, and evaluated observer impressions under two presentation modalities: a physical robot and a recorded video. Results indicate that late-phase motion timing, particularly during withdrawal, plays an important role in impression formation and that physical embodiment enhances the interpretability of motion cues. These findings provide insights for designing expressive robot motions based on human timing behavior.
<div id='section'>Paperid: <span id='pid'>1316, <a href='https://arxiv.org/pdf/2510.24257.pdf' target='_blank'>https://arxiv.org/pdf/2510.24257.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziqi Ma, Changda Tian, Yue Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.24257">Manipulate as Human: Learning Task-oriented Manipulation Skills by Adversarial Motion Priors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, there has been growing interest in developing robots and autonomous systems that can interact with human in a more natural and intuitive way. One of the key challenges in achieving this goal is to enable these systems to manipulate objects and tools in a manner that is similar to that of humans. In this paper, we propose a novel approach for learning human-style manipulation skills by using adversarial motion priors, which we name HMAMP. The approach leverages adversarial networks to model the complex dynamics of tool and object manipulation, as well as the aim of the manipulation task. The discriminator is trained using a combination of real-world data and simulation data executed by the agent, which is designed to train a policy that generates realistic motion trajectories that match the statistical properties of human motion. We evaluated HMAMP on one challenging manipulation task: hammering, and the results indicate that HMAMP is capable of learning human-style manipulation skills that outperform current baseline methods. Additionally, we demonstrate that HMAMP has potential for real-world applications by performing real robot arm hammering tasks. In general, HMAMP represents a significant step towards developing robots and autonomous systems that can interact with humans in a more natural and intuitive way, by learning to manipulate tools and objects in a manner similar to how humans do.
<div id='section'>Paperid: <span id='pid'>1317, <a href='https://arxiv.org/pdf/2510.20425.pdf' target='_blank'>https://arxiv.org/pdf/2510.20425.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyang Li, Chunfeng Cui, Jiaxin Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.20425">Projecting onto the Unit Dual Quaternion Set</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dual quaternions have gained significant attention due to their wide applications in areas such as multi-agent formation control, 3D motion modeling, and robotics. A fundamental aspect in dual quaternion research involves the projection onto unit dual quaternion sets. In this paper, we systematically study such projections under the $2^R$-norm, which is commonly used in practical applications. We identify several distinct cases based on the relationship between the standard and dual parts in vector form, and demonstrate the effectiveness of the proposed algorithm through numerical experiments.
<div id='section'>Paperid: <span id='pid'>1318, <a href='https://arxiv.org/pdf/2508.20734.pdf' target='_blank'>https://arxiv.org/pdf/2508.20734.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Reza Akbari Movahed, Abuzar Rezaee, Arezoo Zakeri, Colin Berry, Edmond S. L. Ho, Ali Gooya
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.20734">CardioMorphNet: Cardiac Motion Prediction Using a Shape-Guided Bayesian Recurrent Deep Network</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate cardiac motion estimation from cine cardiac magnetic resonance (CMR) images is vital for assessing cardiac function and detecting its abnormalities. Existing methods often struggle to capture heart motion accurately because they rely on intensity-based image registration similarity losses that may overlook cardiac anatomical regions. To address this, we propose CardioMorphNet, a recurrent Bayesian deep learning framework for 3D cardiac shape-guided deformable registration using short-axis (SAX) CMR images. It employs a recurrent variational autoencoder to model spatio-temporal dependencies over the cardiac cycle and two posterior models for bi-ventricular segmentation and motion estimation. The derived loss function from the Bayesian formulation guides the framework to focus on anatomical regions by recursively registering segmentation maps without using intensity-based image registration similarity loss, while leveraging sequential SAX volumes and spatio-temporal features. The Bayesian modelling also enables computation of uncertainty maps for the estimated motion fields. Validated on the UK Biobank dataset by comparing warped mask shapes with ground truth masks, CardioMorphNet demonstrates superior performance in cardiac motion estimation, outperforming state-of-the-art methods. Uncertainty assessment shows that it also yields lower uncertainty values for estimated motion fields in the cardiac region compared with other probabilistic-based cardiac registration methods, indicating higher confidence in its predictions.
<div id='section'>Paperid: <span id='pid'>1319, <a href='https://arxiv.org/pdf/2508.18525.pdf' target='_blank'>https://arxiv.org/pdf/2508.18525.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Eleni Tselepi, Spyridon Thermos, Gerasimos Potamianos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.18525">Controllable Single-shot Animation Blending with Temporal Conditioning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Training a generative model on a single human skeletal motion sequence without being bound to a specific kinematic tree has drawn significant attention from the animation community. Unlike text-to-motion generation, single-shot models allow animators to controllably generate variations of existing motion patterns without requiring additional data or extensive retraining. However, existing single-shot methods do not explicitly offer a controllable framework for blending two or more motions within a single generative pass. In this paper, we present the first single-shot motion blending framework that enables seamless blending by temporally conditioning the generation process. Our method introduces a skeleton-aware normalization mechanism to guide the transition between motions, allowing smooth, data-driven control over when and how motions blend. We perform extensive quantitative and qualitative evaluations across various animation styles and different kinematic skeletons, demonstrating that our approach produces plausible, smooth, and controllable motion blends in a unified and efficient manner.
<div id='section'>Paperid: <span id='pid'>1320, <a href='https://arxiv.org/pdf/2508.09960.pdf' target='_blank'>https://arxiv.org/pdf/2508.09960.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifei Yao, Chengyuan Luo, Jiaheng Du, Wentao He, Jun-Guo Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09960">GBC: Generalized Behavior-Cloning Framework for Whole-Body Humanoid Imitation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The creation of human-like humanoid robots is hindered by a fundamental fragmentation: data processing and learning algorithms are rarely universal across different robot morphologies. This paper introduces the Generalized Behavior Cloning (GBC) framework, a comprehensive and unified solution designed to solve this end-to-end challenge. GBC establishes a complete pathway from human motion to robot action through three synergistic innovations. First, an adaptive data pipeline leverages a differentiable IK network to automatically retarget any human MoCap data to any humanoid. Building on this foundation, our novel DAgger-MMPPO algorithm with its MMTransformer architecture learns robust, high-fidelity imitation policies. To complete the ecosystem, the entire framework is delivered as an efficient, open-source platform based on Isaac Lab, empowering the community to deploy the full workflow via simple configuration scripts. We validate the power and generality of GBC by training policies on multiple heterogeneous humanoids, demonstrating excellent performance and transfer to novel motions. This work establishes the first practical and unified pathway for creating truly generalized humanoid controllers.
<div id='section'>Paperid: <span id='pid'>1321, <a href='https://arxiv.org/pdf/2508.05514.pdf' target='_blank'>https://arxiv.org/pdf/2508.05514.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zewei Wu, CÃ©sar Teixeira, Wei Ke, Zhang Xiong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05514">Head Anchor Enhanced Detection and Association for Crowded Pedestrian Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual pedestrian tracking represents a promising research field, with extensive applications in intelligent surveillance, behavior analysis, and human-computer interaction. However, real-world applications face significant occlusion challenges. When multiple pedestrians interact or overlap, the loss of target features severely compromises the tracker's ability to maintain stable trajectories. Traditional tracking methods, which typically rely on full-body bounding box features extracted from {Re-ID} models and linear constant-velocity motion assumptions, often struggle in severe occlusion scenarios. To address these limitations, this work proposes an enhanced tracking framework that leverages richer feature representations and a more robust motion model. Specifically, the proposed method incorporates detection features from both the regression and classification branches of an object detector, embedding spatial and positional information directly into the feature representations. To further mitigate occlusion challenges, a head keypoint detection model is introduced, as the head is less prone to occlusion compared to the full body. In terms of motion modeling, we propose an iterative Kalman filtering approach designed to align with modern detector assumptions, integrating 3D priors to better complete motion trajectories in complex scenes. By combining these advancements in appearance and motion modeling, the proposed method offers a more robust solution for multi-object tracking in crowded environments where occlusions are prevalent.
<div id='section'>Paperid: <span id='pid'>1322, <a href='https://arxiv.org/pdf/2508.03578.pdf' target='_blank'>https://arxiv.org/pdf/2508.03578.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jonas Leo Mueller, Lukas Engel, Eva Dorschky, Daniel Krauss, Ingrid Ullmann, Martin Vossiek, Bjoern M. Eskofier
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.03578">RadProPoser: A Framework for Human Pose Estimation with Uncertainty Quantification from Raw Radar Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Radar-based human pose estimation (HPE) provides a privacy-preserving, illumination-invariant sensing modality but is challenged by noisy, multipath-affected measurements. We introduce RadProPoser, a probabilistic encoder-decoder architecture that processes complex-valued radar tensors from a compact 3-transmitter, 4-receiver MIMO radar. By incorporating variational inference into keypoint regression, RadProPoser jointly predicts 26 three-dimensional joint locations alongside heteroscedastic aleatoric uncertainties and can be recalibrated to predict total uncertainty. We explore different probabilistic formulations using both Gaussian and Laplace distributions for latent priors and likelihoods. On our newly released dataset with optical motion-capture ground truth, RadProPoser achieves an overall mean per-joint position error (MPJPE) of 6.425 cm, with 5.678 cm at the 45 degree aspect angle. The learned uncertainties exhibit strong alignment with actual pose errors and can be calibrated to produce reliable prediction intervals, with our best configuration achieving an expected calibration error of 0.021. As an additional demonstration, sampling from these latent distributions enables effective data augmentation for downstream activity classification, resulting in an F1 score of 0.870. To our knowledge, this is the first end-to-end radar tensor-based HPE system to explicitly model and quantify per-joint uncertainty from raw radar tensor data, establishing a foundation for explainable and reliable human motion analysis in radar applications.
<div id='section'>Paperid: <span id='pid'>1323, <a href='https://arxiv.org/pdf/2508.00939.pdf' target='_blank'>https://arxiv.org/pdf/2508.00939.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haodong Huang, Shilong Sun, Yuanpeng Wang, Chiyao Li, Hailin Huang, Wenfu Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.00939">BarlowWalk: Self-supervised Representation Learning for Legged Robot Terrain-adaptive Locomotion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reinforcement learning (RL), driven by data-driven methods, has become an effective solution for robot leg motion control problems. However, the mainstream RL methods for bipedal robot terrain traversal, such as teacher-student policy knowledge distillation, suffer from long training times, which limit development efficiency. To address this issue, this paper proposes BarlowWalk, an improved Proximal Policy Optimization (PPO) method integrated with self-supervised representation learning. This method employs the Barlow Twins algorithm to construct a decoupled latent space, mapping historical observation sequences into low-dimensional representations and implementing self-supervision. Meanwhile, the actor requires only proprioceptive information to achieve self-supervised learning over continuous time steps, significantly reducing the dependence on external terrain perception. Simulation experiments demonstrate that this method has significant advantages in complex terrain scenarios. To enhance the credibility of the evaluation, this study compares BarlowWalk with advanced algorithms through comparative tests, and the experimental results verify the effectiveness of the proposed method.
<div id='section'>Paperid: <span id='pid'>1324, <a href='https://arxiv.org/pdf/2508.00299.pdf' target='_blank'>https://arxiv.org/pdf/2508.00299.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Danzhen Fu, Jiagao Hu, Daiguo Zhou, Fei Wang, Zepeng Wang, Wenhua Liao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.00299">Controllable Pedestrian Video Editing for Multi-View Driving Scenarios via Motion Sequence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pedestrian detection models in autonomous driving systems often lack robustness due to insufficient representation of dangerous pedestrian scenarios in training datasets. To address this limitation, we present a novel framework for controllable pedestrian video editing in multi-view driving scenarios by integrating video inpainting and human motion control techniques. Our approach begins by identifying pedestrian regions of interest across multiple camera views, expanding detection bounding boxes with a fixed ratio, and resizing and stitching these regions into a unified canvas while preserving cross-view spatial relationships. A binary mask is then applied to designate the editable area, within which pedestrian editing is guided by pose sequence control conditions. This enables flexible editing functionalities, including pedestrian insertion, replacement, and removal. Extensive experiments demonstrate that our framework achieves high-quality pedestrian editing with strong visual realism, spatiotemporal coherence, and cross-view consistency. These results establish the proposed method as a robust and versatile solution for multi-view pedestrian video generation, with broad potential for applications in data augmentation and scenario simulation in autonomous driving.
<div id='section'>Paperid: <span id='pid'>1325, <a href='https://arxiv.org/pdf/2507.14097.pdf' target='_blank'>https://arxiv.org/pdf/2507.14097.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hari Iyer, Neel Macwan, Atharva Jitendra Hude, Heejin Jeong, Shenghan Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.14097">Generative AI-Driven High-Fidelity Human Motion Simulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion simulation (HMS) supports cost-effective evaluation of worker behavior, safety, and productivity in industrial tasks. However, existing methods often suffer from low motion fidelity. This study introduces Generative-AI-Enabled HMS (G-AI-HMS), which integrates text-to-text and text-to-motion models to enhance simulation quality for physical tasks. G-AI-HMS tackles two key challenges: (1) translating task descriptions into motion-aware language using Large Language Models aligned with MotionGPT's training vocabulary, and (2) validating AI-enhanced motions against real human movements using computer vision. Posture estimation algorithms are applied to real-time videos to extract joint landmarks, and motion similarity metrics are used to compare them with AI-enhanced sequences. In a case study involving eight tasks, the AI-enhanced motions showed lower error than human created descriptions in most scenarios, performing better in six tasks based on spatial accuracy, four tasks based on alignment after pose normalization, and seven tasks based on overall temporal similarity. Statistical analysis showed that AI-enhanced prompts significantly (p $<$ 0.0001) reduced joint error and temporal misalignment while retaining comparable posture accuracy.
<div id='section'>Paperid: <span id='pid'>1326, <a href='https://arxiv.org/pdf/2507.04955.pdf' target='_blank'>https://arxiv.org/pdf/2507.04955.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fathinah Izzati, Xinyue Li, Gus Xia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.04955">EXPOTION: Facial Expression and Motion Control for Multimodal Music Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose Expotion (Facial Expression and Motion Control for Multimodal Music Generation), a generative model leveraging multimodal visual controls - specifically, human facial expressions and upper-body motion - as well as text prompts to produce expressive and temporally accurate music. We adopt parameter-efficient fine-tuning (PEFT) on the pretrained text-to-music generation model, enabling fine-grained adaptation to the multimodal controls using a small dataset. To ensure precise synchronization between video and music, we introduce a temporal smoothing strategy to align multiple modalities. Experiments demonstrate that integrating visual features alongside textual descriptions enhances the overall quality of generated music in terms of musicality, creativity, beat-tempo consistency, temporal alignment with the video, and text adherence, surpassing both proposed baselines and existing state-of-the-art video-to-music generation models. Additionally, we introduce a novel dataset consisting of 7 hours of synchronized video recordings capturing expressive facial and upper-body gestures aligned with corresponding music, providing significant potential for future research in multimodal and interactive music generation.
<div id='section'>Paperid: <span id='pid'>1327, <a href='https://arxiv.org/pdf/2506.22459.pdf' target='_blank'>https://arxiv.org/pdf/2506.22459.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wending Heng, Chaoyuan Liang, Yihui Zhao, Zhiqiang Zhang, Glen Cooper, Zhenhong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.22459">Physics-Embedded Neural Networks for sEMG-based Continuous Motion Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurately decoding human motion intentions from surface electromyography (sEMG) is essential for myoelectric control and has wide applications in rehabilitation robotics and assistive technologies. However, existing sEMG-based motion estimation methods often rely on subject-specific musculoskeletal (MSK) models that are difficult to calibrate, or purely data-driven models that lack physiological consistency. This paper introduces a novel Physics-Embedded Neural Network (PENN) that combines interpretable MSK forward-dynamics with data-driven residual learning, thereby preserving physiological consistency while achieving accurate motion estimation. The PENN employs a recursive temporal structure to propagate historical estimates and a lightweight convolutional neural network for residual correction, leading to robust and temporally coherent estimations. A two-phase training strategy is designed for PENN. Experimental evaluations on six healthy subjects show that PENN outperforms state-of-the-art baseline methods in both root mean square error (RMSE) and $R^2$ metrics.
<div id='section'>Paperid: <span id='pid'>1328, <a href='https://arxiv.org/pdf/2506.09411.pdf' target='_blank'>https://arxiv.org/pdf/2506.09411.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vaclav Knapp, Matyas Bohacek
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.09411">Synthetic Human Action Video Data Generation with Pose Transfer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In video understanding tasks, particularly those involving human motion, synthetic data generation often suffers from uncanny features, diminishing its effectiveness for training. Tasks such as sign language translation, gesture recognition, and human motion understanding in autonomous driving have thus been unable to exploit the full potential of synthetic data. This paper proposes a method for generating synthetic human action video data using pose transfer (specifically, controllable 3D Gaussian avatar models). We evaluate this method on the Toyota Smarthome and NTU RGB+D datasets and show that it improves performance in action recognition tasks. Moreover, we demonstrate that the method can effectively scale few-shot datasets, making up for groups underrepresented in the real training data and adding diverse backgrounds. We open-source the method along with RANDOM People, a dataset with videos and avatars of novel human identities for pose transfer crowd-sourced from the internet.
<div id='section'>Paperid: <span id='pid'>1329, <a href='https://arxiv.org/pdf/2505.22111.pdf' target='_blank'>https://arxiv.org/pdf/2505.22111.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Woonho Ko, Jin Bok Park, Il Yong Chun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.22111">Autoregression-free video prediction using diffusion model for mitigating error propagation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing long-term video prediction methods often rely on an autoregressive video prediction mechanism. However, this approach suffers from error propagation, particularly in distant future frames. To address this limitation, this paper proposes the first AutoRegression-Free (ARFree) video prediction framework using diffusion models. Different from an autoregressive video prediction mechanism, ARFree directly predicts any future frame tuples from the context frame tuple. The proposed ARFree consists of two key components: 1) a motion prediction module that predicts a future motion using motion feature extracted from the context frame tuple; 2) a training method that improves motion continuity and contextual consistency between adjacent future frame tuples. Our experiments with two benchmark datasets show that the proposed ARFree video prediction framework outperforms several state-of-the-art video prediction methods.
<div id='section'>Paperid: <span id='pid'>1330, <a href='https://arxiv.org/pdf/2505.21566.pdf' target='_blank'>https://arxiv.org/pdf/2505.21566.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gao Huayu, Huang Tengjiu, Ye Xiaolong, Tsuyoshi Okita
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.21566">Diffusion Model-based Activity Completion for AI Motion Capture from Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>AI-based motion capture is an emerging technology that offers a cost-effective alternative to traditional motion capture systems. However, current AI motion capture methods rely entirely on observed video sequences, similar to conventional motion capture. This means that all human actions must be predefined, and movements outside the observed sequences are not possible. To address this limitation, we aim to apply AI motion capture to virtual humans, where flexible actions beyond the observed sequences are required. We assume that while many action fragments exist in the training data, the transitions between them may be missing. To bridge these gaps, we propose a diffusion-model-based action completion technique that generates complementary human motion sequences, ensuring smooth and continuous movements. By introducing a gate module and a position-time embedding module, our approach achieves competitive results on the Human3.6M dataset. Our experimental results show that (1) MDC-Net outperforms existing methods in ADE, FDE, and MMADE but is slightly less accurate in MMFDE, (2) MDC-Net has a smaller model size (16.84M) compared to HumanMAC (28.40M), and (3) MDC-Net generates more natural and coherent motion sequences. Additionally, we propose a method for extracting sensor data, including acceleration and angular velocity, from human motion sequences.
<div id='section'>Paperid: <span id='pid'>1331, <a href='https://arxiv.org/pdf/2505.19976.pdf' target='_blank'>https://arxiv.org/pdf/2505.19976.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Naoki Agata, Takeo Igarashi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.19976">MAMM: Motion Control via Metric-Aligning Motion Matching</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a novel method for controlling a motion sequence using an arbitrary temporal control sequence using temporal alignment. Temporal alignment of motion has gained significant attention owing to its applications in motion control and retargeting. Traditional methods rely on either learned or hand-craft cross-domain mappings between frames in the original and control domains, which often require large, paired, or annotated datasets and time-consuming training. Our approach, named Metric-Aligning Motion Matching, achieves alignment by solely considering within-domain distances. It computes distances among patches in each domain and seeks a matching that optimally aligns the two within-domain distances. This framework allows for the alignment of a motion sequence to various types of control sequences, including sketches, labels, audio, and another motion sequence, all without the need for manually defined mappings or training with annotated data. We demonstrate the effectiveness of our approach through applications in efficient motion control, showcasing its potential in practical scenarios.
<div id='section'>Paperid: <span id='pid'>1332, <a href='https://arxiv.org/pdf/2504.12702.pdf' target='_blank'>https://arxiv.org/pdf/2504.12702.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziqi Wang, Jingyue Zhao, Jichao Yang, Yaohua Wang, Xun Xiao, Yuan Li, Chao Xiao, Lei Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.12702">Embodied Neuromorphic Control Applied on a 7-DOF Robotic Manipulator</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The development of artificial intelligence towards real-time interaction with the environment is a key aspect of embodied intelligence and robotics. Inverse dynamics is a fundamental robotics problem, which maps from joint space to torque space of robotic systems. Traditional methods for solving it rely on direct physical modeling of robots which is difficult or even impossible due to nonlinearity and external disturbance. Recently, data-based model-learning algorithms are adopted to address this issue. However, they often require manual parameter tuning and high computational costs. Neuromorphic computing is inherently suitable to process spatiotemporal features in robot motion control at extremely low costs. However, current research is still in its infancy: existing works control only low-degree-of-freedom systems and lack performance quantification and comparison. In this paper, we propose a neuromorphic control framework to control 7 degree-of-freedom robotic manipulators. We use Spiking Neural Network to leverage the spatiotemporal continuity of the motion data to improve control accuracy, and eliminate manual parameters tuning. We validated the algorithm on two robotic platforms, which reduces torque prediction error by at least 60% and performs a target position tracking task successfully. This work advances embodied neuromorphic control by one step forward from proof of concept to applications in complex real-world tasks.
<div id='section'>Paperid: <span id='pid'>1333, <a href='https://arxiv.org/pdf/2504.11150.pdf' target='_blank'>https://arxiv.org/pdf/2504.11150.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mahir Gulzar, Yar Muhammad, Naveed Muhammad
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.11150">GC-GAT: Multimodal Vehicular Trajectory Prediction using Graph Goal Conditioning and Cross-context Attention</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Predicting future trajectories of surrounding vehicles heavily relies on what contextual information is given to a motion prediction model. The context itself can be static (lanes, regulatory elements, etc) or dynamic (traffic participants). This paper presents a lane graph-based motion prediction model that first predicts graph-based goal proposals and later fuses them with cross attention over multiple contextual elements. We follow the famous encoder-interactor-decoder architecture where the encoder encodes scene context using lightweight Gated Recurrent Units, the interactor applies cross-context attention over encoded scene features and graph goal proposals, and the decoder regresses multimodal trajectories via Laplacian Mixture Density Network from the aggregated encodings. Using cross-attention over graph-based goal proposals gives robust trajectory estimates since the model learns to attend to future goal-relevant scene elements for the intended agent. We evaluate our work on nuScenes motion prediction dataset, achieving state-of-the-art results.
<div id='section'>Paperid: <span id='pid'>1334, <a href='https://arxiv.org/pdf/2504.10676.pdf' target='_blank'>https://arxiv.org/pdf/2504.10676.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhanbo Huang, Xiaoming Liu, Yu Kong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.10676">H-MoRe: Learning Human-centric Motion Representation for Action Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose H-MoRe, a novel pipeline for learning precise human-centric motion representation. Our approach dynamically preserves relevant human motion while filtering out background movement. Notably, unlike previous methods relying on fully supervised learning from synthetic data, H-MoRe learns directly from real-world scenarios in a self-supervised manner, incorporating both human pose and body shape information. Inspired by kinematics, H-MoRe represents absolute and relative movements of each body point in a matrix format that captures nuanced motion details, termed world-local flows. H-MoRe offers refined insights into human motion, which can be integrated seamlessly into various action-related applications. Experimental results demonstrate that H-MoRe brings substantial improvements across various downstream tasks, including gait recognition(CL@R1: +16.01%), action recognition(Acc@1: +8.92%), and video generation(FVD: -67.07%). Additionally, H-MoRe exhibits high inference efficiency (34 fps), making it suitable for most real-time scenarios. Models and code will be released upon publication.
<div id='section'>Paperid: <span id='pid'>1335, <a href='https://arxiv.org/pdf/2504.10275.pdf' target='_blank'>https://arxiv.org/pdf/2504.10275.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Harsh Yadav, Maximilian Schaefer, Kun Zhao, Tobias Meisen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.10275">LMFormer: Lane based Motion Prediction Transformer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion prediction plays an important role in autonomous driving. This study presents LMFormer, a lane-aware transformer network for trajectory prediction tasks. In contrast to previous studies, our work provides a simple mechanism to dynamically prioritize the lanes and shows that such a mechanism introduces explainability into the learning behavior of the network. Additionally, LMFormer uses the lane connection information at intersections, lane merges, and lane splits, in order to learn long-range dependency in lane structure. Moreover, we also address the issue of refining the predicted trajectories and propose an efficient method for iterative refinement through stacked transformer layers. For benchmarking, we evaluate LMFormer on the nuScenes dataset and demonstrate that it achieves SOTA performance across multiple metrics. Furthermore, the Deep Scenario dataset is used to not only illustrate cross-dataset network performance but also the unification capabilities of LMFormer to train on multiple datasets and achieve better performance.
<div id='section'>Paperid: <span id='pid'>1336, <a href='https://arxiv.org/pdf/2504.05629.pdf' target='_blank'>https://arxiv.org/pdf/2504.05629.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haodong Huang, Shilong Sun, Zida Zhao, Hailin Huang, Changqing Shen, Wenfu Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.05629">PTRL: Prior Transfer Deep Reinforcement Learning for Legged Robots Locomotion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the field of legged robot motion control, reinforcement learning (RL) holds great promise but faces two major challenges: high computational cost for training individual robots and poor generalization of trained models. To address these problems, this paper proposes a novel framework called Prior Transfer Reinforcement Learning (PTRL), which improves both training efficiency and model transferability across different robots. Drawing inspiration from model transfer techniques in deep learning, PTRL introduces a fine-tuning mechanism that selectively freezes layers of the policy network during transfer, making it the first to apply such a method in RL. The framework consists of three stages: pre-training on a source robot using the Proximal Policy Optimization (PPO) algorithm, transferring the learned policy to a target robot, and fine-tuning with partial network freezing. Extensive experiments on various robot platforms confirm that this approach significantly reduces training time while maintaining or even improving performance. Moreover, the study quantitatively analyzes how the ratio of frozen layers affects transfer results, providing valuable insights into optimizing the process. The experimental outcomes show that PTRL achieves better walking control performance and demonstrates strong generalization and adaptability, offering a promising solution for efficient and scalable RL-based control of legged robots.
<div id='section'>Paperid: <span id='pid'>1337, <a href='https://arxiv.org/pdf/2504.04862.pdf' target='_blank'>https://arxiv.org/pdf/2504.04862.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunxiang Liu, Hongkuo Niu, Jianlin Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.04862">GAMDTP: Dynamic Trajectory Prediction with Graph Attention Mamba Network</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate motion prediction of traffic agents is crucial for the safety and stability of autonomous driving systems. In this paper, we introduce GAMDTP, a novel graph attention-based network tailored for dynamic trajectory prediction. Specifically, we fuse the result of self attention and mamba-ssm through a gate mechanism, leveraging the strengths of both to extract features more efficiently and accurately, in each graph convolution layer. GAMDTP encodes the high-definition map(HD map) data and the agents' historical trajectory coordinates and decodes the network's output to generate the final prediction results. Additionally, recent approaches predominantly focus on dynamically fusing historical forecast results and rely on two-stage frameworks including proposal and refinement. To further enhance the performance of the two-stage frameworks we also design a scoring mechanism to evaluate the prediction quality during the proposal and refinement processes. Experiments on the Argoverse dataset demonstrates that GAMDTP achieves state-of-the-art performance, achieving superior accuracy in dynamic trajectory prediction.
<div id='section'>Paperid: <span id='pid'>1338, <a href='https://arxiv.org/pdf/2503.23171.pdf' target='_blank'>https://arxiv.org/pdf/2503.23171.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shayan Sepahvand, Niloufar Amiri, Farrokh Janabi-Sharifi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.23171">Deep Visual Servoing of an Aerial Robot Using Keypoint Feature Extraction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The problem of image-based visual servoing (IBVS) of an aerial robot using deep-learning-based keypoint detection is addressed in this article. A monocular RGB camera mounted on the platform is utilized to collect the visual data. A convolutional neural network (CNN) is then employed to extract the features serving as the visual data for the servoing task. This paper contributes to the field by circumventing not only the challenge stemming from the need for man-made marker detection in conventional visual servoing techniques, but also enhancing the robustness against undesirable factors including occlusion, varying illumination, clutter, and background changes, thereby broadening the applicability of perception-guided motion control tasks in aerial robots. Additionally, extensive physics-based ROS Gazebo simulations are conducted to assess the effectiveness of this method, in contrast to many existing studies that rely solely on physics-less simulations. A demonstration video is available at https://youtu.be/Dd2Her8Ly-E.
<div id='section'>Paperid: <span id='pid'>1339, <a href='https://arxiv.org/pdf/2503.19984.pdf' target='_blank'>https://arxiv.org/pdf/2503.19984.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ido Rachbuch, Sinwook Park, Yuval Katz, Touvia Miloh, Gilad Yossifon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.19984">Hybrid Magnetically and Electrically Powered Metallo-Dielectric Janus Microrobots: Enhanced Motion Control and Operation Beyond Planar Limits</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study introduces the integration of hybrid magnetic and electric actuation mechanisms to achieve advanced motion capabilities for Janus particle (JP) microrobots. We demonstrate enhanced in-plane motion control through versatile control strategies and present the concepts of interplanar transitions and 2.5-dimensional (2.5D) trajectories, enabled by magnetic levitation and electrostatic trapping. These innovations expand the mobility of JPs into 3D space, allowing dynamic operation beyond the limitations of traditional surface-bound motion. Key functionalities include obstacle crossing, transitions to elevated surfaces, and discrete surface patterning enabling highly localized interventions. Using this set of tools, we also showcase the controlled out-of-plane transport of both synthetic and biological cargo. Together, these advancements lay the groundwork for novel microrobot-related applications in microfluidic systems and biomedical research.
<div id='section'>Paperid: <span id='pid'>1340, <a href='https://arxiv.org/pdf/2503.17458.pdf' target='_blank'>https://arxiv.org/pdf/2503.17458.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jean C. Pereira, Valter J. S. Leite, Guilherme V. Raffo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.17458">Stabilizing NMPC Approaches for Underactuated Mechanical Systems on the SE(3) Manifold</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper addresses the motion control problem for underactuated mechanical systems with full attitude control and one translational force input to manage the six degrees of freedom involved in the three-dimensional Euclidean space. These systems are often classified as second-order nonholonomic due to their completely nonintegrable acceleration constraints. To tackle this complex control problem, we propose two nonlinear model predictive control (NMPC) schemes that ensure closed-loop stability and recursive feasibility without terminal conditions. The system dynamics are modeled on the SE(3) manifold for a globally and unique description of rigid body configurations. One NMPC scheme also aims to reduce mission time as an economic criterion. The controllers' effectiveness is validated through numerical experiments on a quadrotor UAV.
<div id='section'>Paperid: <span id='pid'>1341, <a href='https://arxiv.org/pdf/2503.13090.pdf' target='_blank'>https://arxiv.org/pdf/2503.13090.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>VÃ¡clav TruhlaÅÃ­k, TomÃ¡Å¡ PivoÅka, Michal Kasarda, Libor PÅeuÄil
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.13090">Multi-Platform Teach-and-Repeat Navigation by Visual Place Recognition Based on Deep-Learned Local Features</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Uniform and variable environments still remain a challenge for stable visual localization and mapping in mobile robot navigation. One of the possible approaches suitable for such environments is appearance-based teach-and-repeat navigation, relying on simplified localization and reactive robot motion control - all without a need for standard mapping. This work brings an innovative solution to such a system based on visual place recognition techniques. Here, the major contributions stand in the employment of a new visual place recognition technique, a novel horizontal shift computation approach, and a multi-platform system design for applications across various types of mobile robots. Secondly, a new public dataset for experimental testing of appearance-based navigation methods is introduced. Moreover, the work also provides real-world experimental testing and performance comparison of the introduced navigation system against other state-of-the-art methods. The results confirm that the new system outperforms existing methods in several testing scenarios, is capable of operation indoors and outdoors, and exhibits robustness to day and night scene variations.
<div id='section'>Paperid: <span id='pid'>1342, <a href='https://arxiv.org/pdf/2503.13034.pdf' target='_blank'>https://arxiv.org/pdf/2503.13034.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tinghui Li, Pamuditha Somarathne, Zhanna Sarsenbayeva, Anusha Withana
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.13034">TA-GNN: Physics Inspired Time-Agnostic Graph Neural Network for Finger Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Continuous prediction of finger joint movement using historical joint positions/rotations is vital in a multitude of applications, especially related to virtual reality, computer graphics, robotics, and rehabilitation. However, finger motions are highly articulated with multiple degrees of freedom, making them significantly harder to model and predict. To address this challenge, we propose a physics-inspired time-agnostic graph neural network (TA-GNN) to accurately predict human finger motions. The proposed encoder comprises a kinematic feature extractor to generate filtered velocity and acceleration and a physics-based encoder that follows linear kinematics. The model is designed to be prediction-time-agnostic so that it can seamlessly provide continuous predictions. The graph-based decoder for learning the topological motion between finger joints is designed to address the higher degree articulation of fingers. We show the superiority of our model performance in virtual reality context. This novel approach enhances finger tracking without additional sensors, enabling predictive interactions such as haptic re-targeting and improving predictive rendering quality.
<div id='section'>Paperid: <span id='pid'>1343, <a href='https://arxiv.org/pdf/2503.11072.pdf' target='_blank'>https://arxiv.org/pdf/2503.11072.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haotian Tan, Yuan-Hua Ni
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.11072">A High-Speed Time-Optimal Trajectory Generation Strategy via a Two-layer Planning Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion planning and trajectory generation are crucial technologies in various domains including the control of Unmanned Aerial Vehicles, manipulators, and rockets. However, optimization-based real-time motion planning becomes increasingly challenging due to the problem's probable non-convexity and the inherent limitations of non-linear programming algorithms. Highly nonlinear dynamics, obstacle avoidance constraints, and non-convex inputs can exacerbate these difficulties. In order to enhance the robustness and reduce the computational burden, this paper proposes a two-layer trajectory generating algorithm for intelligent ground vehicles with convex optimization methods, aiming to provide real-time guarantees for trajectory optimization and to improve the calculate speed of motion prediction. Our approach involves breaking down the original problem into small horizon-based planning cycles with fixed final times, referred to as planning cycles. Each planning cycle is then solved within a series of restricted convex sets constructed by some customized search algorithms incrementally. We rigorously establish these advantages through mathematical analysis under moderate assumptions and comprehensive experimental validations. For linear vehicle models, comparative experiments with general sequential convex programming algorithms demonstrate the superior performance of our proposed method, particularly in terms of the computational efficiency in dynamic maps and the reduced final time.
<div id='section'>Paperid: <span id='pid'>1344, <a href='https://arxiv.org/pdf/2503.10488.pdf' target='_blank'>https://arxiv.org/pdf/2503.10488.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Evgeniia Vu, Andrei Boiarov, Dmitry Vetrov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.10488">Streaming Generation of Co-Speech Gestures via Accelerated Rolling Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating co-speech gestures in real time requires both temporal coherence and efficient sampling. We introduce Accelerated Rolling Diffusion, a novel framework for streaming gesture generation that extends rolling diffusion models with structured progressive noise scheduling, enabling seamless long-sequence motion synthesis while preserving realism and diversity. We further propose Rolling Diffusion Ladder Acceleration (RDLA), a new approach that restructures the noise schedule into a stepwise ladder, allowing multiple frames to be denoised simultaneously. This significantly improves sampling efficiency while maintaining motion consistency, achieving up to a 2x speedup with high visual fidelity and temporal coherence. We evaluate our approach on ZEGGS and BEAT, strong benchmarks for real-world applicability. Our framework is universally applicable to any diffusion-based gesture generation model, transforming it into a streaming approach. Applied to three state-of-the-art methods, it consistently outperforms them, demonstrating its effectiveness as a generalizable and efficient solution for real-time, high-fidelity co-speech gesture synthesis.
<div id='section'>Paperid: <span id='pid'>1345, <a href='https://arxiv.org/pdf/2503.06050.pdf' target='_blank'>https://arxiv.org/pdf/2503.06050.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alexander Schperberg, Marcel Menner, Stefano Di Cairano
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.06050">Energy-Efficient Motion Planner for Legged Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose an online motion planner for legged robot locomotion with the primary objective of achieving energy efficiency. The conceptual idea is to leverage a placement set of footstep positions based on the robot's body position to determine when and how to execute steps. In particular, the proposed planner uses virtual placement sets beneath the hip joints of the legs and executes a step when the foot is outside of such placement set. Furthermore, we propose a parameter design framework that considers both energy-efficiency and robustness measures to optimize the gait by changing the shape of the placement set along with other parameters, such as step height and swing time, as a function of walking speed. We show that the planner produces trajectories that have a low Cost of Transport (CoT) and high robustness measure, and evaluate our approach against model-free Reinforcement Learning (RL) and motion imitation using biological dog motion priors as the reference. Overall, within low to medium velocity range, we show a 50.4% improvement in CoT and improved robustness over model-free RL, our best performing baseline. Finally, we show ability to handle slippery surfaces, gait transitions, and disturbances in simulation and hardware with the Unitree A1 robot.
<div id='section'>Paperid: <span id='pid'>1346, <a href='https://arxiv.org/pdf/2503.00576.pdf' target='_blank'>https://arxiv.org/pdf/2503.00576.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gerard GÃ³mez-Izquierdo, Javier Laplaza, Alberto Sanfeliu, AnaÃ­s Garrell
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.00576">Enhancing Context-Aware Human Motion Prediction for Efficient Robot Handovers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate human motion prediction (HMP) is critical for seamless human-robot collaboration, particularly in handover tasks that require real-time adaptability. Despite the high accuracy of state-of-the-art models, their computational complexity limits practical deployment in real-world robotic applications. In this work, we enhance human motion forecasting for handover tasks by leveraging siMLPe [1], a lightweight yet powerful architecture, and introducing key improvements. Our approach, named IntentMotion incorporates intention-aware conditioning, task-specific loss functions, and a novel intention classifier, significantly improving motion prediction accuracy while maintaining efficiency. Experimental results demonstrate that our method reduces body loss error by over 50%, achieves 200x faster inference, and requires only 3% of the parameters compared to existing state-of-the-art HMP models. These advancements establish our framework as a highly efficient and scalable solution for real-time human-robot interaction.
<div id='section'>Paperid: <span id='pid'>1347, <a href='https://arxiv.org/pdf/2502.20037.pdf' target='_blank'>https://arxiv.org/pdf/2502.20037.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongyu Deng, Tianfan Xue, He Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.20037">FuseGrasp: Radar-Camera Fusion for Robotic Grasping of Transparent Objects</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Transparent objects are prevalent in everyday environments, but their distinct physical properties pose significant challenges for camera-guided robotic arms. Current research is mainly dependent on camera-only approaches, which often falter in suboptimal conditions, such as low-light environments. In response to this challenge, we present FuseGrasp, the first radar-camera fusion system tailored to enhance the transparent objects manipulation. FuseGrasp exploits the weak penetrating property of millimeter-wave (mmWave) signals, which causes transparent materials to appear opaque, and combines it with the precise motion control of a robotic arm to acquire high-quality mmWave radar images of transparent objects. The system employs a carefully designed deep neural network to fuse radar and camera imagery, thereby improving depth completion and elevating the success rate of object grasping. Nevertheless, training FuseGrasp effectively is non-trivial, due to limited radar image datasets for transparent objects. We address this issue utilizing large RGB-D dataset, and propose an effective two-stage training approach: we first pre-train FuseGrasp on a large public RGB-D dataset of transparent objects, then fine-tune it on a self-built small RGB-D-Radar dataset. Furthermore, as a byproduct, FuseGrasp can determine the composition of transparent objects, such as glass or plastic, leveraging the material identification capability of mmWave radar. This identification result facilitates the robotic arm in modulating its grip force appropriately. Extensive testing reveals that FuseGrasp significantly improves the accuracy of depth reconstruction and material identification for transparent objects. Moreover, real-world robotic trials have confirmed that FuseGrasp markedly enhances the handling of transparent items. A video demonstration of FuseGrasp is available at https://youtu.be/MWDqv0sRSok.
<div id='section'>Paperid: <span id='pid'>1348, <a href='https://arxiv.org/pdf/2502.19056.pdf' target='_blank'>https://arxiv.org/pdf/2502.19056.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Iliana Loi, Konstantinos Moustakas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.19056">Fatigue-PINN: Physics-Informed Fatigue-Driven Motion Modulation and Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fatigue modeling is essential for motion synthesis tasks to model human motions under fatigued conditions and biomechanical engineering applications, such as investigating the variations in movement patterns and posture due to fatigue, defining injury risk mitigation and prevention strategies, formulating fatigue minimization schemes and creating improved ergonomic designs. Nevertheless, employing data-driven methods for synthesizing the impact of fatigue on motion, receives little to no attention in the literature. In this work, we present Fatigue-PINN, a deep learning framework based on Physics-Informed Neural Networks, for modeling fatigued human movements, while providing joint-specific fatigue configurations for adaptation and mitigation of motion artifacts on a joint level, resulting in more realistic animations. To account for muscle fatigue, we simulate the fatigue-induced fluctuations in the maximum exerted joint torques by leveraging a PINN adaptation of the Three-Compartment Controller model to exploit physics-domain knowledge for improving accuracy. This model also introduces parametric motion alignment with respect to joint-specific fatigue, hence avoiding sharp frame transitions. Our results indicate that Fatigue-PINN accurately simulates the effects of externally perceived fatigue on open-type human movements being consistent with findings from real-world experimental fatigue studies. Since fatigue is incorporated in torque space, Fatigue-PINN provides an end-to-end encoder-decoder-like architecture, to ensure transforming joint angles to joint torques and vice-versa, thus, being compatible with motion synthesis frameworks operating on joint angles.
<div id='section'>Paperid: <span id='pid'>1349, <a href='https://arxiv.org/pdf/2502.18696.pdf' target='_blank'>https://arxiv.org/pdf/2502.18696.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Christos Papandreou, Michail Mathioudakis, Theodoros Stouraitis, Petros Iatropoulos, Antonios Nikitakis, Stavros Paschalakis, Konstantinos Kyriakopoulos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.18696">Interpretable Data-Driven Ship Dynamics Model: Enhancing Physics-Based Motion Prediction with Parameter Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The deployment of autonomous navigation systems on ships necessitates accurate motion prediction models tailored to individual vessels. Traditional physics-based models, while grounded in hydrodynamic principles, often fail to account for ship-specific behaviors under real-world conditions. Conversely, purely data-driven models offer specificity but lack interpretability and robustness in edge cases. This study proposes a data-driven physics-based model that integrates physics-based equations with data-driven parameter optimization, leveraging the strengths of both approaches to ensure interpretability and adaptability. The model incorporates physics-based components such as 3-DoF dynamics, rudder, and propeller forces, while parameters such as resistance curve and rudder coefficients are optimized using synthetic data. By embedding domain knowledge into the parameter optimization process, the fitted model maintains physical consistency. Validation of the approach is realized with two container ships by comparing, both qualitatively and quantitatively, predictions against ground-truth trajectories. The results demonstrate significant improvements, in predictive accuracy and reliability, of the data-driven physics-based models over baseline physics-based models tuned with traditional marine engineering practices. The fitted models capture ship-specific behaviors in diverse conditions with their predictions being, 51.6% (ship A) and 57.8% (ship B) more accurate, 72.36% (ship A) and 89.67% (ship B) more consistent.
<div id='section'>Paperid: <span id='pid'>1350, <a href='https://arxiv.org/pdf/2501.13804.pdf' target='_blank'>https://arxiv.org/pdf/2501.13804.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Michail Mathioudakis, Christos Papandreou, Theodoros Stouraitis, Vicky Margari, Antonios Nikitakis, Stavros Paschalakis, Konstantinos Kyriakopoulos, Kostas J. Spyrou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.13804">Towards Real-World Validation of a Physics-Based Ship Motion Prediction Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The maritime industry aims towards a sustainable future, which requires significant improvements in operational efficiency. Current approaches focus on minimising fuel consumption and emissions through greater autonomy. Efficient and safe autonomous navigation requires high-fidelity ship motion models applicable to real-world conditions. Although physics-based ship motion models can predict ships' motion with sub-second resolution, their validation in real-world conditions is rarely found in the literature. This study presents a physics-based 3D dynamics motion model that is tailored to a container-ship, and compares its predictions against real-world voyages. The model integrates vessel motion over time and accounts for its hydrodynamic behavior under different environmental conditions. The model's predictions are evaluated against real vessel data both visually and using multiple distance measures. Both methodologies demonstrate that the model's predictions align closely with the real-world trajectories of the container-ship.
<div id='section'>Paperid: <span id='pid'>1351, <a href='https://arxiv.org/pdf/2501.05744.pdf' target='_blank'>https://arxiv.org/pdf/2501.05744.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Loay Rashid, Siddharth Roheda, Amit Unde
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.05744">LLVD: LSTM-based Explicit Motion Modeling in Latent Space for Blind Video Denoising</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video restoration plays a pivotal role in revitalizing degraded video content by rectifying imperfections caused by various degradations introduced during capturing (sensor noise, motion blur, etc.), saving/sharing (compression, resizing, etc.) and editing. This paper introduces a novel algorithm designed for scenarios where noise is introduced during video capture, aiming to enhance the visual quality of videos by reducing unwanted noise artifacts. We propose the Latent space LSTM Video Denoiser (LLVD), an end-to-end blind denoising model. LLVD uniquely combines spatial and temporal feature extraction, employing Long Short Term Memory (LSTM) within the encoded feature domain. This integration of LSTM layers is crucial for maintaining continuity and minimizing flicker in the restored video. Moreover, processing frames in the encoded feature domain significantly reduces computations, resulting in a very lightweight architecture. LLVD's blind nature makes it versatile for real, in-the-wild denoising scenarios where prior information about noise characteristics is not available. Experiments reveal that LLVD demonstrates excellent performance for both synthetic and captured noise. Specifically, LLVD surpasses the current State-Of-The-Art (SOTA) in RAW denoising by 0.3dB, while also achieving a 59\% reduction in computational complexity.
<div id='section'>Paperid: <span id='pid'>1352, <a href='https://arxiv.org/pdf/2412.17344.pdf' target='_blank'>https://arxiv.org/pdf/2412.17344.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Akane Tsuboya, Yu Kono, Tatsuji Takahashi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.17344">Reinforcement Learning with a Focus on Adjusting Policies to Reach Targets</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The objective of a reinforcement learning agent is to discover better actions through exploration. However, typical exploration techniques aim to maximize rewards, often incurring high costs in both exploration and learning processes. We propose a novel deep reinforcement learning method, which prioritizes achieving an aspiration level over maximizing expected return. This method flexibly adjusts the degree of exploration based on the proportion of target achievement. Through experiments on a motion control task and a navigation task, this method achieved returns equal to or greater than other standard methods. The results of the analysis showed two things: our method flexibly adjusts the exploration scope, and it has the potential to enable the agent to adapt to non-stationary environments. These findings indicated that this method may have effectiveness in improving exploration efficiency in practical applications of reinforcement learning.
<div id='section'>Paperid: <span id='pid'>1353, <a href='https://arxiv.org/pdf/2412.14088.pdf' target='_blank'>https://arxiv.org/pdf/2412.14088.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lucas Dal'Col, Miguel Oliveira, VÃ­tor Santos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.14088">Joint Perception and Prediction for Autonomous Driving: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Perception and prediction modules are critical components of autonomous driving systems, enabling vehicles to navigate safely through complex environments. The perception module is responsible for perceiving the environment, including static and dynamic objects, while the prediction module is responsible for predicting the future behavior of these objects. These modules are typically divided into three tasks: object detection, object tracking, and motion prediction. Traditionally, these tasks are developed and optimized independently, with outputs passed sequentially from one to the next. However, this approach has significant limitations: computational resources are not shared across tasks, the lack of joint optimization can amplify errors as they propagate throughout the pipeline, and uncertainty is rarely propagated between modules, resulting in significant information loss. To address these challenges, the joint perception and prediction paradigm has emerged, integrating perception and prediction into a unified model through multi-task learning. This strategy not only overcomes the limitations of previous methods, but also enables the three tasks to have direct access to raw sensor data, allowing richer and more nuanced environmental interpretations. This paper presents the first comprehensive survey of joint perception and prediction for autonomous driving. We propose a taxonomy that categorizes approaches based on input representation, scene context modeling, and output representation, highlighting their contributions and limitations. Additionally, we present a qualitative analysis and quantitative comparison of existing methods. Finally, we discuss future research directions based on identified gaps in the state-of-the-art.
<div id='section'>Paperid: <span id='pid'>1354, <a href='https://arxiv.org/pdf/2412.11360.pdf' target='_blank'>https://arxiv.org/pdf/2412.11360.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ehsan Asali, Prashant Doshi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.11360">Visual IRL for Human-Like Robotic Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a novel method for collaborative robots (cobots) to learn manipulation tasks and perform them in a human-like manner. Our method falls under the learn-from-observation (LfO) paradigm, where robots learn to perform tasks by observing human actions, which facilitates quicker integration into industrial settings compared to programming from scratch. We introduce Visual IRL that uses the RGB-D keypoints in each frame of the observed human task performance directly as state features, which are input to inverse reinforcement learning (IRL). The inversely learned reward function, which maps keypoints to reward values, is transferred from the human to the cobot using a novel neuro-symbolic dynamics model, which maps human kinematics to the cobot arm. This model allows similar end-effector positioning while minimizing joint adjustments, aiming to preserve the natural dynamics of human motion in robotic manipulation. In contrast with previous techniques that focus on end-effector placement only, our method maps multiple joint angles of the human arm to the corresponding cobot joints. Moreover, it uses an inverse kinematics model to then minimally adjust the joint angles, for accurate end-effector positioning. We evaluate the performance of this approach on two different realistic manipulation tasks. The first task is produce processing, which involves picking, inspecting, and placing onions based on whether they are blemished. The second task is liquid pouring, where the robot picks up bottles, pours the contents into designated containers, and disposes of the empty bottles. Our results demonstrate advances in human-like robotic manipulation, leading to more human-robot compatibility in manufacturing applications.
<div id='section'>Paperid: <span id='pid'>1355, <a href='https://arxiv.org/pdf/2412.04649.pdf' target='_blank'>https://arxiv.org/pdf/2412.04649.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Simone Borelli, Francesco Giovinazzo, Francesco Grella, Giorgio Cannata
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.04649">Generating Whole-Body Avoidance Motion through Localized Proximity Sensing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a novel control algorithm for robotic manipulators in unstructured environments using proximity sensors partially distributed on the platform. The proposed approach exploits arrays of multi zone Time-of-Flight (ToF) sensors to generate a sparse point cloud representation of the robot surroundings. By employing computational geometry techniques, we fuse the knowledge of robot geometric model with ToFs sensory feedback to generate whole-body motion tasks, allowing to move both sensorized and non-sensorized links in response to unpredictable events such as human motion. In particular, the proposed algorithm computes the pair of closest points between the environment cloud and the robot links, generating a dynamic avoidance motion that is implemented as the highest priority task in a two-level hierarchical architecture. Such a design choice allows the robot to work safely alongside humans even without a complete sensorization over the whole surface. Experimental validation demonstrates the algorithm effectiveness both in static and dynamic scenarios, achieving comparable performances with respect to well established control techniques that aim to move the sensors mounting positions on the robot body. The presented algorithm exploits any arbitrary point on the robot surface to perform avoidance motion, showing improvements in the distance margin up to 100 mm, due to the rendering of virtual avoidance tasks on non-sensorized links.
<div id='section'>Paperid: <span id='pid'>1356, <a href='https://arxiv.org/pdf/2411.18377.pdf' target='_blank'>https://arxiv.org/pdf/2411.18377.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Denys Rozumnyi, Nadine Bertsch, Othman Sbai, Filippo Arcadu, Yuhua Chen, Artsiom Sanakoyeu, Manoj Kumar, Catherine Herold, Robin Kips
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.18377">XR-MBT: Multi-modal Full Body Tracking for XR through Self-Supervision with Learned Depth Point Cloud Registration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tracking the full body motions of users in XR (AR/VR) devices is a fundamental challenge to bring a sense of authentic social presence. Due to the absence of dedicated leg sensors, currently available body tracking methods adopt a synthesis approach to generate plausible motions given a 3-point signal from the head and controller tracking. In order to enable mixed reality features, modern XR devices are capable of estimating depth information of the headset surroundings using available sensors combined with dedicated machine learning models. Such egocentric depth sensing cannot drive the body directly, as it is not registered and is incomplete due to limited field-of-view and body self-occlusions. For the first time, we propose to leverage the available depth sensing signal combined with self-supervision to learn a multi-modal pose estimation model capable of tracking full body motions in real time on XR devices. We demonstrate how current 3-point motion synthesis models can be extended to point cloud modalities using a semantic point cloud encoder network combined with a residual network for multi-modal pose estimation. These modules are trained jointly in a self-supervised way, leveraging a combination of real unregistered point clouds and simulated data obtained from motion capture. We compare our approach against several state-of-the-art systems for XR body tracking and show that our method accurately tracks a diverse range of body motions. XR-MBT tracks legs in XR for the first time, whereas traditional synthesis approaches based on partial body tracking are blind.
<div id='section'>Paperid: <span id='pid'>1357, <a href='https://arxiv.org/pdf/2411.13856.pdf' target='_blank'>https://arxiv.org/pdf/2411.13856.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dexian Ma, Yirong Liu, Wenbo Liu, Bo Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.13856">A Data-Driven Modeling and Motion Control of Heavy-Load Hydraulic Manipulators via Reversible Transformation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work proposes a data-driven modeling and the corresponding hybrid motion control framework for unmanned and automated operation of industrial heavy-load hydraulic manipulator. Rather than the direct use of a neural network black box, we construct a reversible nonlinear model by using multilayer perceptron to approximate dynamics in the physical integrator chain system after reversible transformations. The reversible nonlinear model is trained offline using supervised learning techniques, and the data are obtained from simulations or experiments. Entire hybrid motion control framework consists of the model inversion controller that compensates for the nonlinear dynamics and proportional-derivative controller that enhances the robustness. The stability is proved with Lyapunov theory. Co-simulation and Experiments show the effectiveness of proposed modeling and hybrid control framework. With a commercial 39-ton class hydraulic excavator for motion control tasks, the root mean square error of trajectory tracking error decreases by at least 50\% compared to traditional control methods. In addition, by analyzing the system model, the proposed framework can be rapidly applied to different control plants.
<div id='section'>Paperid: <span id='pid'>1358, <a href='https://arxiv.org/pdf/2411.04151.pdf' target='_blank'>https://arxiv.org/pdf/2411.04151.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kehua Qu, Rui Ding, Jin Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.04151">UnityGraph: Unified Learning of Spatio-temporal features for Multi-person Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-person motion prediction is a complex and emerging field with significant real-world applications. Current state-of-the-art methods typically adopt dual-path networks to separately modeling spatial features and temporal features. However, the uncertain compatibility of the two networks brings a challenge for spatio-temporal features fusion and violate the spatio-temporal coherence and coupling of human motions by nature. To address this issue, we propose a novel graph structure, UnityGraph, which treats spatio-temporal features as a whole, enhancing model coherence and coupling.spatio-temporal features as a whole, enhancing model coherence and coupling. Specifically, UnityGraph is a hypervariate graph based network. The flexibility of the hypergraph allows us to consider the observed motions as graph nodes. We then leverage hyperedges to bridge these nodes for exploring spatio-temporal features. This perspective considers spatio-temporal dynamics unitedly and reformulates multi-person motion prediction into a problem on a single graph. Leveraging the dynamic message passing based on this hypergraph, our model dynamically learns from both types of relations to generate targeted messages that reflect the relevance among nodes. Extensive experiments on several datasets demonstrates that our method achieves state-of-the-art performance, confirming its effectiveness and innovative design.
<div id='section'>Paperid: <span id='pid'>1359, <a href='https://arxiv.org/pdf/2411.03729.pdf' target='_blank'>https://arxiv.org/pdf/2411.03729.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kehua Qu, Rui Ding, Jin Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.03729">Relation Learning and Aggregate-attention for Multi-person Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-person motion prediction is an emerging and intricate task with broad real-world applications. Unlike single person motion prediction, it considers not just the skeleton structures or human trajectories but also the interactions between others. Previous methods use various networks to achieve impressive predictions but often overlook that the joints relations within an individual (intra-relation) and interactions among groups (inter-relation) are distinct types of representations. These methods often lack explicit representation of inter&intra-relations, and inevitably introduce undesired dependencies. To address this issue, we introduce a new collaborative framework for multi-person motion prediction that explicitly modeling these relations:a GCN-based network for intra-relations and a novel reasoning network for inter-relations.Moreover, we propose a novel plug-and-play aggregation module called the Interaction Aggregation Module (IAM), which employs an aggregate-attention mechanism to seamlessly integrate these relations. Experiments indicate that the module can also be applied to other dual-path models. Extensive experiments on the 3DPW, 3DPW-RC, CMU-Mocap, MuPoTS-3D, as well as synthesized datasets Mix1 & Mix2 (9 to 15 persons), demonstrate that our method achieves state-of-the-art performance.
<div id='section'>Paperid: <span id='pid'>1360, <a href='https://arxiv.org/pdf/2410.19606.pdf' target='_blank'>https://arxiv.org/pdf/2410.19606.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kai-Yin Hong, Chieh-Chih Wang, Wen-Chieh Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.19606">Multi-modal Motion Prediction using Temporal Ensembling with Learning-based Aggregation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent years have seen a shift towards learning-based methods for trajectory prediction, with challenges remaining in addressing uncertainty and capturing multi-modal distributions. This paper introduces Temporal Ensembling with Learning-based Aggregation, a meta-algorithm designed to mitigate the issue of missing behaviors in trajectory prediction, which leads to inconsistent predictions across consecutive frames. Unlike conventional model ensembling, temporal ensembling leverages predictions from nearby frames to enhance spatial coverage and prediction diversity. By confirming predictions from multiple frames, temporal ensembling compensates for occasional errors in individual frame predictions. Furthermore, trajectory-level aggregation, often utilized in model ensembling, is insufficient for temporal ensembling due to a lack of consideration of traffic context and its tendency to assign candidate trajectories with incorrect driving behaviors to final predictions. We further emphasize the necessity of learning-based aggregation by utilizing mode queries within a DETR-like architecture for our temporal ensembling, leveraging the characteristics of predictions from nearby frames. Our method, validated on the Argoverse 2 dataset, shows notable improvements: a 4% reduction in minADE, a 5% decrease in minFDE, and a 1.16% reduction in the miss rate compared to the strongest baseline, QCNet, highlighting its efficacy and potential in autonomous driving.
<div id='section'>Paperid: <span id='pid'>1361, <a href='https://arxiv.org/pdf/2410.12023.pdf' target='_blank'>https://arxiv.org/pdf/2410.12023.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mykhaylo Andriluka, Baruch Tabanpour, C. Daniel Freeman, Cristian Sminchisescu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.12023">Learned Neural Physics Simulation for Articulated 3D Human Pose Reconstruction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a novel neural network approach, LARP (Learned Articulated Rigid body Physics), to model the dynamics of articulated human motion with contact. Our goal is to develop a faster and more convenient methodological alternative to traditional physics simulators for use in computer vision tasks such as human motion reconstruction from video. To that end we introduce a training procedure and model components that support the construction of a recurrent neural architecture to accurately simulate articulated rigid body dynamics. Our neural architecture supports features typically found in traditional physics simulators, such as modeling of joint motors, variable dimensions of body parts, contact between body parts and objects, and is an order of magnitude faster than traditional systems when multiple simulations are run in parallel. To demonstrate the value of LARP we use it as a drop-in replacement for a state of the art classical non-differentiable simulator in an existing video-based reconstruction framework and show comparative or better 3D human pose reconstruction accuracy.
<div id='section'>Paperid: <span id='pid'>1362, <a href='https://arxiv.org/pdf/2410.11491.pdf' target='_blank'>https://arxiv.org/pdf/2410.11491.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Niklas Gunnarsson, Jens SjÃ¶lund, Peter Kimstrand, Thomas. B SchÃ¶n
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.11491">Online learning in motion modeling for intra-interventional image sequences</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image monitoring and guidance during medical examinations can aid both diagnosis and treatment. However, the sampling frequency is often too low, which creates a need to estimate the missing images. We present a probabilistic motion model for sequential medical images, with the ability to both estimate motion between acquired images and forecast the motion ahead of time. The core is a low-dimensional temporal process based on a linear Gaussian state-space model with analytically tractable solutions for forecasting, simulation, and imputation of missing samples. The results, from two experiments on publicly available cardiac datasets, show reliable motion estimates and an improved forecasting performance using patient-specific adaptation by online learning.
<div id='section'>Paperid: <span id='pid'>1363, <a href='https://arxiv.org/pdf/2409.17790.pdf' target='_blank'>https://arxiv.org/pdf/2409.17790.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Harsh Yadav, Maximilian Schaefer, Kun Zhao, Tobias Meisen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.17790">CASPFormer: Trajectory Prediction from BEV Images with Deformable Attention</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion prediction is an important aspect for Autonomous Driving (AD) and Advance Driver Assistance Systems (ADAS). Current state-of-the-art motion prediction methods rely on High Definition (HD) maps for capturing the surrounding context of the ego vehicle. Such systems lack scalability in real-world deployment as HD maps are expensive to produce and update in real-time. To overcome this issue, we propose Context Aware Scene Prediction Transformer (CASPFormer), which can perform multi-modal motion prediction from rasterized Bird-Eye-View (BEV) images. Our system can be integrated with any upstream perception module that is capable of generating BEV images. Moreover, CASPFormer directly decodes vectorized trajectories without any postprocessing. Trajectories are decoded recurrently using deformable attention, as it is computationally efficient and provides the network with the ability to focus its attention on the important spatial locations of the BEV images. In addition, we also address the issue of mode collapse for generating multiple scene-consistent trajectories by incorporating learnable mode queries. We evaluate our model on the nuScenes dataset and show that it reaches state-of-the-art across multiple metrics
<div id='section'>Paperid: <span id='pid'>1364, <a href='https://arxiv.org/pdf/2409.12140.pdf' target='_blank'>https://arxiv.org/pdf/2409.12140.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sai Shashank Kalakonda, Shubh Maheshwari, Ravi Kiran Sarvadevabhatla
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.12140">MoRAG -- Multi-Fusion Retrieval Augmented Generation for Human Motion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce MoRAG, a novel multi-part fusion based retrieval-augmented generation strategy for text-based human motion generation. The method enhances motion diffusion models by leveraging additional knowledge obtained through an improved motion retrieval process. By effectively prompting large language models (LLMs), we address spelling errors and rephrasing issues in motion retrieval. Our approach utilizes a multi-part retrieval strategy to improve the generalizability of motion retrieval across the language space. We create diverse samples through the spatial composition of the retrieved motions. Furthermore, by utilizing low-level, part-specific motion information, we can construct motion samples for unseen text descriptions. Our experiments demonstrate that our framework can serve as a plug-and-play module, improving the performance of motion diffusion models. Code, pretrained models and sample videos are available at: https://motion-rag.github.io/
<div id='section'>Paperid: <span id='pid'>1365, <a href='https://arxiv.org/pdf/2409.11623.pdf' target='_blank'>https://arxiv.org/pdf/2409.11623.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dayuan Tan, Mohamed Younis, Wassila Lalouani, Shuyao Fan, Guozhi Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.11623">A novel pedestrian road crossing simulator for dynamic traffic light scheduling systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The major advances in intelligent transportation systems are pushing societal services toward autonomy where road management is to be more agile in order to cope with changes and continue to yield optimal performance. However, the pedestrian experience is not sufficiently considered. Particularly, signalized intersections are expected to be popular if not dominant in urban settings where pedestrian density is high. This paper presents the design of a novel environment for simulating human motion on signalized crosswalks at a fine-grained level. Such a simulation not only captures typical behavior, but also handles cases where large pedestrian groups cross from both directions. The proposed simulator is instrumental for optimized road configuration management where the pedestrians' quality of experience, for example, waiting time, is factored in. The validation results using field data show that an accuracy of 98.37 percent can be obtained for the estimated crossing time. Other results using synthetic data show that our simulator enables optimized traffic light scheduling that diminishes pedestrians' waiting time without sacrificing vehicular throughput.
<div id='section'>Paperid: <span id='pid'>1366, <a href='https://arxiv.org/pdf/2409.07798.pdf' target='_blank'>https://arxiv.org/pdf/2409.07798.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Liang Feng, Zhixuan Shen, Lihua Wen, Shiyao Li, Ming Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.07798">GateAttentionPose: Enhancing Pose Estimation with Agent Attention and Improved Gated Convolutions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces GateAttentionPose, an innovative approach that enhances the UniRepLKNet architecture for pose estimation tasks. We present two key contributions: the Agent Attention module and the Gate-Enhanced Feedforward Block (GEFB). The Agent Attention module replaces large kernel convolutions, significantly improving computational efficiency while preserving global context modeling. The GEFB augments feature extraction and processing capabilities, particularly in complex scenes. Extensive evaluations on COCO and MPII datasets demonstrate that GateAttentionPose outperforms existing state-of-the-art methods, including the original UniRepLKNet, achieving superior or comparable results with improved efficiency. Our approach offers a robust solution for pose estimation across diverse applications, including autonomous driving, human motion capture, and virtual reality.
<div id='section'>Paperid: <span id='pid'>1367, <a href='https://arxiv.org/pdf/2409.07752.pdf' target='_blank'>https://arxiv.org/pdf/2409.07752.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Liang Feng, Ming Xu, Lihua Wen, Zhixuan Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.07752">GatedUniPose: A Novel Approach for Pose Estimation Combining UniRepLKNet and Gated Convolution</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pose estimation is a crucial task in computer vision, with wide applications in autonomous driving, human motion capture, and virtual reality. However, existing methods still face challenges in achieving high accuracy, particularly in complex scenes. This paper proposes a novel pose estimation method, GatedUniPose, which combines UniRepLKNet and Gated Convolution and introduces the GLACE module for embedding. Additionally, we enhance the feature map concatenation method in the head layer by using DySample upsampling. Compared to existing methods, GatedUniPose excels in handling complex scenes and occlusion challenges. Experimental results on the COCO, MPII, and CrowdPose datasets demonstrate that GatedUniPose achieves significant performance improvements with a relatively small number of parameters, yielding better or comparable results to models with similar or larger parameter sizes.
<div id='section'>Paperid: <span id='pid'>1368, <a href='https://arxiv.org/pdf/2409.03421.pdf' target='_blank'>https://arxiv.org/pdf/2409.03421.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiong Yang, Hao Ren, Dong Guo, Zhengrong Ling, Tieshan Zhang, Gen Li, Yifeng Tang, Haoxiang Zhao, Jiale Wang, Hongyuan Chang, Jia Dong, Yajing Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.03421">F3T: A soft tactile unit with 3D force and temperature mathematical decoupling ability for robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The human skin exhibits remarkable capability to perceive contact forces and environmental temperatures, providing intricate information essential for nuanced manipulation. Despite recent advancements in soft tactile sensors, a significant challenge remains in accurately decoupling signals - specifically, separating force from directional orientation and temperature - resulting in fail to meet the advanced application requirements of robots. This research proposes a multi-layered soft sensor unit (F3T) designed to achieve isolated measurements and mathematical decoupling of normal pressure, omnidirectional tangential forces, and temperature. We developed a circular coaxial magnetic film featuring a floating-mountain multi-layer capacitor, facilitating the physical decoupling of normal and tangential forces in all directions. Additionally, we incorporated an ion gel-based temperature sensing film atop the tactile sensor. This sensor is resilient to external pressure and deformation, enabling it to measure temperature and, crucially, eliminate capacitor errors induced by environmental temperature changes. This innovative design allows for the decoupled measurement of multiple signals, paving the way for advancements in higher-level robot motion control, autonomous decision-making, and task planning.
<div id='section'>Paperid: <span id='pid'>1369, <a href='https://arxiv.org/pdf/2409.01591.pdf' target='_blank'>https://arxiv.org/pdf/2409.01591.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sohan Anisetty, James Hays
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.01591">Dynamic Motion Synthesis: Masked Audio-Text Conditioned Spatio-Temporal Transformers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Our research presents a novel motion generation framework designed to produce whole-body motion sequences conditioned on multiple modalities simultaneously, specifically text and audio inputs. Leveraging Vector Quantized Variational Autoencoders (VQVAEs) for motion discretization and a bidirectional Masked Language Modeling (MLM) strategy for efficient token prediction, our approach achieves improved processing efficiency and coherence in the generated motions. By integrating spatial attention mechanisms and a token critic we ensure consistency and naturalness in the generated motions. This framework expands the possibilities of motion generation, addressing the limitations of existing approaches and opening avenues for multimodal motion synthesis.
<div id='section'>Paperid: <span id='pid'>1370, <a href='https://arxiv.org/pdf/2409.01241.pdf' target='_blank'>https://arxiv.org/pdf/2409.01241.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sorin Grigorescu, Mihai Zaha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.01241">CyberCortex.AI: An AI-based Operating System for Autonomous Robotics and Complex Automation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The underlying framework for controlling autonomous robots and complex automation applications are Operating Systems (OS) capable of scheduling perception-and-control tasks, as well as providing real-time data communication to other robotic peers and remote cloud computers. In this paper, we introduce CyberCortex AI, a robotics OS designed to enable heterogeneous AI-based robotics and complex automation applications. CyberCortex AI is a decentralized distributed OS which enables robots to talk to each other, as well as to High Performance Computers (HPC) in the cloud. Sensory and control data from the robots is streamed towards HPC systems with the purpose of training AI algorithms, which are afterwards deployed on the robots. Each functionality of a robot (e.g. sensory data acquisition, path planning, motion control, etc.) is executed within a so-called DataBlock of Filters shared through the internet, where each filter is computed either locally on the robot itself, or remotely on a different robotic system. The data is stored and accessed via a so-called Temporal Addressable Memory (TAM), which acts as a gateway between each filter's input and output. CyberCortex AI has two main components: i) the CyberCortex AI inference system, which is a real-time implementation of the DataBlock running on the robots' embedded hardware, and ii) the CyberCortex AI dojo, which runs on an HPC computer in the cloud, and it is used to design, train and deploy AI algorithms. We present a quantitative and qualitative performance analysis of the proposed approach using two collaborative robotics applications: i) a forest fires prevention system based on an Unitree A1 legged robot and an Anafi Parrot 4K drone, as well as ii) an autonomous driving system which uses CyberCortex AI for collaborative perception and motion control.
<div id='section'>Paperid: <span id='pid'>1371, <a href='https://arxiv.org/pdf/2408.14885.pdf' target='_blank'>https://arxiv.org/pdf/2408.14885.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sven Goblirsch, Marcel Weinmann, Johannes Betz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.14885">Three-Dimensional Vehicle Dynamics State Estimation for High-Speed Race Cars under varying Signal Quality</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work aims to present a three-dimensional vehicle dynamics state estimation under varying signal quality. Few researchers have investigated the impact of three-dimensional road geometries on the state estimation and, thus, neglect road inclination and banking. Especially considering high velocities and accelerations, the literature does not address these effects. Therefore, we compare two- and three-dimensional state estimation schemes to outline the impact of road geometries. We use an Extended Kalman Filter with a point-mass motion model and extend it by an additional formulation of reference angles. Furthermore, virtual velocity measurements significantly improve the estimation of road angles and the vehicle's side slip angle. We highlight the importance of steady estimations for vehicle motion control algorithms and demonstrate the challenges of degraded signal quality and Global Navigation Satellite System dropouts. The proposed adaptive covariance facilitates a smooth estimation and enables stable controller behavior. The developed state estimation has been deployed on a high-speed autonomous race car at various racetracks. Our findings indicate that our approach outperforms state-of-the-art vehicle dynamics state estimators and an industry-grade Inertial Navigation System. Further studies are needed to investigate the performance under varying track conditions and on other vehicle types.
<div id='section'>Paperid: <span id='pid'>1372, <a href='https://arxiv.org/pdf/2408.05940.pdf' target='_blank'>https://arxiv.org/pdf/2408.05940.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Eunsoo Im, Changhyun Jee, Jung Kwon Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.05940">Spb3DTracker: A Robust LiDAR-Based Person Tracker for Noisy Environment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Person detection and tracking (PDT) has seen significant advancements with 2D camera-based systems in the autonomous vehicle field, leading to widespread adoption of these algorithms. However, growing privacy concerns have recently emerged as a major issue, prompting a shift towards LiDAR-based PDT as a viable alternative. Within this domain, "Tracking-by-Detection" (TBD) has become a prominent methodology. Despite its effectiveness, LiDAR-based PDT has not yet achieved the same level of performance as camera-based PDT. This paper examines key components of the LiDAR-based PDT framework, including detection post-processing, data association, motion modeling, and lifecycle management. Building upon these insights, we introduce SpbTrack, a robust person tracker designed for diverse environments. Our method achieves superior performance on noisy datasets and state-of-the-art results on KITTI Dataset benchmarks and custom office indoor dataset among LiDAR-based trackers.
<div id='section'>Paperid: <span id='pid'>1373, <a href='https://arxiv.org/pdf/2408.02855.pdf' target='_blank'>https://arxiv.org/pdf/2408.02855.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aleksa Marusic, Louis Annabi, Sao Msi Nguyen, Adriana Tapus
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.02855">Analyzing Data Efficiency and Performance of Machine Learning Algorithms for Assessing Low Back Pain Physical Rehabilitation Exercises</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Analyzing human motion is an active research area, with various applications. In this work, we focus on human motion analysis in the context of physical rehabilitation using a robot coach system. Computer-aided assessment of physical rehabilitation entails evaluation of patient performance in completing prescribed rehabilitation exercises, based on processing movement data captured with a sensory system, such as RGB and RGB-D cameras. As 2D and 3D human pose estimation from RGB images had made impressive improvements, we aim to compare the assessment of physical rehabilitation exercises using movement data obtained from both RGB-D camera (Microsoft Kinect) and estimation from RGB videos (OpenPose and BlazePose algorithms). A Gaussian Mixture Model (GMM) is employed from position (and orientation) features, with performance metrics defined based on the log-likelihood values from GMM. The evaluation is performed on a medical database of clinical patients carrying out low back-pain rehabilitation exercises, previously coached by robot Poppy.
<div id='section'>Paperid: <span id='pid'>1374, <a href='https://arxiv.org/pdf/2407.18140.pdf' target='_blank'>https://arxiv.org/pdf/2407.18140.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alexandre Girard, Jean-SÃ©bastien Plante
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.18140">Influence Vectors Control for Robots Using Cellular-like Binary Actuators</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robots using cellular-like redundant binary actuators could outmatch electric-gearmotor robotic systems in terms of reliability, force-to-weight ratio and cost. This paper presents a robust fault tolerant control scheme that is designed to meet the control challenges encountered by such robots, i.e., discrete actuator inputs, complex system modeling and cross-coupling between actuators. In the proposed scheme, a desired vectorial system output, such as a position or a force, is commanded by recruiting actuators based on their influence vectors on the output. No analytical model of the system is needed; influence vectors are identified experimentally by sequentially activating each actuator. For position control tasks, the controller uses a probabilistic approach and a genetic algorithm to determine an optimal combination of actuators to recruit. For motion control tasks, the controller uses a sliding mode approach and independent recruiting decision for each actuator. Experimental results on a four degrees of freedom binary manipulator with twenty actuators confirm the method's effectiveness, and its ability to tolerate massive perturbations and numerous actuator failures.
<div id='section'>Paperid: <span id='pid'>1375, <a href='https://arxiv.org/pdf/2407.16788.pdf' target='_blank'>https://arxiv.org/pdf/2407.16788.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Su Li, Wang Liang, Jianye Wang, Ziheng Zhang, Lei Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.16788">Occlusion-Aware 3D Motion Interpretation for Abnormal Behavior Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Estimating abnormal posture based on 3D pose is vital in human pose analysis, yet it presents challenges, especially when reconstructing 3D human poses from monocular datasets with occlusions. Accurate reconstructions enable the restoration of 3D movements, which assist in the extraction of semantic details necessary for analyzing abnormal behaviors. However, most existing methods depend on predefined key points as a basis for estimating the coordinates of occluded joints, where variations in data quality have adversely affected the performance of these models. In this paper, we present OAD2D, which discriminates against motion abnormalities based on reconstructing 3D coordinates of mesh vertices and human joints from monocular videos. The OAD2D employs optical flow to capture motion prior information in video streams, enriching the information on occluded human movements and ensuring temporal-spatial alignment of poses. Moreover, we reformulate the abnormal posture estimation by coupling it with Motion to Text (M2T) model in which, the VQVAE is employed to quantize motion features. This approach maps motion tokens to text tokens, allowing for a semantically interpretable analysis of motion, and enhancing the generalization of abnormal posture detection boosted by Language model. Our approach demonstrates the robustness of abnormal behavior detection against severe and self-occlusions, as it reconstructs human motion trajectories in global coordinates to effectively mitigate occlusion issues. Our method, validated using the Human3.6M, 3DPW, and NTU RGB+D datasets, achieves a high $F_1-$Score of 0.94 on the NTU RGB+D dataset for medical condition detection. And we will release all of our code and data.
<div id='section'>Paperid: <span id='pid'>1376, <a href='https://arxiv.org/pdf/2407.00738.pdf' target='_blank'>https://arxiv.org/pdf/2407.00738.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Momir AdÅ¾emoviÄ, Predrag TadiÄ, Andrija PetroviÄ, Mladen NikoliÄ
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.00738">Engineering an Efficient Object Tracker for Non-Linear Motion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The goal of multi-object tracking is to detect and track all objects in a scene while maintaining unique identifiers for each, by associating their bounding boxes across video frames. This association relies on matching motion and appearance patterns of detected objects. This task is especially hard in case of scenarios involving dynamic and non-linear motion patterns. In this paper, we introduce DeepMoveSORT, a novel, carefully engineered multi-object tracker designed specifically for such scenarios. In addition to standard methods of appearance-based association, we improve motion-based association by employing deep learnable filters (instead of the most commonly used Kalman filter) and a rich set of newly proposed heuristics. Our improvements to motion-based association methods are severalfold. First, we propose a new transformer-based filter architecture, TransFilter, which uses an object's motion history for both motion prediction and noise filtering. We further enhance the filter's performance by careful handling of its motion history and accounting for camera motion. Second, we propose a set of heuristics that exploit cues from the position, shape, and confidence of detected bounding boxes to improve association performance. Our experimental evaluation demonstrates that DeepMoveSORT outperforms existing trackers in scenarios featuring non-linear motion, surpassing state-of-the-art results on three such datasets. We also perform a thorough ablation study to evaluate the contributions of different tracker components which we proposed. Based on our study, we conclude that using a learnable filter instead of the Kalman filter, along with appearance-based association is key to achieving strong general tracking performance.
<div id='section'>Paperid: <span id='pid'>1377, <a href='https://arxiv.org/pdf/2406.07169.pdf' target='_blank'>https://arxiv.org/pdf/2406.07169.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mirgahney Mohamed, Harry Jake Cunningham, Marc P. Deisenroth, Lourdes Agapito
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.07169">RecMoDiffuse: Recurrent Flow Diffusion for Human Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion generation has paramount importance in computer animation. It is a challenging generative temporal modelling task due to the vast possibilities of human motion, high human sensitivity to motion coherence and the difficulty of accurately generating fine-grained motions. Recently, diffusion methods have been proposed for human motion generation due to their high sample quality and expressiveness. However, generated sequences still suffer from motion incoherence, and are limited to short duration, and simpler motion and take considerable time during inference. To address these limitations, we propose \textit{RecMoDiffuse: Recurrent Flow Diffusion}, a new recurrent diffusion formulation for temporal modelling. Unlike previous work, which applies diffusion to the whole sequence without any temporal dependency, an approach that inherently makes temporal consistency hard to achieve. Our method explicitly enforces temporal constraints with the means of normalizing flow models in the diffusion process and thereby extends diffusion to the temporal dimension. We demonstrate the effectiveness of RecMoDiffuse in the temporal modelling of human motion. Our experiments show that RecMoDiffuse achieves comparable results with state-of-the-art methods while generating coherent motion sequences and reducing the computational overhead in the inference stage.
<div id='section'>Paperid: <span id='pid'>1378, <a href='https://arxiv.org/pdf/2405.12616.pdf' target='_blank'>https://arxiv.org/pdf/2405.12616.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Till Hielscher, Lukas Heuer, Frederik Wulle, Luigi Palmieri
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.12616">Towards Using Fast Embedded Model Predictive Control for Human-Aware Predictive Robot Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Predictive planning is a key capability for robots to efficiently and safely navigate populated environments. Particularly in densely crowded scenes, with uncertain human motion predictions, predictive path planning, and control can become expensive to compute in real time due to the curse of dimensionality. With the goal of achieving pro-active and legible robot motion in shared environments, in this paper we present HuMAN-MPC, a computationally efficient algorithm for Human Motion Aware Navigation using fast embedded Model Predictive Control. The approach consists of a novel model predictive control (MPC) formulation that leverages a fast state-of-the-art optimization backend based on a sequential quadratic programming real-time iteration scheme while also providing feasibility monitoring. Our experiments, in simulation and on a fully integrated ROS-based platform, show that the approach achieves great scalability with fast computation times without penalizing path quality and efficiency of the resulting avoidance behavior.
<div id='section'>Paperid: <span id='pid'>1379, <a href='https://arxiv.org/pdf/2404.17837.pdf' target='_blank'>https://arxiv.org/pdf/2404.17837.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiming Bao, Xu Zhao, Dahong Qian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.17837">Hybrid 3D Human Pose Estimation with Monocular Video and Sparse IMUs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Temporal 3D human pose estimation from monocular videos is a challenging task in human-centered computer vision due to the depth ambiguity of 2D-to-3D lifting. To improve accuracy and address occlusion issues, inertial sensor has been introduced to provide complementary source of information. However, it remains challenging to integrate heterogeneous sensor data for producing physically rational 3D human poses. In this paper, we propose a novel framework, Real-time Optimization and Fusion (RTOF), to address this issue. We first incorporate sparse inertial orientations into a parametric human skeleton to refine 3D poses in kinematics. The poses are then optimized by energy functions built on both visual and inertial observations to reduce the temporal jitters. Our framework outputs smooth and biomechanically plausible human motion. Comprehensive experiments with ablation studies demonstrate its rationality and efficiency. On Total Capture dataset, the pose estimation error is significantly decreased compared to the baseline method.
<div id='section'>Paperid: <span id='pid'>1380, <a href='https://arxiv.org/pdf/2404.15371.pdf' target='_blank'>https://arxiv.org/pdf/2404.15371.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aman Kumar, Mark Litterick, Samuele Candido
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.15371">Efficient Verification of a RADAR SoC Using Formal and Simulation-Based Methods</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As the demand for Internet of Things (IoT) and Human-to-Machine Interaction (HMI) increases, modern System-on-Chips (SoCs) offering such solutions are becoming increasingly complex. This intricate design poses significant challenges for verification, particularly when time-to-market is a crucial factor for consumer electronics products. This paper presents a case study based on our work to verify a complex Radio Detection And Ranging (RADAR) based SoC that performs on-chip sensing of human motion with millimetre accuracy. We leverage both formal and simulation-based methods to complement each other and achieve verification sign-off with high confidence. While employing a requirements-driven flow approach, we demonstrate the use of different verification methods to cater to multiple requirements and highlight our know-how from the project. Additionally, we used Machine Learning (ML) based methods, specifically the Xcelium ML tool from Cadence, to improve verification throughput.
<div id='section'>Paperid: <span id='pid'>1381, <a href='https://arxiv.org/pdf/2404.11576.pdf' target='_blank'>https://arxiv.org/pdf/2404.11576.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fei Cui, Jiaojiao Fang, Xiaojiang Wu, Zelong Lai, Mengke Yang, Menghan Jia, Guizhong Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.11576">State-space Decomposition Model for Video Prediction Considering Long-term Motion Trend</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Stochastic video prediction enables the consideration of uncertainty in future motion, thereby providing a better reflection of the dynamic nature of the environment. Stochastic video prediction methods based on image auto-regressive recurrent models need to feed their predictions back into the latent space. Conversely, the state-space models, which decouple frame synthesis and temporal prediction, proves to be more efficient. However, inferring long-term temporal information about motion and generalizing to dynamic scenarios under non-stationary assumptions remains an unresolved challenge. In this paper, we propose a state-space decomposition stochastic video prediction model that decomposes the overall video frame generation into deterministic appearance prediction and stochastic motion prediction. Through adaptive decomposition, the model's generalization capability to dynamic scenarios is enhanced. In the context of motion prediction, obtaining a prior on the long-term trend of future motion is crucial. Thus, in the stochastic motion prediction branch, we infer the long-term motion trend from conditional frames to guide the generation of future frames that exhibit high consistency with the conditional frames. Experimental results demonstrate that our model outperforms baselines on multiple datasets.
<div id='section'>Paperid: <span id='pid'>1382, <a href='https://arxiv.org/pdf/2404.02263.pdf' target='_blank'>https://arxiv.org/pdf/2404.02263.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Youshaa Murhij, Dmitry Yudin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.02263">OFMPNet: Deep End-to-End Model for Occupancy and Flow Prediction in Urban Environment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The task of motion prediction is pivotal for autonomous driving systems, providing crucial data to choose a vehicle behavior strategy within its surroundings. Existing motion prediction techniques primarily focus on predicting the future trajectory of each agent in the scene individually, utilizing its past trajectory data. In this paper, we introduce an end-to-end neural network methodology designed to predict the future behaviors of all dynamic objects in the environment. This approach leverages the occupancy map and the scene's motion flow. We are investigatin various alternatives for constructing a deep encoder-decoder model called OFMPNet. This model uses a sequence of bird's-eye-view road images, occupancy grid, and prior motion flow as input data. The encoder of the model can incorporate transformer, attention-based, or convolutional units. The decoder considers the use of both convolutional modules and recurrent blocks. Additionally, we propose a novel time-weighted motion flow loss, whose application has shown a substantial decrease in end-point error. Our approach has achieved state-of-the-art results on the Waymo Occupancy and Flow Prediction benchmark, with a Soft IoU of 52.1% and an AUC of 76.75% on Flow-Grounded Occupancy.
<div id='section'>Paperid: <span id='pid'>1383, <a href='https://arxiv.org/pdf/2404.00163.pdf' target='_blank'>https://arxiv.org/pdf/2404.00163.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi-Heng Cao, Vincent Bourbonne, FranÃ§ois Lucia, Ulrike Schick, Julien Bert, Vincent Jaouen, Dimitris Visvikis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.00163">CT respiratory motion synthesis using joint supervised and adversarial learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Objective: Four-dimensional computed tomography (4DCT) imaging consists in reconstructing a CT acquisition into multiple phases to track internal organ and tumor motion. It is commonly used in radiotherapy treatment planning to establish planning target volumes. However, 4DCT increases protocol complexity, may not align with patient breathing during treatment, and lead to higher radiation delivery. Approach: In this study, we propose a deep synthesis method to generate pseudo respiratory CT phases from static images for motion-aware treatment planning. The model produces patient-specific deformation vector fields (DVFs) by conditioning synthesis on external patient surface-based estimation, mimicking respiratory monitoring devices. A key methodological contribution is to encourage DVF realism through supervised DVF training while using an adversarial term jointly not only on the warped image but also on the magnitude of the DVF itself. This way, we avoid excessive smoothness typically obtained through deep unsupervised learning, and encourage correlations with the respiratory amplitude. Main results: Performance is evaluated using real 4DCT acquisitions with smaller tumor volumes than previously reported. Results demonstrate for the first time that the generated pseudo-respiratory CT phases can capture organ and tumor motion with similar accuracy to repeated 4DCT scans of the same patient. Mean inter-scans tumor center-of-mass distances and Dice similarity coefficients were $1.97$mm and $0.63$, respectively, for real 4DCT phases and $2.35$mm and $0.71$ for synthetic phases, and compares favorably to a state-of-the-art technique (RMSim).
<div id='section'>Paperid: <span id='pid'>1384, <a href='https://arxiv.org/pdf/2403.12214.pdf' target='_blank'>https://arxiv.org/pdf/2403.12214.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gerry Chen, Tristan Al-Haddad, Frank Dellaert, Seth Hutchinson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.12214">Architectural-Scale Artistic Brush Painting with a Hybrid Cable Robot</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robot art presents an opportunity to both showcase and advance state-of-the-art robotics through the challenging task of creating art. Creating large-scale artworks in particular engages the public in a way that small-scale works cannot, and the distinct qualities of brush strokes contribute to an organic and human-like quality. Combining the large scale of murals with the strokes of the brush medium presents an especially impactful result, but also introduces unique challenges in maintaining precise, dextrous motion control of the brush across such a large workspace. In this work, we present the first robot to our knowledge that can paint architectural-scale murals with a brush. We create a hybrid robot consisting of a cable-driven parallel robot and 4 degree of freedom (DoF) serial manipulator to paint a 27m by 3.7m mural on windows spanning 2-stories of a building. We discuss our approach to achieving both the scale and accuracy required for brush-painting a mural through a combination of novel mechanical design elements, coordinated planning and control, and on-site calibration algorithms with experimental validations.
<div id='section'>Paperid: <span id='pid'>1385, <a href='https://arxiv.org/pdf/2403.06994.pdf' target='_blank'>https://arxiv.org/pdf/2403.06994.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zeyuan Qu, Tiange Huang, Yuxin Ji, Yongjun Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.06994">Physics Sensor Based Deep Learning Fall Detection System</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fall detection based on embedded sensor is a practical and popular research direction in recent years. In terms of a specific application: fall detection methods based upon physics sensors such as [gyroscope and accelerator] have been exploited using traditional hand crafted features and feed them in machine learning models like Markov chain or just threshold based classification methods. In this paper, we build a complete system named TSFallDetect including data receiving device based on embedded sensor, mobile deep-learning model deploying platform, and a simple server, which will be used to gather models and data for future expansion. On the other hand, we exploit the sequential deep-learning methods to address this falling motion prediction problem based on data collected by inertial and film pressure sensors. We make a empirical study based on existing datasets and our datasets collected from our system separately, which shows that the deep-learning model has more potential advantage than other traditional methods, and we proposed a new deep-learning model based on the time series data to predict the fall, and it may be superior to other sequential models in this particular field.
<div id='section'>Paperid: <span id='pid'>1386, <a href='https://arxiv.org/pdf/2403.06164.pdf' target='_blank'>https://arxiv.org/pdf/2403.06164.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>PaweÅ A. Pierzchlewicz, Caio O. da Silva, R. James Cotton, Fabian H. Sinz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.06164">Platypose: Calibrated Zero-Shot Multi-Hypothesis 3D Human Motion Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Single camera 3D pose estimation is an ill-defined problem due to inherent ambiguities from depth, occlusion or keypoint noise. Multi-hypothesis pose estimation accounts for this uncertainty by providing multiple 3D poses consistent with the 2D measurements. Current research has predominantly concentrated on generating multiple hypotheses for single frame static pose estimation or single hypothesis motion estimation. In this study we focus on the new task of multi-hypothesis motion estimation. Multi-hypothesis motion estimation is not simply multi-hypothesis pose estimation applied to multiple frames, which would ignore temporal correlation across frames. Instead, it requires distributions which are capable of generating temporally consistent samples, which is significantly more challenging than multi-hypothesis pose estimation or single-hypothesis motion estimation. To this end, we introduce Platypose, a framework that uses a diffusion model pretrained on 3D human motion sequences for zero-shot 3D pose sequence estimation. Platypose outperforms baseline methods on multiple hypotheses for motion estimation. Additionally, Platypose also achieves state-of-the-art calibration and competitive joint error when tested on static poses from Human3.6M, MPI-INF-3DHP and 3DPW. Finally, because it is zero-shot, our method generalizes flexibly to different settings such as multi-camera inference.
<div id='section'>Paperid: <span id='pid'>1387, <a href='https://arxiv.org/pdf/2403.04954.pdf' target='_blank'>https://arxiv.org/pdf/2403.04954.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Edgar Medina, Leyong Loh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.04954">Fooling Neural Networks for Motion Forecasting via Adversarial Attacks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion prediction is still an open problem, which is extremely important for autonomous driving and safety applications. Although there are great advances in this area, the widely studied topic of adversarial attacks has not been applied to multi-regression models such as GCNs and MLP-based architectures in human motion prediction. This work intends to reduce this gap using extensive quantitative and qualitative experiments in state-of-the-art architectures similar to the initial stages of adversarial attacks in image classification. The results suggest that models are susceptible to attacks even on low levels of perturbation. We also show experiments with 3D transformations that affect the model performance, in particular, we show that most models are sensitive to simple rotations and translations which do not alter joint distances. We conclude that similar to earlier CNN models, motion forecasting tasks are susceptible to small perturbations and simple 3D transformations.
<div id='section'>Paperid: <span id='pid'>1388, <a href='https://arxiv.org/pdf/2403.00613.pdf' target='_blank'>https://arxiv.org/pdf/2403.00613.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vishnu Veeraraghavan, Kyle Hunte, Jingang Yi, Kaiyan Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.00613">Complete and Near-Optimal Robotic Crack Coverage and Filling in Civil Infrastructure</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a simultaneous sensor-based inspection and footprint coverage (SIFC) planning and control design with applications to autonomous robotic crack mapping and filling. The main challenge of the SIFC problem lies in the coupling of complete sensing (for mapping) and robotic footprint (for filling) coverage tasks. Initially, we assume known target information (e.g., cracks) and employ classic cell decomposition methods to achieve complete sensing coverage of the workspace and complete robotic footprint coverage using the least-cost route. Subsequently, we generalize the algorithm to handle unknown target information, allowing the robot to scan and incrementally construct the target map online while conducting robotic footprint coverage. The online polynomial-time SIFC planning algorithm minimizes the total robot traveling distance, guarantees complete sensing coverage of the entire workspace, and achieves near-optimal robotic footprint coverage, as demonstrated through experiments. For the demonstrated application, we design coordinated nozzle motion control with the planned robot trajectory to efficiently fill all cracks within the robot's footprint. Experimental results illustrate the algorithm's design, performance, and comparisons. The SIFC algorithm offers a high-efficiency motion planning solution for various robotic applications requiring simultaneous sensing and actuation coverage.
<div id='section'>Paperid: <span id='pid'>1389, <a href='https://arxiv.org/pdf/2402.13172.pdf' target='_blank'>https://arxiv.org/pdf/2402.13172.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhi-Yi Lin, Bofan Lyu, Judith Cueto Fernandez, Eline van der Kruk, Ajay Seth, Xucong Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.13172">3D Kinematics Estimation from Video with a Biomechanical Model and Synthetic Training Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate 3D kinematics estimation of human body is crucial in various applications for human health and mobility, such as rehabilitation, injury prevention, and diagnosis, as it helps to understand the biomechanical loading experienced during movement. Conventional marker-based motion capture is expensive in terms of financial investment, time, and the expertise required. Moreover, due to the scarcity of datasets with accurate annotations, existing markerless motion capture methods suffer from challenges including unreliable 2D keypoint detection, limited anatomic accuracy, and low generalization capability. In this work, we propose a novel biomechanics-aware network that directly outputs 3D kinematics from two input views with consideration of biomechanical prior and spatio-temporal information. To train the model, we create synthetic dataset ODAH with accurate kinematics annotations generated by aligning the body mesh from the SMPL-X model and a full-body OpenSim skeletal model. Our extensive experiments demonstrate that the proposed approach, only trained on synthetic data, outperforms previous state-of-the-art methods when evaluated across multiple datasets, revealing a promising direction for enhancing video-based human motion capture
<div id='section'>Paperid: <span id='pid'>1390, <a href='https://arxiv.org/pdf/2401.12919.pdf' target='_blank'>https://arxiv.org/pdf/2401.12919.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sara GarcÃ­a-de-Villa, David Casillas-PÃ©rez, Ana JimÃ©nez-MartÃ­n, Juan JesÃºs GarcÃ­a-DomÃ­nguez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.12919">Inertial Sensors for Human Motion Analysis: A Comprehensive Review</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Inertial motion analysis is having a growing interest during the last decades due to its advantages over classical optical systems. The technological solution based on inertial measurement units allows the measurement of movements in daily living environments, such as in everyday life, which is key for a realistic assessment and understanding of movements. This is why research in this field is still developing and different approaches are proposed. This presents a systematic review of the different proposals for inertial motion analysis found in the literature. The search strategy has been carried out on eight different platforms, including journal articles and conference proceedings, which are written in English and published until August 2022. The results are analyzed in terms of the publishers, the sensors used, the applications, the monitored units, the algorithms of use, the participants of the studies, and the validation systems employed. In addition, we delve deeply into the machine learning techniques proposed in recent years and in the approaches to reduce the estimation error. In this way, we show an overview of the research carried out in this field, going into more detail in recent years, and providing some research directions for future work
<div id='section'>Paperid: <span id='pid'>1391, <a href='https://arxiv.org/pdf/2401.11783.pdf' target='_blank'>https://arxiv.org/pdf/2401.11783.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Feiyu Yao, Zongkai Wu, Li Yi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.11783">Full-Body Motion Reconstruction with Sparse Sensing from Graph Perspective</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Estimating 3D full-body pose from sparse sensor data is a pivotal technique employed for the reconstruction of realistic human motions in Augmented Reality and Virtual Reality. However, translating sparse sensor signals into comprehensive human motion remains a challenge since the sparsely distributed sensors in common VR systems fail to capture the motion of full human body. In this paper, we use well-designed Body Pose Graph (BPG) to represent the human body and translate the challenge into a prediction problem of graph missing nodes. Then, we propose a novel full-body motion reconstruction framework based on BPG. To establish BPG, nodes are initially endowed with features extracted from sparse sensor signals. Features from identifiable joint nodes across diverse sensors are amalgamated and processed from both temporal and spatial perspectives. Temporal dynamics are captured using the Temporal Pyramid Structure, while spatial relations in joint movements inform the spatial attributes. The resultant features serve as the foundational elements of the BPG nodes. To further refine the BPG, node features are updated through a graph neural network that incorporates edge reflecting varying joint relations. Our method's effectiveness is evidenced by the attained state-of-the-art performance, particularly in lower body motion, outperforming other baseline methods. Additionally, an ablation study validates the efficacy of each module in our proposed framework.
<div id='section'>Paperid: <span id='pid'>1392, <a href='https://arxiv.org/pdf/2512.24673.pdf' target='_blank'>https://arxiv.org/pdf/2512.24673.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yongsheng Zhao, Lei Zhao, Baoping Cheng, Gongxin Yao, Xuanzhang Wen, Han Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.24673">VLA-RAIL: A Real-Time Asynchronous Inference Linker for VLA Models and Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models have achieved remarkable breakthroughs in robotics, with the action chunk playing a dominant role in these advances. Given the real-time and continuous nature of robotic motion control, the strategies for fusing a queue of successive action chunks have a profound impact on the overall performance of VLA models. Existing methods suffer from jitter, stalling, or even pauses in robotic action execution, which not only limits the achievable execution speed but also reduces the overall success rate of task completion. This paper introduces VLA-RAIL (A Real-Time Asynchronous Inference Linker), a novel framework designed to address these issues by conducting model inference and robot motion control asynchronously and guaranteeing smooth, continuous, and high-speed action execution. The core contributions of the paper are two fold: a Trajectory Smoother that effectively filters out the noise and jitter in the trajectory of one action chunk using polynomial fitting and a Chunk Fuser that seamlessly align the current executing trajectory and the newly arrived chunk, ensuring position, velocity, and acceleration continuity between two successive action chunks. We validate the effectiveness of VLA-RAIL on a benchmark of dynamic simulation tasks and several real-world manipulation tasks. Experimental results demonstrate that VLA-RAIL significantly reduces motion jitter, enhances execution speed, and improves task success rates, which will become a key infrastructure for the large-scale deployment of VLA models.
<div id='section'>Paperid: <span id='pid'>1393, <a href='https://arxiv.org/pdf/2512.24200.pdf' target='_blank'>https://arxiv.org/pdf/2512.24200.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yujie Yang, Zhichao Zhang, Jiazhou Chen, Zichao Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.24200">PartMotionEdit: Fine-Grained Text-Driven 3D Human Motion Editing via Part-Level Modulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing text-driven 3D human motion editing methods have demonstrated significant progress, but are still difficult to precisely control over detailed, part-specific motions due to their global modeling nature. In this paper, we propose PartMotionEdit, a novel fine-grained motion editing framework that operates via part-level semantic modulation. The core of PartMotionEdit is a Part-aware Motion Modulation (PMM) module, which builds upon a predefined five-part body decomposition. PMM dynamically predicts time-varying modulation weights for each body part, enabling precise and interpretable editing of local motions. To guide the training of PMM, we also introduce a part-level similarity curve supervision mechanism enhanced with dual-layer normalization. This mechanism assists PMM in learning semantically consistent and editable distributions across all body parts. Furthermore, we design a Bidirectional Motion Interaction (BMI) module. It leverages bidirectional cross-modal attention to achieve more accurate semantic alignment between textual instructions and motion semantics. Extensive quantitative and qualitative evaluations on a well-known benchmark demonstrate that PartMotionEdit outperforms the state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>1394, <a href='https://arxiv.org/pdf/2512.21573.pdf' target='_blank'>https://arxiv.org/pdf/2512.21573.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhangzheng Tu, Kailun Su, Shaolong Zhu, Yukun Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.21573">World-Coordinate Human Motion Retargeting via SAM 3D Body</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recovering world-coordinate human motion from monocular videos with humanoid robot retargeting is significant for embodied intelligence and robotics. To avoid complex SLAM pipelines or heavy temporal models, we propose a lightweight, engineering-oriented framework that leverages SAM 3D Body (3DB) as a frozen perception backbone and uses the Momentum HumanRig (MHR) representation as a robot-friendly intermediate. Our method (i) locks the identity and skeleton-scale parameters of per tracked subject to enforce temporally consistent bone lengths, (ii) smooths per-frame predictions via efficient sliding-window optimization in the low-dimensional MHR latent space, and (iii) recovers physically plausible global root trajectories with a differentiable soft foot-ground contact model and contact-aware global optimization. Finally, we retarget the reconstructed motion to the Unitree G1 humanoid using a kinematics-aware two-stage inverse kinematics pipeline. Results on real monocular videos show that our method has stable world trajectories and reliable robot retargeting, indicating that structured human representations with lightweight physical constraints can yield robot-ready motion from monocular input.
<div id='section'>Paperid: <span id='pid'>1395, <a href='https://arxiv.org/pdf/2512.08859.pdf' target='_blank'>https://arxiv.org/pdf/2512.08859.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lars Ole Häusler, Lena Uhlenberg, Göran Köber, Diyora Salimova, Oliver Amft
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.08859">Refining Diffusion Models for Motion Synthesis with an Acceleration Loss to Generate Realistic IMU Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a text-to-IMU (inertial measurement unit) motion-synthesis framework to obtain realistic IMU data by fine-tuning a pretrained diffusion model with an acceleration-based second-order loss (L_acc). L_acc enforces consistency in the discrete second-order temporal differences of the generated motion, thereby aligning the diffusion prior with IMU-specific acceleration patterns. We integrate L_acc into the training objective of an existing diffusion model, finetune the model to obtain an IMU-specific motion prior, and evaluate the model with an existing text-to-IMU framework that comprises surface modelling and virtual sensor simulation. We analysed acceleration signal fidelity and differences between synthetic motion representation and actual IMU recordings. As a downstream application, we evaluated Human Activity Recognition (HAR) and compared the classification performance using data of our method with the earlier diffusion model and two additional diffusion model baselines. When we augmented the earlier diffusion model objective with L_acc and continued training, L_acc decreased by 12.7% relative to the original model. The improvements were considerably larger in high-dynamic activities (i.e., running, jumping) compared to low-dynamic activities~(i.e., sitting, standing). In a low-dimensional embedding, the synthetic IMU data produced by our refined model shifts closer to the distribution of real IMU recordings. HAR classification trained exclusively on our refined synthetic IMU data improved performance by 8.7% compared to the earlier diffusion model and by 7.6% over the best-performing comparison diffusion model. We conclude that acceleration-aware diffusion refinement provides an effective approach to align motion generation and IMU synthesis and highlights how flexible deep learning pipelines are for specialising generic text-to-motion priors to sensor-specific tasks.
<div id='section'>Paperid: <span id='pid'>1396, <a href='https://arxiv.org/pdf/2512.04499.pdf' target='_blank'>https://arxiv.org/pdf/2512.04499.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuduo Jin, Brandon Haworth
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.04499">Back to Basics: Motion Representation Matters for Human Motion Generation Using Diffusion Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion models have emerged as a widely utilized and successful methodology in human motion synthesis. Task-oriented diffusion models have significantly advanced action-to-motion, text-to-motion, and audio-to-motion applications. In this paper, we investigate fundamental questions regarding motion representations and loss functions in a controlled study, and we enumerate the impacts of various decisions in the workflow of the generative motion diffusion model. To answer these questions, we conduct empirical studies based on a proxy motion diffusion model (MDM). We apply v loss as the prediction objective on MDM (vMDM), where v is the weighted sum of motion data and noise. We aim to enhance the understanding of latent data distributions and provide a foundation for improving the state of conditional motion diffusion models. First, we evaluate the six common motion representations in the literature and compare their performance in terms of quality and diversity metrics. Second, we compare the training time under various configurations to shed light on how to speed up the training process of motion diffusion models. Finally, we also conduct evaluation analysis on a large motion dataset. The results of our experiments indicate clear performance differences across motion representations in diverse datasets. Our results also demonstrate the impacts of distinct configurations on model training and suggest the importance and effectiveness of these decisions on the outcomes of motion diffusion models.
<div id='section'>Paperid: <span id='pid'>1397, <a href='https://arxiv.org/pdf/2512.04487.pdf' target='_blank'>https://arxiv.org/pdf/2512.04487.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Eunjong Lee, Eunhee Kim, Sanghoon Hong, Eunho Jung, Jihoon Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.04487">Controllable Long-term Motion Generation with Extended Joint Targets</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating stable and controllable character motion in real-time is a key challenge in computer animation. Existing methods often fail to provide fine-grained control or suffer from motion degradation over long sequences, limiting their use in interactive applications. We propose COMET, an autoregressive framework that runs in real time, enabling versatile character control and robust long-horizon synthesis. Our efficient Transformer-based conditional VAE allows for precise, interactive control over arbitrary user-specified joints for tasks like goal-reaching and in-betweening from a single model. To ensure long-term temporal stability, we introduce a novel reference-guided feedback mechanism that prevents error accumulation. This mechanism also serves as a plug-and-play stylization module, enabling real-time style transfer. Extensive evaluations demonstrate that COMET robustly generates high-quality motion at real-time speeds, significantly outperforming state-of-the-art approaches in complex motion control tasks and confirming its readiness for demanding interactive applications.
<div id='section'>Paperid: <span id='pid'>1398, <a href='https://arxiv.org/pdf/2511.17387.pdf' target='_blank'>https://arxiv.org/pdf/2511.17387.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yusuf Baran Ates, Omer Morgul
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.17387">Human Imitated Bipedal Locomotion with Frequency Based Gait Generator Network</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning human-like, robust bipedal walking remains difficult due to hybrid dynamics and terrain variability. We propose a lightweight framework that combines a gait generator network learned from human motion with Proximal Policy Optimization (PPO) controller for torque control. Despite being trained only on flat or mildly sloped ground, the learned policies generalize to steeper ramps and rough surfaces. Results suggest that pairing spectral motion priors with Deep Reinforcement Learning (DRL) offers a practical path toward natural and robust bipedal locomotion with modest training cost.
<div id='section'>Paperid: <span id='pid'>1399, <a href='https://arxiv.org/pdf/2511.16406.pdf' target='_blank'>https://arxiv.org/pdf/2511.16406.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Luis Luna, Isaac Chairez, Andrey Polyakov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.16406">Homogeneous Proportional-Integral-Derivative Controller in Mobile Robotic Manipulators</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Mobile robotic manipulators (MRMs), which integrate mobility and manipulation capabilities, present significant control challenges due to their nonlinear dynamics, underactuation, and coupling between the base and manipulator subsystems. This paper proposes a novel homogeneous Proportional-Integral-Derivative (hPID) control strategy tailored for MRMs to achieve robust and coordinated motion control. Unlike classical PID controllers, the hPID controller leverages the mathematical framework of homogeneous control theory to systematically enhance the stability and convergence properties of the closed-loop system, even in the presence of dynamic uncertainties and external disturbances involved into a system in a homogeneous way. A homogeneous PID structure is designed, ensuring improved convergence of tracking errors through a graded homogeneity approach that generalizes traditional PID gains to nonlinear, state-dependent functions. Stability analysis is conducted using Lyapunov-based methods, demonstrating that the hPID controller guarantees global asymptotic stability and finite-time convergence under mild assumptions. Experimental results on a representative MRM model validate the effectiveness of the hPID controller in achieving high-precision trajectory tracking for both the mobile base and manipulator arm, outperforming conventional linear PID controllers in terms of response time, steady-state error, and robustness to model uncertainties. This research contributes a scalable and analytically grounded control framework for enhancing the autonomy and reliability of next-generation mobile manipulation systems in structured and unstructured environments.
<div id='section'>Paperid: <span id='pid'>1400, <a href='https://arxiv.org/pdf/2511.08763.pdf' target='_blank'>https://arxiv.org/pdf/2511.08763.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mincong, Huang, Stefan T. Radev
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.08763">Modeling multi-agent motion dynamics in immersive rooms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Immersive rooms are increasingly popular augmented reality systems that support multi-agent interactions within a virtual world. However, despite extensive content creation and technological developments, insights about perceptually-driven social dynamics, such as the complex movement patterns during virtual world navigation, remain largely underexplored. Computational models of motion dynamics can help us understand the underlying mechanism of human interaction in immersive rooms and develop applications that better support spatially distributed interaction. In this work, we propose a new agent-based model of emergent human motion dynamics. The model represents human agents as simple spatial geometries in the room that relocate and reorient themselves based on the salient virtual spatial objects they approach. Agent motion is modeled as an interactive process combining external diffusion-driven influences from the environment with internal self-propelling interactions among agents. Further, we leverage simulation-based inference (SBI) to show that the governing parameters of motion patterns can be estimated from simple observables. Our results indicate that the model successfully captures action-related agent properties but exposes local non-identifiability linked to environmental awareness. We argue that our simulation-based approach paves the way for creating adaptive, responsive immersive rooms -- spaces that adjust their interfaces and interactions based on human collective movement patterns and spatial attention.
<div id='section'>Paperid: <span id='pid'>1401, <a href='https://arxiv.org/pdf/2511.00011.pdf' target='_blank'>https://arxiv.org/pdf/2511.00011.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alexander Okupnik, Johannes Schneider, Kyriakos Flouris
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.00011">Generative human motion mimicking through feature extraction in denoising diffusion settings</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent success with large language models has sparked a new wave of verbal human-AI interaction. While such models support users in a variety of creative tasks, they lack the embodied nature of human interaction. Dance, as a primal form of human expression, is predestined to complement this experience. To explore creative human-AI interaction exemplified by dance, we build an interactive model based on motion capture (MoCap) data. It generates an artificial other by partially mimicking and also "creatively" enhancing an incoming sequence of movement data. It is the first model, which leverages single-person motion data and high level features in order to do so and, thus, it does not rely on low level human-human interaction data. It combines ideas of two diffusion models, motion inpainting, and motion style transfer to generate movement representations that are both temporally coherent and responsive to a chosen movement reference. The success of the model is demonstrated by quantitatively assessing the convergence of the feature distribution of the generated samples and the test set which serves as simulating the human performer. We show that our generations are first steps to creative dancing with AI as they are both diverse showing various deviations from the human partner while appearing realistic.
<div id='section'>Paperid: <span id='pid'>1402, <a href='https://arxiv.org/pdf/2508.20740.pdf' target='_blank'>https://arxiv.org/pdf/2508.20740.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuki Tanaka, Seiichiro Katsura
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.20740">Non-expert to Expert Motion Translation Using Generative Adversarial Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Decreasing skilled workers is a very serious problem in the world. To deal with this problem, the skill transfer from experts to robots has been researched. These methods which teach robots by human motion are called imitation learning. Experts' skills generally appear in not only position data, but also force data. Thus, position and force data need to be saved and reproduced. To realize this, a lot of research has been conducted in the framework of a motion-copying system. Recent research uses machine learning methods to generate motion commands. However, most of them could not change tasks by following human intention. Some of them can change tasks by conditional training, but the labels are limited. Thus, we propose the flexible motion translation method by using Generative Adversarial Networks. The proposed method enables users to teach robots tasks by inputting data, and skills by a trained model. We evaluated the proposed system with a 3-DOF calligraphy robot.
<div id='section'>Paperid: <span id='pid'>1403, <a href='https://arxiv.org/pdf/2508.05091.pdf' target='_blank'>https://arxiv.org/pdf/2508.05091.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingxuan He, Busheng Su, Finn Wong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05091">PoseGen: In-Context LoRA Finetuning for Pose-Controllable Long Human Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating long, temporally coherent videos with precise control over subject identity and motion is a formidable challenge for current diffusion models, which often suffer from identity drift and are limited to short clips. We introduce PoseGen, a novel framework that generates arbitrarily long videos of a specific subject from a single reference image and a driving pose sequence. Our core innovation is an in-context LoRA finetuning strategy that injects subject appearance at the token level for identity preservation, while simultaneously conditioning on pose information at the channel level for fine-grained motion control. To overcome duration limits, PoseGen pioneers an interleaved segment generation method that seamlessly stitches video clips together, using a shared KV cache mechanism and a specialized transition process to ensure background consistency and temporal smoothness. Trained on a remarkably small 33-hour video dataset, extensive experiments show that PoseGen significantly outperforms state-of-the-art methods in identity fidelity, pose accuracy, and its unique ability to produce coherent, artifact-free videos of unlimited duration.
<div id='section'>Paperid: <span id='pid'>1404, <a href='https://arxiv.org/pdf/2508.04847.pdf' target='_blank'>https://arxiv.org/pdf/2508.04847.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Md Zahidul Hasan, A. Ben Hamza, Nizar Bouguila
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.04847">LuKAN: A Kolmogorov-Arnold Network Framework for 3D Human Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The goal of 3D human motion prediction is to forecast future 3D poses of the human body based on historical motion data. Existing methods often face limitations in achieving a balance between prediction accuracy and computational efficiency. In this paper, we present LuKAN, an effective model based on Kolmogorov-Arnold Networks (KANs) with Lucas polynomial activations. Our model first applies the discrete wavelet transform to encode temporal information in the input motion sequence. Then, a spatial projection layer is used to capture inter-joint dependencies, ensuring structural consistency of the human body. At the core of LuKAN is the Temporal Dependency Learner, which employs a KAN layer parameterized by Lucas polynomials for efficient function approximation. These polynomials provide computational efficiency and an enhanced capability to handle oscillatory behaviors. Finally, the inverse discrete wavelet transform reconstructs motion sequences in the time domain, generating temporally coherent predictions. Extensive experiments on three benchmark datasets demonstrate the competitive performance of our model compared to strong baselines, as evidenced by both quantitative and qualitative evaluations. Moreover, its compact architecture coupled with the linear recurrence of Lucas polynomials, ensures computational efficiency.
<div id='section'>Paperid: <span id='pid'>1405, <a href='https://arxiv.org/pdf/2507.23350.pdf' target='_blank'>https://arxiv.org/pdf/2507.23350.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mahmoud Ghorab, Matthias Lorenzen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.23350">Multi-Waypoint Path Planning and Motion Control for Non-holonomic Mobile Robots in Agricultural Applications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>There is a growing demand for autonomous mobile robots capable of navigating unstructured agricultural environments. Tasks such as weed control in meadows require efficient path planning through an unordered set of coordinates while minimizing travel distance and adhering to curvature constraints to prevent soil damage and protect vegetation. This paper presents an integrated navigation framework combining a global path planner based on the Dubins Traveling Salesman Problem (DTSP) with a Nonlinear Model Predictive Control (NMPC) strategy for local path planning and control. The DTSP generates a minimum-length, curvature-constrained path that efficiently visits all targets, while the NMPC leverages this path to compute control signals to accurately reach each waypoint. The system's performance was validated through comparative simulation analysis on real-world field datasets, demonstrating that the coupled DTSP-based planner produced smoother and shorter paths, with a reduction of about 16% in the provided scenario, compared to decoupled methods. Based thereon, the NMPC controller effectively steered the robot to the desired waypoints, while locally optimizing the trajectory and ensuring adherence to constraints. These findings demonstrate the potential of the proposed framework for efficient autonomous navigation in agricultural environments.
<div id='section'>Paperid: <span id='pid'>1406, <a href='https://arxiv.org/pdf/2507.19119.pdf' target='_blank'>https://arxiv.org/pdf/2507.19119.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanghong Liu, Xingping Dong, Ming Li, Weixing Zhang, Yidong Lou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.19119">PatchTraj: Unified Time-Frequency Representation Learning via Dynamic Patches for Trajectory Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pedestrian trajectory prediction is crucial for autonomous driving and robotics. While existing point-based and grid-based methods expose two main limitations: insufficiently modeling human motion dynamics, as they fail to balance local motion details with long-range spatiotemporal dependencies, and the time representations lack interaction with their frequency components in jointly modeling trajectory sequences. To address these challenges, we propose PatchTraj, a dynamic patch-based framework that integrates time-frequency joint modeling for trajectory prediction. Specifically, we decompose the trajectory into raw time sequences and frequency components, and employ dynamic patch partitioning to perform multi-scale segmentation, capturing hierarchical motion patterns. Each patch undergoes adaptive embedding with scale-aware feature extraction, followed by hierarchical feature aggregation to model both fine-grained and long-range dependencies. The outputs of the two branches are further enhanced via cross-modal attention, facilitating complementary fusion of temporal and spectral cues. The resulting enhanced embeddings exhibit strong expressive power, enabling accurate predictions even when using a vanilla Transformer architecture. Extensive experiments on ETH-UCY, SDD, NBA, and JRDB datasets demonstrate that our method achieves state-of-the-art performance. Notably, on the egocentric JRDB dataset, PatchTraj attains significant relative improvements of 26.7% in ADE and 17.4% in FDE, underscoring its substantial potential in embodied intelligence.
<div id='section'>Paperid: <span id='pid'>1407, <a href='https://arxiv.org/pdf/2507.18979.pdf' target='_blank'>https://arxiv.org/pdf/2507.18979.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Deokjin Lee, Junho Song, Alireza Karimi, Sehoon Oh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.18979">Frequency Response Data-Driven Disturbance Observer Design for Flexible Joint Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion control of flexible joint robots (FJR) is challenged by inherent flexibility and configuration-dependent variations in system dynamics. While disturbance observers (DOB) can enhance system robustness, their performance is often limited by the elasticity of the joints and the variations in system parameters, which leads to a conservative design of the DOB. This paper presents a novel frequency response function (FRF)-based optimization method aimed at improving DOB performance, even in the presence of flexibility and system variability. The proposed method maximizes control bandwidth and effectively suppresses vibrations, thus enhancing overall system performance. Closed-loop stability is rigorously proven using the Nyquist stability criterion. Experimental validation on a FJR demonstrates that the proposed approach significantly improves robustness and motion performance, even under conditions of joint flexibility and system variation.
<div id='section'>Paperid: <span id='pid'>1408, <a href='https://arxiv.org/pdf/2507.18052.pdf' target='_blank'>https://arxiv.org/pdf/2507.18052.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>David Sinclair, Ademyemi Ademola, Babis Koniaris, Kenny Mitchell
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.18052">DanceGraph: A Complementary Architecture for Synchronous Dancing Online</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>DanceGraph is an architecture for synchronized online dancing overcoming the latency of networked body pose sharing. We break down this challenge by developing a real-time bandwidth-efficient architecture to minimize lag and reduce the timeframe of required motion prediction for synchronization with the music's rhythm. In addition, we show an interactive method for the parameterized stylization of dance motions for rhythmic dance using online dance correctives.
<div id='section'>Paperid: <span id='pid'>1409, <a href='https://arxiv.org/pdf/2507.13179.pdf' target='_blank'>https://arxiv.org/pdf/2507.13179.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyu Zhong, BjÃ¶rn Landfeldt, GÃ¼nter Alce, Hector A Caltenco
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.13179">Predictability-Aware Motion Prediction for Edge XR via High-Order Error-State Kalman Filtering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As 6G networks are developed and defined, offloading of XR applications is emerging as one of the strong new use cases. The reduced 6G latency coupled with edge processing infrastructure will for the first time provide a realistic offloading scenario in cellular networks where several computationally intensive functions, including rendering, can migrate from the user device and into the network. A key advantage of doing so is the lowering of the battery needs in the user devices and the possibility to design new devices with smaller form factors. However, offloading introduces increased delays compared to local execution, primarily due to network transmission latency and queuing delays at edge servers, especially under multi-user concurrency. Despite the computational power of edge platforms, the resulting motion-to-photon (MTP) latency negatively impacts user experience. To mitigate this, motion prediction has been proposed to offset delays. Existing approaches build on either deep learning or Kalman filtering. Deep learning techniques face scalability limitations at the resource-constrained edge, as their computational expense intensifies with increasing user concurrency, while Kalman filtering suffers from poor handling of complex movements and fragility to packet loss inherent in 6G's high-frequency radio interfaces. In this work, we introduce a context-aware error-state Kalman filter (ESKF) prediction framework, which forecasts the user's head motion trajectory to compensate for MTP latency in remote XR. By integrating a motion classifier that categorizes head motions based on their predictability, our algorithm demonstrates reduced prediction error across different motion classes. Our findings demonstrate that the optimized ESKF not only surpasses traditional Kalman filters in positional and orientational accuracy but also exhibits enhanced robustness and resilience to packet loss.
<div id='section'>Paperid: <span id='pid'>1410, <a href='https://arxiv.org/pdf/2507.12138.pdf' target='_blank'>https://arxiv.org/pdf/2507.12138.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Michal Heker, Sefy Kararlitsky, David Tolpin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.12138">Neural Human Pose Prior</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a principled, data-driven approach for modeling a neural prior over human body poses using normalizing flows. Unlike heuristic or low-expressivity alternatives, our method leverages RealNVP to learn a flexible density over poses represented in the 6D rotation format. We address the challenge of modeling distributions on the manifold of valid 6D rotations by inverting the Gram-Schmidt process during training, enabling stable learning while preserving downstream compatibility with rotation-based frameworks. Our architecture and training pipeline are framework-agnostic and easily reproducible. We demonstrate the effectiveness of the learned prior through both qualitative and quantitative evaluations, and we analyze its impact via ablation studies. This work provides a sound probabilistic foundation for integrating pose priors into human motion capture and reconstruction pipelines.
<div id='section'>Paperid: <span id='pid'>1411, <a href='https://arxiv.org/pdf/2507.06530.pdf' target='_blank'>https://arxiv.org/pdf/2507.06530.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kazi Mahathir Rahman, Naveed Imtiaz Nafis, Md. Farhan Sadik, Mohammad Al Rafi, Mehedi Hasan Shahed
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06530">Speak2Sign3D: A Multi-modal Pipeline for English Speech to American Sign Language Animation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Helping deaf and hard-of-hearing people communicate more easily is the main goal of Automatic Sign Language Translation. Although most past research has focused on turning sign language into text, doing the reverse, turning spoken English into sign language animations, has been largely overlooked. That's because it involves multiple steps, such as understanding speech, translating it into sign-friendly grammar, and generating natural human motion. In this work, we introduce a complete pipeline that converts English speech into smooth, realistic 3D sign language animations. Our system starts with Whisper to translate spoken English into text. Then, we use a MarianMT machine translation model to translate that text into American Sign Language (ASL) gloss, a simplified version of sign language that captures meaning without grammar. This model performs well, reaching BLEU scores of 0.7714 and 0.8923. To make the gloss translation more accurate, we also use word embeddings such as Word2Vec and FastText to understand word meanings. Finally, we animate the translated gloss using a 3D keypoint-based motion system trained on Sign3D-WLASL, a dataset we created by extracting body, hand, and face key points from real ASL videos in the WLASL dataset. To support the gloss translation stage, we also built a new dataset called BookGlossCorpus-CG, which turns everyday English sentences from the BookCorpus dataset into ASL gloss using grammar rules. Our system stitches everything together by smoothly interpolating between signs to create natural, continuous animations. Unlike previous works like How2Sign and Phoenix-2014T that focus on recognition or use only one type of data, our pipeline brings together audio, text, and motion in a single framework that goes all the way from spoken English to lifelike 3D sign language animation.
<div id='section'>Paperid: <span id='pid'>1412, <a href='https://arxiv.org/pdf/2507.05867.pdf' target='_blank'>https://arxiv.org/pdf/2507.05867.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nikita Savin, Elena Ambrosovskaya, Dmitry Romaev, Anton Proskurnikov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.05867">Assessing Linear Control Strategies for Zero-Speed Fin Roll Damping</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Roll stabilization is a critical aspect of ship motion control, particularly for vessels operating in low-speed or zero-speed conditions, where traditional hydrodynamic fins lose their effectiveness. In this paper, we consider a roll damping system, developed by Navis JSC, based on two actively controlled zero-speed fins. Unlike conventional fin stabilizers, zero-speed fins employ a drag-based mechanism and active oscillations to generate stabilizing forces even when the vessel is stationary. We propose a simple linear control architecture that, however, accounts for nonlinear drag forces and actuator limitations. Simulation results on a high-fidelity vessel model used for HIL testing demonstrate the effectiveness of the proposed approach.
<div id='section'>Paperid: <span id='pid'>1413, <a href='https://arxiv.org/pdf/2507.00676.pdf' target='_blank'>https://arxiv.org/pdf/2507.00676.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Edward Effendy, Kuan-Wei Tseng, Rei Kawakami
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.00676">A Unified Transformer-Based Framework with Pretraining For Whole Body Grasping Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accepted in the ICIP 2025
  We present a novel transformer-based framework for whole-body grasping that addresses both pose generation and motion infilling, enabling realistic and stable object interactions. Our pipeline comprises three stages: Grasp Pose Generation for full-body grasp generation, Temporal Infilling for smooth motion continuity, and a LiftUp Transformer that refines downsampled joints back to high-resolution markers. To overcome the scarcity of hand-object interaction data, we introduce a data-efficient Generalized Pretraining stage on large, diverse motion datasets, yielding robust spatio-temporal representations transferable to grasping tasks. Experiments on the GRAB dataset show that our method outperforms state-of-the-art baselines in terms of coherence, stability, and visual realism. The modular design also supports easy adaptation to other human-motion applications.
<div id='section'>Paperid: <span id='pid'>1414, <a href='https://arxiv.org/pdf/2506.17996.pdf' target='_blank'>https://arxiv.org/pdf/2506.17996.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>David Tolpin, Sefy Kagarlitsky
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.17996">Fast Neural Inverse Kinematics on Human Body Motions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Markerless motion capture enables the tracking of human motion without requiring physical markers or suits, offering increased flexibility and reduced costs compared to traditional systems. However, these advantages often come at the expense of higher computational demands and slower inference, limiting their applicability in real-time scenarios. In this technical report, we present a fast and reliable neural inverse kinematics framework designed for real-time capture of human body motions from 3D keypoints. We describe the network architecture, training methodology, and inference procedure in detail. Our framework is evaluated both qualitatively and quantitatively, and we support key design decisions through ablation studies.
<div id='section'>Paperid: <span id='pid'>1415, <a href='https://arxiv.org/pdf/2506.14563.pdf' target='_blank'>https://arxiv.org/pdf/2506.14563.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jesse St. Amand, Leonardo Gizzi, Martin A. Giese
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.14563">Single-Example Learning in a Mixture of GPDMs with Latent Geometries</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present the Gaussian process dynamical mixture model (GPDMM) and show its utility in single-example learning of human motion data. The Gaussian process dynamical model (GPDM) is a form of the Gaussian process latent variable model (GPLVM), but optimized with a hidden Markov model dynamical prior. The GPDMM combines multiple GPDMs in a probabilistic mixture-of-experts framework, utilizing embedded geometric features to allow for diverse sequences to be encoded in a single latent space, enabling the categorization and generation of each sequence class. GPDMs and our mixture model are particularly advantageous in addressing the challenges of modeling human movement in scenarios where data is limited and model interpretability is vital, such as in patient-specific medical applications like prosthesis control. We score the GPDMM on classification accuracy and generative ability in single-example learning, showcase model variations, and benchmark it against LSTMs, VAEs, and transformers.
<div id='section'>Paperid: <span id='pid'>1416, <a href='https://arxiv.org/pdf/2506.14428.pdf' target='_blank'>https://arxiv.org/pdf/2506.14428.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruihao Xi, Xuekuan Wang, Yongcheng Li, Shuhua Li, Zichen Wang, Yiwei Wang, Feng Wei, Cairong Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.14428">Toward Rich Video Human-Motion2D Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating realistic and controllable human motions, particularly those involving rich multi-character interactions, remains a significant challenge due to data scarcity and the complexities of modeling inter-personal dynamics. To address these limitations, we first introduce a new large-scale rich video human motion 2D dataset (Motion2D-Video-150K) comprising 150,000 video sequences. Motion2D-Video-150K features a balanced distribution of diverse single-character and, crucially, double-character interactive actions, each paired with detailed textual descriptions. Building upon this dataset, we propose a novel diffusion-based rich video human motion2D generation (RVHM2D) model. RVHM2D incorporates an enhanced textual conditioning mechanism utilizing either dual text encoders (CLIP-L/B) or T5-XXL with both global and local features. We devise a two-stage training strategy: the model is first trained with a standard diffusion objective, and then fine-tuned using reinforcement learning with an FID-based reward to further enhance motion realism and text alignment. Extensive experiments demonstrate that RVHM2D achieves leading performance on the Motion2D-Video-150K benchmark in generating both single and interactive double-character scenarios.
<div id='section'>Paperid: <span id='pid'>1417, <a href='https://arxiv.org/pdf/2506.09836.pdf' target='_blank'>https://arxiv.org/pdf/2506.09836.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junli Deng, Ping Shi, Qipei Li, Jinyang Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.09836">DynaSplat: Dynamic-Static Gaussian Splatting with Hierarchical Motion Decomposition for Scene Reconstruction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reconstructing intricate, ever-changing environments remains a central ambition in computer vision, yet existing solutions often crumble before the complexity of real-world dynamics. We present DynaSplat, an approach that extends Gaussian Splatting to dynamic scenes by integrating dynamic-static separation and hierarchical motion modeling. First, we classify scene elements as static or dynamic through a novel fusion of deformation offset statistics and 2D motion flow consistency, refining our spatial representation to focus precisely where motion matters. We then introduce a hierarchical motion modeling strategy that captures both coarse global transformations and fine-grained local movements, enabling accurate handling of intricate, non-rigid motions. Finally, we integrate physically-based opacity estimation to ensure visually coherent reconstructions, even under challenging occlusions and perspective shifts. Extensive experiments on challenging datasets reveal that DynaSplat not only surpasses state-of-the-art alternatives in accuracy and realism but also provides a more intuitive, compact, and efficient route to dynamic scene reconstruction.
<div id='section'>Paperid: <span id='pid'>1418, <a href='https://arxiv.org/pdf/2506.04680.pdf' target='_blank'>https://arxiv.org/pdf/2506.04680.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ping-Kong Huang, Chien-Wu Lan, Chin-Tien Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.04680">Application of SDRE to Achieve Gait Control in a Bipedal Robot for Knee-Type Exoskeleton Testing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Exoskeletons are widely used in rehabilitation and industrial applications to assist human motion. However, direct human testing poses risks due to possible exoskeleton malfunctions and inconsistent movement replication. To provide a safer and more repeatable testing environment, this study employs a bipedal robot platform to reproduce human gait, allowing for controlled exoskeleton evaluations. A control strategy based on the State-Dependent Riccati Equation (SDRE) is formulated to achieve optimal torque control for accurate gait replication. The bipedal robot dynamics are represented using double pendulum model, where SDRE-optimized control inputs minimize deviations from human motion trajectories. To align with motor behavior constraints, a parameterized control method is introduced to simplify the control process while effectively replicating human gait. The proposed approach initially adopts a ramping trapezoidal velocity model, which is then adapted into a piecewise linear velocity-time representation through motor command overwriting. This modification enables finer control over gait phase transitions while ensuring compatibility with motor dynamics. The corresponding cost function optimizes the control parameters to minimize errors in joint angles, velocities, and torques relative to SDRE control result. By structuring velocity transitions in accordance with motor limitations, the method reduce the computational load associated with real-time control. Experimental results verify the feasibility of the proposed parameterized control method in reproducing human gait. The bipedal robot platform provides a reliable and repeatable testing mechanism for knee-type exoskeletons, offering insights into exoskeleton performance under controlled conditions.
<div id='section'>Paperid: <span id='pid'>1419, <a href='https://arxiv.org/pdf/2506.03084.pdf' target='_blank'>https://arxiv.org/pdf/2506.03084.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zizhao Wu, Yingying Sun, Yiming Chen, Xiaoling Gu, Ruyu Liu, Jiazhou Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.03084">InterMamba: Efficient Human-Human Interaction Generation with Adaptive Spatio-Temporal Mamba</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human-human interaction generation has garnered significant attention in motion synthesis due to its vital role in understanding humans as social beings. However, existing methods typically rely on transformer-based architectures, which often face challenges related to scalability and efficiency. To address these issues, we propose a novel, efficient human-human interaction generation method based on the Mamba framework, designed to meet the demands of effectively capturing long-sequence dependencies while providing real-time feedback. Specifically, we introduce an adaptive spatio-temporal Mamba framework that utilizes two parallel SSM branches with an adaptive mechanism to integrate the spatial and temporal features of motion sequences. To further enhance the model's ability to capture dependencies within individual motion sequences and the interactions between different individual sequences, we develop two key modules: the self-adaptive spatio-temporal Mamba module and the cross-adaptive spatio-temporal Mamba module, enabling efficient feature learning. Extensive experiments demonstrate that our method achieves state-of-the-art results on two interaction datasets with remarkable quality and efficiency. Compared to the baseline method InterGen, our approach not only improves accuracy but also requires a minimal parameter size of just 66M ,only 36% of InterGen's, while achieving an average inference speed of 0.57 seconds, which is 46% of InterGen's execution time.
<div id='section'>Paperid: <span id='pid'>1420, <a href='https://arxiv.org/pdf/2505.20370.pdf' target='_blank'>https://arxiv.org/pdf/2505.20370.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Martine Dyring Hansen, Elena Celledoni, Benjamin Kwanen Tapley
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.20370">Learning mechanical systems from real-world data using discrete forced Lagrangian dynamics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a data-driven method for learning the equations of motion of mechanical systems directly from position measurements, without requiring access to velocity data. This is particularly relevant in system identification tasks where only positional information is available, such as motion capture, pixel data or low-resolution tracking. Our approach takes advantage of the discrete Lagrange-d'Alembert principle and the forced discrete Euler-Lagrange equations to construct a physically grounded model of the system's dynamics. We decompose the dynamics into conservative and non-conservative components, which are learned separately using feed-forward neural networks. In the absence of external forces, our method reduces to a variational discretization of the action principle naturally preserving the symplectic structure of the underlying Hamiltonian system. We validate our approach on a variety of synthetic and real-world datasets, demonstrating its effectiveness compared to baseline methods. In particular, we apply our model to (1) measured human motion data and (2) latent embeddings obtained via an autoencoder trained on image sequences. We demonstrate that we can faithfully reconstruct and separate both the conservative and forced dynamics, yielding interpretable and physically consistent predictions.
<div id='section'>Paperid: <span id='pid'>1421, <a href='https://arxiv.org/pdf/2505.14858.pdf' target='_blank'>https://arxiv.org/pdf/2505.14858.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fernando Coutinho, Nicolas Lizarralde, Fernando Lizarralde
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.14858">Coordinated motion control of a wire arc additive manufacturing robotic system for multi-directional building parts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work investigates the manufacturing of complex shapes parts with wire arc additive manufacturing (WAAM). In order to guarantee the integrity and quality of each deposited layer that composes the final piece, the deposition process is usually carried out in a flat position. However, for complex geometry parts with non-flat surfaces, this strategy causes unsupported overhangs and staircase effect, which contribute to a poor surface finishing. Generally, the build direction is not constant for every deposited section or layer in complex geometry parts. As a result, there is an additional concern to ensure the build direction is aligned with gravity, thus improving the quality of the final part. This paper proposes an algorithm to control the torch motion with respect to a deposition substrate as well as the torch orientation with respect to an inertial frame. The control scheme is based on task augmentation applied to an extended kinematic chain composed by two robots, which constitutes a coordinated control problem, and allows the deposition trajectory to be planned with respect to the deposition substrate coordinate frame while aligning each layer buildup direction with gravity (or any other direction defined for an inertial frame). Parts with complex geometry aspects have been produced in a WAAM cell composed by two robots (a manipulator with a welding torch and a positioning table holding the workpiece) in order to validate the proposed approach.
<div id='section'>Paperid: <span id='pid'>1422, <a href='https://arxiv.org/pdf/2505.12327.pdf' target='_blank'>https://arxiv.org/pdf/2505.12327.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Albert Zhao, Stefano Soatto
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.12327">Robust Planning for Autonomous Driving via Mixed Adversarial Diffusion Predictions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We describe a robust planning method for autonomous driving that mixes normal and adversarial agent predictions output by a diffusion model trained for motion prediction. We first train a diffusion model to learn an unbiased distribution of normal agent behaviors. We then generate a distribution of adversarial predictions by biasing the diffusion model at test time to generate predictions that are likely to collide with a candidate plan. We score plans using expected cost with respect to a mixture distribution of normal and adversarial predictions, leading to a planner that is robust against adversarial behaviors but not overly conservative when agents behave normally. Unlike current approaches, we do not use risk measures that over-weight adversarial behaviors while placing little to no weight on low-cost normal behaviors or use hard safety constraints that may not be appropriate for all driving scenarios. We show the effectiveness of our method on single-agent and multi-agent jaywalking scenarios as well as a red light violation scenario.
<div id='section'>Paperid: <span id='pid'>1423, <a href='https://arxiv.org/pdf/2505.09379.pdf' target='_blank'>https://arxiv.org/pdf/2505.09379.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ali Rida Sahili, Najett Neji, Hedi Tabia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.09379">Text-driven Motion Generation: Overview, Challenges and Directions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-driven motion generation offers a powerful and intuitive way to create human movements directly from natural language. By removing the need for predefined motion inputs, it provides a flexible and accessible approach to controlling animated characters. This makes it especially useful in areas like virtual reality, gaming, human-computer interaction, and robotics. In this review, we first revisit the traditional perspective on motion synthesis, where models focused on predicting future poses from observed initial sequences, often conditioned on action labels. We then provide a comprehensive and structured survey of modern text-to-motion generation approaches, categorizing them from two complementary perspectives: (i) architectural, dividing methods into VAE-based, diffusion-based, and hybrid models; and (ii) motion representation, distinguishing between discrete and continuous motion generation strategies. In addition, we explore the most widely used datasets, evaluation methods, and recent benchmarks that have shaped progress in this area. With this survey, we aim to capture where the field currently stands, bring attention to its key challenges and limitations, and highlight promising directions for future exploration. We hope this work offers a valuable starting point for researchers and practitioners working to push the boundaries of language-driven human motion synthesis.
<div id='section'>Paperid: <span id='pid'>1424, <a href='https://arxiv.org/pdf/2505.04660.pdf' target='_blank'>https://arxiv.org/pdf/2505.04660.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sana Alamgeer, Yasine Souissi, Anne H. H. Ngu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.04660">AI-Generated Fall Data: Assessing LLMs and Diffusion Model for Wearable Fall Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Training fall detection systems is challenging due to the scarcity of real-world fall data, particularly from elderly individuals. To address this, we explore the potential of Large Language Models (LLMs) for generating synthetic fall data. This study evaluates text-to-motion (T2M, SATO, ParCo) and text-to-text models (GPT4o, GPT4, Gemini) in simulating realistic fall scenarios. We generate synthetic datasets and integrate them with four real-world baseline datasets to assess their impact on fall detection performance using a Long Short-Term Memory (LSTM) model. Additionally, we compare LLM-generated synthetic data with a diffusion-based method to evaluate their alignment with real accelerometer distributions. Results indicate that dataset characteristics significantly influence the effectiveness of synthetic data, with LLM-generated data performing best in low-frequency settings (e.g., 20Hz) while showing instability in high-frequency datasets (e.g., 200Hz). While text-to-motion models produce more realistic biomechanical data than text-to-text models, their impact on fall detection varies. Diffusion-based synthetic data demonstrates the closest alignment to real data but does not consistently enhance model performance. An ablation study further confirms that the effectiveness of synthetic data depends on sensor placement and fall representation. These findings provide insights into optimizing synthetic data generation for fall detection models.
<div id='section'>Paperid: <span id='pid'>1425, <a href='https://arxiv.org/pdf/2504.08280.pdf' target='_blank'>https://arxiv.org/pdf/2504.08280.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiong Li, Shulei Liu, Xingning Chen, Yisong Wu, Dong Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.08280">PNE-SGAN: Probabilistic NDT-Enhanced Semantic Graph Attention Network for LiDAR Loop Closure Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>LiDAR loop closure detection (LCD) is crucial for consistent Simultaneous Localization and Mapping (SLAM) but faces challenges in robustness and accuracy. Existing methods, including semantic graph approaches, often suffer from coarse geometric representations and lack temporal robustness against noise, dynamics, and viewpoint changes. We introduce PNE-SGAN, a Probabilistic NDT-Enhanced Semantic Graph Attention Network, to overcome these limitations. PNE-SGAN enhances semantic graphs by using Normal Distributions Transform (NDT) covariance matrices as rich, discriminative geometric node features, processed via a Graph Attention Network (GAT). Crucially, it integrates graph similarity scores into a probabilistic temporal filtering framework (modeled as an HMM/Bayes filter), incorporating uncertain odometry for motion modeling and utilizing forward-backward smoothing to effectively handle ambiguities. Evaluations on challenging KITTI sequences (00 and 08) demonstrate state-of-the-art performance, achieving Average Precision of 96.2\% and 95.1\%, respectively. PNE-SGAN significantly outperforms existing methods, particularly in difficult bidirectional loop scenarios where others falter. By synergizing detailed NDT geometry with principled probabilistic temporal reasoning, PNE-SGAN offers a highly accurate and robust solution for LiDAR LCD, enhancing SLAM reliability in complex, large-scale environments.
<div id='section'>Paperid: <span id='pid'>1426, <a href='https://arxiv.org/pdf/2504.01338.pdf' target='_blank'>https://arxiv.org/pdf/2504.01338.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Manolo Canales Cuba, VinÃ­cius do Carmo MelÃ­cio, JoÃ£o Paulo Gois
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.01338">FlowMotion: Target-Predictive Conditional Flow Matching for Jitter-Reduced Text-Driven Human Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Achieving high-fidelity and temporally smooth 3D human motion generation remains a challenge, particularly within resource-constrained environments. We introduce FlowMotion, a novel method leveraging Conditional Flow Matching (CFM). FlowMotion incorporates a training objective within CFM that focuses on more accurately predicting target motion in 3D human motion generation, resulting in enhanced generation fidelity and temporal smoothness while maintaining the fast synthesis times characteristic of flow-matching-based methods. FlowMotion achieves state-of-the-art jitter performance, achieving the best jitter in the KIT dataset and the second-best jitter in the HumanML3D dataset, and a competitive FID value in both datasets. This combination provides robust and natural motion sequences, offering a promising equilibrium between generation quality and temporal naturalness.
<div id='section'>Paperid: <span id='pid'>1427, <a href='https://arxiv.org/pdf/2504.01024.pdf' target='_blank'>https://arxiv.org/pdf/2504.01024.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yufei He, Xucong Zhang, Arno H. A. Stienen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.01024">Gaze-Guided 3D Hand Motion Prediction for Detecting Intent in Egocentric Grasping Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human intention detection with hand motion prediction is critical to drive the upper-extremity assistive robots in neurorehabilitation applications. However, the traditional methods relying on physiological signal measurement are restrictive and often lack environmental context. We propose a novel approach that predicts future sequences of both hand poses and joint positions. This method integrates gaze information, historical hand motion sequences, and environmental object data, adapting dynamically to the assistive needs of the patient without prior knowledge of the intended object for grasping. Specifically, we use a vector-quantized variational autoencoder for robust hand pose encoding with an autoregressive generative transformer for effective hand motion sequence prediction. We demonstrate the usability of these novel techniques in a pilot study with healthy subjects. To train and evaluate the proposed method, we collect a dataset consisting of various types of grasp actions on different objects from multiple subjects. Through extensive experiments, we demonstrate that the proposed method can successfully predict sequential hand movement. Especially, the gaze information shows significant enhancements in prediction capabilities, particularly with fewer input frames, highlighting the potential of the proposed method for real-world applications.
<div id='section'>Paperid: <span id='pid'>1428, <a href='https://arxiv.org/pdf/2503.20237.pdf' target='_blank'>https://arxiv.org/pdf/2503.20237.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vineela Reddy Pippera Badguna, Aliasghar Arab, Durga Avinash Kodavalla
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.20237">A Virtual Fencing Framework for Safe and Efficient Collaborative Robotics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Collaborative robots (cobots) increasingly operate alongside humans, demanding robust real-time safeguarding. Current safety standards (e.g., ISO 10218, ANSI/RIA 15.06, ISO/TS 15066) require risk assessments but offer limited guidance for real-time responses. We propose a virtual fencing approach that detects and predicts human motion, ensuring safe cobot operation. Safety and performance tradeoffs are modeled as an optimization problem and solved via sequential quadratic programming. Experimental validation shows that our method minimizes operational pauses while maintaining safety, providing a modular solution for human-robot collaboration.
<div id='section'>Paperid: <span id='pid'>1429, <a href='https://arxiv.org/pdf/2503.18386.pdf' target='_blank'>https://arxiv.org/pdf/2503.18386.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sicong Feng, Jielong Yang, Li Peng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.18386">Resource-Efficient Motion Control for Video Generation via Dynamic Mask Guidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in diffusion models bring new vitality to visual content creation. However, current text-to-video generation models still face significant challenges such as high training costs, substantial data requirements, and difficulties in maintaining consistency between given text and motion of the foreground object. To address these challenges, we propose mask-guided video generation, which can control video generation through mask motion sequences, while requiring limited training data. Our model enhances existing architectures by incorporating foreground masks for precise text-position matching and motion trajectory control. Through mask motion sequences, we guide the video generation process to maintain consistent foreground objects throughout the sequence. Additionally, through a first-frame sharing strategy and autoregressive extension approach, we achieve more stable and longer video generation. Extensive qualitative and quantitative experiments demonstrate that this approach excels in various video generation tasks, such as video editing and generating artistic videos, outperforming previous methods in terms of consistency and quality. Our generated results can be viewed in the supplementary materials.
<div id='section'>Paperid: <span id='pid'>1430, <a href='https://arxiv.org/pdf/2503.06897.pdf' target='_blank'>https://arxiv.org/pdf/2503.06897.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xingzu Zhan, Chen Xie, Honghang Chen, Haoran Sun, Xiaochun Mai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.06897">Multi-granular body modeling with Redundancy-Free Spatiotemporal Fusion for Text-Driven Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-to-motion generation sits at the intersection of multimodal learning and computer graphics and is gaining momentum because it can simplify content creation for games, animation, robotics and virtual reality. Most current methods stack spatial and temporal features in a straightforward way, which adds redundancy and still misses subtle joint-level cues. We introduce HiSTF Mamba, a framework with three parts: Dual-Spatial Mamba, Bi-Temporal Mamba and a Dynamic Spatiotemporal Fusion Module (DSFM). The Dual-Spatial module runs part-based and whole-body models in parallel, capturing both overall coordination and fine-grained joint motion. The Bi-Temporal module scans sequences forward and backward to encode short-term details and long-term dependencies. DSFM removes redundant temporal information, extracts complementary cues and fuses them with spatial features to build a richer spatiotemporal representation. Experiments on the HumanML3D benchmark show that HiSTF Mamba performs well across several metrics, achieving high fidelity and tight semantic alignment between text and motion.
<div id='section'>Paperid: <span id='pid'>1431, <a href='https://arxiv.org/pdf/2503.06119.pdf' target='_blank'>https://arxiv.org/pdf/2503.06119.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shinichi Tanaka, Zhao Wang, Yoichi Kato, Jun Ohya
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.06119">Unlocking Pretrained LLMs for Motion-Related Multimodal Generation: A Fine-Tuning Approach to Unify Diffusion and Next-Token Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose a unified framework that leverages a single pretrained LLM for Motion-related Multimodal Generation, referred to as MoMug. MoMug integrates diffusion-based continuous motion generation with the model's inherent autoregressive discrete text prediction capabilities by fine-tuning a pretrained LLM. This enables seamless switching between continuous motion output and discrete text token prediction within a single model architecture, effectively combining the strengths of both diffusion- and LLM-based approaches. Experimental results show that, compared to the most recent LLM-based baseline, MoMug improves FID by 38% and mean accuracy across seven metrics by 16.61% on the text-to-motion task. Additionally, it improves mean accuracy across eight metrics by 8.44% on the text-to-motion task. To the best of our knowledge, this is the first approach to integrate diffusion- and LLM-based generation within a single model for motion-related multimodal tasks while maintaining low training costs. This establishes a foundation for future advancements in motion-related generation, paving the way for high-quality yet cost-efficient motion synthesis.
<div id='section'>Paperid: <span id='pid'>1432, <a href='https://arxiv.org/pdf/2503.02869.pdf' target='_blank'>https://arxiv.org/pdf/2503.02869.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>M. van der Hulst, R. A. GonzÃ¡lez, K. Classens, P. Tacx, N. Dirkx, J. van de Wijdeven, T. Oomen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.02869">Frequency domain identification for multivariable motion control systems: Applied to a prototype wafer stage</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multivariable parametric models are essential for optimizing the performance of high-tech systems. The main objective of this paper is to develop an identification strategy that provides accurate parametric models for complex multivariable systems. To achieve this, an additive model structure is adopted, offering advantages over traditional black-box model structures when considering physical systems. The introduced method minimizes a weighted least-squares criterion and uses an iterative linear regression algorithm to solve the estimation problem, achieving local optimality upon convergence. Experimental validation is conducted on a prototype wafer-stage system, featuring a large number of spatially distributed actuators and sensors and exhibiting complex flexible dynamic behavior, to evaluate performance and demonstrate the effectiveness of the proposed method.
<div id='section'>Paperid: <span id='pid'>1433, <a href='https://arxiv.org/pdf/2503.00602.pdf' target='_blank'>https://arxiv.org/pdf/2503.00602.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Zhang, Xiaoyu Shi, Tongyang Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.00602">Zero-Power Backscatter Sensing and Communication Proof-of-Concept</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we present an experimental setup to evaluate the performance of a radio frequency identification (RFID)-based integrated sensing and communication (ISAC) system. We focus on both the communication and sensing capabilities of the system. Our experiments evaluate the system's performance in various channel fading scenarios and with different substrate materials, including wood, plastic, wall, and glass. Additionally, we utilize radio tomographic imaging (RTI) to detect human motion by analyzing received signal strength indicator (RSSI) data. Our results demonstrate the impact of different materials and environments on RSSI and highlight the potential of RFID-based systems for effective sensing and communication in diverse applications.
<div id='section'>Paperid: <span id='pid'>1434, <a href='https://arxiv.org/pdf/2502.21207.pdf' target='_blank'>https://arxiv.org/pdf/2502.21207.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>ThÃ©o Cheynel, Thomas Rossi, Baptiste Bellot-Gurlet, Damien Rohmer, Marie-Paule Cani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.21207">ReConForM : Real-time Contact-aware Motion Retargeting for more Diverse Character Morphologies</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Preserving semantics, in particular in terms of contacts, is a key challenge when retargeting motion between characters of different morphologies. Our solution relies on a low-dimensional embedding of the character's mesh, based on rigged key vertices that are automatically transferred from the source to the target. Motion descriptors are extracted from the trajectories of these key vertices, providing an embedding that contains combined semantic information about both shape and pose. A novel, adaptive algorithm is then used to automatically select and weight the most relevant features over time, enabling us to efficiently optimize the target motion until it conforms to these constraints, so as to preserve the semantics of the source motion. Our solution allows extensions to several novel use-cases where morphology and mesh contacts were previously overlooked, such as multi-character retargeting and motion transfer on uneven terrains. As our results show, our method is able to achieve real-time retargeting onto a wide variety of characters. Extensive experiments and comparison with state-of-the-art methods using several relevant metrics demonstrate improved results, both in terms of motion smoothness and contact accuracy.
<div id='section'>Paperid: <span id='pid'>1435, <a href='https://arxiv.org/pdf/2502.20824.pdf' target='_blank'>https://arxiv.org/pdf/2502.20824.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fadeel Sher Khan, Joshua Ebenezer, Hamid Sheikh, Seok-Jun Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.20824">MFSR-GAN: Multi-Frame Super-Resolution with Handheld Motion Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Smartphone cameras have become ubiquitous imaging tools, yet their small sensors and compact optics often limit spatial resolution and introduce distortions. Combining information from multiple low-resolution (LR) frames to produce a high-resolution (HR) image has been explored to overcome the inherent limitations of smartphone cameras. Despite the promise of multi-frame super-resolution (MFSR), current approaches are hindered by datasets that fail to capture the characteristic noise and motion patterns found in real-world handheld burst images. In this work, we address this gap by introducing a novel synthetic data engine that uses multi-exposure static images to synthesize LR-HR training pairs while preserving sensor-specific noise characteristics and image motion found during handheld burst photography. We also propose MFSR-GAN: a multi-scale RAW-to-RGB network for MFSR. Compared to prior approaches, MFSR-GAN emphasizes a "base frame" throughout its architecture to mitigate artifacts. Experimental results on both synthetic and real data demonstrates that MFSR-GAN trained with our synthetic engine yields sharper, more realistic reconstructions than existing methods for real-world MFSR.
<div id='section'>Paperid: <span id='pid'>1436, <a href='https://arxiv.org/pdf/2502.16134.pdf' target='_blank'>https://arxiv.org/pdf/2502.16134.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Huaiqu Feng, Guoyang Zhao, Cheng Liu, Yongwei Wang, Jun Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.16134">Motion-Coupled Mapping Algorithm for Hybrid Rice Canopy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a motion-coupled mapping algorithm for contour mapping of hybrid rice canopies, specifically designed for Agricultural Unmanned Ground Vehicles (Agri-UGV) navigating complex and unknown rice fields. Precise canopy mapping is essential for Agri-UGVs to plan efficient routes and avoid protected zones. The motion control of Agri-UGVs, tasked with impurity removal and other operations, depends heavily on accurate estimation of rice canopy height and structure. To achieve this, the proposed algorithm integrates real-time RGB-D sensor data with kinematic and inertial measurements, enabling efficient mapping and proprioceptive localization. The algorithm produces grid-based elevation maps that reflect the probabilistic distribution of canopy contours, accounting for motion-induced uncertainties. It is implemented on a high-clearance Agri-UGV platform and tested in various environments, including both controlled and dynamic rice field settings. This approach significantly enhances the mapping accuracy and operational reliability of Agri-UGVs, contributing to more efficient autonomous agricultural operations.
<div id='section'>Paperid: <span id='pid'>1437, <a href='https://arxiv.org/pdf/2502.15956.pdf' target='_blank'>https://arxiv.org/pdf/2502.15956.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Canxuan Gang, Yiran Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.15956">Human Motion Prediction, Reconstruction, and Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This report reviews recent advancements in human motion prediction, reconstruction, and generation. Human motion prediction focuses on forecasting future poses and movements from historical data, addressing challenges like nonlinear dynamics, occlusions, and motion style variations. Reconstruction aims to recover accurate 3D human body movements from visual inputs, often leveraging transformer-based architectures, diffusion models, and physical consistency losses to handle noise and complex poses. Motion generation synthesizes realistic and diverse motions from action labels, textual descriptions, or environmental constraints, with applications in robotics, gaming, and virtual avatars. Additionally, text-to-motion generation and human-object interaction modeling have gained attention, enabling fine-grained and context-aware motion synthesis for augmented reality and robotics. This review highlights key methodologies, datasets, challenges, and future research directions driving progress in these fields.
<div id='section'>Paperid: <span id='pid'>1438, <a href='https://arxiv.org/pdf/2502.00665.pdf' target='_blank'>https://arxiv.org/pdf/2502.00665.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fuxi Ling, Hongye Liu, Guoqiang Huang, Jing Li, Hong Wu, Zhihao Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.00665">Cross-Modal Synergies: Unveiling the Potential of Motion-Aware Fusion Networks in Handling Dynamic and Static ReID Scenarios</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Navigating the complexities of person re-identification (ReID) in varied surveillance scenarios, particularly when occlusions occur, poses significant challenges. We introduce an innovative Motion-Aware Fusion (MOTAR-FUSE) network that utilizes motion cues derived from static imagery to significantly enhance ReID capabilities. This network incorporates a dual-input visual adapter capable of processing both images and videos, thereby facilitating more effective feature extraction. A unique aspect of our approach is the integration of a motion consistency task, which empowers the motion-aware transformer to adeptly capture the dynamics of human motion. This technique substantially improves the recognition of features in scenarios where occlusions are prevalent, thereby advancing the ReID process. Our comprehensive evaluations across multiple ReID benchmarks, including holistic, occluded, and video-based scenarios, demonstrate that our MOTAR-FUSE network achieves superior performance compared to existing approaches.
<div id='section'>Paperid: <span id='pid'>1439, <a href='https://arxiv.org/pdf/2502.00215.pdf' target='_blank'>https://arxiv.org/pdf/2502.00215.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fabio Spada, Purnanand Elango, BehÃ§et AÃ§Ä±kmeÅe
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.00215">Impulsive Relative Motion Control with Continuous-Time Constraint Satisfaction for Cislunar Space Missions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent investments in cislunar applications open new frontiers for space missions within highly nonlinear dynamical regimes. In this paper, we propose a method based on Sequential Convex Programming (SCP) to loiter around a given target with impulsive actuation while satisfying path constraints continuously over the finite time-horizon, i.e., independently of the number of nodes in which domain is discretized. Location, timing, magnitude, and direction of a fixed number of impulses are optimized in a model predictive framework, exploiting the exact nonlinear dynamics of non-stationary orbital regimes. The proposed approach is first validated on a relative orbiting problem with respect to a selenocentric near rectilinear halo orbit. The approach is then compared to a formulation with path constraints imposed only at nodes and with mesh refined to ensure complete satisfaction of path constraints over the continuous-time horizon. CPU time per iteration of 400 ms for the refined-mesh approach reduce to 5.5 ms for the proposed approach.
<div id='section'>Paperid: <span id='pid'>1440, <a href='https://arxiv.org/pdf/2501.18729.pdf' target='_blank'>https://arxiv.org/pdf/2501.18729.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anthony Richardson, Felix Putze
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.18729">Motion Diffusion Autoencoders: Enabling Attribute Manipulation in Human Motion Demonstrated on Karate Techniques</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Attribute manipulation deals with the problem of changing individual attributes of a data point or a time series, while leaving all other aspects unaffected. This work focuses on the domain of human motion, more precisely karate movement patterns. To the best of our knowledge, it presents the first success at manipulating attributes of human motion data. One of the key requirements for achieving attribute manipulation on human motion is a suitable pose representation. Therefore, we design a novel continuous, rotation-based pose representation that enables the disentanglement of the human skeleton and the motion trajectory, while still allowing an accurate reconstruction of the original anatomy. The core idea of the manipulation approach is to use a transformer encoder for discovering high-level semantics, and a diffusion probabilistic model for modeling the remaining stochastic variations. We show that the embedding space obtained from the transformer encoder is semantically meaningful and linear. This enables the manipulation of high-level attributes, by discovering their linear direction of change in the semantic embedding space and moving the embedding along said direction. All code and data is made publicly available.
<div id='section'>Paperid: <span id='pid'>1441, <a href='https://arxiv.org/pdf/2501.15058.pdf' target='_blank'>https://arxiv.org/pdf/2501.15058.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Jiang, Yixing Chen, Xingyang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.15058">KETA: Kinematic-Phrases-Enhanced Text-to-Motion Generation via Fine-grained Alignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion synthesis plays a vital role in various fields of artificial intelligence. Among the various conditions of motion generation, text can describe motion details elaborately and is easy to acquire, making text-to-motion(T2M) generation important. State-of-the-art T2M techniques mainly leverage diffusion models to generate motions with text prompts as guidance, tackling the many-to-many nature of T2M tasks. However, existing T2M approaches face challenges, given the gap between the natural language domain and the physical domain, making it difficult to generate motions fully consistent with the texts.
  We leverage kinematic phrases(KP), an intermediate representation that bridges these two modalities, to solve this. Our proposed method, KETA, decomposes the given text into several decomposed texts via a language model. It trains an aligner to align decomposed texts with the KP segments extracted from the generated motions. Thus, it's possible to restrict the behaviors for diffusion-based T2M models. During the training stage, we deploy the text-KP alignment loss as an auxiliary goal to supervise the models. During the inference stage, we refine our generated motions for multiple rounds in our decoder structure, where we compute the text-KP distance as the guidance signal in each new round. Experiments demonstrate that KETA achieves up to 1.19x, 2.34x better R precision and FID value on both backbones of the base model, motion diffusion model. Compared to a wide range of T2M generation models. KETA achieves either the best or the second-best performance.
<div id='section'>Paperid: <span id='pid'>1442, <a href='https://arxiv.org/pdf/2501.04793.pdf' target='_blank'>https://arxiv.org/pdf/2501.04793.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Caner OdabaÅ, Ãmer MorgÃ¼l
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.04793">A Novel Observer Design for LuGre Friction Estimation and Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dynamic components of the friction may directly impact the stability and performance of the motion control systems. The LuGre model is a prevalent friction model utilized to express this dynamic behavior. Since the LuGre model is very comprehensive, friction compensation based on it might be challenging. Inspired by this, we develop a novel observer to estimate and compensate for LuGre friction. Furthermore, we present a Lyapunov stability analysis to show that observer dynamics are asymptotically stable under certain conditions. Compared to its counterparts, the proposed observer constitutes a simple and standalone scheme that can be utilized with arbitrary control inputs in a straightforward way. As a primary difference, the presented observer estimates velocity and uses the velocity error to estimate friction in addition to control input. The extensive simulations revealed that the introduced observer enhances position and velocity tracking performance in the presence of friction.
<div id='section'>Paperid: <span id='pid'>1443, <a href='https://arxiv.org/pdf/2412.15819.pdf' target='_blank'>https://arxiv.org/pdf/2412.15819.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cheng Wang, Ziyang Feng, Pin Zhang, Manjiang Cao, Yiming Yuan, Tengfei Chang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.15819">Robustness-enhanced Myoelectric Control with GAN-based Open-set Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Electromyography (EMG) signals are widely used in human motion recognition and medical rehabilitation, yet their variability and susceptibility to noise significantly limit the reliability of myoelectric control systems. Existing recognition algorithms often fail to handle unfamiliar actions effectively, leading to system instability and errors. This paper proposes a novel framework based on Generative Adversarial Networks (GANs) to enhance the robustness and usability of myoelectric control systems by enabling open-set recognition. The method incorporates a GAN-based discriminator to identify and reject unknown actions, maintaining system stability by preventing misclassifications. Experimental evaluations on publicly available and self-collected datasets demonstrate a recognition accuracy of 97.6\% for known actions and a 23.6\% improvement in Active Error Rate (AER) after rejecting unknown actions. The proposed approach is computationally efficient and suitable for deployment on edge devices, making it practical for real-world applications.
<div id='section'>Paperid: <span id='pid'>1444, <a href='https://arxiv.org/pdf/2412.04820.pdf' target='_blank'>https://arxiv.org/pdf/2412.04820.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Charles Dietzel, Patrick J. Martin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.04820">Assessing Similarity Measures for the Evaluation of Human-Robot Motion Correspondence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>One key area of research in Human-Robot Interaction is solving the human-robot correspondence problem, which asks how a robot can learn to reproduce a human motion demonstration when the human and robot have different dynamics and kinematic structures. Evaluating these correspondence problem solutions often requires the use of qualitative surveys that can be time consuming to design and administer. Additionally, qualitative survey results vary depending on the population of survey participants. In this paper, we propose the use of heterogeneous time-series similarity measures as a quantitative evaluation metric for evaluating motion correspondence to complement these qualitative surveys. To assess the suitability of these measures, we develop a behavioral cloning-based motion correspondence model, and evaluate it with a qualitative survey as well as quantitative measures. By comparing the resulting similarity scores with the human survey results, we identify Gromov Dynamic Time Warping as a promising quantitative measure for evaluating motion correspondence.
<div id='section'>Paperid: <span id='pid'>1445, <a href='https://arxiv.org/pdf/2412.04097.pdf' target='_blank'>https://arxiv.org/pdf/2412.04097.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Meenakshi Gupta, Mingyuan Lei, Tat-Jen Cham, Hwee Kuan Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.04097">D-LORD for Motion Stylization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces a novel framework named D-LORD (Double Latent Optimization for Representation Disentanglement), which is designed for motion stylization (motion style transfer and motion retargeting). The primary objective of this framework is to separate the class and content information from a given motion sequence using a data-driven latent optimization approach. Here, class refers to person-specific style, such as a particular emotion or an individual's identity, while content relates to the style-agnostic aspect of an action, such as walking or jumping, as universally understood concepts. The key advantage of D-LORD is its ability to perform style transfer without needing paired motion data. Instead, it utilizes class and content labels during the latent optimization process. By disentangling the representation, the framework enables the transformation of one motion sequences style to another's style using Adaptive Instance Normalization. The proposed D-LORD framework is designed with a focus on generalization, allowing it to handle different class and content labels for various applications. Additionally, it can generate diverse motion sequences when specific class and content labels are provided. The framework's efficacy is demonstrated through experimentation on three datasets: the CMU XIA dataset for motion style transfer, the MHAD dataset, and the RRIS Ability dataset for motion retargeting. Notably, this paper presents the first generalized framework for motion style transfer and motion retargeting, showcasing its potential contributions in this area.
<div id='section'>Paperid: <span id='pid'>1446, <a href='https://arxiv.org/pdf/2412.03299.pdf' target='_blank'>https://arxiv.org/pdf/2412.03299.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sam A. Scivier, Tarje Nissen-Meyer, Paula Koelemeijer, AtÄ±lÄ±m GÃ¼neÅ Baydin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.03299">Gaussian Processes for Probabilistic Estimates of Earthquake Ground Shaking: A 1-D Proof-of-Concept</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Estimates of seismic wave speeds in the Earth (seismic velocity models) are key input parameters to earthquake simulations for ground motion prediction. Owing to the non-uniqueness of the seismic inverse problem, typically many velocity models exist for any given region. The arbitrary choice of which velocity model to use in earthquake simulations impacts ground motion predictions. However, current hazard analysis methods do not account for this source of uncertainty. We present a proof-of-concept ground motion prediction workflow for incorporating uncertainties arising from inconsistencies between existing seismic velocity models. Our analysis is based on the probabilistic fusion of overlapping seismic velocity models using scalable Gaussian process (GP) regression. Specifically, we fit a GP to two synthetic 1-D velocity profiles simultaneously, and show that the predictive uncertainty accounts for the differences between the models. We subsequently draw velocity model samples from the predictive distribution and estimate peak ground displacement using acoustic wave propagation through the velocity models. The resulting distribution of possible ground motion amplitudes is much wider than would be predicted by simulating shaking using only the two input velocity models. This proof-of-concept illustrates the importance of probabilistic methods for physics-based seismic hazard analysis.
<div id='section'>Paperid: <span id='pid'>1447, <a href='https://arxiv.org/pdf/2411.18745.pdf' target='_blank'>https://arxiv.org/pdf/2411.18745.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zheyan Zhang, Diego Klabjan, Renee CB Manworren
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.18745">DiffMVR: Diffusion-based Automated Multi-Guidance Video Restoration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we address a challenge in video inpainting: reconstructing occluded regions in dynamic, real-world scenarios. Motivated by the need for continuous human motion monitoring in healthcare settings, where facial features are frequently obscured, we propose a diffusion-based video-level inpainting model, DiffMVR. Our approach introduces a dynamic dual-guided image prompting system, leveraging adaptive reference frames to guide the inpainting process. This enables the model to capture both fine-grained details and smooth transitions between video frames, offering precise control over inpainting direction and significantly improving restoration accuracy in challenging, dynamic environments. DiffMVR represents a significant advancement in the field of diffusion-based inpainting, with practical implications for real-time applications in various dynamic settings.
<div id='section'>Paperid: <span id='pid'>1448, <a href='https://arxiv.org/pdf/2411.17745.pdf' target='_blank'>https://arxiv.org/pdf/2411.17745.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiarui Song, Yingbo Sun, Qing Dong, Xuewu Ji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.17745">A Parameter Adaptive Trajectory Tracking and Motion Control Framework for Autonomous Vehicle</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper studies the trajectory tracking and motion control problems for autonomous vehicles (AVs). A parameter adaptive control framework for AVs is proposed to enhance tracking accuracy and yaw stability. While establishing linear quadratic regulator (LQR) and three robust controllers, the control framework addresses trajectory tracking and motion control in a modular fashion, without introducing complexity into each controller. The robust performance has been guaranteed in three robust controllers by considering the parameter uncertainties, mismatch of unmodeled subsystem as well as external disturbance, comprehensively. Also, the dynamic characteristics of uncertain parameters are identified by Recursive Least Squares (RLS) algorithm, while the boundaries of three robust factors are determined through combining Gaussian Process Regression (GPR) and Bayesian optimization machine learning methods, reducing the conservatism of the controller. Sufficient conditions for closed-loop stability under the diverse robust factors are provided by the Lyapunov method analytically. The simulation results on MATLAB/Simulink and Carsim joint platform demonstrate that the proposed methodology considerably improves tracking accuracy, driving stability, and robust performance, guaranteeing the feasibility and capability of driving in extreme scenarios.
<div id='section'>Paperid: <span id='pid'>1449, <a href='https://arxiv.org/pdf/2411.16802.pdf' target='_blank'>https://arxiv.org/pdf/2411.16802.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Omar El Assal, Carlos M. Mateo, Sebastien Ciron, David Fofi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.16802">Leveraging Foundation Models To learn the shape of semi-fluid deformable objects</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>One of the difficulties imposed on the manipulation of deformable objects is their characterization and the detection of representative keypoints for the purpose of manipulation. A keen interest was manifested by researchers in the last decade to characterize and manipulate deformable objects of non-fluid nature, such as clothes and ropes. Even though several propositions were made in the regard of object characterization, however researchers were always confronted with the need of pixel-level information of the object through images to extract relevant information. This usually is accomplished by means of segmentation networks trained on manually labeled data for this purpose. In this paper, we address the subject of characterizing weld pool to define stable features that serve as information for further motion control objectives. We achieve this by employing different pipelines. The first one consists of characterizing fluid deformable objects through the use of a generative model that is trained using a teacher-student framework. And in the second one we leverage foundation models by using them as teachers to characterize the object in the image, without the need of any pre-training and any dataset. The performance of knowledge distillation from foundation models into a smaller generative model shows prominent results in the characterization of deformable objects. The student network was capable of learning to retrieve the keypoitns of the object with an error of 13.4 pixels. And the teacher was evaluated based on its capacities to retrieve pixel level information represented by the object mask, with a mean Intersection Over Union (mIoU) of 75.26%.
<div id='section'>Paperid: <span id='pid'>1450, <a href='https://arxiv.org/pdf/2411.05734.pdf' target='_blank'>https://arxiv.org/pdf/2411.05734.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Agamdeep Singh, Sujit PB, Mayank Vatsa
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.05734">Poze: Sports Technique Feedback under Data Constraints</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Access to expert coaching is essential for developing technique in sports, yet economic barriers often place it out of reach for many enthusiasts. To bridge this gap, we introduce Poze, an innovative video processing framework that provides feedback on human motion, emulating the insights of a professional coach. Poze combines pose estimation with sequence comparison and is optimized to function effectively with minimal data. Poze surpasses state-of-the-art vision-language models in video question-answering frameworks, achieving 70% and 196% increase in accuracy over GPT4V and LLaVAv1.6 7b, respectively.
<div id='section'>Paperid: <span id='pid'>1451, <a href='https://arxiv.org/pdf/2410.17741.pdf' target='_blank'>https://arxiv.org/pdf/2410.17741.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zexu Huang, Sarah Monazam Erfani, Siying Lu, Mingming Gong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.17741">Efficient Neural Implicit Representation for 3D Human Reconstruction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>High-fidelity digital human representations are increasingly in demand in the digital world, particularly for interactive telepresence, AR/VR, 3D graphics, and the rapidly evolving metaverse. Even though they work well in small spaces, conventional methods for reconstructing 3D human motion frequently require the use of expensive hardware and have high processing costs. This study presents HumanAvatar, an innovative approach that efficiently reconstructs precise human avatars from monocular video sources. At the core of our methodology, we integrate the pre-trained HuMoR, a model celebrated for its proficiency in human motion estimation. This is adeptly fused with the cutting-edge neural radiance field technology, Instant-NGP, and the state-of-the-art articulated model, Fast-SNARF, to enhance the reconstruction fidelity and speed. By combining these two technologies, a system is created that can render quickly and effectively while also providing estimation of human pose parameters that are unmatched in accuracy. We have enhanced our system with an advanced posture-sensitive space reduction technique, which optimally balances rendering quality with computational efficiency. In our detailed experimental analysis using both artificial and real-world monocular videos, we establish the advanced performance of our approach. HumanAvatar consistently equals or surpasses contemporary leading-edge reconstruction techniques in quality. Furthermore, it achieves these complex reconstructions in minutes, a fraction of the time typically required by existing methods. Our models achieve a training speed that is 110X faster than that of State-of-The-Art (SoTA) NeRF-based models. Our technique performs noticeably better than SoTA dynamic human NeRF methods if given an identical runtime limit. HumanAvatar can provide effective visuals after only 30 seconds of training.
<div id='section'>Paperid: <span id='pid'>1452, <a href='https://arxiv.org/pdf/2410.16623.pdf' target='_blank'>https://arxiv.org/pdf/2410.16623.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sudarshan Harithas, Srinath Sridhar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.16623">MotionGlot: A Multi-Embodied Motion Generation Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces MotionGlot, a model that can generate motion across multiple embodiments with different action dimensions, such as quadruped robots and human bodies. By leveraging the well-established training procedures commonly used in large language models (LLMs), we introduce an instruction-tuning template specifically designed for motionrelated tasks. Our approach demonstrates that the principles underlying LLM training can be successfully adapted to learn a wide range of motion generation tasks across multiple embodiments with different action dimensions. We demonstrate the various abilities of MotionGlot on a set of 6 tasks and report an average improvement of 35.3% across tasks. Additionally, we contribute two new datasets: (1) a dataset of expert-controlled quadruped locomotion with approximately 48,000 trajectories paired with direction-based text annotations, and (2) a dataset of over 23,000 situational text prompts for human motion generation tasks. Finally, we conduct hardware experiments to validate the capabilities of our system in real-world applications.
<div id='section'>Paperid: <span id='pid'>1453, <a href='https://arxiv.org/pdf/2409.19647.pdf' target='_blank'>https://arxiv.org/pdf/2409.19647.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shiming Fang, Kaiyan Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.19647">Fine-Tuning Hybrid Physics-Informed Neural Networks for Vehicle Dynamics Model Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate dynamic modeling is critical for autonomous racing vehicles, especially during high-speed and agile maneuvers where precise motion prediction is essential for safety. Traditional parameter estimation methods face limitations such as reliance on initial guesses, labor-intensive fitting procedures, and complex testing setups. On the other hand, purely data-driven machine learning methods struggle to capture inherent physical constraints and typically require large datasets for optimal performance. To address these challenges, this paper introduces the Fine-Tuning Hybrid Dynamics (FTHD) method, which integrates supervised and unsupervised Physics-Informed Neural Networks (PINNs), combining physics-based modeling with data-driven techniques. FTHD fine-tunes a pre-trained Deep Dynamics Model (DDM) using a smaller training dataset, delivering superior performance compared to state-of-the-art methods such as the Deep Pacejka Model (DPM) and outperforming the original DDM. Furthermore, an Extended Kalman Filter (EKF) is embedded within FTHD (EKF-FTHD) to effectively manage noisy real-world data, ensuring accurate denoising while preserving the vehicle's essential physical characteristics. The proposed FTHD framework is validated through scaled simulations using the BayesRace Physics-based Simulator and full-scale real-world experiments from the Indy Autonomous Challenge. Results demonstrate that the hybrid approach significantly improves parameter estimation accuracy, even with reduced data, and outperforms existing models. EKF-FTHD enhances robustness by denoising real-world data while maintaining physical insights, representing a notable advancement in vehicle dynamics modeling for high-speed autonomous racing.
<div id='section'>Paperid: <span id='pid'>1454, <a href='https://arxiv.org/pdf/2409.13251.pdf' target='_blank'>https://arxiv.org/pdf/2409.13251.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingdian Liu, Yilin Liu, Gurunandan Krishnan, Karl S Bayer, Bing Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.13251">T2M-X: Learning Expressive Text-to-Motion Generation from Partially Annotated Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The generation of humanoid animation from text prompts can profoundly impact animation production and AR/VR experiences. However, existing methods only generate body motion data, excluding facial expressions and hand movements. This limitation, primarily due to a lack of a comprehensive whole-body motion dataset, inhibits their readiness for production use. Recent attempts to create such a dataset have resulted in either motion inconsistency among different body parts in the artificially augmented data or lower quality in the data extracted from RGB videos. In this work, we propose T2M-X, a two-stage method that learns expressive text-to-motion generation from partially annotated data. T2M-X trains three separate Vector Quantized Variational AutoEncoders (VQ-VAEs) for body, hand, and face on respective high-quality data sources to ensure high-quality motion outputs, and a Multi-indexing Generative Pretrained Transformer (GPT) model with motion consistency loss for motion generation and coordination among different body parts. Our results show significant improvements over the baselines both quantitatively and qualitatively, demonstrating its robustness against the dataset limitations.
<div id='section'>Paperid: <span id='pid'>1455, <a href='https://arxiv.org/pdf/2409.13208.pdf' target='_blank'>https://arxiv.org/pdf/2409.13208.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiyana Figuera, Soogeun Park, Hyemin Ahn
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.13208">Redefining Data Pairing for Motion Retargeting Leveraging a Human Body Prior</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose MR HuBo(Motion Retargeting leveraging a HUman BOdy prior), a cost-effective and convenient method to collect high-quality upper body paired <robot, human> pose data, which is essential for data-driven motion retargeting methods. Unlike existing approaches which collect <robot, human> pose data by converting human MoCap poses into robot poses, our method goes in reverse. We first sample diverse random robot poses, and then convert them into human poses. However, since random robot poses can result in extreme and infeasible human poses, we propose an additional technique to sort out extreme poses by exploiting a human body prior trained from a large amount of human pose data. Our data collection method can be used for any humanoid robots, if one designs or optimizes the system's hyperparameters which include a size scale factor and the joint angle ranges for sampling. In addition to this data collection method, we also present a two-stage motion retargeting neural network that can be trained via supervised learning on a large amount of paired data. Compared to other learning-based methods trained via unsupervised learning, we found that our deep neural network trained with ample high-quality paired data achieved notable performance. Our experiments also show that our data filtering method yields better retargeting results than training the model with raw and noisy data. Our code and video results are available on https://sites.google.com/view/mr-hubo/
<div id='section'>Paperid: <span id='pid'>1456, <a href='https://arxiv.org/pdf/2409.09326.pdf' target='_blank'>https://arxiv.org/pdf/2409.09326.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Deng Junli, Luo Yihao, Yang Xueting, Li Siyou, Wang Wei, Guo Jinyang, Shi Ping
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.09326">LawDNet: Enhanced Audio-Driven Lip Synthesis via Local Affine Warping Deformation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the domain of photorealistic avatar generation, the fidelity of audio-driven lip motion synthesis is essential for realistic virtual interactions. Existing methods face two key challenges: a lack of vivacity due to limited diversity in generated lip poses and noticeable anamorphose motions caused by poor temporal coherence. To address these issues, we propose LawDNet, a novel deep-learning architecture enhancing lip synthesis through a Local Affine Warping Deformation mechanism. This mechanism models the intricate lip movements in response to the audio input by controllable non-linear warping fields. These fields consist of local affine transformations focused on abstract keypoints within deep feature maps, offering a novel universal paradigm for feature warping in networks. Additionally, LawDNet incorporates a dual-stream discriminator for improved frame-to-frame continuity and employs face normalization techniques to handle pose and scene variations. Extensive evaluations demonstrate LawDNet's superior robustness and lip movement dynamism performance compared to previous methods. The advancements presented in this paper, including the methodologies, training data, source codes, and pre-trained models, will be made accessible to the research community.
<div id='section'>Paperid: <span id='pid'>1457, <a href='https://arxiv.org/pdf/2408.04106.pdf' target='_blank'>https://arxiv.org/pdf/2408.04106.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sagar Ojha, Karl Leodler, Lou Barbieri, TseHuai Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.04106">Force-Motion Control For A Six Degree-Of-Freedom Robotic Manipulator</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a unified algorithm for motion and force control for a six degree-of-freedom spatial manipulator. The motion-force controller performs trajectory tracking, maneuvering the manipulator's end-effector through desired position, orientations and rates. When contacting an obstacle or target object, the force module of the controller restricts the manipulator movements with a novel force exertion method, which prevents damage to the manipulator, the end-effector, and the objects during the contact or collision. The core strategy presented in this paper is to design the linear acceleration for the end-effector which ensures both trajectory tracking and restriction of any contact force at the end-effector. The design of the controller is validated through numerical simulations and digital twin validation.
<div id='section'>Paperid: <span id='pid'>1458, <a href='https://arxiv.org/pdf/2406.18031.pdf' target='_blank'>https://arxiv.org/pdf/2406.18031.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Juan David Adarve, Robert Mahony
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.18031">Real-time Structure Flow</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This article introduces the structure flow field; a flow field that can provide high-speed robo-centric motion information for motion control of highly dynamic robotic devices and autonomous vehicles. Structure flow is the angular 3D velocity of the scene at a given pixel. We show that structure flow posses an elegant evolution model in the form of a Partial Differential Equation (PDE) that enables us to create dense flow predictions forward in time. We exploit this structure to design a predictor-update algorithm to compute structure flow in real time using image and depth measurements. The prediction stage takes the previous estimate of the structure flow and propagates it forward in time using a numerical implementation of the structure flow PDE. The predicted flow is then updated using new image and depth data. The algorithm runs up to 600 Hz on a Desktop GPU machine for 512x512 images with flow values up to 8 pixels. We provide ground truth validation on high-speed synthetic image sequences as well as results on real-life video on driving scenarios.
<div id='section'>Paperid: <span id='pid'>1459, <a href='https://arxiv.org/pdf/2406.13419.pdf' target='_blank'>https://arxiv.org/pdf/2406.13419.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yide Liu, Xiyan Liu, Dongqi Wang, Wei Yang, shaoxing Qu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.13419">An eight-neuron network for quadruped locomotion with hip-knee joint control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The gait generator, which is capable of producing rhythmic signals for coordinating multiple joints, is an essential component in the quadruped robot locomotion control framework. The biological counterpart of the gait generator is the Central Pattern Generator (abbreviated as CPG), a small neural network consisting of interacting neurons. Inspired by this architecture, researchers have designed artificial neural networks composed of simulated neurons or oscillator equations. Despite the widespread application of these designed CPGs in various robot locomotion controls, some issues remain unaddressed, including: (1) Simplistic network designs often overlook the symmetry between signal and network structure, resulting in fewer gait patterns than those found in nature. (2) Due to minimal architectural consideration, quadruped control CPGs typically consist of only four neurons, which restricts the network's direct control to leg phases rather than joint coordination. (3) Gait changes are achieved by varying the neuron couplings or the assignment between neurons and legs, rather than through external stimulation. We apply symmetry theory to design an eight-neuron network, composed of Stein neuronal models, capable of achieving five gaits and coordinated control of the hip-knee joints. We validate the signal stability of this network as a gait generator through numerical simulations, which reveal various results and patterns encountered during gait transitions using neuronal stimulation. Based on these findings, we have developed several successful gait transition strategies through neuronal stimulations. Using a commercial quadruped robot model, we demonstrate the usability and feasibility of this network by implementing motion control and gait transitions.
<div id='section'>Paperid: <span id='pid'>1460, <a href='https://arxiv.org/pdf/2405.09109.pdf' target='_blank'>https://arxiv.org/pdf/2405.09109.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Stanley Mugisha, Vamsi Krishna Guda, Christine Chevallereau, Damien Chablat, Matteo Zoppi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.09109">Motion Prediction with Gaussian Processes for Safe Human-Robot Interaction in Virtual Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humans use collaborative robots as tools for accomplishing various tasks. The interaction between humans and robots happens in tight shared workspaces. However, these machines must be safe to operate alongside humans to minimize the risk of accidental collisions. Ensuring safety imposes many constraints, such as reduced torque and velocity limits during operation, thus increasing the time to accomplish many tasks. However, for applications such as using collaborative robots as haptic interfaces with intermittent contacts for virtual reality applications, speed limitations result in poor user experiences. This research aims to improve the efficiency of a collaborative robot while improving the safety of the human user. We used Gaussian process models to predict human hand motion and developed strategies for human intention detection based on hand motion and gaze to improve the time for the robot and human security in a virtual environment. We then studied the effect of prediction. Results from comparisons show that the prediction models improved the robot time by 3\% and safety by 17\%. When used alongside gaze, prediction with Gaussian process models resulted in an improvement of the robot time by 2\% and the safety by 13\%.
<div id='section'>Paperid: <span id='pid'>1461, <a href='https://arxiv.org/pdf/2405.06778.pdf' target='_blank'>https://arxiv.org/pdf/2405.06778.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kebing Xue, Hyewon Seo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.06778">Shape Conditioned Human Motion Generation with Diffusion Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion synthesis is an important task in computer graphics and computer vision. While focusing on various conditioning signals such as text, action class, or audio to guide the generation process, most existing methods utilize skeleton-based pose representation, requiring additional skinning to produce renderable meshes. Given that human motion is a complex interplay of bones, joints, and muscles, considering solely the skeleton for generation may neglect their inherent interdependency, which can limit the variability and precision of the generated results. To address this issue, we propose a Shape-conditioned Motion Diffusion model (SMD), which enables the generation of motion sequences directly in mesh format, conditioned on a specified target mesh. In SMD, the input meshes are transformed into spectral coefficients using graph Laplacian, to efficiently represent meshes. Subsequently, we propose a Spectral-Temporal Autoencoder (STAE) to leverage cross-temporal dependencies within the spectral domain. Extensive experimental evaluations show that SMD not only produces vivid and realistic motions but also achieves competitive performance in text-to-motion and action-to-motion tasks when compared to state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>1462, <a href='https://arxiv.org/pdf/2405.05428.pdf' target='_blank'>https://arxiv.org/pdf/2405.05428.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Thomas Carr, Depeng Xu, Aidong Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.05428">Adversary-Guided Motion Retargeting for Skeleton Anonymization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Skeleton-based motion visualization is a rising field in computer vision, especially in the case of virtual reality (VR). With further advancements in human-pose estimation and skeleton extracting sensors, more and more applications that utilize skeleton data have come about. These skeletons may appear to be anonymous but they contain embedded personally identifiable information (PII). In this paper we present a new anonymization technique that is based on motion retargeting, utilizing adversary classifiers to further remove PII embedded in the skeleton. Motion retargeting is effective in anonymization as it transfers the movement of the user onto the a dummy skeleton. In doing so, any PII linked to the skeleton will be based on the dummy skeleton instead of the user we are protecting. We propose a Privacy-centric Deep Motion Retargeting model (PMR) which aims to further clear the retargeted skeleton of PII through adversarial learning. In our experiments, PMR achieves motion retargeting utility performance on par with state of the art models while also reducing the performance of privacy attacks.
<div id='section'>Paperid: <span id='pid'>1463, <a href='https://arxiv.org/pdf/2404.10880.pdf' target='_blank'>https://arxiv.org/pdf/2404.10880.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Arnab Kumar Mondal, Stefano Alletto, Denis Tome
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.10880">HumMUSS: Human Motion Understanding using State Space Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding human motion from video is essential for a range of applications, including pose estimation, mesh recovery and action recognition. While state-of-the-art methods predominantly rely on transformer-based architectures, these approaches have limitations in practical scenarios. Transformers are slower when sequentially predicting on a continuous stream of frames in real-time, and do not generalize to new frame rates. In light of these constraints, we propose a novel attention-free spatiotemporal model for human motion understanding building upon recent advancements in state space models. Our model not only matches the performance of transformer-based models in various motion understanding tasks but also brings added benefits like adaptability to different video frame rates and enhanced training speed when working with longer sequence of keypoints. Moreover, the proposed model supports both offline and real-time applications. For real-time sequential prediction, our model is both memory efficient and several times faster than transformer-based approaches while maintaining their high accuracy.
<div id='section'>Paperid: <span id='pid'>1464, <a href='https://arxiv.org/pdf/2404.01576.pdf' target='_blank'>https://arxiv.org/pdf/2404.01576.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jesudara Omidokun, Darlington Egeonu, Bochen Jia, Liang Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.01576">Leveraging Digital Perceptual Technologies for Remote Perception and Analysis of Human Biomechanical Processes: A Contactless Approach for Workload and Joint Force Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study presents an innovative computer vision framework designed to analyze human movements in industrial settings, aiming to enhance biomechanical analysis by integrating seamlessly with existing software. Through a combination of advanced imaging and modeling techniques, the framework allows for comprehensive scrutiny of human motion, providing valuable insights into kinematic patterns and kinetic data. Utilizing Convolutional Neural Networks (CNNs), Direct Linear Transform (DLT), and Long Short-Term Memory (LSTM) networks, the methodology accurately detects key body points, reconstructs 3D landmarks, and generates detailed 3D body meshes. Extensive evaluations across various movements validate the framework's effectiveness, demonstrating comparable results to traditional marker-based models with minor differences in joint angle estimations and precise estimations of weight and height. Statistical analyses consistently support the framework's reliability, with joint angle estimations showing less than a 5-degree difference for hip flexion, elbow flexion, and knee angle methods. Additionally, weight estimation exhibits an average error of less than 6 % for weight and less than 2 % for height when compared to ground-truth values from 10 subjects. The integration of the Biomech-57 landmark skeleton template further enhances the robustness and reinforces the framework's credibility. This framework shows significant promise for meticulous biomechanical analysis in industrial contexts, eliminating the need for cumbersome markers and extending its utility to diverse research domains, including the study of specific exoskeleton devices' impact on facilitating the prompt return of injured workers to their tasks.
<div id='section'>Paperid: <span id='pid'>1465, <a href='https://arxiv.org/pdf/2403.15444.pdf' target='_blank'>https://arxiv.org/pdf/2403.15444.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Abhi Kamboj, Minh Do
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.15444">A Survey of IMU Based Cross-Modal Transfer Learning in Human Activity Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite living in a multi-sensory world, most AI models are limited to textual and visual understanding of human motion and behavior. In fact, full situational awareness of human motion could best be understood through a combination of sensors. In this survey we investigate how knowledge can be transferred and utilized amongst modalities for Human Activity/Action Recognition (HAR), i.e. cross-modality transfer learning. We motivate the importance and potential of IMU data and its applicability in cross-modality learning as well as the importance of studying the HAR problem. We categorize HAR related tasks by time and abstractness and then compare various types of multimodal HAR datasets. We also distinguish and expound on many related but inconsistently used terms in the literature, such as transfer learning, domain adaptation, representation learning, sensor fusion, and multimodal learning, and describe how cross-modal learning fits with all these concepts. We then review the literature in IMU-based cross-modal transfer for HAR. The two main approaches for cross-modal transfer are instance-based transfer, where instances of one modality are mapped to another (e.g. knowledge is transferred in the input space), or feature-based transfer, where the model relates the modalities in an intermediate latent space (e.g. knowledge is transferred in the feature space). Finally, we discuss future research directions and applications in cross-modal HAR.
<div id='section'>Paperid: <span id='pid'>1466, <a href='https://arxiv.org/pdf/2403.13905.pdf' target='_blank'>https://arxiv.org/pdf/2403.13905.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anegi James, Efstathios Bakolas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.13905">Motion Prediction of Multi-agent systems with Multi-view clustering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a method for future motion prediction of multi-agent systems by including group formation information and future intent. Formation of groups depends on a physics-based clustering method that follows the agglomerative hierarchical clustering algorithm. We identify clusters that incorporate the minimum cost-to-go function of a relevant optimal control problem as a metric for clustering between the groups among agents, where groups with similar associated costs are assumed to be likely to move together. The cost metric accounts for proximity to other agents as well as the intended goal of each agent. An unscented Kalman filter based approach is used to update the established clusters as well as add new clusters when new information is obtained. Our approach is verified through non-trivial numerical simulations implementing the proposed algorithm on different datasets pertaining to a variety of scenarios and agents.
<div id='section'>Paperid: <span id='pid'>1467, <a href='https://arxiv.org/pdf/2403.06569.pdf' target='_blank'>https://arxiv.org/pdf/2403.06569.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sharmita Dey, Sarath R. Nair
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.06569">Enhancing Joint Motion Prediction for Individuals with Limb Loss Through Model Reprogramming</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Mobility impairment caused by limb loss is a significant challenge faced by millions of individuals worldwide. The development of advanced assistive technologies, such as prosthetic devices, has the potential to greatly improve the quality of life for amputee patients. A critical component in the design of such technologies is the accurate prediction of reference joint motion for the missing limb. However, this task is hindered by the scarcity of joint motion data available for amputee patients, in contrast to the substantial quantity of data from able-bodied subjects. To overcome this, we leverage deep learning's reprogramming property to repurpose well-trained models for a new goal without altering the model parameters. With only data-level manipulation, we adapt models originally designed for able-bodied people to forecast joint motion in amputees. The findings in this study have significant implications for advancing assistive tech and amputee mobility.
<div id='section'>Paperid: <span id='pid'>1468, <a href='https://arxiv.org/pdf/2401.05018.pdf' target='_blank'>https://arxiv.org/pdf/2401.05018.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sarmad Idrees, Jongeun Choi, Seokman Sohn
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.05018">AdvMT: Adversarial Motion Transformer for Long-term Human Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To achieve seamless collaboration between robots and humans in a shared environment, accurately predicting future human movements is essential. Human motion prediction has traditionally been approached as a sequence prediction problem, leveraging historical human motion data to estimate future poses. Beginning with vanilla recurrent networks, the research community has investigated a variety of methods for learning human motion dynamics, encompassing graph-based and generative approaches. Despite these efforts, achieving accurate long-term predictions continues to be a significant challenge. In this regard, we present the Adversarial Motion Transformer (AdvMT), a novel model that integrates a transformer-based motion encoder and a temporal continuity discriminator. This combination effectively captures spatial and temporal dependencies simultaneously within frames. With adversarial training, our method effectively reduces the unwanted artifacts in predictions, thereby ensuring the learning of more realistic and fluid human motions. The evaluation results indicate that AdvMT greatly enhances the accuracy of long-term predictions while also delivering robust short-term predictions
<div id='section'>Paperid: <span id='pid'>1469, <a href='https://arxiv.org/pdf/2401.02620.pdf' target='_blank'>https://arxiv.org/pdf/2401.02620.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Song Bai, Jie Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.02620">Progress and Prospects in 3D Generative AI: A Technical Overview including 3D human</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While AI-generated text and 2D images continue to expand its territory, 3D generation has gradually emerged as a trend that cannot be ignored. Since the year 2023 an abundant amount of research papers has emerged in the domain of 3D generation. This growth encompasses not just the creation of 3D objects, but also the rapid development of 3D character and motion generation. Several key factors contribute to this progress. The enhanced fidelity in stable diffusion, coupled with control methods that ensure multi-view consistency, and realistic human models like SMPL-X, contribute synergistically to the production of 3D models with remarkable consistency and near-realistic appearances. The advancements in neural network-based 3D storing and rendering models, such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS), have accelerated the efficiency and realism of neural rendered models. Furthermore, the multimodality capabilities of large language models have enabled language inputs to transcend into human motion outputs. This paper aims to provide a comprehensive overview and summary of the relevant papers published mostly during the latter half year of 2023. It will begin by discussing the AI generated object models in 3D, followed by the generated 3D human models, and finally, the generated 3D human motions, culminating in a conclusive summary and a vision for the future.
<div id='section'>Paperid: <span id='pid'>1470, <a href='https://arxiv.org/pdf/2512.17183.pdf' target='_blank'>https://arxiv.org/pdf/2512.17183.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.17183">Semantic Co-Speech Gesture Synthesis and Real-Time Control for Humanoid Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present an innovative end-to-end framework for synthesizing semantically meaningful co-speech gestures and deploying them in real-time on a humanoid robot. This system addresses the challenge of creating natural, expressive non-verbal communication for robots by integrating advanced gesture generation techniques with robust physical control. Our core innovation lies in the meticulous integration of a semantics-aware gesture synthesis module, which derives expressive reference motions from speech input by leveraging a generative retrieval mechanism based on large language models (LLMs) and an autoregressive Motion-GPT model. This is coupled with a high-fidelity imitation learning control policy, the MotionTracker, which enables the Unitree G1 humanoid robot to execute these complex motions dynamically and maintain balance. To ensure feasibility, we employ a robust General Motion Retargeting (GMR) method to bridge the embodiment gap between human motion data and the robot platform. Through comprehensive evaluation, we demonstrate that our combined system produces semantically appropriate and rhythmically coherent gestures that are accurately tracked and executed by the physical robot. To our knowledge, this work represents a significant step toward general real-world use by providing a complete pipeline for automatic, semantic-aware, co-speech gesture generation and synchronized real-time physical deployment on a humanoid robot.
<div id='section'>Paperid: <span id='pid'>1471, <a href='https://arxiv.org/pdf/2512.05008.pdf' target='_blank'>https://arxiv.org/pdf/2512.05008.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haroon Hublikar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.05008">Contact-Implicit Modeling and Simulation of a Snake Robot on Compliant and Granular Terrain</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This thesis presents a unified modeling and simulation framework for analyzing sidewinding and tumbling locomotion of the COBRA snake robot across rigid, compliant, and granular terrains. A contact-implicit formulation is used to model distributed frictional interactions during sidewinding, and validated through MATLAB Simscape simulations and physical experiments on rigid ground and loose sand. To capture terrain deformation effects, Project Chrono's Soil Contact Model (SCM) is integrated with the articulated multibody dynamics, enabling prediction of slip, sinkage, and load redistribution that reduce stride efficiency on deformable substrates. For high-energy rolling locomotion on steep slopes, the Chrono DEM Engine is used to simulate particle-resolved granular interactions, revealing soil failure, intermittent lift-off, and energy dissipation mechanisms not captured by rigid models. Together, these methods span real-time control-oriented simulation and high-fidelity granular physics. Results demonstrate that rigid-ground models provide accurate short-horizon motion prediction, while continuum and particle-based terrain modeling becomes necessary for reliable mobility analysis in soft and highly dynamic environments. This work establishes a hierarchical simulation pipeline that advances robust, terrain-aware locomotion for robots operating in challenging unstructured settings.
<div id='section'>Paperid: <span id='pid'>1472, <a href='https://arxiv.org/pdf/2511.21641.pdf' target='_blank'>https://arxiv.org/pdf/2511.21641.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Michael Ruderman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.21641">Model-free practical PI-Lead control design by ultimate sensitivity principle</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Practical design and tuning of feedback controllers has to do often without any model of the given dynamic process. Only some general assumptions about the process, in this work type-one stable behavior, can be available for engineers, in particular in motion control systems. This paper proposes a practical and simple in realization procedure for designing a robust PI-Lead control without modeling. The developed method derives from the ultimate sensitivity principles, known in the empirical Ziegler-Nichols tuning of PID control, and makes use of some general characteristics of loop shaping. A three-steps procedure is proposed to determine the integration time constant, control gain, and Lead-element in a way to guarantee a sufficient phase margin, while all steps are served by only experimental observations of the output value. The proposed method is also evaluated with experiments on a noise-perturbed electro-mechanical actuator system with translational motion.
<div id='section'>Paperid: <span id='pid'>1473, <a href='https://arxiv.org/pdf/2511.11644.pdf' target='_blank'>https://arxiv.org/pdf/2511.11644.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiantang Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.11644">Slow - Motion Video Synthesis for Basketball Using Frame Interpolation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Basketball broadcast footage is traditionally captured at 30-60 fps, limiting viewers' ability to appreciate rapid plays such as dunks and crossovers. We present a real-time slow-motion synthesis system that produces high-quality basketball-specific interpolated frames by fine-tuning the recent Real-Time Intermediate Flow Estimation (RIFE) network on the SportsSloMo dataset. Our pipeline isolates the basketball subset of SportsSloMo, extracts training triplets, and fine-tunes RIFE with human-aware random cropping. We compare the resulting model against Super SloMo and the baseline RIFE model using Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity (SSIM) on held-out clips. The fine-tuned RIFE attains a mean PSNR of 34.3 dB and SSIM of 0.949, outperforming Super SloMo by 2.1 dB and the baseline RIFE by 1.3 dB. A lightweight Gradio interface demonstrates end-to-end 4x slow-motion generation on a single RTX 4070 Ti Super at approximately 30 fps. These results indicate that task-specific adaptation is crucial for sports slow-motion, and that RIFE provides an attractive accuracy-speed trade-off for consumer applications.
<div id='section'>Paperid: <span id='pid'>1474, <a href='https://arxiv.org/pdf/2511.01194.pdf' target='_blank'>https://arxiv.org/pdf/2511.01194.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Minmin Zeng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.01194">A Topology-Aware Graph Convolutional Network for Human Pose Similarity and Action Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Action Quality Assessment (AQA) requires fine-grained understanding of human motion and precise evaluation of pose similarity. This paper proposes a topology-aware Graph Convolutional Network (GCN) framework, termed GCN-PSN, which models the human skeleton as a graph to learn discriminative, topology-sensitive pose embeddings. Using a Siamese architecture trained with a contrastive regression objective, our method outperforms coordinate-based baselines and achieves competitive performance on AQA-7 and FineDiving benchmarks. Experimental results and ablation studies validate the effectiveness of leveraging skeletal topology for pose similarity and action quality assessment.
<div id='section'>Paperid: <span id='pid'>1475, <a href='https://arxiv.org/pdf/2507.16850.pdf' target='_blank'>https://arxiv.org/pdf/2507.16850.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohamed Adjel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.16850">Toward a Real-Time Framework for Accurate Monocular 3D Human Pose Estimation with Geometric Priors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Monocular 3D human pose estimation remains a challenging and ill-posed problem, particularly in real-time settings and unconstrained environments. While direct imageto-3D approaches require large annotated datasets and heavy models, 2D-to-3D lifting offers a more lightweight and flexible alternative-especially when enhanced with prior knowledge. In this work, we propose a framework that combines real-time 2D keypoint detection with geometry-aware 2D-to-3D lifting, explicitly leveraging known camera intrinsics and subject-specific anatomical priors. Our approach builds on recent advances in self-calibration and biomechanically-constrained inverse kinematics to generate large-scale, plausible 2D-3D training pairs from MoCap and synthetic datasets. We discuss how these ingredients can enable fast, personalized, and accurate 3D pose estimation from monocular images without requiring specialized hardware. This proposal aims to foster discussion on bridging data-driven learning and model-based priors to improve accuracy, interpretability, and deployability of 3D human motion capture on edge devices in the wild.
<div id='section'>Paperid: <span id='pid'>1476, <a href='https://arxiv.org/pdf/2506.14287.pdf' target='_blank'>https://arxiv.org/pdf/2506.14287.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanwei Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.14287">Steering Robots with Inference-Time Interactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Imitation learning has driven the development of generalist policies capable of autonomously solving multiple tasks. However, when a pretrained policy makes errors during deployment, there are limited mechanisms for users to correct its behavior. While collecting additional data for finetuning can address such issues, doing so for each downstream use case is inefficient at deployment. My research proposes an alternative: keeping pretrained policies frozen as a fixed skill repertoire while allowing user interactions to guide behavior generation toward user preferences at inference time. By making pretrained policies steerable, users can help correct policy errors when the model struggles to generalize-without needing to finetune the policy. Specifically, I propose (1) inference-time steering, which leverages user interactions to switch between discrete skills, and (2) task and motion imitation, which enables user interactions to edit continuous motions while satisfying task constraints defined by discrete symbolic plans. These frameworks correct misaligned policy predictions without requiring additional training, maximizing the utility of pretrained models while achieving inference-time user objectives.
<div id='section'>Paperid: <span id='pid'>1477, <a href='https://arxiv.org/pdf/2505.16535.pdf' target='_blank'>https://arxiv.org/pdf/2505.16535.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Asrar Alruwayqi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.16535">SHaDe: Compact and Consistent Dynamic 3D Reconstruction via Tri-Plane Deformation and Latent Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a novel framework for dynamic 3D scene reconstruction that integrates three key components: an explicit tri-plane deformation field, a view-conditioned canonical radiance field with spherical harmonics (SH) attention, and a temporally-aware latent diffusion prior. Our method encodes 4D scenes using three orthogonal 2D feature planes that evolve over time, enabling efficient and compact spatiotemporal representation. These features are explicitly warped into a canonical space via a deformation offset field, eliminating the need for MLP-based motion modeling.
  In canonical space, we replace traditional MLP decoders with a structured SH-based rendering head that synthesizes view-dependent color via attention over learned frequency bands improving both interpretability and rendering efficiency. To further enhance fidelity and temporal consistency, we introduce a transformer-guided latent diffusion module that refines the tri-plane and deformation features in a compressed latent space. This generative module denoises scene representations under ambiguous or out-of-distribution (OOD) motion, improving generalization.
  Our model is trained in two stages: the diffusion module is first pre-trained independently, and then fine-tuned jointly with the full pipeline using a combination of image reconstruction, diffusion denoising, and temporal consistency losses. We demonstrate state-of-the-art results on synthetic benchmarks, surpassing recent methods such as HexPlane and 4D Gaussian Splatting in visual quality, temporal coherence, and robustness to sparse-view dynamic inputs.
<div id='section'>Paperid: <span id='pid'>1478, <a href='https://arxiv.org/pdf/2505.12631.pdf' target='_blank'>https://arxiv.org/pdf/2505.12631.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Li Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.12631">Multi-Resolution Haar Network: Enhancing human motion prediction via Haar transform</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The 3D human pose is vital for modern computer vision and computer graphics, and its prediction has drawn attention in recent years. 3D human pose prediction aims at forecasting a human's future motion from the previous sequence. Ignoring that the arbitrariness of human motion sequences has a firm origin in transition in both temporal and spatial axes limits the performance of state-of-the-art methods, leading them to struggle with making precise predictions on complex cases, e.g., arbitrarily posing or greeting. To alleviate this problem, a network called HaarMoDic is proposed in this paper, which utilizes the 2D Haar transform to project joints to higher resolution coordinates where the network can access spatial and temporal information simultaneously. An ablation study proves that the significant contributing module within the HaarModic Network is the Multi-Resolution Haar (MR-Haar) block. Instead of mining in one of two axes or extracting separately, the MR-Haar block projects whole motion sequences to a mixed-up coordinate in higher resolution with 2D Haar Transform, allowing the network to give scope to information from both axes in different resolutions. With the MR-Haar block, the HaarMoDic network can make predictions referring to a broader range of information. Experimental results demonstrate that HaarMoDic surpasses state-of-the-art methods in every testing interval on the Human3.6M dataset in the Mean Per Joint Position Error (MPJPE) metric.
<div id='section'>Paperid: <span id='pid'>1479, <a href='https://arxiv.org/pdf/2504.21316.pdf' target='_blank'>https://arxiv.org/pdf/2504.21316.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Michael Ruderman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.21316">Reduced order asymptotic observer of friction in motion control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>An asymptotic observer of the motion state variables with nonlinear friction [1] benefits from a robust to the noise state-space representation of the dynamic friction force, including pre-sliding transitions, and implements the reduced order Luenberger observation law with only measurable output displacement. The uniform asymptotic stability and convergence analysis of the proposed observer are elaborated by using the Lyapunov function-based stability criteria by Ignatyev and imposing the parametric constraints on the time dependent eigenvalues to be always negative real. A design procedure for assigning a dominant (thus slowest) real pole of the observer system matrix is proposed. A thorough experimental evaluation is given for the proposed observer-based friction compensation, which is performed for positioning and tracking tasks and compared with an optimally tuned PID feedback control.
<div id='section'>Paperid: <span id='pid'>1480, <a href='https://arxiv.org/pdf/2503.18348.pdf' target='_blank'>https://arxiv.org/pdf/2503.18348.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ji-Hong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.18348">MCE-based Direct FTC Method for Dynamic Positioning of Underwater Vehicles with Thruster Redundancy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents an active model-based FTC (fault-tolerant control) method for the dynamic positioning of a class of underwater vehicles with thruster redundancy. Compared to the widely used state and parameter estimation methods, this proposed scheme directly utilizes the vehicle's motion control error (MCE) to construct a residual for detecting thruster faults and failures in the steady state of the control system. In the case of thruster fault identification, the most difficult aspect is that the actual control input with thruster faults is unknown. However, through a detailed and precise analyses of MCE variation trends in the case of thruster faults, highly useful information about this unknown control input can be extracted. This characteristic also serves as the foundation for the novel scheme proposed in this paper. As for control reconfiguration, it is straightforward since the thrust losses can be directly estimated as a result of the identification process. Numerical studies with the real world vehicle model are also carried out to demonstrate the effectiveness of the proposed method.
<div id='section'>Paperid: <span id='pid'>1481, <a href='https://arxiv.org/pdf/2503.04879.pdf' target='_blank'>https://arxiv.org/pdf/2503.04879.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sammy Christen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.04879">Modeling Dynamic Hand-Object Interactions with Applications to Human-Robot Handovers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humans frequently grasp, manipulate, and move objects. Interactive systems assist humans in these tasks, enabling applications in Embodied AI, human-robot interaction, and virtual reality. However, current methods in hand-object synthesis often neglect dynamics and focus on generating static grasps. The first part of this dissertation introduces dynamic grasp synthesis, where a hand grasps and moves an object to a target pose. We approach this task using physical simulation and reinforcement learning. We then extend this to bimanual manipulation and articulated objects, requiring fine-grained coordination between hands. In the second part of this dissertation, we study human-to-robot handovers. We integrate captured human motion into simulation and introduce a student-teacher framework that adapts to human behavior and transfers from sim to real. To overcome data scarcity, we generate synthetic interactions, increasing training diversity by 100x. Our user study finds no difference between policies trained on synthetic vs. real motions.
<div id='section'>Paperid: <span id='pid'>1482, <a href='https://arxiv.org/pdf/2503.02577.pdf' target='_blank'>https://arxiv.org/pdf/2503.02577.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Boseong Jeon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.02577">SPG: Improving Motion Diffusion by Smooth Perturbation Guidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a test-time guidance method to improve the output quality of the human motion diffusion models without requiring additional training. To have negative guidance, Smooth Perturbation Guidance (SPG) builds a weak model by temporally smoothing the motion in the denoising steps. Compared to model-agnostic methods originating from the image generation field, SPG effectively mitigates out-of-distribution issues when perturbing motion diffusion models. In SPG guidance, the nature of motion structure remains intact. This work conducts a comprehensive analysis across distinct model architectures and tasks. Despite its extremely simple implementation and no need for additional training requirements, SPG consistently enhances motion fidelity. Project page can be found at https://spg-blind.vercel.app/
<div id='section'>Paperid: <span id='pid'>1483, <a href='https://arxiv.org/pdf/2502.19364.pdf' target='_blank'>https://arxiv.org/pdf/2502.19364.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ali Ismail-Fawaz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.19364">Deep Learning For Time Series Analysis With Application On Human Motion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Time series data, defined by equally spaced points over time, is essential in fields like medicine, telecommunications, and energy. Analyzing it involves tasks such as classification, clustering, prototyping, and regression. Classification identifies normal vs. abnormal movements in skeleton-based motion sequences, clustering detects stock market behavior patterns, prototyping expands physical therapy datasets, and regression predicts patient recovery. Deep learning has recently gained traction in time series analysis due to its success in other domains. This thesis leverages deep learning to enhance classification with feature engineering, introduce foundation models, and develop a compact yet state-of-the-art architecture. We also address limited labeled data with self-supervised learning. Our contributions apply to real-world tasks, including human motion analysis for action recognition and rehabilitation. We introduce a generative model for human motion data, valuable for cinematic production and gaming. For prototyping, we propose a shape-based synthetic sample generation method to support regression models when data is scarce. Lastly, we critically evaluate discriminative and generative models, identifying limitations in current methodologies and advocating for a robust, standardized evaluation framework. Our experiments on public datasets provide novel insights and methodologies, advancing time series analysis with practical applications.
<div id='section'>Paperid: <span id='pid'>1484, <a href='https://arxiv.org/pdf/2502.00685.pdf' target='_blank'>https://arxiv.org/pdf/2502.00685.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Emre Sariyildiz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.00685">IEEEICM25: "A High-Performance Disturbance Observer"</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper proposes a novel Disturbance Observer, termed the High-Performance Disturbance Observer, which achieves more accurate disturbance estimation compared to the conventional disturbance observer, thereby delivering significant improvements in robustness and performance for motion control systems.
<div id='section'>Paperid: <span id='pid'>1485, <a href='https://arxiv.org/pdf/2502.00683.pdf' target='_blank'>https://arxiv.org/pdf/2502.00683.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Emre Sariyildiz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.00683">IEEEICM25: "Stability of Digital Robust Motion Control Systems with Disturbance Observer"</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, new stability analysis methods are proposed for digital robust motion control systems implemented using a disturbance observer.
<div id='section'>Paperid: <span id='pid'>1486, <a href='https://arxiv.org/pdf/2501.18726.pdf' target='_blank'>https://arxiv.org/pdf/2501.18726.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Canxuan Gang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.18726">Strong and Controllable 3D Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion generation is a significant pursuit in generative computer vision with widespread applications in film-making, video games, AR/VR, and human-robot interaction. Current methods mainly utilize either diffusion-based generative models or autoregressive models for text-to-motion generation. However, they face two significant challenges: (1) The generation process is time-consuming, posing a major obstacle for real-time applications such as gaming, robot manipulation, and other online settings. (2) These methods typically learn a relative motion representation guided by text, making it difficult to generate motion sequences with precise joint-level control. These challenges significantly hinder progress and limit the real-world application of human motion generation techniques. To address this gap, we propose a simple yet effective architecture consisting of two key components. Firstly, we aim to improve hardware efficiency and computational complexity in transformer-based diffusion models for human motion generation. By customizing flash linear attention, we can optimize these models specifically for generating human motion efficiently. Furthermore, we will customize the consistency model in the motion latent space to further accelerate motion generation. Secondly, we introduce Motion ControlNet, which enables more precise joint-level control of human motion compared to previous text-to-motion generation methods. These contributions represent a significant advancement for text-to-motion generation, bringing it closer to real-world applications.
<div id='section'>Paperid: <span id='pid'>1487, <a href='https://arxiv.org/pdf/2501.07026.pdf' target='_blank'>https://arxiv.org/pdf/2501.07026.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Emre Sariyildiz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.07026">IEEE_TIE25: Analysis and Synthesis of DOb-based Robust Motion Controllers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>By employing a unified state-space design framework, this paper proposes a novel systematic analysis and synthesis method that facilitates the implementation of both conventional zero-order (ZO) and high-order (HO) DObs. Furthermore, this design method supports the development of advanced DObs (e.g., the proposed High-Performance (HP) DOb in this paper), enabling more accurate disturbance estimation and, consequently, enhancing the robust stability and performance of motion control systems. Lyapunov direct method is employed in the discrete-time domain to analyse the stability of the proposed digital robust motion controllers. The analysis demonstrates that the proposed DObs are stable in the sense that the estimation error is uniformly ultimately bounded when subjected to bounded disturbances. Additionally, they are proven to be asymptotically stable under specific disturbance conditions, such as constant disturbances for the ZO and HP DObs. Stability constraints on the design parameters of the DObs are analytically derived, providing effective synthesis tools for the implementation of the digital robust motion controllers. The discrete-time analysis facilitates the derivation of more practical design constraints. The proposed analysis and synthesis methods have been rigorously validated through experimental evaluations, confirming their effectiveness.
<div id='section'>Paperid: <span id='pid'>1488, <a href='https://arxiv.org/pdf/2412.11632.pdf' target='_blank'>https://arxiv.org/pdf/2412.11632.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Juncheng Zou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.11632">Multi-Scale Incremental Modeling for Enhanced Human Motion Prediction in Human-Robot Collaboration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate human motion prediction is crucial for safe human-robot collaboration but remains challenging due to the complexity of modeling intricate and variable human movements. This paper presents Parallel Multi-scale Incremental Prediction (PMS), a novel framework that explicitly models incremental motion across multiple spatio-temporal scales to capture subtle joint evolutions and global trajectory shifts. PMS encodes these multi-scale increments using parallel sequence branches, enabling iterative refinement of predictions. A multi-stage training procedure with a full-timeline loss integrates temporal context. Extensive experiments on four datasets demonstrate substantial improvements in continuity, biomechanical consistency, and long-term forecast stability by modeling inter-frame increments. PMS achieves state-of-the-art performance, increasing prediction accuracy by 16.3%-64.2% over previous methods. The proposed multi-scale incremental approach provides a powerful technique for advancing human motion prediction capabilities critical for seamless human-robot interaction.
<div id='section'>Paperid: <span id='pid'>1489, <a href='https://arxiv.org/pdf/2412.07797.pdf' target='_blank'>https://arxiv.org/pdf/2412.07797.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongjie Fu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.07797">Mogo: RQ Hierarchical Causal Transformer for High-Quality 3D Human Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the field of text-to-motion generation, Bert-type Masked Models (MoMask, MMM) currently produce higher-quality outputs compared to GPT-type autoregressive models (T2M-GPT). However, these Bert-type models often lack the streaming output capability required for applications in video game and multimedia environments, a feature inherent to GPT-type models. Additionally, they demonstrate weaker performance in out-of-distribution generation. To surpass the quality of BERT-type models while leveraging a GPT-type structure, without adding extra refinement models that complicate scaling data, we propose a novel architecture, Mogo (Motion Only Generate Once), which generates high-quality lifelike 3D human motions by training a single transformer model. Mogo consists of only two main components: 1) RVQ-VAE, a hierarchical residual vector quantization variational autoencoder, which discretizes continuous motion sequences with high precision; 2) Hierarchical Causal Transformer, responsible for generating the base motion sequences in an autoregressive manner while simultaneously inferring residuals across different layers. Experimental results demonstrate that Mogo can generate continuous and cyclic motion sequences up to 260 frames (13 seconds), surpassing the 196 frames (10 seconds) length limitation of existing datasets like HumanML3D. On the HumanML3D test set, Mogo achieves a FID score of 0.079, outperforming both the GPT-type model T2M-GPT (FID = 0.116), AttT2M (FID = 0.112) and the BERT-type model MMM (FID = 0.080). Furthermore, our model achieves the best quantitative performance in out-of-distribution generation.
<div id='section'>Paperid: <span id='pid'>1490, <a href='https://arxiv.org/pdf/2411.19495.pdf' target='_blank'>https://arxiv.org/pdf/2411.19495.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Michael Ruderman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.19495">Loop Shaping of Hybrid Motion Control with Contact Transition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A standard motion control with feedback of the output displacement cannot handle unforeseen contact with environment without penetrating into the soft, i.e. viscoelastic, materials or even damaging the fragile materials. Robotics and mechatronics with tactile and haptic capabilities, and in particular medical robotics for example, place special demands on the advanced motion control systems that should enable the safe and harmless contact transitions. This paper shows how the basic principles of loop shaping can be easily used to handle sufficiently stiff motion control in such a way that it is extended by sensor-free dynamic reconfiguration upon contact with the environment. A thereupon based hybrid control scheme is proposed. A remarkable feature of the developed approach is that no measurement of the contact force is required and the input signal and the measured output displacement are the only quantities used for design and operation. Experiments on 1-DOF actuator are shown, where the moving tool comes into contact with grapes that are soft and simultaneously penetrable.
<div id='section'>Paperid: <span id='pid'>1491, <a href='https://arxiv.org/pdf/2410.07200.pdf' target='_blank'>https://arxiv.org/pdf/2410.07200.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>SK Hasan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.07200">A Realistic Model Reference Computed Torque Control Strategy for Human Lower Limb Exoskeletons</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Exoskeleton robots have become a promising tool in neurorehabilitation, offering effective physical therapy and recovery monitoring. The success of these therapies relies on precise motion control systems. Although computed torque control based on inverse dynamics provides a robust theoretical foundation, its practical application in rehabilitation is limited by its sensitivity to model accuracy, making it less effective when dealing with unpredictable payloads. To overcome these limitations, this study introduces a novel model reference computed torque controller that accounts for parametric uncertainties while optimizing computational efficiency. A dynamic model of a seven-degree-of-freedom human lower limb exoskeleton is developed, incorporating a realistic joint friction model to accurately reflect the physical behavior of the robot. To reduce computational demands, the control system is split into two loops: a slower loop that predicts joint torque requirements based on input trajectories and robot dynamics, and a faster PID loop that corrects trajectory tracking errors. Coriolis and centrifugal forces are excluded from the model due to their minimal impact on system dynamics relative to their computational cost. Experimental results show high accuracy in trajectory tracking, and statistical analyses confirm the controller's robustness and effectiveness in handling parametric uncertainties. This approach presents a promising advancement for improving the stability and performance of exoskeleton-based neurorehabilitation.
<div id='section'>Paperid: <span id='pid'>1492, <a href='https://arxiv.org/pdf/2409.19686.pdf' target='_blank'>https://arxiv.org/pdf/2409.19686.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xingyu Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.19686">Text-driven Human Motion Generation with Motion Masked Diffusion Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-driven human motion generation is a multimodal task that synthesizes human motion sequences conditioned on natural language. It requires the model to satisfy textual descriptions under varying conditional inputs, while generating plausible and realistic human actions with high diversity. Existing diffusion model-based approaches have outstanding performance in the diversity and multimodality of generation. However, compared to autoregressive methods that train motion encoders before inference, diffusion methods lack in fitting the distribution of human motion features which leads to an unsatisfactory FID score. One insight is that the diffusion model lack the ability to learn the motion relations among spatio-temporal semantics through contextual reasoning. To solve this issue, in this paper, we proposed Motion Masked Diffusion Model \textbf{(MMDM)}, a novel human motion masked mechanism for diffusion model to explicitly enhance its ability to learn the spatio-temporal relationships from contextual joints among motion sequences. Besides, considering the complexity of human motion data with dynamic temporal characteristics and spatial structure, we designed two mask modeling strategies: \textbf{time frames mask} and \textbf{body parts mask}. During training, MMDM masks certain tokens in the motion embedding space. Then, the diffusion decoder is designed to learn the whole motion sequence from masked embedding in each sampling step, this allows the model to recover a complete sequence from incomplete representations. Experiments on HumanML3D and KIT-ML dataset demonstrate that our mask strategy is effective by balancing motion quality and text-motion consistency.
<div id='section'>Paperid: <span id='pid'>1493, <a href='https://arxiv.org/pdf/2409.05295.pdf' target='_blank'>https://arxiv.org/pdf/2409.05295.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Farhad Aghili
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.05295">Adaptive Visual Servoing for On-Orbit Servicing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents an adaptive visual servoing framework for robotic on-orbit servicing (OOS), specifically designed for capturing tumbling satellites. The vision-guided robotic system is capable of selecting optimal control actions in the event of partial or complete vision system failure, particularly in the short term. The autonomous system accounts for physical and operational constraints, executing visual servoing tasks to minimize a cost function. A hierarchical control architecture is developed, integrating a variant of the Iterative Closest Point (ICP) algorithm for image registration, a constrained noise-adaptive Kalman filter, fault detection and recovery logic, and a constrained optimal path planner. The dynamic estimator provides real-time estimates of unknown states and uncertain parameters essential for motion prediction, while ensuring consistency through a set of inequality constraints. It also adjusts the Kalman filter parameters adaptively in response to unexpected vision errors. In the event of vision system faults, a recovery strategy is activated, guided by fault detection logic that monitors the visual feedback via the metric fit error of image registration. The estimated/predicted pose and parameters are subsequently fed into an optimal path planner, which directs the robot's end-effector to the target's grasping point. This process is subject to multiple constraints, including acceleration limits, smooth capture, and line-of-sight maintenance with the target. Experimental results demonstrate that the proposed visual servoing system successfully captured a free-floating object, despite complete occlusion of the vision system.
<div id='section'>Paperid: <span id='pid'>1494, <a href='https://arxiv.org/pdf/2409.02636.pdf' target='_blank'>https://arxiv.org/pdf/2409.02636.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Toshiaki Tsuji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.02636">Mamba as a motion encoder for robotic imitation learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in imitation learning, particularly with the integration of LLM techniques, are set to significantly improve robots' dexterity and adaptability. This paper proposes using Mamba, a state-of-the-art architecture with potential applications in LLMs, for robotic imitation learning, highlighting its ability to function as an encoder that effectively captures contextual information. By reducing the dimensionality of the state space, Mamba operates similarly to an autoencoder. It effectively compresses the sequential information into state variables while preserving the essential temporal dynamics necessary for accurate motion prediction. Experimental results in tasks such as cup placing and case loading demonstrate that despite exhibiting higher estimation errors, Mamba achieves superior success rates compared to Transformers in practical task execution. This performance is attributed to Mamba's structure, which encompasses the state space model. Additionally, the study investigates Mamba's capacity to serve as a real-time motion generator with a limited amount of training data.
<div id='section'>Paperid: <span id='pid'>1495, <a href='https://arxiv.org/pdf/2407.13330.pdf' target='_blank'>https://arxiv.org/pdf/2407.13330.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Arunabh Bora
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.13330">Exploring Robot Trajectory Planning -- A Comparative Analysis of Algorithms And Software Implementations in Dynamic Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Trajectory Planning is a crucial word in Modern & Advanced Robotics. It's a way of generating a smooth and feasible path for the robot to follow over time. The process primarily takes several factors to generate the path, such as velocity, acceleration and jerk. The process deals with how the robot can follow a desired motion path in a suitable environment. This trajectory planning is extensively used in Automobile Industrial Robot, Manipulators, and Mobile Robots. Trajectory planning is a fundamental component of motion control systems. To perform tasks like pick and place operations, assembly, welding, painting, path following, and obstacle avoidance. This paper introduces a comparative analysis of trajectory planning algorithms and their key software elements working strategy in complex and dynamic environments. Adaptability and real-time analysis are the most common problems in trajectory planning. The paper primarily focuses on getting a better understanding of these unpredictable environments.
<div id='section'>Paperid: <span id='pid'>1496, <a href='https://arxiv.org/pdf/2405.01284.pdf' target='_blank'>https://arxiv.org/pdf/2405.01284.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Liu Qiyuan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.01284">Behavior Imitation for Manipulator Control and Grasping with Deep Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The existing Motion Imitation models typically require expert data obtained through MoCap devices, but the vast amount of training data needed is difficult to acquire, necessitating substantial investments of financial resources, manpower, and time. This project combines 3D human pose estimation with reinforcement learning, proposing a novel model that simplifies Motion Imitation into a prediction problem of joint angle values in reinforcement learning. This significantly reduces the reliance on vast amounts of training data, enabling the agent to learn an imitation policy from just a few seconds of video and exhibit strong generalization capabilities. It can quickly apply the learned policy to imitate human arm motions in unfamiliar videos. The model first extracts skeletal motions of human arms from a given video using 3D human pose estimation. These extracted arm motions are then morphologically retargeted onto a robotic manipulator. Subsequently, the retargeted motions are used to generate reference motions. Finally, these reference motions are used to formulate a reinforcement learning problem, enabling the agent to learn a policy for imitating human arm motions. This project excels at imitation tasks and demonstrates robust transferability, accurately imitating human arm motions from other unfamiliar videos. This project provides a lightweight, convenient, efficient, and accurate Motion Imitation model. While simplifying the complex process of Motion Imitation, it achieves notably outstanding performance.
<div id='section'>Paperid: <span id='pid'>1497, <a href='https://arxiv.org/pdf/2401.01644.pdf' target='_blank'>https://arxiv.org/pdf/2401.01644.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hanxiao Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.01644">Motion Control of Interactive Robotic Arms Based on Mixed Reality Development</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Mixed Reality (MR) is constantly evolving to inspire new patterns of robot manipulation for more advanced Human- Robot Interaction under the 4th Industrial Revolution Paradigm. Consider that Mixed Reality aims to connect physical and digital worlds to provide special immersive experiences, it is necessary to establish the information exchange platform and robot control systems within the developed MR scenarios. In this work, we mainly present multiple effective motion control methods applied on different interactive robotic arms (e.g., UR5, UR5e, myCobot) for the Unity-based development of MR applications, including GUI control panel, text input control panel, end-effector object dynamic tracking and ROS-Unity digital-twin connection.
