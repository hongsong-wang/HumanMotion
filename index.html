<!DOCTYPE html>
<html>
<head>
<title>Paper collected by Wang</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<style type="text/css">


/* BODY
=============================================================================*/

body {
  font-family: Helvetica, arial, freesans, clean, sans-serif;
  font-size: 14px;
  line-height: 1.6;
  color: #333;
  background-color: #fff;
  padding: 20px;
  max-width: 960px;
  margin: 0 auto;
}

span#pid {
  color:red;
  
}
span#filename{
  font-style: oblique;
}

span#title{
  font-family: Times New Roman, freesans, clean, sans-serif;
  font-style: italic;
  font-size: 20px;
  border:1px solid #B50;
}
span#abs{
  font-family: Times New Roman, freesans, clean, sans-serif;
  font-style: oblique;
  font-size: 18px;
}
</style>
</head>
<body>

</p></br></br><div id='section'>Paperid: <span id='pid'>1, <a href='https://arxiv.org/pdf/2510.06219.pdf' target='_blank'>https://arxiv.org/pdf/2510.06219.pdf</a></span>   <span><a href='https://github.com/fanegg/Human3R' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yue Chen, Xingyu Chen, Yuxuan Xue, Anpei Chen, Yuliang Xiu, Gerard Pons-Moll
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.06219">Human3R: Everyone Everywhere All at Once</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present Human3R, a unified, feed-forward framework for online 4D human-scene reconstruction, in the world frame, from casually captured monocular videos. Unlike previous approaches that rely on multi-stage pipelines, iterative contact-aware refinement between humans and scenes, and heavy dependencies, e.g., human detection, depth estimation, and SLAM pre-processing, Human3R jointly recovers global multi-person SMPL-X bodies ("everyone"), dense 3D scene ("everywhere"), and camera trajectories in a single forward pass ("all-at-once"). Our method builds upon the 4D online reconstruction model CUT3R, and uses parameter-efficient visual prompt tuning, to strive to preserve CUT3R's rich spatiotemporal priors, while enabling direct readout of multiple SMPL-X bodies. Human3R is a unified model that eliminates heavy dependencies and iterative refinement. After being trained on the relatively small-scale synthetic dataset BEDLAM for just one day on one GPU, it achieves superior performance with remarkable efficiency: it reconstructs multiple humans in a one-shot manner, along with 3D scenes, in one stage, at real-time speed (15 FPS) with a low memory footprint (8 GB). Extensive experiments demonstrate that Human3R delivers state-of-the-art or competitive performance across tasks, including global human motion estimation, local human mesh recovery, video depth estimation, and camera pose estimation, with a single unified model. We hope that Human3R will serve as a simple yet strong baseline, be easily extended for downstream applications.Code available in https://fanegg.github.io/Human3R
<div id='section'>Paperid: <span id='pid'>2, <a href='https://arxiv.org/pdf/2510.03031.pdf' target='_blank'>https://arxiv.org/pdf/2510.03031.pdf</a></span>   <span><a href='https://github.com/test-bai-cpu/LHMP-with-MoDs.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yufei Zhu, Andrey Rudenko, Tomasz P. Kucner, Achim J. Lilienthal, Martin Magnusson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03031">Long-Term Human Motion Prediction Using Spatio-Temporal Maps of Dynamics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Long-term human motion prediction (LHMP) is important for the safe and efficient operation of autonomous robots and vehicles in environments shared with humans. Accurate predictions are important for applications including motion planning, tracking, human-robot interaction, and safety monitoring. In this paper, we exploit Maps of Dynamics (MoDs), which encode spatial or spatio-temporal motion patterns as environment features, to achieve LHMP for horizons of up to 60 seconds. We propose an MoD-informed LHMP framework that supports various types of MoDs and includes a ranking method to output the most likely predicted trajectory, improving practical utility in robotics. Further, a time-conditioned MoD is introduced to capture motion patterns that vary across different times of day. We evaluate MoD-LHMP instantiated with three types of MoDs. Experiments on two real-world datasets show that MoD-informed method outperforms learning-based ones, with up to 50\% improvement in average displacement error, and the time-conditioned variant achieves the highest accuracy overall. Project code is available at https://github.com/test-bai-cpu/LHMP-with-MoDs.git
<div id='section'>Paperid: <span id='pid'>3, <a href='https://arxiv.org/pdf/2510.02722.pdf' target='_blank'>https://arxiv.org/pdf/2510.02722.pdf</a></span>   <span><a href='https://github.com/JunyuShi02/MoGIC' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Junyu Shi, Yong Sun, Zhiyuan Zhang, Lijiang Liu, Zhengjie Zhang, Yuxin He, Qiang Nie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.02722">MoGIC: Boosting Motion Generation via Intention Understanding and Visual Context</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing text-driven motion generation methods often treat synthesis as a bidirectional mapping between language and motion, but remain limited in capturing the causal logic of action execution and the human intentions that drive behavior. The absence of visual grounding further restricts precision and personalization, as language alone cannot specify fine-grained spatiotemporal details. We propose MoGIC, a unified framework that integrates intention modeling and visual priors into multimodal motion synthesis. By jointly optimizing multimodal-conditioned motion generation and intention prediction, MoGIC uncovers latent human goals, leverages visual priors to enhance generation, and exhibits versatile multimodal generative capability. We further introduce a mixture-of-attention mechanism with adaptive scope to enable effective local alignment between conditional tokens and motion subsequences. To support this paradigm, we curate Mo440H, a 440-hour benchmark from 21 high-quality motion datasets. Experiments show that after finetuning, MoGIC reduces FID by 38.6\% on HumanML3D and 34.6\% on Mo440H, surpasses LLM-based methods in motion captioning with a lightweight text head, and further enables intention prediction and vision-conditioned generation, advancing controllable motion synthesis and intention understanding. The code is available at https://github.com/JunyuShi02/MoGIC
<div id='section'>Paperid: <span id='pid'>4, <a href='https://arxiv.org/pdf/2510.02252.pdf' target='_blank'>https://arxiv.org/pdf/2510.02252.pdf</a></span>   <span><a href='https://github.com/YanjieZe/GMR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Joao Pedro Araujo, Yanjie Ze, Pei Xu, Jiajun Wu, C. Karen Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.02252">Retargeting Matters: General Motion Retargeting for Humanoid Motion Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humanoid motion tracking policies are central to building teleoperation pipelines and hierarchical controllers, yet they face a fundamental challenge: the embodiment gap between humans and humanoid robots. Current approaches address this gap by retargeting human motion data to humanoid embodiments and then training reinforcement learning (RL) policies to imitate these reference trajectories. However, artifacts introduced during retargeting, such as foot sliding, self-penetration, and physically infeasible motion are often left in the reference trajectories for the RL policy to correct. While prior work has demonstrated motion tracking abilities, they often require extensive reward engineering and domain randomization to succeed. In this paper, we systematically evaluate how retargeting quality affects policy performance when excessive reward tuning is suppressed. To address issues that we identify with existing retargeting methods, we propose a new retargeting method, General Motion Retargeting (GMR). We evaluate GMR alongside two open-source retargeters, PHC and ProtoMotions, as well as with a high-quality closed-source dataset from Unitree. Using BeyondMimic for policy training, we isolate retargeting effects without reward tuning. Our experiments on a diverse subset of the LAFAN1 dataset reveal that while most motions can be tracked, artifacts in retargeted data significantly reduce policy robustness, particularly for dynamic or long sequences. GMR consistently outperforms existing open-source methods in both tracking performance and faithfulness to the source motion, achieving perceptual fidelity and policy success rates close to the closed-source baseline. Website: https://jaraujo98.github.io/retargeting_matters. Code: https://github.com/YanjieZe/GMR.
<div id='section'>Paperid: <span id='pid'>5, <a href='https://arxiv.org/pdf/2509.19252.pdf' target='_blank'>https://arxiv.org/pdf/2509.19252.pdf</a></span>   <span><a href='https://github.com/TeCSAR-UNCC/Pose-Quantization' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Gabriel Maldonado, Narges Rashvand, Armin Danesh Pazho, Ghazal Alinezhad Noghre, Vinit Katariya, Hamed Tabkhi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.19252">Adversarially-Refined VQ-GAN with Dense Motion Tokenization for Spatio-Temporal Heatmaps</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Continuous human motion understanding remains a core challenge in computer vision due to its high dimensionality and inherent redundancy. Efficient compression and representation are crucial for analyzing complex motion dynamics. In this work, we introduce an adversarially-refined VQ-GAN framework with dense motion tokenization for compressing spatio-temporal heatmaps while preserving the fine-grained traces of human motion. Our approach combines dense motion tokenization with adversarial refinement, which eliminates reconstruction artifacts like motion smearing and temporal misalignment observed in non-adversarial baselines. Our experiments on the CMU Panoptic dataset provide conclusive evidence of our method's superiority, outperforming the dVAE baseline by 9.31% SSIM and reducing temporal instability by 37.1%. Furthermore, our dense tokenization strategy enables a novel analysis of motion complexity, revealing that 2D motion can be optimally represented with a compact 128-token vocabulary, while 3D motion's complexity demands a much larger 1024-token codebook for faithful reconstruction. These results establish practical deployment feasibility across diverse motion analysis applications. The code base for this work is available at https://github.com/TeCSAR-UNCC/Pose-Quantization.
<div id='section'>Paperid: <span id='pid'>6, <a href='https://arxiv.org/pdf/2509.17323.pdf' target='_blank'>https://arxiv.org/pdf/2509.17323.pdf</a></span>   <span><a href='https://github.com/warriordby/DepTR-MOT' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/warriordby/DepTR-MOT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Buyin Deng, Lingxin Huang, Kai Luo, Fei Teng, Kailun Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17323">DepTR-MOT: Unveiling the Potential of Depth-Informed Trajectory Refinement for Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual Multi-Object Tracking (MOT) is a crucial component of robotic perception, yet existing Tracking-By-Detection (TBD) methods often rely on 2D cues, such as bounding boxes and motion modeling, which struggle under occlusions and close-proximity interactions. Trackers relying on these 2D cues are particularly unreliable in robotic environments, where dense targets and frequent occlusions are common. While depth information has the potential to alleviate these issues, most existing MOT datasets lack depth annotations, leading to its underexploited role in the domain. To unveil the potential of depth-informed trajectory refinement, we introduce DepTR-MOT, a DETR-based detector enhanced with instance-level depth information. Specifically, we propose two key innovations: (i) foundation model-based instance-level soft depth label supervision, which refines depth prediction, and (ii) the distillation of dense depth maps to maintain global depth consistency. These strategies enable DepTR-MOT to output instance-level depth during inference, without requiring foundation models and without additional computational cost. By incorporating depth cues, our method enhances the robustness of the TBD paradigm, effectively resolving occlusion and close-proximity challenges. Experiments on both the QuadTrack and DanceTrack datasets demonstrate the effectiveness of our approach, achieving HOTA scores of 27.59 and 44.47, respectively. In particular, results on QuadTrack, a robotic platform MOT dataset, highlight the advantages of our method in handling occlusion and close-proximity challenges in robotic tracking. The source code will be made publicly available at https://github.com/warriordby/DepTR-MOT.
<div id='section'>Paperid: <span id='pid'>7, <a href='https://arxiv.org/pdf/2509.15781.pdf' target='_blank'>https://arxiv.org/pdf/2509.15781.pdf</a></span>   <span><a href='https://github.com/2025-LSVOS-3rd-place/MOSEv2_3rd_place' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chang Soo Lim, Joonyoung Moon, Donghyeon Cho
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15781">Enriched Feature Representation and Motion Prediction Module for MOSEv2 Track of 7th LSVOS Challenge: 3rd Place Solution</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video object segmentation (VOS) is a challenging task with wide applications such as video editing and autonomous driving. While Cutie provides strong query-based segmentation and SAM2 offers enriched representations via a pretrained ViT encoder, each has limitations in feature capacity and temporal modeling. In this report, we propose a framework that integrates their complementary strengths by replacing the encoder of Cutie with the ViT encoder of SAM2 and introducing a motion prediction module for temporal stability. We further adopt an ensemble strategy combining Cutie, SAM2, and our variant, achieving 3rd place in the MOSEv2 track of the 7th LSVOS Challenge. We refer to our final model as SCOPE (SAM2-CUTIE Object Prediction Ensemble). This demonstrates the effectiveness of enriched feature representation and motion prediction for robust video object segmentation. The code is available at https://github.com/2025-LSVOS-3rd-place/MOSEv2_3rd_place.
<div id='section'>Paperid: <span id='pid'>8, <a href='https://arxiv.org/pdf/2509.11090.pdf' target='_blank'>https://arxiv.org/pdf/2509.11090.pdf</a></span>   <span><a href='https://github.com/Joechencc/CAAPolicy' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chao Chen, Shunyu Yao, Yuanwu He, Tao Feng, Ruojing Song, Yuliang Guo, Xinyu Huang, Chenxu Wu, Ren Liu, Chen Feng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11090">End-to-End Visual Autonomous Parking via Control-Aided Attention</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Precise parking requires an end-to-end system where perception adaptively provides policy-relevant details-especially in critical areas where fine control decisions are essential. End-to-end learning offers a unified framework by directly mapping sensor inputs to control actions, but existing approaches lack effective synergy between perception and control. We find that transformer-based self-attention, when used alone, tends to produce unstable and temporally inconsistent spatial attention, which undermines the reliability of downstream policy decisions over time. Instead, we propose CAA-Policy, an end-to-end imitation learning system that allows control signal to guide the learning of visual attention via a novel Control-Aided Attention (CAA) mechanism. For the first time, we train such an attention module in a self-supervised manner, using backpropagated gradients from the control outputs instead of from the training loss. This strategy encourages the attention to focus on visual features that induce high variance in action outputs, rather than merely minimizing the training loss-a shift we demonstrate leads to a more robust and generalizable policy. To further enhance stability, CAA-Policy integrates short-horizon waypoint prediction as an auxiliary task, and introduces a separately trained motion prediction module to robustly track the target spot over time. Extensive experiments in the CARLA simulator show that \titlevariable~consistently surpasses both the end-to-end learning baseline and the modular BEV segmentation + hybrid A* pipeline, achieving superior accuracy, robustness, and interpretability. Code is released at https://github.com/Joechencc/CAAPolicy.
<div id='section'>Paperid: <span id='pid'>9, <a href='https://arxiv.org/pdf/2509.09555.pdf' target='_blank'>https://arxiv.org/pdf/2509.09555.pdf</a></span>   <span><a href='https://github.com/wzyabcas/InterAct,' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sirui Xu, Dongting Li, Yucheng Zhang, Xiyan Xu, Qi Long, Ziyin Wang, Yunzhi Lu, Shuchang Dong, Hezi Jiang, Akshat Gupta, Yu-Xiong Wang, Liang-Yan Gui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09555">InterAct: Advancing Large-Scale Versatile 3D Human-Object Interaction Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While large-scale human motion capture datasets have advanced human motion generation, modeling and generating dynamic 3D human-object interactions (HOIs) remain challenging due to dataset limitations. Existing datasets often lack extensive, high-quality motion and annotation and exhibit artifacts such as contact penetration, floating, and incorrect hand motions. To address these issues, we introduce InterAct, a large-scale 3D HOI benchmark featuring dataset and methodological advancements. First, we consolidate and standardize 21.81 hours of HOI data from diverse sources, enriching it with detailed textual annotations. Second, we propose a unified optimization framework to enhance data quality by reducing artifacts and correcting hand motions. Leveraging the principle of contact invariance, we maintain human-object relationships while introducing motion variations, expanding the dataset to 30.70 hours. Third, we define six benchmarking tasks and develop a unified HOI generative modeling perspective, achieving state-of-the-art performance. Extensive experiments validate the utility of our dataset as a foundational resource for advancing 3D human-object interaction generation. To support continued research in this area, the dataset is publicly available at https://github.com/wzyabcas/InterAct, and will be actively maintained.
<div id='section'>Paperid: <span id='pid'>10, <a href='https://arxiv.org/pdf/2509.03883.pdf' target='_blank'>https://arxiv.org/pdf/2509.03883.pdf</a></span>   <span><a href='https://github.com/Winn1y/Awesome-Human-Motion-Video-Generation' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/Winn1y/Awesome-Human-Motion-Video-Generation' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haiwei Xue, Xiangyang Luo, Zhanghao Hu, Xin Zhang, Xunzhi Xiang, Yuqin Dai, Jianzhuang Liu, Zhensong Zhang, Minglei Li, Jian Yang, Fei Ma, Zhiyong Wu, Changpeng Yang, Zonghong Dai, Fei Richard Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03883">Human Motion Video Generation: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion video generation has garnered significant research interest due to its broad applications, enabling innovations such as photorealistic singing heads or dynamic avatars that seamlessly dance to music. However, existing surveys in this field focus on individual methods, lacking a comprehensive overview of the entire generative process. This paper addresses this gap by providing an in-depth survey of human motion video generation, encompassing over ten sub-tasks, and detailing the five key phases of the generation process: input, motion planning, motion video generation, refinement, and output. Notably, this is the first survey that discusses the potential of large language models in enhancing human motion video generation. Our survey reviews the latest developments and technological trends in human motion video generation across three primary modalities: vision, text, and audio. By covering over two hundred papers, we offer a thorough overview of the field and highlight milestone works that have driven significant technological breakthroughs. Our goal for this survey is to unveil the prospects of human motion video generation and serve as a valuable resource for advancing the comprehensive applications of digital humans. A complete list of the models examined in this survey is available in Our Repository https://github.com/Winn1y/Awesome-Human-Motion-Video-Generation.
<div id='section'>Paperid: <span id='pid'>11, <a href='https://arxiv.org/pdf/2508.20920.pdf' target='_blank'>https://arxiv.org/pdf/2508.20920.pdf</a></span>   <span><a href='https://github.com/PARCO-LAB/COMETH' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Enrico Martini, Ho Jin Choi, Nadia Figueroa, Nicola Bombieri
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.20920">COMETH: Convex Optimization for Multiview Estimation and Tracking of Humans</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the era of Industry 5.0, monitoring human activity is essential for ensuring both ergonomic safety and overall well-being. While multi-camera centralized setups improve pose estimation accuracy, they often suffer from high computational costs and bandwidth requirements, limiting scalability and real-time applicability. Distributing processing across edge devices can reduce network bandwidth and computational load. On the other hand, the constrained resources of edge devices lead to accuracy degradation, and the distribution of computation leads to temporal and spatial inconsistencies. We address this challenge by proposing COMETH (Convex Optimization for Multiview Estimation and Tracking of Humans), a lightweight algorithm for real-time multi-view human pose fusion that relies on three concepts: it integrates kinematic and biomechanical constraints to increase the joint positioning accuracy; it employs convex optimization-based inverse kinematics for spatial fusion; and it implements a state observer to improve temporal consistency. We evaluate COMETH on both public and industrial datasets, where it outperforms state-of-the-art methods in localization, detection, and tracking accuracy. The proposed fusion pipeline enables accurate and scalable human motion tracking, making it well-suited for industrial and safety-critical applications. The code is publicly available at https://github.com/PARCO-LAB/COMETH.
<div id='section'>Paperid: <span id='pid'>12, <a href='https://arxiv.org/pdf/2508.20615.pdf' target='_blank'>https://arxiv.org/pdf/2508.20615.pdf</a></span>   <span><a href='https://github.com/GVCLab/EmoCAST' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiguo Jiang, Xiaodong Cun, Yong Zhang, Yudian Zheng, Fan Tang, Chi-Man Pun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.20615">EmoCAST: Emotional Talking Portrait via Emotive Text Description</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Emotional talking head synthesis aims to generate talking portrait videos with vivid expressions. Existing methods still exhibit limitations in control flexibility, motion naturalness, and expression quality. Moreover, currently available datasets are primarily collected in lab settings, further exacerbating these shortcomings. Consequently, these limitations substantially hinder practical applications in real-world scenarios. To address these challenges, we propose EmoCAST, a diffusion-based framework with two key modules for precise text-driven emotional synthesis. In appearance modeling, emotional prompts are integrated through a text-guided decoupled emotive module, enhancing the spatial knowledge to improve emotion comprehension. To improve the relationship between audio and emotion, we introduce an emotive audio attention module to capture the interplay between controlled emotion and driving audio, generating emotion-aware features to guide more precise facial motion synthesis. Additionally, we construct an emotional talking head dataset with comprehensive emotive text descriptions to optimize the framework's performance. Based on the proposed dataset, we propose an emotion-aware sampling training strategy and a progressive functional training strategy that further improve the model's ability to capture nuanced expressive features and achieve accurate lip-synchronization. Overall, EmoCAST achieves state-of-the-art performance in generating realistic, emotionally expressive, and audio-synchronized talking-head videos. Project Page: https://github.com/GVCLab/EmoCAST
<div id='section'>Paperid: <span id='pid'>13, <a href='https://arxiv.org/pdf/2508.10897.pdf' target='_blank'>https://arxiv.org/pdf/2508.10897.pdf</a></span>   <span><a href='https://github.com/BradleyWang0416/Human-in-Context' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mengyuan Liu, Xinshun Wang, Zhongbin Fang, Deheng Ye, Xia Li, Tao Tang, Songtao Wu, Xiangtai Li, Ming-Hsuan Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10897">Human-in-Context: Unified Cross-Domain 3D Human Motion Modeling via In-Context Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper aims to model 3D human motion across domains, where a single model is expected to handle multiple modalities, tasks, and datasets. Existing cross-domain models often rely on domain-specific components and multi-stage training, which limits their practicality and scalability. To overcome these challenges, we propose a new setting to train a unified cross-domain model through a single process, eliminating the need for domain-specific components and multi-stage training. We first introduce Pose-in-Context (PiC), which leverages in-context learning to create a pose-centric cross-domain model. While PiC generalizes across multiple pose-based tasks and datasets, it encounters difficulties with modality diversity, prompting strategy, and contextual dependency handling. We thus propose Human-in-Context (HiC), an extension of PiC that broadens generalization across modalities, tasks, and datasets. HiC combines pose and mesh representations within a unified framework, expands task coverage, and incorporates larger-scale datasets. Additionally, HiC introduces a max-min similarity prompt sampling strategy to enhance generalization across diverse domains and a network architecture with dual-branch context injection for improved handling of contextual dependencies. Extensive experimental results show that HiC performs better than PiC in terms of generalization, data scale, and performance across a wide range of domains. These results demonstrate the potential of HiC for building a unified cross-domain 3D human motion model with improved flexibility and scalability. The source codes and models are available at https://github.com/BradleyWang0416/Human-in-Context.
<div id='section'>Paperid: <span id='pid'>14, <a href='https://arxiv.org/pdf/2508.09575.pdf' target='_blank'>https://arxiv.org/pdf/2508.09575.pdf</a></span>   <span><a href='https://github.com/jwonkm/DRF' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiwon Kim, Pureum Kim, SeonHwa Kim, Soobin Park, Eunju Cha, Kyong Hwan Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09575">Dual Recursive Feedback on Generation and Appearance Latents for Pose-Robust Text-to-Image Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in controllable text-to-image (T2I) diffusion models, such as Ctrl-X and FreeControl, have demonstrated robust spatial and appearance control without requiring auxiliary module training. However, these models often struggle to accurately preserve spatial structures and fail to capture fine-grained conditions related to object poses and scene layouts. To address these challenges, we propose a training-free Dual Recursive Feedback (DRF) system that properly reflects control conditions in controllable T2I models. The proposed DRF consists of appearance feedback and generation feedback that recursively refines the intermediate latents to better reflect the given appearance information and the user's intent. This dual-update mechanism guides latent representations toward reliable manifolds, effectively integrating structural and appearance attributes. Our approach enables fine-grained generation even between class-invariant structure-appearance fusion, such as transferring human motion onto a tiger's form. Extensive experiments demonstrate the efficacy of our method in producing high-quality, semantically coherent, and structurally consistent image generations. Our source code is available at https://github.com/jwonkm/DRF.
<div id='section'>Paperid: <span id='pid'>15, <a href='https://arxiv.org/pdf/2508.09404.pdf' target='_blank'>https://arxiv.org/pdf/2508.09404.pdf</a></span>   <span><a href='https://github.com/GuangxunZhu/Waymo-3DSkelMo' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Guangxun Zhu, Shiyu Fan, Hang Dai, Edmond S. L. Ho
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09404">Waymo-3DSkelMo: A Multi-Agent 3D Skeletal Motion Dataset for Pedestrian Interaction Modeling in Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large-scale high-quality 3D motion datasets with multi-person interactions are crucial for data-driven models in autonomous driving to achieve fine-grained pedestrian interaction understanding in dynamic urban environments. However, existing datasets mostly rely on estimating 3D poses from monocular RGB video frames, which suffer from occlusion and lack of temporal continuity, thus resulting in unrealistic and low-quality human motion. In this paper, we introduce Waymo-3DSkelMo, the first large-scale dataset providing high-quality, temporally coherent 3D skeletal motions with explicit interaction semantics, derived from the Waymo Perception dataset. Our key insight is to utilize 3D human body shape and motion priors to enhance the quality of the 3D pose sequences extracted from the raw LiDRA point clouds. The dataset covers over 14,000 seconds across more than 800 real driving scenarios, including rich interactions among an average of 27 agents per scene (with up to 250 agents in the largest scene). Furthermore, we establish 3D pose forecasting benchmarks under varying pedestrian densities, and the results demonstrate its value as a foundational resource for future research on fine-grained human behavior understanding in complex urban environments. The dataset and code will be available at https://github.com/GuangxunZhu/Waymo-3DSkelMo
<div id='section'>Paperid: <span id='pid'>16, <a href='https://arxiv.org/pdf/2508.02605.pdf' target='_blank'>https://arxiv.org/pdf/2508.02605.pdf</a></span>   <span><a href='https://github.com/AIGeeksGroup/ReMoMask' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhengdao Li, Siheng Wang, Zeyu Zhang, Hao Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02605">ReMoMask: Retrieval-Augmented Masked Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-to-Motion (T2M) generation aims to synthesize realistic and semantically aligned human motion sequences from natural language descriptions. However, current approaches face dual challenges: Generative models (e.g., diffusion models) suffer from limited diversity, error accumulation, and physical implausibility, while Retrieval-Augmented Generation (RAG) methods exhibit diffusion inertia, partial-mode collapse, and asynchronous artifacts. To address these limitations, we propose ReMoMask, a unified framework integrating three key innovations: 1) A Bidirectional Momentum Text-Motion Model decouples negative sample scale from batch size via momentum queues, substantially improving cross-modal retrieval precision; 2) A Semantic Spatio-temporal Attention mechanism enforces biomechanical constraints during part-level fusion to eliminate asynchronous artifacts; 3) RAG-Classier-Free Guidance incorporates minor unconditional generation to enhance generalization. Built upon MoMask's RVQ-VAE, ReMoMask efficiently generates temporally coherent motions in minimal steps. Extensive experiments on standard benchmarks demonstrate the state-of-the-art performance of ReMoMask, achieving a 3.88% and 10.97% improvement in FID scores on HumanML3D and KIT-ML, respectively, compared to the previous SOTA method RAG-T2M. Code: https://github.com/AIGeeksGroup/ReMoMask. Website: https://aigeeksgroup.github.io/ReMoMask.
<div id='section'>Paperid: <span id='pid'>17, <a href='https://arxiv.org/pdf/2508.02605.pdf' target='_blank'>https://arxiv.org/pdf/2508.02605.pdf</a></span>   <span><a href='https://github.com/AIGeeksGroup/ReMoMask' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhengdao Li, Siheng Wang, Zeyu Zhang, Hao Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02605">ReMoMask: Retrieval-Augmented Masked Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-to-Motion (T2M) generation aims to synthesize realistic and semantically aligned human motion sequences from natural language descriptions. However, current approaches face dual challenges: Generative models (e.g., diffusion models) suffer from limited diversity, error accumulation, and physical implausibility, while Retrieval-Augmented Generation (RAG) methods exhibit diffusion inertia, partial-mode collapse, and asynchronous artifacts. To address these limitations, we propose ReMoMask, a unified framework integrating three key innovations: 1) A Bidirectional Momentum Text-Motion Model decouples negative sample scale from batch size via momentum queues, substantially improving cross-modal retrieval precision; 2) A Semantic Spatio-temporal Attention mechanism enforces biomechanical constraints during part-level fusion to eliminate asynchronous artifacts; 3) RAG-Classier-Free Guidance incorporates minor unconditional generation to enhance generalization. Built upon MoMask's RVQ-VAE, ReMoMask efficiently generates temporally coherent motions in minimal steps. Extensive experiments on standard benchmarks demonstrate the state-of-the-art performance of ReMoMask, achieving a 3.88% and 10.97% improvement in FID scores on HumanML3D and KIT-ML, respectively, compared to the previous SOTA method RAG-T2M. Code: https://github.com/AIGeeksGroup/ReMoMask. Website: https://aigeeksgroup.github.io/ReMoMask.
<div id='section'>Paperid: <span id='pid'>18, <a href='https://arxiv.org/pdf/2508.02140.pdf' target='_blank'>https://arxiv.org/pdf/2508.02140.pdf</a></span>   <span><a href='https://github.com/DriverlessMobility/AID4AD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Daniel Lengerer, Mathias Pechinger, Klaus Bogenberger, Carsten Markgraf
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02140">AID4AD: Aerial Image Data for Automated Driving Perception</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work investigates the integration of spatially aligned aerial imagery into perception tasks for automated vehicles (AVs). As a central contribution, we present AID4AD, a publicly available dataset that augments the nuScenes dataset with high-resolution aerial imagery precisely aligned to its local coordinate system. The alignment is performed using SLAM-based point cloud maps provided by nuScenes, establishing a direct link between aerial data and nuScenes local coordinate system. To ensure spatial fidelity, we propose an alignment workflow that corrects for localization and projection distortions. A manual quality control process further refines the dataset by identifying a set of high-quality alignments, which we publish as ground truth to support future research on automated registration. We demonstrate the practical value of AID4AD in two representative tasks: in online map construction, aerial imagery serves as a complementary input that improves the mapping process; in motion prediction, it functions as a structured environmental representation that replaces high-definition maps. Experiments show that aerial imagery leads to a 15-23% improvement in map construction accuracy and a 2% gain in trajectory prediction performance. These results highlight the potential of aerial imagery as a scalable and adaptable source of environmental context in automated vehicle systems, particularly in scenarios where high-definition maps are unavailable, outdated, or costly to maintain. AID4AD, along with evaluation code and pretrained models, is publicly released to foster further research in this direction: https://github.com/DriverlessMobility/AID4AD.
<div id='section'>Paperid: <span id='pid'>19, <a href='https://arxiv.org/pdf/2508.01984.pdf' target='_blank'>https://arxiv.org/pdf/2508.01984.pdf</a></span>   <span><a href='https://github.com/LUNAProject22/IMoRe' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chen Li, Chinthani Sugandhika, Yeo Keat Ee, Eric Peh, Hao Zhang, Hong Yang, Deepu Rajan, Basura Fernando
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01984">IMoRe: Implicit Program-Guided Reasoning for Human Motion Q&A</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing human motion Q\&A methods rely on explicit program execution, where the requirement for manually defined functional modules may limit the scalability and adaptability. To overcome this, we propose an implicit program-guided motion reasoning (IMoRe) framework that unifies reasoning across multiple query types without manually designed modules. Unlike existing implicit reasoning approaches that infer reasoning operations from question words, our model directly conditions on structured program functions, ensuring a more precise execution of reasoning steps. Additionally, we introduce a program-guided reading mechanism, which dynamically selects multi-level motion representations from a pretrained motion Vision Transformer (ViT), capturing both high-level semantics and fine-grained motion cues. The reasoning module iteratively refines memory representations, leveraging structured program functions to extract relevant information for different query types. Our model achieves state-of-the-art performance on Babel-QA and generalizes to a newly constructed motion Q\&A dataset based on HuMMan, demonstrating its adaptability across different motion reasoning datasets. Code and dataset are available at: https://github.com/LUNAProject22/IMoRe.
<div id='section'>Paperid: <span id='pid'>20, <a href='https://arxiv.org/pdf/2507.22567.pdf' target='_blank'>https://arxiv.org/pdf/2507.22567.pdf</a></span>   <span><a href='https://github.com/JoeyBGOfficial/Low-Cost-Accurate-Radar-Based-Human-Motion-Direction-Determination' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Weicheng Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.22567">Exploration of Low-Cost but Accurate Radar-Based Human Motion Direction Determination</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work is completed on a whim after discussions with my junior colleague. The motion direction angle affects the micro-Doppler spectrum width, thus determining the human motion direction can provide important prior information for downstream tasks such as gait recognition. However, Doppler-Time map (DTM)-based methods still have room for improvement in achieving feature augmentation and motion determination simultaneously. In response, a low-cost but accurate radar-based human motion direction determination (HMDD) method is explored in this paper. In detail, the radar-based human gait DTMs are first generated, and then the feature augmentation is achieved using feature linking model. Subsequently, the HMDD is implemented through a lightweight and fast Vision Transformer-Convolutional Neural Network hybrid model structure. The effectiveness of the proposed method is verified through open-source dataset. The open-source code of this work is released at: https://github.com/JoeyBGOfficial/Low-Cost-Accurate-Radar-Based-Human-Motion-Direction-Determination.
<div id='section'>Paperid: <span id='pid'>21, <a href='https://arxiv.org/pdf/2507.18237.pdf' target='_blank'>https://arxiv.org/pdf/2507.18237.pdf</a></span>   <span><a href='https://github.com/ChengchangTian/DATA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chengchang Tian, Jianwei Ma, Yan Huang, Zhanye Chen, Honghao Wei, Hui Zhang, Wei Hong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.18237">DATA: Domain-And-Time Alignment for High-Quality Feature Fusion in Collaborative Perception</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Feature-level fusion shows promise in collaborative perception (CP) through balanced performance and communication bandwidth trade-off. However, its effectiveness critically relies on input feature quality. The acquisition of high-quality features faces domain gaps from hardware diversity and deployment conditions, alongside temporal misalignment from transmission delays. These challenges degrade feature quality with cumulative effects throughout the collaborative network. In this paper, we present the Domain-And-Time Alignment (DATA) network, designed to systematically align features while maximizing their semantic representations for fusion. Specifically, we propose a Consistency-preserving Domain Alignment Module (CDAM) that reduces domain gaps through proximal-region hierarchical downsampling and observability-constrained discriminator. We further propose a Progressive Temporal Alignment Module (PTAM) to handle transmission delays via multi-scale motion modeling and two-stage compensation. Building upon the aligned features, an Instance-focused Feature Aggregation Module (IFAM) is developed to enhance semantic representations. Extensive experiments demonstrate that DATA achieves state-of-the-art performance on three typical datasets, maintaining robustness with severe communication delays and pose errors. The code will be released at https://github.com/ChengchangTian/DATA.
<div id='section'>Paperid: <span id='pid'>22, <a href='https://arxiv.org/pdf/2507.15266.pdf' target='_blank'>https://arxiv.org/pdf/2507.15266.pdf</a></span>   <span><a href='https://github.com/henryhcliu/vlmudmc.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haichao Liu, Haoren Guo, Pei Liu, Benshan Ma, Yuxiang Zhang, Jun Ma, Tong Heng Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.15266">VLM-UDMC: VLM-Enhanced Unified Decision-Making and Motion Control for Urban Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scene understanding and risk-aware attentions are crucial for human drivers to make safe and effective driving decisions. To imitate this cognitive ability in urban autonomous driving while ensuring the transparency and interpretability, we propose a vision-language model (VLM)-enhanced unified decision-making and motion control framework, named VLM-UDMC. This framework incorporates scene reasoning and risk-aware insights into an upper-level slow system, which dynamically reconfigures the optimal motion planning for the downstream fast system. The reconfiguration is based on real-time environmental changes, which are encoded through context-aware potential functions. More specifically, the upper-level slow system employs a two-step reasoning policy with Retrieval-Augmented Generation (RAG), leveraging foundation models to process multimodal inputs and retrieve contextual knowledge, thereby generating risk-aware insights. Meanwhile, a lightweight multi-kernel decomposed LSTM provides real-time trajectory predictions for heterogeneous traffic participants by extracting smoother trend representations for short-horizon trajectory prediction. The effectiveness of the proposed VLM-UDMC framework is verified via both simulations and real-world experiments with a full-size autonomous vehicle. It is demonstrated that the presented VLM-UDMC effectively leverages scene understanding and attention decomposition for rational driving decisions, thus improving the overall urban driving performance. Our open-source project is available at https://github.com/henryhcliu/vlmudmc.git.
<div id='section'>Paperid: <span id='pid'>23, <a href='https://arxiv.org/pdf/2507.10792.pdf' target='_blank'>https://arxiv.org/pdf/2507.10792.pdf</a></span>   <span><a href='https://github.com/511205787/Phy_SSM-ICML2025' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuchen Wang, Hongjue Zhao, Haohong Lin, Enze Xu, Lifang He, Huajie Shao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.10792">A Generalizable Physics-Enhanced State Space Model for Long-Term Dynamics Forecasting in Complex Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work aims to address the problem of long-term dynamic forecasting in complex environments where data are noisy and irregularly sampled. While recent studies have introduced some methods to improve prediction performance, these approaches still face a significant challenge in handling long-term extrapolation tasks under such complex scenarios. To overcome this challenge, we propose Phy-SSM, a generalizable method that integrates partial physics knowledge into state space models (SSMs) for long-term dynamics forecasting in complex environments. Our motivation is that SSMs can effectively capture long-range dependencies in sequential data and model continuous dynamical systems, while the incorporation of physics knowledge improves generalization ability. The key challenge lies in how to seamlessly incorporate partially known physics into SSMs. To achieve this, we decompose partially known system dynamics into known and unknown state matrices, which are integrated into a Phy-SSM unit. To further enhance long-term prediction performance, we introduce a physics state regularization term to make the estimated latent states align with system dynamics. Besides, we theoretically analyze the uniqueness of the solutions for our method. Extensive experiments on three real-world applications, including vehicle motion prediction, drone state prediction, and COVID-19 epidemiology forecasting, demonstrate the superior performance of Phy-SSM over the baselines in both long-term interpolation and extrapolation tasks. The code is available at https://github.com/511205787/Phy_SSM-ICML2025.
<div id='section'>Paperid: <span id='pid'>24, <a href='https://arxiv.org/pdf/2507.09672.pdf' target='_blank'>https://arxiv.org/pdf/2507.09672.pdf</a></span>   <span><a href='https://github.com/CarmenQing/VST-Pose' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/CarmenQing/VST-Pose' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinyu Zhang, Zhonghao Ye, Jingwei Zhang, Xiang Tian, Zhisheng Liang, Shipeng Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.09672">VST-Pose: A Velocity-Integrated Spatiotem-poral Attention Network for Human WiFi Pose Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>WiFi-based human pose estimation has emerged as a promising non-visual alternative approaches due to its pene-trability and privacy advantages. This paper presents VST-Pose, a novel deep learning framework for accurate and continuous pose estimation using WiFi channel state information. The proposed method introduces ViSTA-Former, a spatiotemporal attention backbone with dual-stream architecture that adopts a dual-stream architecture to separately capture temporal dependencies and structural relationships among body joints. To enhance sensitivity to subtle human motions, a velocity modeling branch is integrated into the framework, which learns short-term keypoint dis-placement patterns and improves fine-grained motion representation. We construct a 2D pose dataset specifically designed for smart home care scenarios and demonstrate that our method achieves 92.2% accuracy on the PCK@50 metric, outperforming existing methods by 8.3% in PCK@50 on the self-collected dataset. Further evaluation on the public MMFi dataset confirms the model's robustness and effectiveness in 3D pose estimation tasks. The proposed system provides a reliable and privacy-aware solution for continuous human motion analysis in indoor environments. Our codes are available in https://github.com/CarmenQing/VST-Pose.
<div id='section'>Paperid: <span id='pid'>25, <a href='https://arxiv.org/pdf/2507.09446.pdf' target='_blank'>https://arxiv.org/pdf/2507.09446.pdf</a></span>   <span><a href='https://github.com/Yuanhong-Zheng/EMPMP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuanhong Zheng, Ruixuan Yu, Jian Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.09446">Efficient Multi-Person Motion Prediction by Lightweight Spatial and Temporal Interactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D multi-person motion prediction is a highly complex task, primarily due to the dependencies on both individual past movements and the interactions between agents. Moreover, effectively modeling these interactions often incurs substantial computational costs. In this work, we propose a computationally efficient model for multi-person motion prediction by simplifying spatial and temporal interactions. Our approach begins with the design of lightweight dual branches that learn local and global representations for individual and multiple persons separately. Additionally, we introduce a novel cross-level interaction block to integrate the spatial and temporal representations from both branches. To further enhance interaction modeling, we explicitly incorporate the spatial inter-person distance embedding. With above efficient temporal and spatial design, we achieve state-of-the-art performance for multiple metrics on standard datasets of CMU-Mocap, MuPoTS-3D, and 3DPW, while significantly reducing the computational cost. Code is available at https://github.com/Yuanhong-Zheng/EMPMP.
<div id='section'>Paperid: <span id='pid'>26, <a href='https://arxiv.org/pdf/2507.08028.pdf' target='_blank'>https://arxiv.org/pdf/2507.08028.pdf</a></span>   <span><a href='https://github.com/dolphin-in-a-coma/sssumo' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Evgenii Rudakov, Jonathan Shock, Otto Lappi, Benjamin Ultan Cowley
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.08028">SSSUMO: Real-Time Semi-Supervised Submovement Decomposition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces a SSSUMO, semi-supervised deep learning approach for submovement decomposition that achieves state-of-the-art accuracy and speed. While submovement analysis offers valuable insights into motor control, existing methods struggle with reconstruction accuracy, computational cost, and validation, due to the difficulty of obtaining hand-labeled data. We address these challenges using a semi-supervised learning framework. This framework learns from synthetic data, initially generated from minimum-jerk principles and then iteratively refined through adaptation to unlabeled human movement data. Our fully convolutional architecture with differentiable reconstruction significantly surpasses existing methods on both synthetic and diverse human motion datasets, demonstrating robustness even in high-noise conditions. Crucially, the model operates in real-time (less than a millisecond per input second), a substantial improvement over optimization-based techniques. This enhanced performance facilitates new applications in human-computer interaction, rehabilitation medicine, and motor control studies. We demonstrate the model's effectiveness across diverse human-performed tasks such as steering, rotation, pointing, object moving, handwriting, and mouse-controlled gaming, showing notable improvements particularly on challenging datasets where traditional methods largely fail. Training and benchmarking source code, along with pre-trained model weights, are made publicly available at https://github.com/dolphin-in-a-coma/sssumo.
<div id='section'>Paperid: <span id='pid'>27, <a href='https://arxiv.org/pdf/2507.07515.pdf' target='_blank'>https://arxiv.org/pdf/2507.07515.pdf</a></span>   <span><a href='https://github.com/inkcat520/GGMotion.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuaijin Wan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07515">GGMotion: Group Graph Dynamics-Kinematics Networks for Human Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion is a continuous physical process in 3D space, governed by complex dynamic and kinematic constraints. Existing methods typically represent the human pose as an abstract graph structure, neglecting the intrinsic physical dependencies between joints, which increases learning difficulty and makes the model prone to generating unrealistic motions. In this paper, we propose GGMotion, a group graph dynamics-kinematics network that models human topology in groups to better leverage dynamics and kinematics priors. To preserve the geometric equivariance in 3D space, we propose a novel radial field for the graph network that captures more comprehensive spatio-temporal dependencies by aggregating joint features through spatial and temporal edges. Inter-group and intra-group interaction modules are employed to capture the dependencies of joints at different scales. Combined with equivariant multilayer perceptrons (MLP), joint position features are updated in each group through parallelized dynamics-kinematics propagation to improve physical plausibility. Meanwhile, we introduce an auxiliary loss to supervise motion priors during training. Extensive experiments on three standard benchmarks, including Human3.6M, CMU-Mocap, and 3DPW, demonstrate the effectiveness and superiority of our approach, achieving a significant performance margin in short-term motion prediction. The code is available at https://github.com/inkcat520/GGMotion.git.
<div id='section'>Paperid: <span id='pid'>28, <a href='https://arxiv.org/pdf/2507.07095.pdf' target='_blank'>https://arxiv.org/pdf/2507.07095.pdf</a></span>   <span><a href='https://github.com/VankouF/MotionMillion-Codes' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ke Fan, Shunlin Lu, Minyue Dai, Runyi Yu, Lixing Xiao, Zhiyang Dou, Junting Dong, Lizhuang Ma, Jingbo Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07095">Go to Zero: Towards Zero-shot Motion Generation with Million-scale Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating diverse and natural human motion sequences based on textual descriptions constitutes a fundamental and challenging research area within the domains of computer vision, graphics, and robotics. Despite significant advancements in this field, current methodologies often face challenges regarding zero-shot generalization capabilities, largely attributable to the limited size of training datasets. Moreover, the lack of a comprehensive evaluation framework impedes the advancement of this task by failing to identify directions for improvement. In this work, we aim to push text-to-motion into a new era, that is, to achieve the generalization ability of zero-shot. To this end, firstly, we develop an efficient annotation pipeline and introduce MotionMillion-the largest human motion dataset to date, featuring over 2,000 hours and 2 million high-quality motion sequences. Additionally, we propose MotionMillion-Eval, the most comprehensive benchmark for evaluating zero-shot motion generation. Leveraging a scalable architecture, we scale our model to 7B parameters and validate its performance on MotionMillion-Eval. Our results demonstrate strong generalization to out-of-domain and complex compositional motions, marking a significant step toward zero-shot human motion generation. The code is available at https://github.com/VankouF/MotionMillion-Codes.
<div id='section'>Paperid: <span id='pid'>29, <a href='https://arxiv.org/pdf/2507.04060.pdf' target='_blank'>https://arxiv.org/pdf/2507.04060.pdf</a></span>   <span><a href='https://github.com/hyqlat/TCL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianwei Tang, Jiangxin Sun, Xiaotong Lin, Lifang Zhang, Wei-Shi Zheng, Jian-Fang Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.04060">Temporal Continual Learning with Prior Compensation for Human Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human Motion Prediction (HMP) aims to predict future poses at different moments according to past motion sequences. Previous approaches have treated the prediction of various moments equally, resulting in two main limitations: the learning of short-term predictions is hindered by the focus on long-term predictions, and the incorporation of prior information from past predictions into subsequent predictions is limited. In this paper, we introduce a novel multi-stage training framework called Temporal Continual Learning (TCL) to address the above challenges. To better preserve prior information, we introduce the Prior Compensation Factor (PCF). We incorporate it into the model training to compensate for the lost prior information. Furthermore, we derive a more reasonable optimization objective through theoretical derivation. It is important to note that our TCL framework can be easily integrated with different HMP backbone models and adapted to various datasets and applications. Extensive experiments on four HMP benchmark datasets demonstrate the effectiveness and flexibility of TCL. The code is available at https://github.com/hyqlat/TCL.
<div id='section'>Paperid: <span id='pid'>30, <a href='https://arxiv.org/pdf/2507.01340.pdf' target='_blank'>https://arxiv.org/pdf/2507.01340.pdf</a></span>   <span><a href='https://github.com/cuongle1206/Phys-GRD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Cuong Le, Huy-Phuong Le, Duc Le, Minh-Thien Duong, Van-Binh Nguyen, My-Ha Le
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.01340">Physics-informed Ground Reaction Dynamics from Human Motion Capture</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Body dynamics are crucial information for the analysis of human motions in important research fields, ranging from biomechanics, sports science to computer vision and graphics. Modern approaches collect the body dynamics, external reactive force specifically, via force plates, synchronizing with human motion capture data, and learn to estimate the dynamics from a black-box deep learning model. Being specialized devices, force plates can only be installed in laboratory setups, imposing a significant limitation on the learning of human dynamics. To this end, we propose a novel method for estimating human ground reaction dynamics directly from the more reliable motion capture data with physics laws and computational simulation as constrains. We introduce a highly accurate and robust method for computing ground reaction forces from motion capture data using Euler's integration scheme and PD algorithm. The physics-based reactive forces are used to inform the learning model about the physics-informed motion dynamics thus improving the estimation accuracy. The proposed approach was tested on the GroundLink dataset, outperforming the baseline model on: 1) the ground reaction force estimation accuracy compared to the force plates measurement; and 2) our simulated root trajectory precision. The implementation code is available at https://github.com/cuongle1206/Phys-GRD
<div id='section'>Paperid: <span id='pid'>31, <a href='https://arxiv.org/pdf/2507.01012.pdf' target='_blank'>https://arxiv.org/pdf/2507.01012.pdf</a></span>   <span><a href='https://github.com/kongzhecn/DAM-VSR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhe Kong, Le Li, Yong Zhang, Feng Gao, Shaoshu Yang, Tao Wang, Kaihao Zhang, Zhuoliang Kang, Xiaoming Wei, Guanying Chen, Wenhan Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.01012">DAM-VSR: Disentanglement of Appearance and Motion for Video Super-Resolution</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Real-world video super-resolution (VSR) presents significant challenges due to complex and unpredictable degradations. Although some recent methods utilize image diffusion models for VSR and have shown improved detail generation capabilities, they still struggle to produce temporally consistent frames. We attempt to use Stable Video Diffusion (SVD) combined with ControlNet to address this issue. However, due to the intrinsic image-animation characteristics of SVD, it is challenging to generate fine details using only low-quality videos. To tackle this problem, we propose DAM-VSR, an appearance and motion disentanglement framework for VSR. This framework disentangles VSR into appearance enhancement and motion control problems. Specifically, appearance enhancement is achieved through reference image super-resolution, while motion control is achieved through video ControlNet. This disentanglement fully leverages the generative prior of video diffusion models and the detail generation capabilities of image super-resolution models. Furthermore, equipped with the proposed motion-aligned bidirectional sampling strategy, DAM-VSR can conduct VSR on longer input videos. DAM-VSR achieves state-of-the-art performance on real-world data and AIGC data, demonstrating its powerful detail generation capabilities.
<div id='section'>Paperid: <span id='pid'>32, <a href='https://arxiv.org/pdf/2507.00792.pdf' target='_blank'>https://arxiv.org/pdf/2507.00792.pdf</a></span>   <span><a href='https://github.com/hvoss-techfak/JAX-IK' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hendric Voss, Stefan Kopp
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.00792">JAX-IK: Real-Time Inverse Kinematics for Generating Multi-Constrained Movements of Virtual Human Characters</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating accurate and realistic virtual human movements in real-time is of high importance for a variety of applications in computer graphics, interactive virtual environments, robotics, and biomechanics. This paper introduces a novel real-time inverse kinematics (IK) solver specifically designed for realistic human-like movement generation. Leveraging the automatic differentiation and just-in-time compilation of TensorFlow, the proposed solver efficiently handles complex articulated human skeletons with high degrees of freedom. By treating forward and inverse kinematics as differentiable operations, our method effectively addresses common challenges such as error accumulation and complicated joint limits in multi-constrained problems, which are critical for realistic human motion modeling. We demonstrate the solver's effectiveness on the SMPLX human skeleton model, evaluating its performance against widely used iterative-based IK algorithms, like Cyclic Coordinate Descent (CCD), FABRIK, and the nonlinear optimization algorithm IPOPT. Our experiments cover both simple end-effector tasks and sophisticated, multi-constrained problems with realistic joint limits. Results indicate that our IK solver achieves real-time performance, exhibiting rapid convergence, minimal computational overhead per iteration, and improved success rates compared to existing methods. The project code is available at https://github.com/hvoss-techfak/JAX-IK
<div id='section'>Paperid: <span id='pid'>33, <a href='https://arxiv.org/pdf/2507.00273.pdf' target='_blank'>https://arxiv.org/pdf/2507.00273.pdf</a></span>   <span><a href='https://github.com/alvister88/og_bruce' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yusuke Tanaka, Alvin Zhu, Quanyou Wang, Dennis Hong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.00273">Mechanical Intelligence-Aware Curriculum Reinforcement Learning for Humanoids with Parallel Actuation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reinforcement learning (RL) has enabled advances in humanoid robot locomotion, yet most learning frameworks do not account for mechanical intelligence embedded in parallel actuation mechanisms due to limitations in simulator support for closed kinematic chains. This omission can lead to inaccurate motion modeling and suboptimal policies, particularly for robots with high actuation complexity. This paper presents general formulations and simulation methods for three types of parallel mechanisms: a differential pulley, a five-bar linkage, and a four-bar linkage, and trains a parallel-mechanism aware policy through an end-to-end curriculum RL framework for BRUCE, a kid-sized humanoid robot. Unlike prior approaches that rely on simplified serial approximations, we simulate all closed-chain constraints natively using GPU-accelerated MuJoCo (MJX), preserving the hardware's mechanical nonlinear properties during training. We benchmark our RL approach against a model predictive controller (MPC), demonstrating better surface generalization and performance in real-world zero-shot deployment. This work highlights the computational approaches and performance benefits of fully simulating parallel mechanisms in end-to-end learning pipelines for legged humanoids. Project codes with parallel mechanisms: https://github.com/alvister88/og_bruce
<div id='section'>Paperid: <span id='pid'>34, <a href='https://arxiv.org/pdf/2506.23135.pdf' target='_blank'>https://arxiv.org/pdf/2506.23135.pdf</a></span>   <span><a href='https://github.com/tsinghua-fib-lab/RoboScape' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Shang, Xin Zhang, Yinzhou Tang, Lei Jin, Chen Gao, Wei Wu, Yong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.23135">RoboScape: Physics-informed Embodied World Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>World models have become indispensable tools for embodied intelligence, serving as powerful simulators capable of generating realistic robotic videos while addressing critical data scarcity challenges. However, current embodied world models exhibit limited physical awareness, particularly in modeling 3D geometry and motion dynamics, resulting in unrealistic video generation for contact-rich robotic scenarios. In this paper, we present RoboScape, a unified physics-informed world model that jointly learns RGB video generation and physics knowledge within an integrated framework. We introduce two key physics-informed joint training tasks: temporal depth prediction that enhances 3D geometric consistency in video rendering, and keypoint dynamics learning that implicitly encodes physical properties (e.g., object shape and material characteristics) while improving complex motion modeling. Extensive experiments demonstrate that RoboScape generates videos with superior visual fidelity and physical plausibility across diverse robotic scenarios. We further validate its practical utility through downstream applications including robotic policy training with generated data and policy evaluation. Our work provides new insights for building efficient physics-informed world models to advance embodied intelligence research. The code is available at: https://github.com/tsinghua-fib-lab/RoboScape.
<div id='section'>Paperid: <span id='pid'>35, <a href='https://arxiv.org/pdf/2506.21249.pdf' target='_blank'>https://arxiv.org/pdf/2506.21249.pdf</a></span>   <span><a href='https://github.com/mengxianghan123/TR2C' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xianghan Meng, Zhengyu Tong, Zhiyuan Huang, Chun-Guang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.21249">Temporal Rate Reduction Clustering for Human Motion Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human Motion Segmentation (HMS), which aims to partition videos into non-overlapping human motions, has attracted increasing research attention recently. Existing approaches for HMS are mainly dominated by subspace clustering methods, which are grounded on the assumption that high-dimensional temporal data align with a Union-of-Subspaces (UoS) distribution. However, the frames in video capturing complex human motions with cluttered backgrounds may not align well with the UoS distribution. In this paper, we propose a novel approach for HMS, named Temporal Rate Reduction Clustering ($\text{TR}^2\text{C}$), which jointly learns structured representations and affinity to segment the sequences of frames in video. Specifically, the structured representations learned by $\text{TR}^2\text{C}$ enjoy temporally consistency and are aligned well with a UoS structure, which is favorable for addressing the HMS task. We conduct extensive experiments on five benchmark HMS datasets and achieve state-of-the-art performances with different feature extractors. The code is available at: https://github.com/mengxianghan123/TR2C.
<div id='section'>Paperid: <span id='pid'>36, <a href='https://arxiv.org/pdf/2506.18343.pdf' target='_blank'>https://arxiv.org/pdf/2506.18343.pdf</a></span>   <span><a href='https://github.com/kawser-ahmed-byte/TritonZ' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kawser Ahmed, Mir Shahriar Fardin, Md Arif Faysal Nayem, Fahim Hafiz, Swakkhar Shatabda
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.18343">TritonZ: A Remotely Operated Underwater Rover with Manipulator Arm for Exploration and Rescue Operations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The increasing demand for underwater exploration and rescue operations enforces the development of advanced wireless or semi-wireless underwater vessels equipped with manipulator arms. This paper presents the implementation of a semi-wireless underwater vehicle, "TritonZ" equipped with a manipulator arm, tailored for effective underwater exploration and rescue operations. The vehicle's compact design enables deployment in different submarine surroundings, addressing the need for wireless systems capable of navigating challenging underwater terrains. The manipulator arm can interact with the environment, allowing the robot to perform sophisticated tasks during exploration and rescue missions in emergency situations. TritonZ is equipped with various sensors such as Pi-Camera, Humidity, and Temperature sensors to send real-time environmental data. Our underwater vehicle controlled using a customized remote controller can navigate efficiently in the water where Pi-Camera enables live streaming of the surroundings. Motion control and video capture are performed simultaneously using this camera. The manipulator arm is designed to perform various tasks, similar to grasping, manipulating, and collecting underwater objects. Experimental results shows the efficacy of the proposed remotely operated vehicle in performing a variety of underwater exploration and rescue tasks. Additionally, the results show that TritonZ can maintain an average of 13.5cm/s with a minimal delay of 2-3 seconds. Furthermore, the vehicle can sustain waves underwater by maintaining its position as well as average velocity. The full project details and source code can be accessed at this link: https://github.com/kawser-ahmed-byte/TritonZ
<div id='section'>Paperid: <span id='pid'>37, <a href='https://arxiv.org/pdf/2506.12848.pdf' target='_blank'>https://arxiv.org/pdf/2506.12848.pdf</a></span>   <span><a href='https://github.com/EGO-False-Sleep/Miga25_track1' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Xu, Lechao Cheng, Yaxiong Wang, Shengeng Tang, Zhun Zhong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.12848">Towards Fine-Grained Emotion Understanding via Skeleton-Based Micro-Gesture Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present our solution to the MiGA Challenge at IJCAI 2025, which aims to recognize micro-gestures (MGs) from skeleton sequences for the purpose of hidden emotion understanding. MGs are characterized by their subtlety, short duration, and low motion amplitude, making them particularly challenging to model and classify. We adopt PoseC3D as the baseline framework and introduce three key enhancements: (1) a topology-aware skeleton representation specifically designed for the iMiGUE dataset to better capture fine-grained motion patterns; (2) an improved temporal processing strategy that facilitates smoother and more temporally consistent motion modeling; and (3) the incorporation of semantic label embeddings as auxiliary supervision to improve the model generalization. Our method achieves a Top-1 accuracy of 67.01\% on the iMiGUE test set. As a result of these contributions, our approach ranks third on the official MiGA Challenge leaderboard. The source code is available at \href{https://github.com/EGO-False-Sleep/Miga25_track1}{https://github.com/EGO-False-Sleep/Miga25\_track1}.
<div id='section'>Paperid: <span id='pid'>38, <a href='https://arxiv.org/pdf/2506.07216.pdf' target='_blank'>https://arxiv.org/pdf/2506.07216.pdf</a></span>   <span><a href='https://github.com/NadaAbodeshish/Random-Cropping-augmentation-HGR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Nada Aboudeshish, Dmitry Ignatov, Radu Timofte
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.07216">AugmentGest: Can Random Data Cropping Augmentation Boost Gesture Recognition Performance?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Data augmentation is a crucial technique in deep learning, particularly for tasks with limited dataset diversity, such as skeleton-based datasets. This paper proposes a comprehensive data augmentation framework that integrates geometric transformations, random cropping, rotation, zooming and intensity-based transformations, brightness and contrast adjustments to simulate real-world variations. Random cropping ensures the preservation of spatio-temporal integrity while addressing challenges such as viewpoint bias and occlusions. The augmentation pipeline generates three augmented versions for each sample in addition to the data set sample, thus quadrupling the data set size and enriching the diversity of gesture representations. The proposed augmentation strategy is evaluated on three models: multi-stream e2eET, FPPR point cloud-based hand gesture recognition (HGR), and DD-Network. Experiments are conducted on benchmark datasets including DHG14/28, SHREC'17, and JHMDB. The e2eET model, recognized as the state-of-the-art for hand gesture recognition on DHG14/28 and SHREC'17. The FPPR-PCD model, the second-best performing model on SHREC'17, excels in point cloud-based gesture recognition. DD-Net, a lightweight and efficient architecture for skeleton-based action recognition, is evaluated on SHREC'17 and the Human Motion Data Base (JHMDB). The results underline the effectiveness and versatility of the proposed augmentation strategy, significantly improving model generalization and robustness across diverse datasets and architectures. This framework not only establishes state-of-the-art results on all three evaluated models but also offers a scalable solution to advance HGR and action recognition applications in real-world scenarios. The framework is available at https://github.com/NadaAbodeshish/Random-Cropping-augmentation-HGR
<div id='section'>Paperid: <span id='pid'>39, <a href='https://arxiv.org/pdf/2506.03408.pdf' target='_blank'>https://arxiv.org/pdf/2506.03408.pdf</a></span>   <span><a href='https://github.com/colorfulfuture/Awesome-Trajectory-Motion-Prediction-Papers' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi Xu, Ruining Yang, Yitian Zhang, Yizhou Wang, Jianglin Lu, Mingyuan Zhang, Lili Su, Yun Fu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.03408">Trajectory Prediction Meets Large Language Models: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in large language models (LLMs) have sparked growing interest in integrating language-driven techniques into trajectory prediction. By leveraging their semantic and reasoning capabilities, LLMs are reshaping how autonomous systems perceive, model, and predict trajectories. This survey provides a comprehensive overview of this emerging field, categorizing recent work into five directions: (1) Trajectory prediction via language modeling paradigms, (2) Direct trajectory prediction with pretrained language models, (3) Language-guided scene understanding for trajectory prediction, (4) Language-driven data generation for trajectory prediction, (5) Language-based reasoning and interpretability for trajectory prediction. For each, we analyze representative methods, highlight core design choices, and identify open challenges. This survey bridges natural language processing and trajectory prediction, offering a unified perspective on how language can enrich trajectory prediction.
<div id='section'>Paperid: <span id='pid'>40, <a href='https://arxiv.org/pdf/2506.03408.pdf' target='_blank'>https://arxiv.org/pdf/2506.03408.pdf</a></span>   <span><a href='https://github.com/colorfulfuture/Awesome-Trajectory-Motion-Prediction-Papers' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi Xu, Ruining Yang, Yitian Zhang, Jianglin Lu, Mingyuan Zhang, Yizhou Wang, Lili Su, Yun Fu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.03408">Trajectory Prediction Meets Large Language Models: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in large language models (LLMs) have sparked growing interest in integrating language-driven techniques into trajectory prediction. By leveraging their semantic and reasoning capabilities, LLMs are reshaping how autonomous systems perceive, model, and predict trajectories. This survey provides a comprehensive overview of this emerging field, categorizing recent work into five directions: (1) Trajectory prediction via language modeling paradigms, (2) Direct trajectory prediction with pretrained language models, (3) Language-guided scene understanding for trajectory prediction, (4) Language-driven data generation for trajectory prediction, (5) Language-based reasoning and interpretability for trajectory prediction. For each, we analyze representative methods, highlight core design choices, and identify open challenges. This survey bridges natural language processing and trajectory prediction, offering a unified perspective on how language can enrich trajectory prediction.
<div id='section'>Paperid: <span id='pid'>41, <a href='https://arxiv.org/pdf/2506.02845.pdf' target='_blank'>https://arxiv.org/pdf/2506.02845.pdf</a></span>   <span><a href='https://github.com/LEI-QI-233/HAR-in-Space' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/LEI-QI-233/HAR-in-Space' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Di Wen, Lei Qi, Kunyu Peng, Kailun Yang, Fei Teng, Ao Luo, Jia Fu, Yufan Chen, Ruiping Liu, Yitian Shi, M. Saquib Sarfraz, Rainer Stiefelhagen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.02845">Go Beyond Earth: Understanding Human Actions and Scenes in Microgravity Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite substantial progress in video understanding, most existing datasets are limited to Earth's gravitational conditions. However, microgravity alters human motion, interactions, and visual semantics, revealing a critical gap for real-world vision systems. This presents a challenge for domain-robust video understanding in safety-critical space applications. To address this, we introduce MicroG-4M, the first benchmark for spatio-temporal and semantic understanding of human activities in microgravity. Constructed from real-world space missions and cinematic simulations, the dataset includes 4,759 clips covering 50 actions, 1,238 context-rich captions, and over 7,000 question-answer pairs on astronaut activities and scene understanding. MicroG-4M supports three core tasks: fine-grained multi-label action recognition, temporal video captioning, and visual question answering, enabling a comprehensive evaluation of both spatial localization and semantic reasoning in microgravity contexts. We establish baselines using state-of-the-art models. All data, annotations, and code are available at https://github.com/LEI-QI-233/HAR-in-Space.
<div id='section'>Paperid: <span id='pid'>42, <a href='https://arxiv.org/pdf/2506.01608.pdf' target='_blank'>https://arxiv.org/pdf/2506.01608.pdf</a></span>   <span><a href='https://github.com/amathislab/EPFL-Smart-Kitchen' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Andy Bonnetto, Haozhe Qi, Franklin Leong, Matea Tashkovska, Mahdi Rad, Solaiman Shokur, Friedhelm Hummel, Silvestro Micera, Marc Pollefeys, Alexander Mathis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.01608">EPFL-Smart-Kitchen-30: Densely annotated cooking dataset with 3D kinematics to challenge video and language models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding behavior requires datasets that capture humans while carrying out complex tasks. The kitchen is an excellent environment for assessing human motor and cognitive function, as many complex actions are naturally exhibited in kitchens from chopping to cleaning. Here, we introduce the EPFL-Smart-Kitchen-30 dataset, collected in a noninvasive motion capture platform inside a kitchen environment. Nine static RGB-D cameras, inertial measurement units (IMUs) and one head-mounted HoloLens~2 headset were used to capture 3D hand, body, and eye movements. The EPFL-Smart-Kitchen-30 dataset is a multi-view action dataset with synchronized exocentric, egocentric, depth, IMUs, eye gaze, body and hand kinematics spanning 29.7 hours of 16 subjects cooking four different recipes. Action sequences were densely annotated with 33.78 action segments per minute. Leveraging this multi-modal dataset, we propose four benchmarks to advance behavior understanding and modeling through 1) a vision-language benchmark, 2) a semantic text-to-motion generation benchmark, 3) a multi-modal action recognition benchmark, 4) a pose-based action segmentation benchmark. We expect the EPFL-Smart-Kitchen-30 dataset to pave the way for better methods as well as insights to understand the nature of ecologically-valid human behavior. Code and data are available at https://github.com/amathislab/EPFL-Smart-Kitchen
<div id='section'>Paperid: <span id='pid'>43, <a href='https://arxiv.org/pdf/2505.20255.pdf' target='_blank'>https://arxiv.org/pdf/2505.20255.pdf</a></span>   <span><a href='https://github.com/MyNiuuu/AniCrafter' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/MyNiuuu/AniCrafter' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Muyao Niu, Mingdeng Cao, Yifan Zhan, Qingtian Zhu, Mingze Ma, Jiancheng Zhao, Yanhong Zeng, Zhihang Zhong, Xiao Sun, Yinqiang Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.20255">AniCrafter: Customizing Realistic Human-Centric Animation via Avatar-Background Conditioning in Video Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in video diffusion models have significantly improved character animation techniques. However, current approaches rely on basic structural conditions such as DWPose or SMPL-X to animate character images, limiting their effectiveness in open-domain scenarios with dynamic backgrounds or challenging human poses. In this paper, we introduce \textbf{AniCrafter}, a diffusion-based human-centric animation model that can seamlessly integrate and animate a given character into open-domain dynamic backgrounds while following given human motion sequences. Built on cutting-edge Image-to-Video (I2V) diffusion architectures, our model incorporates an innovative ''avatar-background'' conditioning mechanism that reframes open-domain human-centric animation as a restoration task, enabling more stable and versatile animation outputs. Experimental results demonstrate the superior performance of our method. Codes are available at https://github.com/MyNiuuu/AniCrafter.
<div id='section'>Paperid: <span id='pid'>44, <a href='https://arxiv.org/pdf/2505.19742.pdf' target='_blank'>https://arxiv.org/pdf/2505.19742.pdf</a></span>   <span><a href='https://github.com/gobunu/HAODiff' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/gobunu/HAODiff' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jue Gong, Tingyu Yang, Jingkai Wang, Zheng Chen, Xing Liu, Hong Gu, Yulun Zhang, Xiaokang Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.19742">HAODiff: Human-Aware One-Step Diffusion via Dual-Prompt Guidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human-centered images often suffer from severe generic degradation during transmission and are prone to human motion blur (HMB), making restoration challenging. Existing research lacks sufficient focus on these issues, as both problems often coexist in practice. To address this, we design a degradation pipeline that simulates the coexistence of HMB and generic noise, generating synthetic degraded data to train our proposed HAODiff, a human-aware one-step diffusion. Specifically, we propose a triple-branch dual-prompt guidance (DPG), which leverages high-quality images, residual noise (LQ minus HQ), and HMB segmentation masks as training targets. It produces a positive-negative prompt pair for classifier-free guidance (CFG) in a single diffusion step. The resulting adaptive dual prompts let HAODiff exploit CFG more effectively, boosting robustness against diverse degradations. For fair evaluation, we introduce MPII-Test, a benchmark rich in combined noise and HMB cases. Extensive experiments show that our HAODiff surpasses existing state-of-the-art (SOTA) methods in terms of both quantitative metrics and visual quality on synthetic and real-world datasets, including our introduced MPII-Test. Code is available at: https://github.com/gobunu/HAODiff.
<div id='section'>Paperid: <span id='pid'>45, <a href='https://arxiv.org/pdf/2505.18229.pdf' target='_blank'>https://arxiv.org/pdf/2505.18229.pdf</a></span>   <span><a href='https://github.com/lostwolves/BEDI' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingning Guo, Mengwei Wu, Jiarun He, Shaoxian Li, Haifeng Li, Chao Tao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.18229">BEDI: A Comprehensive Benchmark for Evaluating Embodied Agents on UAVs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapid advancement of low-altitude remote sensing and Vision-Language Models (VLMs), Embodied Agents based on Unmanned Aerial Vehicles (UAVs) have shown significant potential in autonomous tasks. However, current evaluation methods for UAV-Embodied Agents (UAV-EAs) remain constrained by the lack of standardized benchmarks, diverse testing scenarios and open system interfaces. To address these challenges, we propose BEDI (Benchmark for Embodied Drone Intelligence), a systematic and standardized benchmark designed for evaluating UAV-EAs. Specifically, we introduce a novel Dynamic Chain-of-Embodied-Task paradigm based on the perception-decision-action loop, which decomposes complex UAV tasks into standardized, measurable subtasks. Building on this paradigm, we design a unified evaluation framework encompassing five core sub-skills: semantic perception, spatial perception, motion control, tool utilization, and task planning. Furthermore, we construct a hybrid testing platform that integrates static real-world environments with dynamic virtual scenarios, enabling comprehensive performance assessment of UAV-EAs across varied contexts. The platform also offers open and standardized interfaces, allowing researchers to customize tasks and extend scenarios, thereby enhancing flexibility and scalability in the evaluation process. Finally, through empirical evaluations of several state-of-the-art (SOTA) VLMs, we reveal their limitations in embodied UAV tasks, underscoring the critical role of the BEDI benchmark in advancing embodied intelligence research and model optimization. By filling the gap in systematic and standardized evaluation within this field, BEDI facilitates objective model comparison and lays a robust foundation for future development in this field. Our benchmark will be released at https://github.com/lostwolves/BEDI .
<div id='section'>Paperid: <span id='pid'>46, <a href='https://arxiv.org/pdf/2505.13921.pdf' target='_blank'>https://arxiv.org/pdf/2505.13921.pdf</a></span>   <span><a href='https://github.com/hwj20/APEX_EXP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wanjing Huang, Weixiang Yan, Zhen Zhang, Ambuj Singh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.13921">APEX: Empowering LLMs with Physics-Based Task Planning for Real-time Insight</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs) demonstrate strong reasoning and task planning capabilities but remain fundamentally limited in physical interaction modeling. Existing approaches integrate perception via Vision-Language Models (VLMs) or adaptive decision-making through Reinforcement Learning (RL), but they fail to capture dynamic object interactions or require task-specific training, limiting their real-world applicability. We introduce APEX (Anticipatory Physics-Enhanced Execution), a framework that equips LLMs with physics-driven foresight for real-time task planning. APEX constructs structured graphs to identify and model the most relevant dynamic interactions in the environment, providing LLMs with explicit physical state updates. Simultaneously, APEX provides low-latency forward simulations of physically feasible actions, allowing LLMs to select optimal strategies based on predictive outcomes rather than static observations. We evaluate APEX on three benchmarks designed to assess perception, prediction, and decision-making: (1) Physics Reasoning Benchmark, testing causal inference and object motion prediction; (2) Tetris, evaluating whether physics-informed prediction enhances decision-making performance in long-horizon planning tasks; (3) Dynamic Obstacle Avoidance, assessing the immediate integration of perception and action feasibility analysis. APEX significantly outperforms standard LLMs and VLM-based models, demonstrating the necessity of explicit physics reasoning for bridging the gap between language-based intelligence and real-world task execution. The source code and experiment setup are publicly available at https://github.com/hwj20/APEX_EXP .
<div id='section'>Paperid: <span id='pid'>47, <a href='https://arxiv.org/pdf/2505.11013.pdf' target='_blank'>https://arxiv.org/pdf/2505.11013.pdf</a></span>   <span><a href='https://github.com/zzysteve/MoMADiff' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zongye Zhang, Bohan Kong, Qingjie Liu, Yunhong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.11013">Towards Robust and Controllable Text-to-Motion via Masked Autoregressive Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating 3D human motion from text descriptions remains challenging due to the diverse and complex nature of human motion. While existing methods excel within the training distribution, they often struggle with out-of-distribution motions, limiting their applicability in real-world scenarios. Existing VQVAE-based methods often fail to represent novel motions faithfully using discrete tokens, which hampers their ability to generalize beyond seen data. Meanwhile, diffusion-based methods operating on continuous representations often lack fine-grained control over individual frames. To address these challenges, we propose a robust motion generation framework MoMADiff, which combines masked modeling with diffusion processes to generate motion using frame-level continuous representations. Our model supports flexible user-provided keyframe specification, enabling precise control over both spatial and temporal aspects of motion synthesis. MoMADiff demonstrates strong generalization capability on novel text-to-motion datasets with sparse keyframes as motion prompts. Extensive experiments on two held-out datasets and two standard benchmarks show that our method consistently outperforms state-of-the-art models in motion quality, instruction fidelity, and keyframe adherence. The code is available at: https://github.com/zzysteve/MoMADiff
<div id='section'>Paperid: <span id='pid'>48, <a href='https://arxiv.org/pdf/2505.05638.pdf' target='_blank'>https://arxiv.org/pdf/2505.05638.pdf</a></span>   <span><a href='https://github.com/continental/pred2plan' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohamed-Khalil Bouzidi, Christian Schlauch, Nicole Scheuerer, Yue Yao, Nadja Klein, Daniel GÃ¶hring, JÃ¶rg Reichardt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.05638">Closing the Loop: Motion Prediction Models beyond Open-Loop Benchmarks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fueled by motion prediction competitions and benchmarks, recent years have seen the emergence of increasingly large learning based prediction models, many with millions of parameters, focused on improving open-loop prediction accuracy by mere centimeters. However, these benchmarks fail to assess whether such improvements translate to better performance when integrated into an autonomous driving stack. In this work, we systematically evaluate the interplay between state-of-the-art motion predictors and motion planners. Our results show that higher open-loop accuracy does not always correlate with better closed-loop driving behavior and that other factors, such as temporal consistency of predictions and planner compatibility, also play a critical role. Furthermore, we investigate downsized variants of these models, and, surprisingly, find that in some cases models with up to 86% fewer parameters yield comparable or even superior closed-loop driving performance. Our code is available at https://github.com/continental/pred2plan.
<div id='section'>Paperid: <span id='pid'>49, <a href='https://arxiv.org/pdf/2504.21497.pdf' target='_blank'>https://arxiv.org/pdf/2504.21497.pdf</a></span>   <span><a href='https://github.com/weimengting/MagicPortrait' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mengting Wei, Yante Li, Tuomas Varanka, Yan Jiang, Guoying Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.21497">MagicPortrait: Temporally Consistent Face Reenactment with 3D Geometric Guidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this study, we propose a method for video face reenactment that integrates a 3D face parametric model into a latent diffusion framework, aiming to improve shape consistency and motion control in existing video-based face generation approaches. Our approach employs the FLAME (Faces Learned with an Articulated Model and Expressions) model as the 3D face parametric representation, providing a unified framework for modeling face expressions and head pose. This not only enables precise extraction of motion features from driving videos, but also contributes to the faithful preservation of face shape and geometry. Specifically, we enhance the latent diffusion model with rich 3D expression and detailed pose information by incorporating depth maps, normal maps, and rendering maps derived from FLAME sequences. These maps serve as motion guidance and are encoded into the denoising UNet through a specifically designed Geometric Guidance Encoder (GGE). A multi-layer feature fusion module with integrated self-attention mechanisms is used to combine facial appearance and motion latent features within the spatial domain. By utilizing the 3D face parametric model as motion guidance, our method enables parametric alignment of face identity between the reference image and the motion captured from the driving video. Experimental results on benchmark datasets show that our method excels at generating high-quality face animations with precise expression and head pose variation modeling. In addition, it demonstrates strong generalization performance on out-of-domain images. Code is publicly available at https://github.com/weimengting/MagicPortrait.
<div id='section'>Paperid: <span id='pid'>50, <a href='https://arxiv.org/pdf/2504.19056.pdf' target='_blank'>https://arxiv.org/pdf/2504.19056.pdf</a></span>   <span><a href='https://github.com/llm-lab-org/Generative-AI-for-Character-Animation-Survey' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/llm-lab-org/Generative-AI-for-Character-Animation-Survey' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohammad Mahdi Abootorabi, Omid Ghahroodi, Pardis Sadat Zahraei, Hossein Behzadasl, Alireza Mirrokni, Mobina Salimipanah, Arash Rasouli, Bahar Behzadipour, Sara Azarnoush, Benyamin Maleki, Erfan Sadraiye, Kiarash Kiani Feriz, Mahdi Teymouri Nahad, Ali Moghadasi, Abolfazl Eshagh Abianeh, Nizi Nazar, Hamid R. Rabiee, Mahdieh Soleymani Baghshah, Meisam Ahmadi, Ehsaneddin Asgari
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.19056">Generative AI for Character Animation: A Comprehensive Survey of Techniques, Applications, and Future Directions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generative AI is reshaping art, gaming, and most notably animation. Recent breakthroughs in foundation and diffusion models have reduced the time and cost of producing animated content. Characters are central animation components, involving motion, emotions, gestures, and facial expressions. The pace and breadth of advances in recent months make it difficult to maintain a coherent view of the field, motivating the need for an integrative review. Unlike earlier overviews that treat avatars, gestures, or facial animation in isolation, this survey offers a single, comprehensive perspective on all the main generative AI applications for character animation. We begin by examining the state-of-the-art in facial animation, expression rendering, image synthesis, avatar creation, gesture modeling, motion synthesis, object generation, and texture synthesis. We highlight leading research, practical deployments, commonly used datasets, and emerging trends for each area. To support newcomers, we also provide a comprehensive background section that introduces foundational models and evaluation metrics, equipping readers with the knowledge needed to enter the field. We discuss open challenges and map future research directions, providing a roadmap to advance AI-driven character-animation technologies. This survey is intended as a resource for researchers and developers entering the field of generative AI animation or adjacent fields. Resources are available at: https://github.com/llm-lab-org/Generative-AI-for-Character-Animation-Survey.
<div id='section'>Paperid: <span id='pid'>51, <a href='https://arxiv.org/pdf/2504.14899.pdf' target='_blank'>https://arxiv.org/pdf/2504.14899.pdf</a></span>   <span><a href='https://github.com/ewrfcas/Uni3C' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenjie Cao, Jingkai Zhou, Shikai Li, Jingyun Liang, Chaohui Yu, Fan Wang, Xiangyang Xue, Yanwei Fu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.14899">Uni3C: Unifying Precisely 3D-Enhanced Camera and Human Motion Controls for Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Camera and human motion controls have been extensively studied for video generation, but existing approaches typically address them separately, suffering from limited data with high-quality annotations for both aspects. To overcome this, we present Uni3C, a unified 3D-enhanced framework for precise control of both camera and human motion in video generation. Uni3C includes two key contributions. First, we propose a plug-and-play control module trained with a frozen video generative backbone, PCDController, which utilizes unprojected point clouds from monocular depth to achieve accurate camera control. By leveraging the strong 3D priors of point clouds and the powerful capacities of video foundational models, PCDController shows impressive generalization, performing well regardless of whether the inference backbone is frozen or fine-tuned. This flexibility enables different modules of Uni3C to be trained in specific domains, i.e., either camera control or human motion control, reducing the dependency on jointly annotated data. Second, we propose a jointly aligned 3D world guidance for the inference phase that seamlessly integrates both scenic point clouds and SMPL-X characters to unify the control signals for camera and human motion, respectively. Extensive experiments confirm that PCDController enjoys strong robustness in driving camera motion for fine-tuned backbones of video generation. Uni3C substantially outperforms competitors in both camera controllability and human motion quality. Additionally, we collect tailored validation sets featuring challenging camera movements and human actions to validate the effectiveness of our method.
<div id='section'>Paperid: <span id='pid'>52, <a href='https://arxiv.org/pdf/2504.14305.pdf' target='_blank'>https://arxiv.org/pdf/2504.14305.pdf</a></span>   <span><a href='https://github.com/TeleHuman/ALMI-Open,' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiyuan Shi, Xinzhe Liu, Dewei Wang, Ouyang Lu, SÃ¶ren Schwertfeger, Fuchun Sun, Chenjia Bai, Xuelong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.14305">Adversarial Locomotion and Motion Imitation for Humanoid Policy Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humans exhibit diverse and expressive whole-body movements. However, attaining human-like whole-body coordination in humanoid robots remains challenging, as conventional approaches that mimic whole-body motions often neglect the distinct roles of upper and lower body. This oversight leads to computationally intensive policy learning and frequently causes robot instability and falls during real-world execution. To address these issues, we propose Adversarial Locomotion and Motion Imitation (ALMI), a novel framework that enables adversarial policy learning between upper and lower body. Specifically, the lower body aims to provide robust locomotion capabilities to follow velocity commands while the upper body tracks various motions. Conversely, the upper-body policy ensures effective motion tracking when the robot executes velocity-based movements. Through iterative updates, these policies achieve coordinated whole-body control, which can be extended to loco-manipulation tasks with teleoperation systems. Extensive experiments demonstrate that our method achieves robust locomotion and precise motion tracking in both simulation and on the full-size Unitree H1 robot. Additionally, we release a large-scale whole-body motion control dataset featuring high-quality episodic trajectories from MuJoCo simulations deployable on real robots. The project page is https://almi-humanoid.github.io.
<div id='section'>Paperid: <span id='pid'>53, <a href='https://arxiv.org/pdf/2504.12826.pdf' target='_blank'>https://arxiv.org/pdf/2504.12826.pdf</a></span>   <span><a href='https://github.com/pengxuanyang/UncAD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Pengxuan Yang, Yupeng Zheng, Qichao Zhang, Kefei Zhu, Zebin Xing, Qiao Lin, Yun-Fu Liu, Zhiguo Su, Dongbin Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.12826">UncAD: Towards Safe End-to-end Autonomous Driving via Online Map Uncertainty</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>End-to-end autonomous driving aims to produce planning trajectories from raw sensors directly. Currently, most approaches integrate perception, prediction, and planning modules into a fully differentiable network, promising great scalability. However, these methods typically rely on deterministic modeling of online maps in the perception module for guiding or constraining vehicle planning, which may incorporate erroneous perception information and further compromise planning safety. To address this issue, we delve into the importance of online map uncertainty for enhancing autonomous driving safety and propose a novel paradigm named UncAD. Specifically, UncAD first estimates the uncertainty of the online map in the perception module. It then leverages the uncertainty to guide motion prediction and planning modules to produce multi-modal trajectories. Finally, to achieve safer autonomous driving, UncAD proposes an uncertainty-collision-aware planning selection strategy according to the online map uncertainty to evaluate and select the best trajectory. In this study, we incorporate UncAD into various state-of-the-art (SOTA) end-to-end methods. Experiments on the nuScenes dataset show that integrating UncAD, with only a 1.9% increase in parameters, can reduce collision rates by up to 26% and drivable area conflict rate by up to 42%. Codes, pre-trained models, and demo videos can be accessed at https://github.com/pengxuanyang/UncAD.
<div id='section'>Paperid: <span id='pid'>54, <a href='https://arxiv.org/pdf/2504.06504.pdf' target='_blank'>https://arxiv.org/pdf/2504.06504.pdf</a></span>   <span><a href='https://github.com/XiaohangYang829/STaR' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/XiaohangYang829/STaR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaohang Yang, Qing Wang, Jiahao Yang, Gregory Slabaugh, Shanxin Yuan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.06504">STaR: Seamless Spatial-Temporal Aware Motion Retargeting with Penetration and Consistency Constraints</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion retargeting seeks to faithfully replicate the spatio-temporal motion characteristics of a source character onto a target character with a different body shape. Apart from motion semantics preservation, ensuring geometric plausibility and maintaining temporal consistency are also crucial for effective motion retargeting. However, many existing methods prioritize either geometric plausibility or temporal consistency. Neglecting geometric plausibility results in interpenetration while neglecting temporal consistency leads to motion jitter. In this paper, we propose a novel sequence-to-sequence model for seamless Spatial-Temporal aware motion Retargeting (STaR), with penetration and consistency constraints. STaR consists of two modules: (1) a spatial module that incorporates dense shape representation and a novel limb penetration constraint to ensure geometric plausibility while preserving motion semantics, and (2) a temporal module that utilizes a temporal transformer and a novel temporal consistency constraint to predict the entire motion sequence at once while enforcing multi-level trajectory smoothness. The seamless combination of the two modules helps us achieve a good balance between the semantic, geometric, and temporal targets. Extensive experiments on the Mixamo and ScanRet datasets demonstrate that our method produces plausible and coherent motions while significantly reducing interpenetration rates compared with other approaches. Code page: https://github.com/XiaohangYang829/STaR.
<div id='section'>Paperid: <span id='pid'>55, <a href='https://arxiv.org/pdf/2504.02451.pdf' target='_blank'>https://arxiv.org/pdf/2504.02451.pdf</a></span>   <span><a href='https://github.com/Andyplus1/ConMo' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiayi Gao, Zijin Yin, Changcheng Hua, Yuxin Peng, Kongming Liang, Zhanyu Ma, Jun Guo, Yang Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.02451">ConMo: Controllable Motion Disentanglement and Recomposition for Zero-Shot Motion Transfer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The development of Text-to-Video (T2V) generation has made motion transfer possible, enabling the control of video motion based on existing footage. However, current methods have two limitations: 1) struggle to handle multi-subjects videos, failing to transfer specific subject motion; 2) struggle to preserve the diversity and accuracy of motion as transferring to subjects with varying shapes. To overcome these, we introduce \textbf{ConMo}, a zero-shot framework that disentangle and recompose the motions of subjects and camera movements. ConMo isolates individual subject and background motion cues from complex trajectories in source videos using only subject masks, and reassembles them for target video generation. This approach enables more accurate motion control across diverse subjects and improves performance in multi-subject scenarios. Additionally, we propose soft guidance in the recomposition stage which controls the retention of original motion to adjust shape constraints, aiding subject shape adaptation and semantic transformation. Unlike previous methods, ConMo unlocks a wide range of applications, including subject size and position editing, subject removal, semantic modifications, and camera motion simulation. Extensive experiments demonstrate that ConMo significantly outperforms state-of-the-art methods in motion fidelity and semantic consistency. The code is available at https://github.com/Andyplus1/ConMo.
<div id='section'>Paperid: <span id='pid'>56, <a href='https://arxiv.org/pdf/2503.18211.pdf' target='_blank'>https://arxiv.org/pdf/2503.18211.pdf</a></span>   <span><a href='https://github.com/lzhyu/SimMotionEdit' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhengyuan Li, Kai Cheng, Anindita Ghosh, Uttaran Bhattacharya, Liangyan Gui, Aniket Bera
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.18211">SimMotionEdit: Text-Based Human Motion Editing with Motion Similarity Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-based 3D human motion editing is a critical yet challenging task in computer vision and graphics. While training-free approaches have been explored, the recent release of the MotionFix dataset, which includes source-text-motion triplets, has opened new avenues for training, yielding promising results. However, existing methods struggle with precise control, often leading to misalignment between motion semantics and language instructions. In this paper, we introduce a related task, motion similarity prediction, and propose a multi-task training paradigm, where we train the model jointly on motion editing and motion similarity prediction to foster the learning of semantically meaningful representations. To complement this task, we design an advanced Diffusion-Transformer-based architecture that separately handles motion similarity prediction and motion editing. Extensive experiments demonstrate the state-of-the-art performance of our approach in both editing alignment and fidelity.
<div id='section'>Paperid: <span id='pid'>57, <a href='https://arxiv.org/pdf/2503.14637.pdf' target='_blank'>https://arxiv.org/pdf/2503.14637.pdf</a></span>   <span><a href='https://github.com/amathislab/Kinesis' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Merkourios Simos, Alberto Silvio Chiappa, Alexander Mathis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.14637">Reinforcement learning-based motion imitation for physiologically plausible musculoskeletal motor control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>How do humans move? The quest to understand human motion has broad applications in numerous fields, ranging from computer animation and motion synthesis to neuroscience, human prosthetics and rehabilitation. Although advances in reinforcement learning (RL) have produced impressive results in capturing human motion using simplified humanoids, controlling physiologically accurate models of the body remains an open challenge. In this work, we present a model-free motion imitation framework (KINESIS) to advance the understanding of muscle-based motor control. Using a musculoskeletal model of the lower body with 80 muscle actuators and 20 DoF, we demonstrate that KINESIS achieves strong imitation performance on 1.9 hours of motion capture data, is controllable by natural language through pre-trained text-to-motion generative models, and can be fine-tuned to carry out high-level tasks such as target goal reaching. Importantly, KINESIS generates muscle activity patterns that correlate well with human EMG activity. The physiological plausibility makes KINESIS a promising model for tackling challenging problems in human motor control theory, which we highlight by investigating Bernstein's redundancy problem in the context of locomotion. Code, videos and benchmarks will be available at https://github.com/amathislab/Kinesis.
<div id='section'>Paperid: <span id='pid'>58, <a href='https://arxiv.org/pdf/2503.13300.pdf' target='_blank'>https://arxiv.org/pdf/2503.13300.pdf</a></span>   <span><a href='https://github.com/qinghuannn/PMG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ling-An Zeng, Gaojie Wu, Ancong Wu, Jian-Fang Hu, Wei-Shi Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.13300">Progressive Human Motion Generation Based on Text and Few Motion Frames</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Although existing text-to-motion (T2M) methods can produce realistic human motion from text description, it is still difficult to align the generated motion with the desired postures since using text alone is insufficient for precisely describing diverse postures. To achieve more controllable generation, an intuitive way is to allow the user to input a few motion frames describing precise desired postures. Thus, we explore a new Text-Frame-to-Motion (TF2M) generation task that aims to generate motions from text and very few given frames. Intuitively, the closer a frame is to a given frame, the lower the uncertainty of this frame is when conditioned on this given frame. Hence, we propose a novel Progressive Motion Generation (PMG) method to progressively generate a motion from the frames with low uncertainty to those with high uncertainty in multiple stages. During each stage, new frames are generated by a Text-Frame Guided Generator conditioned on frame-aware semantics of the text, given frames, and frames generated in previous stages. Additionally, to alleviate the train-test gap caused by multi-stage accumulation of incorrectly generated frames during testing, we propose a Pseudo-frame Replacement Strategy for training. Experimental results show that our PMG outperforms existing T2M generation methods by a large margin with even one given frame, validating the effectiveness of our PMG. Code is available at https://github.com/qinghuannn/PMG.
<div id='section'>Paperid: <span id='pid'>59, <a href='https://arxiv.org/pdf/2503.13047.pdf' target='_blank'>https://arxiv.org/pdf/2503.13047.pdf</a></span>   <span><a href='https://github.com/songruiqi/InsightDrive' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruiqi Song, Xianda Guo, Hangbin Wu, Qinggong Wei, Long Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.13047">InsightDrive: Insight Scene Representation for End-to-End Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Directly generating planning results from raw sensors has become increasingly prevalent due to its adaptability and robustness in complex scenarios. Scene representation, as a key module in the pipeline, has traditionally relied on conventional perception, which focus on the global scene. However, in driving scenarios, human drivers typically focus only on regions that directly impact driving, which often coincide with those required for end-to-end autonomous driving. In this paper, a novel end-to-end autonomous driving method called InsightDrive is proposed, which organizes perception by language-guided scene representation. We introduce an instance-centric scene tokenizer that transforms the surrounding environment into map- and object-aware instance tokens. Scene attention language descriptions, which highlight key regions and obstacles affecting the ego vehicle's movement, are generated by a vision-language model that leverages the cognitive reasoning capabilities of foundation models. We then align scene descriptions with visual features using the vision-language model, guiding visual attention through these descriptions to give effectively scene representation. Furthermore, we employ self-attention and cross-attention mechanisms to model the ego-agents and ego-map relationships to comprehensively build the topological relationships of the scene. Finally, based on scene understanding, we jointly perform motion prediction and planning. Extensive experiments on the widely used nuScenes benchmark demonstrate that the proposed InsightDrive achieves state-of-the-art performance in end-to-end autonomous driving. The code is available at https://github.com/songruiqi/InsightDrive
<div id='section'>Paperid: <span id='pid'>60, <a href='https://arxiv.org/pdf/2503.12955.pdf' target='_blank'>https://arxiv.org/pdf/2503.12955.pdf</a></span>   <span><a href='https://github.com/ZJHTerry18/HumanInScene' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiahe Zhao, Ruibing Hou, Zejie Tian, Hong Chang, Shiguang Shan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.12955">HIS-GPT: Towards 3D Human-In-Scene Multimodal Understanding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a new task to benchmark human-in-scene understanding for embodied agents: Human-In-Scene Question Answering (HIS-QA). Given a human motion within a 3D scene, HIS-QA requires the agent to comprehend human states and behaviors, reason about its surrounding environment, and answer human-related questions within the scene. To support this new task, we present HIS-Bench, a multimodal benchmark that systematically evaluates HIS understanding across a broad spectrum, from basic perception to commonsense reasoning and planning. Our evaluation of various vision-language models on HIS-Bench reveals significant limitations in their ability to handle HIS-QA tasks. To this end, we propose HIS-GPT, the first foundation model for HIS understanding. HIS-GPT integrates 3D scene context and human motion dynamics into large language models while incorporating specialized mechanisms to capture human-scene interactions. Extensive experiments demonstrate that HIS-GPT sets a new state-of-the-art on HIS-QA tasks. We hope this work inspires future research on human behavior analysis in 3D scenes, advancing embodied AI and world models. The codes and data: https://github.com/ZJHTerry18/HumanInScene.
<div id='section'>Paperid: <span id='pid'>61, <a href='https://arxiv.org/pdf/2503.09154.pdf' target='_blank'>https://arxiv.org/pdf/2503.09154.pdf</a></span>   <span><a href='https://github.com/PKU-YuanGroup/SwapAnyone' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chengshu Zhao, Yunyang Ge, Xinhua Cheng, Bin Zhu, Yatian Pang, Bin Lin, Fan Yang, Feng Gao, Li Yuan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.09154">SwapAnyone: Consistent and Realistic Video Synthesis for Swapping Any Person into Any Video</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video body-swapping aims to replace the body in an existing video with a new body from arbitrary sources, which has garnered more attention in recent years. Existing methods treat video body-swapping as a composite of multiple tasks instead of an independent task and typically rely on various models to achieve video body-swapping sequentially. However, these methods fail to achieve end-to-end optimization for the video body-swapping which causes issues such as variations in luminance among frames, disorganized occlusion relationships, and the noticeable separation between bodies and background. In this work, we define video body-swapping as an independent task and propose three critical consistencies: identity consistency, motion consistency, and environment consistency. We introduce an end-to-end model named SwapAnyone, treating video body-swapping as a video inpainting task with reference fidelity and motion control. To improve the ability to maintain environmental harmony, particularly luminance harmony in the resulting video, we introduce a novel EnvHarmony strategy for training our model progressively. Additionally, we provide a dataset named HumanAction-32K covering various videos about human actions. Extensive experiments demonstrate that our method achieves State-Of-The-Art (SOTA) performance among open-source methods while approaching or surpassing closed-source models across multiple dimensions. All code, model weights, and the HumanAction-32K dataset will be open-sourced at https://github.com/PKU-YuanGroup/SwapAnyone.
<div id='section'>Paperid: <span id='pid'>62, <a href='https://arxiv.org/pdf/2503.08664.pdf' target='_blank'>https://arxiv.org/pdf/2503.08664.pdf</a></span>   <span><a href='https://github.com/johannwyh/MEAT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuhan Wang, Fangzhou Hong, Shuai Yang, Liming Jiang, Wayne Wu, Chen Change Loy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.08664">MEAT: Multiview Diffusion Model for Human Generation on Megapixels with Mesh Attention</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multiview diffusion models have shown considerable success in image-to-3D generation for general objects. However, when applied to human data, existing methods have yet to deliver promising results, largely due to the challenges of scaling multiview attention to higher resolutions. In this paper, we explore human multiview diffusion models at the megapixel level and introduce a solution called mesh attention to enable training at 1024x1024 resolution. Using a clothed human mesh as a central coarse geometric representation, the proposed mesh attention leverages rasterization and projection to establish direct cross-view coordinate correspondences. This approach significantly reduces the complexity of multiview attention while maintaining cross-view consistency. Building on this foundation, we devise a mesh attention block and combine it with keypoint conditioning to create our human-specific multiview diffusion model, MEAT. In addition, we present valuable insights into applying multiview human motion videos for diffusion training, addressing the longstanding issue of data scarcity. Extensive experiments show that MEAT effectively generates dense, consistent multiview human images at the megapixel level, outperforming existing multiview diffusion methods.
<div id='section'>Paperid: <span id='pid'>63, <a href='https://arxiv.org/pdf/2502.19868.pdf' target='_blank'>https://arxiv.org/pdf/2502.19868.pdf</a></span>   <span><a href='https://github.com/WesLee88524/C-Drag-Official-Repo' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuhao Li, Mirana Claire Angel, Salman Khan, Yu Zhu, Jinqiu Sun, Yanning Zhang, Fahad Shahbaz Khan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.19868">C-Drag: Chain-of-Thought Driven Motion Controller for Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Trajectory-based motion control has emerged as an intuitive and efficient approach for controllable video generation. However, the existing trajectory-based approaches are usually limited to only generating the motion trajectory of the controlled object and ignoring the dynamic interactions between the controlled object and its surroundings. To address this limitation, we propose a Chain-of-Thought-based motion controller for controllable video generation, named C-Drag. Instead of directly generating the motion of some objects, our C-Drag first performs object perception and then reasons the dynamic interactions between different objects according to the given motion control of the objects. Specifically, our method includes an object perception module and a Chain-of-Thought-based motion reasoning module. The object perception module employs visual language models to capture the position and category information of various objects within the image. The Chain-of-Thought-based motion reasoning module takes this information as input and conducts a stage-wise reasoning process to generate motion trajectories for each of the affected objects, which are subsequently fed to the diffusion model for video synthesis. Furthermore, we introduce a new video object interaction (VOI) dataset to evaluate the generation quality of motion controlled video generation methods. Our VOI dataset contains three typical types of interactions and provides the motion trajectories of objects that can be used for accurate performance evaluation. Experimental results show that C-Drag achieves promising performance across multiple metrics, excelling in object motion control. Our benchmark, codes, and models will be available at https://github.com/WesLee88524/C-Drag-Official-Repo.
<div id='section'>Paperid: <span id='pid'>64, <a href='https://arxiv.org/pdf/2502.17327.pdf' target='_blank'>https://arxiv.org/pdf/2502.17327.pdf</a></span>   <span><a href='https://github.com/Anytop2025/Anytop' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Inbar Gat, Sigal Raab, Guy Tevet, Yuval Reshef, Amit H. Bermano, Daniel Cohen-Or
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.17327">AnyTop: Character Animation Diffusion with Any Topology</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating motion for arbitrary skeletons is a longstanding challenge in computer graphics, remaining largely unexplored due to the scarcity of diverse datasets and the irregular nature of the data. In this work, we introduce AnyTop, a diffusion model that generates motions for diverse characters with distinct motion dynamics, using only their skeletal structure as input. Our work features a transformer-based denoising network, tailored for arbitrary skeleton learning, integrating topology information into the traditional attention mechanism. Additionally, by incorporating textual joint descriptions into the latent feature representation, AnyTop learns semantic correspondences between joints across diverse skeletons. Our evaluation demonstrates that AnyTop generalizes well, even with as few as three training examples per topology, and can produce motions for unseen skeletons as well. Furthermore, our model's latent space is highly informative, enabling downstream tasks such as joint correspondence, temporal segmentation and motion editing. Our webpage, https://anytop2025.github.io/Anytop-page, includes links to videos and code.
<div id='section'>Paperid: <span id='pid'>65, <a href='https://arxiv.org/pdf/2502.11644.pdf' target='_blank'>https://arxiv.org/pdf/2502.11644.pdf</a></span>   <span><a href='https://github.com/IDASLab/InTec_Framework' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Habib Larian, Faramarz Safi-Esfahani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.11644">InTec: integrated things-edge computing: a framework for distributing machine learning pipelines in edge AI systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapid expansion of the Internet of Things (IoT), sensors, smartphones, and wearables have become integral to daily life, powering smart applications in home automation, healthcare, and intelligent transportation. However, these advancements face significant challenges due to latency and bandwidth constraints imposed by traditional cloud based machine learning (ML) frameworks. The need for innovative solutions is evident as cloud computing struggles with increased latency and network congestion. Previous attempts to offload parts of the ML pipeline to edge and cloud layers have yet to fully resolve these issues, often worsening system response times and network congestion due to the computational limitations of edge devices. In response to these challenges, this study introduces the InTec (Integrated Things Edge Computing) framework, a groundbreaking innovation in IoT architecture. Unlike existing methods, InTec fully leverages the potential of a three tier architecture by strategically distributing ML tasks across the Things, Edge, and Cloud layers. This comprehensive approach enables real time data processing at the point of data generation, significantly reducing latency, optimizing network traffic, and enhancing system reliability. InTec effectiveness is validated through empirical evaluation using the MHEALTH dataset for human motion detection in smart homes, demonstrating notable improvements in key metrics: an 81.56 percent reduction in response time, a 10.92 percent decrease in network traffic, a 9.82 percent improvement in throughput, a 21.86 percent reduction in edge energy consumption, and a 25.83 percent reduction in cloud energy consumption. These advancements establish InTec as a new benchmark for scalable, responsive, and energy efficient IoT applications, demonstrating its potential to revolutionize how the ML pipeline is integrated into Edge AI (EI) systems.
<div id='section'>Paperid: <span id='pid'>66, <a href='https://arxiv.org/pdf/2502.08244.pdf' target='_blank'>https://arxiv.org/pdf/2502.08244.pdf</a></span>   <span><a href='https://github.com/JinWonjoon/FloVD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wonjoon Jin, Qi Dai, Chong Luo, Seung-Hwan Baek, Sunghyun Cho
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.08244">FloVD: Optical Flow Meets Video Diffusion Model for Enhanced Camera-Controlled Video Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present FloVD, a novel video diffusion model for camera-controllable video generation. FloVD leverages optical flow to represent the motions of the camera and moving objects. This approach offers two key benefits. Since optical flow can be directly estimated from videos, our approach allows for the use of arbitrary training videos without ground-truth camera parameters. Moreover, as background optical flow encodes 3D correlation across different viewpoints, our method enables detailed camera control by leveraging the background motion. To synthesize natural object motion while supporting detailed camera control, our framework adopts a two-stage video synthesis pipeline consisting of optical flow generation and flow-conditioned video synthesis. Extensive experiments demonstrate the superiority of our method over previous approaches in terms of accurate camera control and natural object motion synthesis.
<div id='section'>Paperid: <span id='pid'>67, <a href='https://arxiv.org/pdf/2502.06221.pdf' target='_blank'>https://arxiv.org/pdf/2502.06221.pdf</a></span>   <span><a href='https://github.com/tedhuang96/icp' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhe Huang, Tianchen Ji, Heling Zhang, Fatemeh Cheraghi Pouria, Katherine Driggs-Campbell, Roy Dong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.06221">Interaction-aware Conformal Prediction for Crowd Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>During crowd navigation, robot motion plan needs to consider human motion uncertainty, and the human motion uncertainty is dependent on the robot motion plan. We introduce Interaction-aware Conformal Prediction (ICP) to alternate uncertainty-aware robot motion planning and decision-dependent human motion uncertainty quantification. ICP is composed of a trajectory predictor to predict human trajectories, a model predictive controller to plan robot motion with confidence interval radii added for probabilistic safety, a human simulator to collect human trajectory calibration dataset conditioned on the planned robot motion, and a conformal prediction module to quantify trajectory prediction error on the decision-dependent calibration dataset. Crowd navigation simulation experiments show that ICP strikes a good balance of performance among navigation efficiency, social awareness, and uncertainty quantification compared to previous works. ICP generalizes well to navigation tasks under various crowd densities. The fast runtime and efficient memory usage make ICP practical for real-world applications. Code is available at https://github.com/tedhuang96/icp.
<div id='section'>Paperid: <span id='pid'>68, <a href='https://arxiv.org/pdf/2502.05857.pdf' target='_blank'>https://arxiv.org/pdf/2502.05857.pdf</a></span>   <span><a href='https://github.com/zju3dv/EgoAgent' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lu Chen, Yizhou Wang, Shixiang Tang, Qianhong Ma, Tong He, Wanli Ouyang, Xiaowei Zhou, Hujun Bao, Sida Peng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.05857">EgoAgent: A Joint Predictive Agent Model in Egocentric Worlds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning an agent model that behaves like humans-capable of jointly perceiving the environment, predicting the future, and taking actions from a first-person perspective-is a fundamental challenge in computer vision. Existing methods typically train separate models for these abilities, which fail to capture their intrinsic relationships and prevent them from learning from each other. Inspired by how humans learn through the perception-action loop, we propose EgoAgent, a unified agent model that simultaneously learns to represent, predict, and act within a single transformer. EgoAgent explicitly models the causal and temporal dependencies among these abilities by formulating the task as an interleaved sequence of states and actions. It further introduces a joint embedding-action-prediction architecture with temporally asymmetric predictor and observer branches, enabling synergistic optimization across all three capabilities. Comprehensive evaluations of EgoAgent on representative tasks such as image classification, egocentric future state prediction, and 3D human motion prediction demonstrate the superiority of our method. The code and trained models will be publicly available at https://github.com/zju3dv/EgoAgent.
<div id='section'>Paperid: <span id='pid'>69, <a href='https://arxiv.org/pdf/2502.05792.pdf' target='_blank'>https://arxiv.org/pdf/2502.05792.pdf</a></span>   <span><a href='https://github.com/centiLinda/AToM-human-prediction.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuwen Liao, Muqing Cao, Xinhang Xu, Lihua Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.05792">AToM: Adaptive Theory-of-Mind-Based Human Motion Prediction in Long-Term Human-Robot Interactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humans learn from observations and experiences to adjust their behaviours towards better performance. Interacting with such dynamic humans is challenging, as the robot needs to predict the humans accurately for safe and efficient operations. Long-term interactions with dynamic humans have not been extensively studied by prior works. We propose an adaptive human prediction model based on the Theory-of-Mind (ToM), a fundamental social-cognitive ability that enables humans to infer others' behaviours and intentions. We formulate the human internal belief about others using a game-theoretic model, which predicts the future motions of all agents in a navigation scenario. To estimate an evolving belief, we use an Unscented Kalman Filter to update the behavioural parameters in the human internal model. Our formulation provides unique interpretability to dynamic human behaviours by inferring how the human predicts the robot. We demonstrate through long-term experiments in both simulations and real-world settings that our prediction effectively promotes safety and efficiency in downstream robot planning. Code will be available at https://github.com/centiLinda/AToM-human-prediction.git.
<div id='section'>Paperid: <span id='pid'>70, <a href='https://arxiv.org/pdf/2502.00395.pdf' target='_blank'>https://arxiv.org/pdf/2502.00395.pdf</a></span>   <span><a href='https://github.com/TUMFTM/FlexCloud' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Maximilian Leitenstern, Marko Alten, Christian Bolea-Schaser, Dominik Kulmer, Marcel Weinmann, Markus Lienkamp
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.00395">FlexCloud: Direct, Modular Georeferencing and Drift-Correction of Point Cloud Maps</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current software stacks for real-world applications of autonomous driving leverage map information to ensure reliable localization, path planning, and motion prediction. An important field of research is the generation of point cloud maps, referring to the topic of simultaneous localization and mapping (SLAM). As most recent developments do not include global position data, the resulting point cloud maps suffer from internal distortion and missing georeferencing, preventing their use for map-based localization approaches. Therefore, we propose FlexCloud for an automatic georeferencing of point cloud maps created from SLAM. Our approach is designed to work modularly with different SLAM methods, utilizing only the generated local point cloud map and its odometry. Using the corresponding GNSS positions enables direct georeferencing without additional control points. By leveraging a 3D rubber-sheet transformation, we can correct distortions within the map caused by long-term drift while maintaining its structure. Our approach enables the creation of consistent, globally referenced point cloud maps from data collected by a mobile mapping system (MMS). The source code of our work is available at https://github.com/TUMFTM/FlexCloud.
<div id='section'>Paperid: <span id='pid'>71, <a href='https://arxiv.org/pdf/2501.15648.pdf' target='_blank'>https://arxiv.org/pdf/2501.15648.pdf</a></span>   <span><a href='https://github.com/matyasbohacek/pose-transfer-human-motion' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Vaclav Knapp, Matyas Bohacek
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.15648">Can Pose Transfer Models Generate Realistic Human Motion?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent pose-transfer methods aim to generate temporally consistent and fully controllable videos of human action where the motion from a reference video is reenacted by a new identity. We evaluate three state-of-the-art pose-transfer methods -- AnimateAnyone, MagicAnimate, and ExAvatar -- by generating videos with actions and identities outside the training distribution and conducting a participant study about the quality of these videos. In a controlled environment of 20 distinct human actions, we find that participants, presented with the pose-transferred videos, correctly identify the desired action only 42.92% of the time. Moreover, the participants find the actions in the generated videos consistent with the reference (source) videos only 36.46% of the time. These results vary by method: participants find the splatting-based ExAvatar more consistent and photorealistic than the diffusion-based AnimateAnyone and MagicAnimate.
<div id='section'>Paperid: <span id='pid'>72, <a href='https://arxiv.org/pdf/2501.11260.pdf' target='_blank'>https://arxiv.org/pdf/2501.11260.pdf</a></span>   <span><a href='https://github.com/FengZicai/WMAD-Benchmarks' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/FengZicai/AwesomeWMAD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tuo Feng, Wenguan Wang, Yi Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.11260">A Survey of World Models for Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent breakthroughs in autonomous driving have been propelled by advances in robust world modeling, fundamentally transforming how vehicles interpret dynamic scenes and execute safe decision-making. World models have emerged as a linchpin technology, offering high-fidelity representations of the driving environment that integrate multi-sensor data, semantic cues, and temporal dynamics. This paper systematically reviews recent advances in world models for autonomous driving, proposing a three-tiered taxonomy: (i) Generation of Future Physical World, covering Image-, BEV-, OG-, and PC-based generation methods that enhance scene evolution modeling through diffusion models and 4D occupancy forecasting; (ii) Behavior Planning for Intelligent Agents, combining rule-driven and learning-based paradigms with cost map optimization and reinforcement learning for trajectory generation in complex traffic conditions; (ii) Interaction between Prediction and Planning, achieving multi-agent collaborative decision-making through latent space diffusion and memory-augmented architectures. The study further analyzes training paradigms, including self-supervised learning, multimodal pretraining, and generative data augmentation, while evaluating world models' performance in scene understanding and motion prediction tasks. Future research must address key challenges in self-supervised representation learning, multimodal fusion, and advanced simulation to advance the practical deployment of world models in complex urban environments. Overall, the comprehensive analysis provides a technical roadmap for harnessing the transformative potential of world models in advancing safe and reliable autonomous driving solutions.
<div id='section'>Paperid: <span id='pid'>73, <a href='https://arxiv.org/pdf/2501.10021.pdf' target='_blank'>https://arxiv.org/pdf/2501.10021.pdf</a></span>   <span><a href='https://github.com/bytedance/X-Dyna' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/bytedance/X-Dyna' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Di Chang, Hongyi Xu, You Xie, Yipeng Gao, Zhengfei Kuang, Shengqu Cai, Chenxu Zhang, Guoxian Song, Chao Wang, Yichun Shi, Zeyuan Chen, Shijie Zhou, Linjie Luo, Gordon Wetzstein, Mohammad Soleymani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.10021">X-Dyna: Expressive Dynamic Human Image Animation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce X-Dyna, a novel zero-shot, diffusion-based pipeline for animating a single human image using facial expressions and body movements derived from a driving video, that generates realistic, context-aware dynamics for both the subject and the surrounding environment. Building on prior approaches centered on human pose control, X-Dyna addresses key shortcomings causing the loss of dynamic details, enhancing the lifelike qualities of human video animations. At the core of our approach is the Dynamics-Adapter, a lightweight module that effectively integrates reference appearance context into the spatial attentions of the diffusion backbone while preserving the capacity of motion modules in synthesizing fluid and intricate dynamic details. Beyond body pose control, we connect a local control module with our model to capture identity-disentangled facial expressions, facilitating accurate expression transfer for enhanced realism in animated scenes. Together, these components form a unified framework capable of learning physical human motion and natural scene dynamics from a diverse blend of human and scene videos. Comprehensive qualitative and quantitative evaluations demonstrate that X-Dyna outperforms state-of-the-art methods, creating highly lifelike and expressive animations. The code is available at https://github.com/bytedance/X-Dyna.
<div id='section'>Paperid: <span id='pid'>74, <a href='https://arxiv.org/pdf/2501.08331.pdf' target='_blank'>https://arxiv.org/pdf/2501.08331.pdf</a></span>   <span><a href='https://github.com/Eyeline-Labs/Go-with-the-Flow' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ryan Burgert, Yuancheng Xu, Wenqi Xian, Oliver Pilarski, Pascal Clausen, Mingming He, Li Ma, Yitong Deng, Lingxiao Li, Mohsen Mousavi, Michael Ryoo, Paul Debevec, Ning Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.08331">Go-with-the-Flow: Motion-Controllable Video Diffusion Models Using Real-Time Warped Noise</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generative modeling aims to transform random noise into structured outputs. In this work, we enhance video diffusion models by allowing motion control via structured latent noise sampling. This is achieved by just a change in data: we pre-process training videos to yield structured noise. Consequently, our method is agnostic to diffusion model design, requiring no changes to model architectures or training pipelines. Specifically, we propose a novel noise warping algorithm, fast enough to run in real time, that replaces random temporal Gaussianity with correlated warped noise derived from optical flow fields, while preserving the spatial Gaussianity. The efficiency of our algorithm enables us to fine-tune modern video diffusion base models using warped noise with minimal overhead, and provide a one-stop solution for a wide range of user-friendly motion control: local object motion control, global camera movement control, and motion transfer. The harmonization between temporal coherence and spatial Gaussianity in our warped noise leads to effective motion control while maintaining per-frame pixel quality. Extensive experiments and user studies demonstrate the advantages of our method, making it a robust and scalable approach for controlling motion in video diffusion models. Video results are available on our webpage: https://eyeline-labs.github.io/Go-with-the-Flow. Source code and model checkpoints are available on GitHub: https://github.com/Eyeline-Labs/Go-with-the-Flow.
<div id='section'>Paperid: <span id='pid'>75, <a href='https://arxiv.org/pdf/2501.02530.pdf' target='_blank'>https://arxiv.org/pdf/2501.02530.pdf</a></span>   <span><a href='https://github.com/henryhcliu/udmc_carla.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haichao Liu, Kai Chen, Yulin Li, Zhenmin Huang, Ming Liu, Jun Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.02530">UDMC: Unified Decision-Making and Control Framework for Urban Autonomous Driving with Motion Prediction of Traffic Participants</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current autonomous driving systems often struggle to balance decision-making and motion control while ensuring safety and traffic rule compliance, especially in complex urban environments. Existing methods may fall short due to separate handling of these functionalities, leading to inefficiencies and safety compromises. To address these challenges, we introduce UDMC, an interpretable and unified Level 4 autonomous driving framework. UDMC integrates decision-making and motion control into a single optimal control problem (OCP), considering the dynamic interactions with surrounding vehicles, pedestrians, road lanes, and traffic signals. By employing innovative potential functions to model traffic participants and regulations, and incorporating a specialized motion prediction module, our framework enhances on-road safety and rule adherence. The integrated design allows for real-time execution of flexible maneuvers suited to diverse driving scenarios. High-fidelity simulations conducted in CARLA exemplify the framework's computational efficiency, robustness, and safety, resulting in superior driving performance when compared against various baseline models. Our open-source project is available at https://github.com/henryhcliu/udmc_carla.git.
<div id='section'>Paperid: <span id='pid'>76, <a href='https://arxiv.org/pdf/2412.17210.pdf' target='_blank'>https://arxiv.org/pdf/2412.17210.pdf</a></span>   <span><a href='https://github.com/guijiejie/DCMD-main' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongsong Wang, Andi Xu, Pinle Ding, Jie Gui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.17210">Dual Conditioned Motion Diffusion for Pose-Based Video Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video Anomaly Detection (VAD) is essential for computer vision research. Existing VAD methods utilize either reconstruction-based or prediction-based frameworks. The former excels at detecting irregular patterns or structures, whereas the latter is capable of spotting abnormal deviations or trends. We address pose-based video anomaly detection and introduce a novel framework called Dual Conditioned Motion Diffusion (DCMD), which enjoys the advantages of both approaches. The DCMD integrates conditioned motion and conditioned embedding to comprehensively utilize the pose characteristics and latent semantics of observed movements, respectively. In the reverse diffusion process, a motion transformer is proposed to capture potential correlations from multi-layered characteristics within the spectrum space of human motion. To enhance the discriminability between normal and abnormal instances, we design a novel United Association Discrepancy (UAD) regularization that primarily relies on a Gaussian kernel-based time association and a self-attention-based global association. Finally, a mask completion strategy is introduced during the inference stage of the reverse diffusion process to enhance the utilization of conditioned motion for the prediction branch of anomaly detection. Extensive experiments on four datasets demonstrate that our method dramatically outperforms state-of-the-art methods and exhibits superior generalization performance.
<div id='section'>Paperid: <span id='pid'>77, <a href='https://arxiv.org/pdf/2412.13729.pdf' target='_blank'>https://arxiv.org/pdf/2412.13729.pdf</a></span>   <span><a href='https://github.com/tmralmeida/thor-magni-actions' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tiago Rodrigues de Almeida, Tim Schreiter, Andrey Rudenko, Luigi Palmieiri, Johannes A. Stork, Achim J. Lilienthal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.13729">THÃR-MAGNI Act: Actions for Human Motion Modeling in Robot-Shared Industrial Spaces</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate human activity and trajectory prediction are crucial for ensuring safe and reliable human-robot interactions in dynamic environments, such as industrial settings, with mobile robots. Datasets with fine-grained action labels for moving people in industrial environments with mobile robots are scarce, as most existing datasets focus on social navigation in public spaces. This paper introduces the THÃR-MAGNI Act dataset, a substantial extension of the THÃR-MAGNI dataset, which captures participant movements alongside robots in diverse semantic and spatial contexts. THÃR-MAGNI Act provides 8.3 hours of manually labeled participant actions derived from egocentric videos recorded via eye-tracking glasses. These actions, aligned with the provided THÃR-MAGNI motion cues, follow a long-tailed distribution with diversified acceleration, velocity, and navigation distance profiles. We demonstrate the utility of THÃR-MAGNI Act for two tasks: action-conditioned trajectory prediction and joint action and trajectory prediction. We propose two efficient transformer-based models that outperform the baselines to address these tasks. These results underscore the potential of THÃR-MAGNI Act to develop predictive models for enhanced human-robot interaction in complex environments.
<div id='section'>Paperid: <span id='pid'>78, <a href='https://arxiv.org/pdf/2412.11495.pdf' target='_blank'>https://arxiv.org/pdf/2412.11495.pdf</a></span>   <span><a href='https://github.com/ShiqiYu/OpenGait' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongyang Jin, Chao Fan, Weihua Chen, Shiqi Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.11495">Exploring More from Multiple Gait Modalities for Human Identification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The gait, as a kind of soft biometric characteristic, can reflect the distinct walking patterns of individuals at a distance, exhibiting a promising technique for unrestrained human identification. With largely excluding gait-unrelated cues hidden in RGB videos, the silhouette and skeleton, though visually compact, have acted as two of the most prevailing gait modalities for a long time. Recently, several attempts have been made to introduce more informative data forms like human parsing and optical flow images to capture gait characteristics, along with multi-branch architectures. However, due to the inconsistency within model designs and experiment settings, we argue that a comprehensive and fair comparative study among these popular gait modalities, involving the representational capacity and fusion strategy exploration, is still lacking. From the perspectives of fine vs. coarse-grained shape and whole vs. pixel-wise motion modeling, this work presents an in-depth investigation of three popular gait representations, i.e., silhouette, human parsing, and optical flow, with various fusion evaluations, and experimentally exposes their similarities and differences. Based on the obtained insights, we further develop a C$^2$Fusion strategy, consequently building our new framework MultiGait++. C$^2$Fusion preserves commonalities while highlighting differences to enrich the learning of gait features. To verify our findings and conclusions, extensive experiments on Gait3D, GREW, CCPG, and SUSTech1K are conducted. The code is available at https://github.com/ShiqiYu/OpenGait.
<div id='section'>Paperid: <span id='pid'>79, <a href='https://arxiv.org/pdf/2412.11193.pdf' target='_blank'>https://arxiv.org/pdf/2412.11193.pdf</a></span>   <span><a href='https://github.com/qinghuannn/light-t2m' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ling-An Zeng, Guohong Huang, Gaojie Wu, Wei-Shi Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.11193">Light-T2M: A Lightweight and Fast Model for Text-to-motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite the significant role text-to-motion (T2M) generation plays across various applications, current methods involve a large number of parameters and suffer from slow inference speeds, leading to high usage costs. To address this, we aim to design a lightweight model to reduce usage costs. First, unlike existing works that focus solely on global information modeling, we recognize the importance of local information modeling in the T2M task by reconsidering the intrinsic properties of human motion, leading us to propose a lightweight Local Information Modeling Module. Second, we introduce Mamba to the T2M task, reducing the number of parameters and GPU memory demands, and we have designed a novel Pseudo-bidirectional Scan to replicate the effects of a bidirectional scan without increasing parameter count. Moreover, we propose a novel Adaptive Textual Information Injector that more effectively integrates textual information into the motion during generation. By integrating the aforementioned designs, we propose a lightweight and fast model named Light-T2M. Compared to the state-of-the-art method, MoMask, our Light-T2M model features just 10\% of the parameters (4.48M vs 44.85M) and achieves a 16\% faster inference time (0.152s vs 0.180s), while surpassing MoMask with an FID of \textbf{0.040} (vs. 0.045) on HumanML3D dataset and 0.161 (vs. 0.228) on KIT-ML dataset. The code is available at https://github.com/qinghuannn/light-t2m.
<div id='section'>Paperid: <span id='pid'>80, <a href='https://arxiv.org/pdf/2412.08643.pdf' target='_blank'>https://arxiv.org/pdf/2412.08643.pdf</a></span>   <span><a href='https://github.com/wzzheng/GPD' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/wzzheng/GPD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zixun Xie, Sicheng Zuo, Wenzhao Zheng, Yunpeng Zhang, Dalong Du, Jie Zhou, Jiwen Lu, Shanghang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.08643">GPD-1: Generative Pre-training for Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modeling the evolutions of driving scenarios is important for the evaluation and decision-making of autonomous driving systems. Most existing methods focus on one aspect of scene evolution such as map generation, motion prediction, and trajectory planning. In this paper, we propose a unified Generative Pre-training for Driving (GPD-1) model to accomplish all these tasks altogether without additional fine-tuning. We represent each scene with ego, agent, and map tokens and formulate autonomous driving as a unified token generation problem. We adopt the autoregressive transformer architecture and use a scene-level attention mask to enable intra-scene bi-directional interactions. For the ego and agent tokens, we propose a hierarchical positional tokenizer to effectively encode both 2D positions and headings. For the map tokens, we train a map vector-quantized autoencoder to efficiently compress ego-centric semantic maps into discrete tokens. We pre-train our GPD-1 on the large-scale nuPlan dataset and conduct extensive experiments to evaluate its effectiveness. With different prompts, our GPD-1 successfully generalizes to various tasks without finetuning, including scene generation, traffic simulation, closed-loop simulation, map prediction, and motion planning. Code: https://github.com/wzzheng/GPD.
<div id='section'>Paperid: <span id='pid'>81, <a href='https://arxiv.org/pdf/2412.01343.pdf' target='_blank'>https://arxiv.org/pdf/2412.01343.pdf</a></span>   <span><a href='https://github.com/XiaominLi1997/MoTrans' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaomin Li, Xu Jia, Qinghe Wang, Haiwen Diao, Mengmeng Ge, Pengxiang Li, You He, Huchuan Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.01343">MoTrans: Customized Motion Transfer with Text-driven Video Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing pretrained text-to-video (T2V) models have demonstrated impressive abilities in generating realistic videos with basic motion or camera movement. However, these models exhibit significant limitations when generating intricate, human-centric motions. Current efforts primarily focus on fine-tuning models on a small set of videos containing a specific motion. They often fail to effectively decouple motion and the appearance in the limited reference videos, thereby weakening the modeling capability of motion patterns. To this end, we propose MoTrans, a customized motion transfer method enabling video generation of similar motion in new context. Specifically, we introduce a multimodal large language model (MLLM)-based recaptioner to expand the initial prompt to focus more on appearance and an appearance injection module to adapt appearance prior from video frames to the motion modeling process. These complementary multimodal representations from recaptioned prompt and video frames promote the modeling of appearance and facilitate the decoupling of appearance and motion. In addition, we devise a motion-specific embedding for further enhancing the modeling of the specific motion. Experimental results demonstrate that our method effectively learns specific motion pattern from singular or multiple reference videos, performing favorably against existing methods in customized video generation.
<div id='section'>Paperid: <span id='pid'>82, <a href='https://arxiv.org/pdf/2412.01179.pdf' target='_blank'>https://arxiv.org/pdf/2412.01179.pdf</a></span>   <span><a href='https://github.com/TangTao-PKU/DGTR' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/TangTao-PKU/DGTR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tao Tang, Hong Liu, Yingxuan You, Ti Wang, Wenhao Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.01179">Dual-Branch Graph Transformer Network for 3D Human Mesh Reconstruction from Video</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human Mesh Reconstruction (HMR) from monocular video plays an important role in human-robot interaction and collaboration. However, existing video-based human mesh reconstruction methods face a trade-off between accurate reconstruction and smooth motion. These methods design networks based on either RNNs or attention mechanisms to extract local temporal correlations or global temporal dependencies, but the lack of complementary long-term information and local details limits their performance. To address this problem, we propose a \textbf{D}ual-branch \textbf{G}raph \textbf{T}ransformer network for 3D human mesh \textbf{R}econstruction from video, named DGTR. DGTR employs a dual-branch network including a Global Motion Attention (GMA) branch and a Local Details Refine (LDR) branch to parallelly extract long-term dependencies and local crucial information, helping model global human motion and local human details (e.g., local motion, tiny movement). Specifically, GMA utilizes a global transformer to model long-term human motion. LDR combines modulated graph convolutional networks and the transformer framework to aggregate local information in adjacent frames and extract crucial information of human details. Experiments demonstrate that our DGTR outperforms state-of-the-art video-based methods in reconstruction accuracy and maintains competitive motion smoothness. Moreover, DGTR utilizes fewer parameters and FLOPs, which validate the effectiveness and efficiency of the proposed DGTR. Code is publicly available at \href{https://github.com/TangTao-PKU/DGTR}{\textcolor{myBlue}{https://github.com/TangTao-PKU/DGTR}}.
<div id='section'>Paperid: <span id='pid'>83, <a href='https://arxiv.org/pdf/2412.00420.pdf' target='_blank'>https://arxiv.org/pdf/2412.00420.pdf</a></span>   <span><a href='https://github.com/vita-epfl/TAROT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lan Feng, Fan Nie, Yuejiang Liu, Alexandre Alahi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.00420">TAROT: Targeted Data Selection via Optimal Transport</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose TAROT, a targeted data selection framework grounded in optimal transport theory. Previous targeted data selection methods primarily rely on influence-based greedy heuristics to enhance domain-specific performance. While effective on limited, unimodal data (i.e., data following a single pattern), these methods struggle as target data complexity increases. Specifically, in multimodal distributions, these heuristics fail to account for multiple inherent patterns, leading to suboptimal data selection. This work identifies two primary factors contributing to this limitation: (i) the disproportionate impact of dominant feature components in high-dimensional influence estimation, and (ii) the restrictive linear additive assumptions inherent in greedy selection strategies. To address these challenges, TAROT incorporates whitened feature distance to mitigate dominant feature bias, providing a more reliable measure of data influence. Building on this, TAROT uses whitened feature distance to quantify and minimize the optimal transport distance between the selected data and target domains. Notably, this minimization also facilitates the estimation of optimal selection ratios. We evaluate TAROT across multiple tasks, including semantic segmentation, motion prediction, and instruction tuning. Results consistently show that TAROT outperforms state-of-the-art methods, highlighting its versatility across various deep learning tasks. Code is available at https://github.com/vita-epfl/TAROT.
<div id='section'>Paperid: <span id='pid'>84, <a href='https://arxiv.org/pdf/2411.17383.pdf' target='_blank'>https://arxiv.org/pdf/2411.17383.pdf</a></span>   <span><a href='https://github.com/cangcz/AnchorCrafter' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyi Xu, Ziyao Huang, Juan Cao, Yong Zhang, Xiaodong Cun, Qing Shuai, Yuchen Wang, Linchao Bao, Jintao Li, Fan Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.17383">AnchorCrafter: Animate Cyber-Anchors Selling Your Products via Human-Object Interacting Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The generation of anchor-style product promotion videos presents promising opportunities in e-commerce, advertising, and consumer engagement. Despite advancements in pose-guided human video generation, creating product promotion videos remains challenging. In addressing this challenge, we identify the integration of human-object interactions (HOI) into pose-guided human video generation as a core issue. To this end, we introduce AnchorCrafter, a novel diffusion-based system designed to generate 2D videos featuring a target human and a customized object, achieving high visual fidelity and controllable interactions. Specifically, we propose two key innovations: the HOI-appearance perception, which enhances object appearance recognition from arbitrary multi-view perspectives and disentangles object and human appearance, and the HOI-motion injection, which enables complex human-object interactions by overcoming challenges in object trajectory conditioning and inter-occlusion management. Extensive experiments show that our system improves object appearance preservation by 7.5\% and doubles the object localization accuracy compared to existing state-of-the-art approaches. It also outperforms existing approaches in maintaining human motion consistency and high-quality video generation. Project page including data, code, and Huggingface demo: https://github.com/cangcz/AnchorCrafter.
<div id='section'>Paperid: <span id='pid'>85, <a href='https://arxiv.org/pdf/2411.16805.pdf' target='_blank'>https://arxiv.org/pdf/2411.16805.pdf</a></span>   <span><a href='https://github.com/ILGLJ/LLaMo' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lei Li, Sen Jia, Jianhao Wang, Zhongyu Jiang, Feng Zhou, Ju Dai, Tianfang Zhang, Zongkai Wu, Jenq-Neng Hwang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.16805">Human Motion Instruction Tuning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents LLaMo (Large Language and Human Motion Assistant), a multimodal framework for human motion instruction tuning. In contrast to conventional instruction-tuning approaches that convert non-linguistic inputs, such as video or motion sequences, into language tokens, LLaMo retains motion in its native form for instruction tuning. This method preserves motion-specific details that are often diminished in tokenization, thereby improving the model's ability to interpret complex human behaviors. By processing both video and motion data alongside textual inputs, LLaMo enables a flexible, human-centric analysis. Experimental evaluations across high-complexity domains, including human behaviors and professional activities, indicate that LLaMo effectively captures domain-specific knowledge, enhancing comprehension and prediction in motion-intensive scenarios. We hope LLaMo offers a foundation for future multimodal AI systems with broad applications, from sports analytics to behavioral prediction. Our code and models are available on the project website: https://github.com/ILGLJ/LLaMo.
<div id='section'>Paperid: <span id='pid'>86, <a href='https://arxiv.org/pdf/2411.16758.pdf' target='_blank'>https://arxiv.org/pdf/2411.16758.pdf</a></span>   <span><a href='https://github.com/MyNiuuu/BAGA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Muyao Niu, Yifan Zhan, Qingtian Zhu, Zhuoxiao Li, Wei Wang, Zhihang Zhong, Xiao Sun, Yinqiang Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.16758">Bundle Adjusted Gaussian Avatars Deblurring</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The development of 3D human avatars from multi-view videos represents a significant yet challenging task in the field. Recent advancements, including 3D Gaussian Splattings (3DGS), have markedly progressed this domain. Nonetheless, existing techniques necessitate the use of high-quality sharp images, which are often impractical to obtain in real-world settings due to variations in human motion speed and intensity. In this study, we attempt to explore deriving sharp intrinsic 3D human Gaussian avatars from blurry video footage in an end-to-end manner. Our approach encompasses a 3D-aware, physics-oriented model of blur formation attributable to human movement, coupled with a 3D human motion model to clarify ambiguities found in motion-induced blurry images. This methodology facilitates the concurrent learning of avatar model parameters and the refinement of sub-frame motion parameters from a coarse initialization. We have established benchmarks for this task through a synthetic dataset derived from existing multi-view captures, alongside a real-captured dataset acquired through a 360-degree synchronous hybrid-exposure camera system. Comprehensive evaluations demonstrate that our model surpasses existing baselines.
<div id='section'>Paperid: <span id='pid'>87, <a href='https://arxiv.org/pdf/2411.13079.pdf' target='_blank'>https://arxiv.org/pdf/2411.13079.pdf</a></span>   <span><a href='https://github.com/thu-uav/NeuralIMC' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Feng Gao, Chao Yu, Yu Wang, Yi Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.13079">Neural Internal Model Control: Learning a Robust Control Policy via Predictive Error Feedback</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate motion control in the face of disturbances within complex environments remains a major challenge in robotics. Classical model-based approaches often struggle with nonlinearities and unstructured disturbances, while RL-based methods can be fragile when encountering unseen scenarios. In this paper, we propose a novel framework, Neural Internal Model Control, which integrates model-based control with RL-based control to enhance robustness. Our framework streamlines the predictive model by applying Newton-Euler equations for rigid-body dynamics, eliminating the need to capture complex high-dimensional nonlinearities. This internal model combines model-free RL algorithms with predictive error feedback. Such a design enables a closed-loop control structure to enhance the robustness and generalizability of the control system. We demonstrate the effectiveness of our framework on both quadrotors and quadrupedal robots, achieving superior performance compared to state-of-the-art methods. Furthermore, real-world deployment on a quadrotor with rope-suspended payloads highlights the framework's robustness in sim-to-real transfer. Our code is released at https://github.com/thu-uav/NeuralIMC.
<div id='section'>Paperid: <span id='pid'>88, <a href='https://arxiv.org/pdf/2411.02673.pdf' target='_blank'>https://arxiv.org/pdf/2411.02673.pdf</a></span>   <span><a href='https://github.com/vita-epfl/multi-transmotion' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yang Gao, Po-Chien Luan, Alexandre Alahi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.02673">Multi-Transmotion: Pre-trained Model for Human Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The ability of intelligent systems to predict human behaviors is crucial, particularly in fields such as autonomous vehicle navigation and social robotics. However, the complexity of human motion have prevented the development of a standardized dataset for human motion prediction, thereby hindering the establishment of pre-trained models. In this paper, we address these limitations by integrating multiple datasets, encompassing both trajectory and 3D pose keypoints, to propose a pre-trained model for human motion prediction. We merge seven distinct datasets across varying modalities and standardize their formats. To facilitate multimodal pre-training, we introduce Multi-Transmotion, an innovative transformer-based model designed for cross-modality pre-training. Additionally, we present a novel masking strategy to capture rich representations. Our methodology demonstrates competitive performance across various datasets on several downstream tasks, including trajectory prediction in the NBA and JTA datasets, as well as pose prediction in the AMASS and 3DPW datasets. The code is publicly available: https://github.com/vita-epfl/multi-transmotion
<div id='section'>Paperid: <span id='pid'>89, <a href='https://arxiv.org/pdf/2411.01814.pdf' target='_blank'>https://arxiv.org/pdf/2411.01814.pdf</a></span>   <span><a href='https://github.com/thanhnguyencanh/SGan-TEB.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Thanh Nguyen Canh, Xiem HoangVan, Nak Young Chong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.01814">Enhancing Social Robot Navigation with Integrated Motion Prediction and Trajectory Planning in Dynamic Human Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Navigating safely in dynamic human environments is crucial for mobile service robots, and social navigation is a key aspect of this process. In this paper, we proposed an integrative approach that combines motion prediction and trajectory planning to enable safe and socially-aware robot navigation. The main idea of the proposed method is to leverage the advantages of Socially Acceptable trajectory prediction and Timed Elastic Band (TEB) by incorporating human interactive information including position, orientation, and motion into the objective function of the TEB algorithms. In addition, we designed social constraints to ensure the safety of robot navigation. The proposed system is evaluated through physical simulation using both quantitative and qualitative metrics, demonstrating its superior performance in avoiding human and dynamic obstacles, thereby ensuring safe navigation. The implementations are open source at: \url{https://github.com/thanhnguyencanh/SGan-TEB.git}
<div id='section'>Paperid: <span id='pid'>90, <a href='https://arxiv.org/pdf/2410.24219.pdf' target='_blank'>https://arxiv.org/pdf/2410.24219.pdf</a></span>   <span><a href='https://github.com/PR-Ryan/DEMO' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Penghui Ruan, Pichao Wang, Divya Saxena, Jiannong Cao, Yuhui Shi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.24219">Enhancing Motion in Text-to-Video Generation with Decomposed Encoding and Conditioning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite advancements in Text-to-Video (T2V) generation, producing videos with realistic motion remains challenging. Current models often yield static or minimally dynamic outputs, failing to capture complex motions described by text. This issue stems from the internal biases in text encoding, which overlooks motions, and inadequate conditioning mechanisms in T2V generation models. To address this, we propose a novel framework called DEcomposed MOtion (DEMO), which enhances motion synthesis in T2V generation by decomposing both text encoding and conditioning into content and motion components. Our method includes a content encoder for static elements and a motion encoder for temporal dynamics, alongside separate content and motion conditioning mechanisms. Crucially, we introduce text-motion and video-motion supervision to improve the model's understanding and generation of motion. Evaluations on benchmarks such as MSR-VTT, UCF-101, WebVid-10M, EvalCrafter, and VBench demonstrate DEMO's superior ability to produce videos with enhanced motion dynamics while maintaining high visual quality. Our approach significantly advances T2V generation by integrating comprehensive motion understanding directly from textual descriptions. Project page: https://PR-Ryan.github.io/DEMO-project/
<div id='section'>Paperid: <span id='pid'>91, <a href='https://arxiv.org/pdf/2410.20986.pdf' target='_blank'>https://arxiv.org/pdf/2410.20986.pdf</a></span>   <span><a href='https://github.com/abcyzj/MeshRet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zijie Ye, Jia-Wei Liu, Jia Jia, Shikun Sun, Mike Zheng Shou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.20986">Skinned Motion Retargeting with Dense Geometric Interaction Perception</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Capturing and maintaining geometric interactions among different body parts is crucial for successful motion retargeting in skinned characters. Existing approaches often overlook body geometries or add a geometry correction stage after skeletal motion retargeting. This results in conflicts between skeleton interaction and geometry correction, leading to issues such as jittery, interpenetration, and contact mismatches. To address these challenges, we introduce a new retargeting framework, MeshRet, which directly models the dense geometric interactions in motion retargeting. Initially, we establish dense mesh correspondences between characters using semantically consistent sensors (SCS), effective across diverse mesh topologies. Subsequently, we develop a novel spatio-temporal representation called the dense mesh interaction (DMI) field. This field, a collection of interacting SCS feature vectors, skillfully captures both contact and non-contact interactions between body geometries. By aligning the DMI field during retargeting, MeshRet not only preserves motion semantics but also prevents self-interpenetration and ensures contact preservation. Extensive experiments on the public Mixamo dataset and our newly-collected ScanRet dataset demonstrate that MeshRet achieves state-of-the-art performance. Code available at https://github.com/abcyzj/MeshRet.
<div id='section'>Paperid: <span id='pid'>92, <a href='https://arxiv.org/pdf/2410.16864.pdf' target='_blank'>https://arxiv.org/pdf/2410.16864.pdf</a></span>   <span><a href='https://github.com/dmytrozabolotnii/autoware_mini' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dmytro Zabolotnii, Yar Muhammad, Naveed Muhammad
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.16864">Pedestrian motion prediction evaluation for urban autonomous driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pedestrian motion prediction is a key part of the modular-based autonomous driving pipeline, ensuring safe, accurate, and timely awareness of human agents' possible future trajectories. The autonomous vehicle can use this information to prevent any possible accidents and create a comfortable and pleasant driving experience for the passengers and pedestrians. A wealth of research was done on the topic from the authors of robotics, computer vision, intelligent transportation systems, and other fields. However, a relatively unexplored angle is the integration of the state-of-art solutions into existing autonomous driving stacks and evaluating them in real-life conditions rather than sanitized datasets. We analyze selected publications with provided open-source solutions and provide a perspective obtained by integrating them into existing Autonomous Driving framework - Autoware Mini and performing experiments in natural urban conditions in Tartu, Estonia to determine valuability of traditional motion prediction metrics. This perspective should be valuable to any potential autonomous driving or robotics engineer looking for the real-world performance of the existing state-of-art pedestrian motion prediction problem. The code with instructions on accessing the dataset is available at https://github.com/dmytrozabolotnii/autoware_mini.
<div id='section'>Paperid: <span id='pid'>93, <a href='https://arxiv.org/pdf/2410.15819.pdf' target='_blank'>https://arxiv.org/pdf/2410.15819.pdf</a></span>   <span><a href='https://github.com/Cing2/LiMTR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Camiel Oerlemans, Bram Grooten, Michiel Braat, Alaa Alassi, Emilia Silvas, Decebal Constantin Mocanu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.15819">LiMTR: Time Series Motion Prediction for Diverse Road Users through Multimodal Feature Integration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Predicting the behavior of road users accurately is crucial to enable the safe operation of autonomous vehicles in urban or densely populated areas. Therefore, there has been a growing interest in time series motion prediction research, leading to significant advancements in state-of-the-art techniques in recent years. However, the potential of using LiDAR data to capture more detailed local features, such as a person's gaze or posture, remains largely unexplored. To address this, we develop a novel multimodal approach for motion prediction based on the PointNet foundation model architecture, incorporating local LiDAR features. Evaluation on the Waymo Open Dataset shows a performance improvement of 6.20% and 1.58% in minADE and mAP respectively, when integrated and compared with the previous state-of-the-art MTR. We open-source the code of our LiMTR model.
<div id='section'>Paperid: <span id='pid'>94, <a href='https://arxiv.org/pdf/2410.15582.pdf' target='_blank'>https://arxiv.org/pdf/2410.15582.pdf</a></span>   <span><a href='https://github.com/TangTao-PKU/ARTS' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/TangTao-PKU/ARTS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tao Tang, Hong Liu, Yingxuan You, Ti Wang, Wenhao Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.15582">ARTS: Semi-Analytical Regressor using Disentangled Skeletal Representations for Human Mesh Recovery from Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Although existing video-based 3D human mesh recovery methods have made significant progress, simultaneously estimating human pose and shape from low-resolution image features limits their performance. These image features lack sufficient spatial information about the human body and contain various noises (e.g., background, lighting, and clothing), which often results in inaccurate pose and inconsistent motion. Inspired by the rapid advance in human pose estimation, we discover that compared to image features, skeletons inherently contain accurate human pose and motion. Therefore, we propose a novel semiAnalytical Regressor using disenTangled Skeletal representations for human mesh recovery from videos, called ARTS. Specifically, a skeleton estimation and disentanglement module is proposed to estimate the 3D skeletons from a video and decouple them into disentangled skeletal representations (i.e., joint position, bone length, and human motion). Then, to fully utilize these representations, we introduce a semi-analytical regressor to estimate the parameters of the human mesh model. The regressor consists of three modules: Temporal Inverse Kinematics (TIK), Bone-guided Shape Fitting (BSF), and Motion-Centric Refinement (MCR). TIK utilizes joint position to estimate initial pose parameters and BSF leverages bone length to regress bone-aligned shape parameters. Finally, MCR combines human motion representation with image features to refine the initial human model parameters. Extensive experiments demonstrate that our ARTS surpasses existing state-of-the-art video-based methods in both per-frame accuracy and temporal consistency on popular benchmarks: 3DPW, MPI-INF-3DHP, and Human3.6M. Code is available at https://github.com/TangTao-PKU/ARTS.
<div id='section'>Paperid: <span id='pid'>95, <a href='https://arxiv.org/pdf/2410.13790.pdf' target='_blank'>https://arxiv.org/pdf/2410.13790.pdf</a></span>   <span><a href='https://github.com/liangxuy/MotionBank' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Liang Xu, Shaoyang Hua, Zili Lin, Yifan Liu, Feipeng Ma, Yichao Yan, Xin Jin, Xiaokang Yang, Wenjun Zeng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.13790">MotionBank: A Large-scale Video Motion Benchmark with Disentangled Rule-based Annotations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we tackle the problem of how to build and benchmark a large motion model (LMM). The ultimate goal of LMM is to serve as a foundation model for versatile motion-related tasks, e.g., human motion generation, with interpretability and generalizability. Though advanced, recent LMM-related works are still limited by small-scale motion data and costly text descriptions. Besides, previous motion benchmarks primarily focus on pure body movements, neglecting the ubiquitous motions in context, i.e., humans interacting with humans, objects, and scenes. To address these limitations, we consolidate large-scale video action datasets as knowledge banks to build MotionBank, which comprises 13 video action datasets, 1.24M motion sequences, and 132.9M frames of natural and diverse human motions. Different from laboratory-captured motions, in-the-wild human-centric videos contain abundant motions in context. To facilitate better motion text alignment, we also meticulously devise a motion caption generation algorithm to automatically produce rule-based, unbiased, and disentangled text descriptions via the kinematic characteristics for each motion. Extensive experiments show that our MotionBank is beneficial for general motion-related tasks of human motion generation, motion in-context generation, and motion understanding. Video motions together with the rule-based text annotations could serve as an efficient alternative for larger LMMs. Our dataset, codes, and benchmark will be publicly available at https://github.com/liangxuy/MotionBank.
<div id='section'>Paperid: <span id='pid'>96, <a href='https://arxiv.org/pdf/2410.08669.pdf' target='_blank'>https://arxiv.org/pdf/2410.08669.pdf</a></span>   <span><a href='https://github.com/youngzhou1999/SmartPretrain' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yang Zhou, Hao Shao, Letian Wang, Steven L. Waslander, Hongsheng Li, Yu Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.08669">SmartPretrain: Model-Agnostic and Dataset-Agnostic Representation Learning for Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Predicting the future motion of surrounding agents is essential for autonomous vehicles (AVs) to operate safely in dynamic, human-robot-mixed environments. However, the scarcity of large-scale driving datasets has hindered the development of robust and generalizable motion prediction models, limiting their ability to capture complex interactions and road geometries. Inspired by recent advances in natural language processing (NLP) and computer vision (CV), self-supervised learning (SSL) has gained significant attention in the motion prediction community for learning rich and transferable scene representations. Nonetheless, existing pre-training methods for motion prediction have largely focused on specific model architectures and single dataset, limiting their scalability and generalizability. To address these challenges, we propose SmartPretrain, a general and scalable SSL framework for motion prediction that is both model-agnostic and dataset-agnostic. Our approach integrates contrastive and reconstructive SSL, leveraging the strengths of both generative and discriminative paradigms to effectively represent spatiotemporal evolution and interactions without imposing architectural constraints. Additionally, SmartPretrain employs a dataset-agnostic scenario sampling strategy that integrates multiple datasets, enhancing data volume, diversity, and robustness. Extensive experiments on multiple datasets demonstrate that SmartPretrain consistently improves the performance of state-of-the-art prediction models across datasets, data splits and main metrics. For instance, SmartPretrain significantly reduces the MissRate of Forecast-MAE by 10.6%. These results highlight SmartPretrain's effectiveness as a unified, scalable solution for motion prediction, breaking free from the limitations of the small-data regime. Codes are available at https://github.com/youngzhou1999/SmartPretrain
<div id='section'>Paperid: <span id='pid'>97, <a href='https://arxiv.org/pdf/2410.07795.pdf' target='_blank'>https://arxiv.org/pdf/2410.07795.pdf</a></span>   <span><a href='https://github.com/cuongle1206/OSDCap' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Cuong Le, Viktor Johansson, Manon Kok, Bastian Wandt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.07795">Optimal-state Dynamics Estimation for Physics-based Human Motion Capture from Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion capture from monocular videos has made significant progress in recent years. However, modern approaches often produce temporal artifacts, e.g. in form of jittery motion and struggle to achieve smooth and physically plausible motions. Explicitly integrating physics, in form of internal forces and exterior torques, helps alleviating these artifacts. Current state-of-the-art approaches make use of an automatic PD controller to predict torques and reaction forces in order to re-simulate the input kinematics, i.e. the joint angles of a predefined skeleton. However, due to imperfect physical models, these methods often require simplifying assumptions and extensive preprocessing of the input kinematics to achieve good performance. To this end, we propose a novel method to selectively incorporate the physics models with the kinematics observations in an online setting, inspired by a neural Kalman-filtering approach. We develop a control loop as a meta-PD controller to predict internal joint torques and external reaction forces, followed by a physics-based motion simulation. A recurrent neural network is introduced to realize a Kalman filter that attentively balances the kinematics input and simulated motion, resulting in an optimal-state dynamics prediction. We show that this filtering step is crucial to provide an online supervision that helps balancing the shortcoming of the respective input motions, thus being important for not only capturing accurate global motion trajectories but also producing physically plausible human poses. The proposed approach excels in the physics-based human pose estimation task and demonstrates the physical plausibility of the predictive dynamics, compared to state of the art. The code is available on https://github.com/cuongle1206/OSDCap
<div id='section'>Paperid: <span id='pid'>98, <a href='https://arxiv.org/pdf/2409.19911.pdf' target='_blank'>https://arxiv.org/pdf/2409.19911.pdf</a></span>   <span><a href='https://github.com/ali-vilab/UniAnimate-DiT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiang Wang, Shiwei Zhang, Haonan Qiu, Ruihang Chu, Zekun Li, Yingya Zhang, Changxin Gao, Yuehuan Wang, Chunhua Shen, Nong Sang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.19911">Replace Anyone in Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The field of controllable human-centric video generation has witnessed remarkable progress, particularly with the advent of diffusion models. However, achieving precise and localized control over human motion in videos, such as replacing or inserting individuals while preserving desired motion patterns, still remains a formidable challenge. In this work, we present the ReplaceAnyone framework, which focuses on localized human replacement and insertion featuring intricate backgrounds. Specifically, we formulate this task as an image-conditioned video inpainting paradigm with pose guidance, utilizing a unified end-to-end video diffusion architecture that facilitates image-conditioned video inpainting within masked regions. To prevent shape leakage and enable granular local control, we introduce diverse mask forms involving both regular and irregular shapes. Furthermore, we implement an enriched visual guidance mechanism to enhance appearance alignment, a hybrid inpainting encoder to further preserve the detailed background information in the masked video, and a two-phase optimization methodology to simplify the training difficulty. ReplaceAnyone enables seamless replacement or insertion of characters while maintaining the desired pose motion and reference appearance within a single framework. Extensive experimental results demonstrate the effectiveness of our method in generating realistic and coherent video content. The proposed ReplaceAnyone can be seamlessly applied not only to traditional 3D-UNet base models but also to DiT-based video models such as Wan2.1. The code will be available at https://github.com/ali-vilab/UniAnimate-DiT.
<div id='section'>Paperid: <span id='pid'>99, <a href='https://arxiv.org/pdf/2409.18399.pdf' target='_blank'>https://arxiv.org/pdf/2409.18399.pdf</a></span>   <span><a href='https://github.com/LLsxyc/mine_motion_prediction.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lei Li, Zhifa Chen, Jian Wang, Bin Zhou, Guizhen Yu, Xiaoxuan Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.18399">Multimodal Trajectory Prediction for Autonomous Driving on Unstructured Roads using Deep Convolutional Network</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, the application of autonomous driving in open-pit mining has garnered increasing attention for achieving safe and efficient mineral transportation. Compared to urban structured roads, unstructured roads in mining sites have uneven boundaries and lack clearly defined lane markings. This leads to a lack of sufficient constraint information for predicting the trajectories of other human-driven vehicles, resulting in higher uncertainty in trajectory prediction problems. A method is proposed to predict multiple possible trajectories and their probabilities of the target vehicle. The surrounding environment and historical trajectories of the target vehicle are encoded as a rasterized image, which is used as input to our deep convolutional network to predict the target vehicle's multiple possible trajectories. The method underwent offline testing on a dataset specifically designed for autonomous driving scenarios in open-pit mining and was compared and evaluated against physics-based method. The open-source code and data are available at https://github.com/LLsxyc/mine_motion_prediction.git
<div id='section'>Paperid: <span id='pid'>100, <a href='https://arxiv.org/pdf/2409.14101.pdf' target='_blank'>https://arxiv.org/pdf/2409.14101.pdf</a></span>   <span><a href='https://github.com/CaveSpiderLZJ/PoseAugment-ECCV2024' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhuojun Li, Chun Yu, Chen Liang, Yuanchun Shi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.14101">PoseAugment: Generative Human Pose Data Augmentation with Physical Plausibility for IMU-based Motion Capture</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The data scarcity problem is a crucial factor that hampers the model performance of IMU-based human motion capture. However, effective data augmentation for IMU-based motion capture is challenging, since it has to capture the physical relations and constraints of the human body, while maintaining the data distribution and quality. We propose PoseAugment, a novel pipeline incorporating VAE-based pose generation and physical optimization. Given a pose sequence, the VAE module generates infinite poses with both high fidelity and diversity, while keeping the data distribution. The physical module optimizes poses to satisfy physical constraints with minimal motion restrictions. High-quality IMU data are then synthesized from the augmented poses for training motion capture models. Experiments show that PoseAugment outperforms previous data augmentation and pose generation methods in terms of motion capture accuracy, revealing a strong potential of our method to alleviate the data collection burden for IMU-based motion capture and related tasks driven by human poses.
<div id='section'>Paperid: <span id='pid'>101, <a href='https://arxiv.org/pdf/2409.12202.pdf' target='_blank'>https://arxiv.org/pdf/2409.12202.pdf</a></span>   <span><a href='https://github.com/HanLingsgjk/CSCV' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Han Ling, Yinghui Sun, Quansen Sun, Yuhui Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.12202">ScaleFlow++: Robust and Accurate Estimation of 3D Motion from Video</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Perceiving and understanding 3D motion is a core technology in fields such as autonomous driving, robots, and motion prediction. This paper proposes a 3D motion perception method called ScaleFlow++ that is easy to generalize. With just a pair of RGB images, ScaleFlow++ can robustly estimate optical flow and motion-in-depth (MID). Most existing methods directly regress MID from two RGB frames or optical flow, resulting in inaccurate and unstable results. Our key insight is cross-scale matching, which extracts deep motion clues by matching objects in pairs of images at different scales. Unlike previous methods, ScaleFlow++ integrates optical flow and MID estimation into a unified architecture, estimating optical flow and MID end-to-end based on feature matching. Moreover, we also proposed modules such as global initialization network, global iterative optimizer, and hybrid training pipeline to integrate global motion information, reduce the number of iterations, and prevent overfitting during training. On KITTI, ScaleFlow++ achieved the best monocular scene flow estimation performance, reducing SF-all from 6.21 to 5.79. The evaluation of MID even surpasses RGBD-based methods. In addition, ScaleFlow++ has achieved stunning zero-shot generalization performance in both rigid and nonrigid scenes. Code is available at \url{https://github.com/HanLingsgjk/CSCV}.
<div id='section'>Paperid: <span id='pid'>102, <a href='https://arxiv.org/pdf/2409.12189.pdf' target='_blank'>https://arxiv.org/pdf/2409.12189.pdf</a></span>   <span><a href='https://github.com/felixbmuller/SAST' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Felix B Mueller, Julian Tanke, Juergen Gall
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.12189">Massively Multi-Person 3D Human Motion Forecasting with Scene Context</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Forecasting long-term 3D human motion is challenging: the stochasticity of human behavior makes it hard to generate realistic human motion from the input sequence alone. Information on the scene environment and the motion of nearby people can greatly aid the generation process. We propose a scene-aware social transformer model (SAST) to forecast long-term (10s) human motion motion. Unlike previous models, our approach can model interactions between both widely varying numbers of people and objects in a scene. We combine a temporal convolutional encoder-decoder architecture with a Transformer-based bottleneck that allows us to efficiently combine motion and scene information. We model the conditional motion distribution using denoising diffusion models. We benchmark our approach on the Humans in Kitchens dataset, which contains 1 to 16 persons and 29 to 50 objects that are visible simultaneously. Our model outperforms other approaches in terms of realism and diversity on different metrics and in a user study. Code is available at https://github.com/felixbmuller/SAST.
<div id='section'>Paperid: <span id='pid'>103, <a href='https://arxiv.org/pdf/2409.11676.pdf' target='_blank'>https://arxiv.org/pdf/2409.11676.pdf</a></span>   <span><a href='https://github.com/keshuw95/RHINO-Hypergraph-Motion-Generation' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Keshu Wu, Yang Zhou, Haotian Shi, Dominique Lord, Bin Ran, Xinyue Ye
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.11676">Hypergraph-based Motion Generation with Multi-modal Interaction Relational Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The intricate nature of real-world driving environments, characterized by dynamic and diverse interactions among multiple vehicles and their possible future states, presents considerable challenges in accurately predicting the motion states of vehicles and handling the uncertainty inherent in the predictions. Addressing these challenges requires comprehensive modeling and reasoning to capture the implicit relations among vehicles and the corresponding diverse behaviors. This research introduces an integrated framework for autonomous vehicles (AVs) motion prediction to address these complexities, utilizing a novel Relational Hypergraph Interaction-informed Neural mOtion generator (RHINO). RHINO leverages hypergraph-based relational reasoning by integrating a multi-scale hypergraph neural network to model group-wise interactions among multiple vehicles and their multi-modal driving behaviors, thereby enhancing motion prediction accuracy and reliability. Experimental validation using real-world datasets demonstrates the superior performance of this framework in improving predictive accuracy and fostering socially aware automated driving in dynamic traffic scenarios. The source code is publicly available at https://github.com/keshuw95/RHINO-Hypergraph-Motion-Generation.
<div id='section'>Paperid: <span id='pid'>104, <a href='https://arxiv.org/pdf/2409.11172.pdf' target='_blank'>https://arxiv.org/pdf/2409.11172.pdf</a></span>   <span><a href='https://github.com/valeoai/MF_aWTA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yihong Xu, Victor Letzelter, MickaÃ«l Chen, Ãloi Zablocki, Matthieu Cord
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.11172">Annealed Winner-Takes-All for Motion Forecasting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In autonomous driving, motion prediction aims at forecasting the future trajectories of nearby agents, helping the ego vehicle to anticipate behaviors and drive safely. A key challenge is generating a diverse set of future predictions, commonly addressed using data-driven models with Multiple Choice Learning (MCL) architectures and Winner-Takes-All (WTA) training objectives. However, these methods face initialization sensitivity and training instabilities. Additionally, to compensate for limited performance, some approaches rely on training with a large set of hypotheses, requiring a post-selection step during inference to significantly reduce the number of predictions. To tackle these issues, we take inspiration from annealed MCL, a recently introduced technique that improves the convergence properties of MCL methods through an annealed Winner-Takes-All loss (aWTA). In this paper, we demonstrate how the aWTA loss can be integrated with state-of-the-art motion forecasting models to enhance their performance using only a minimal set of hypotheses, eliminating the need for the cumbersome post-selection step. Our approach can be easily incorporated into any trajectory prediction model normally trained using WTA and yields significant improvements. To facilitate the application of our approach to future motion forecasting models, the code is made publicly available: https://github.com/valeoai/MF_aWTA.
<div id='section'>Paperid: <span id='pid'>105, <a href='https://arxiv.org/pdf/2409.10847.pdf' target='_blank'>https://arxiv.org/pdf/2409.10847.pdf</a></span>   <span><a href='https://github.com/RohollahHS/BAD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>S. Rohollah Hosseyni, Ali Ahmad Rahmani, S. Jamal Seyedmohammadi, Sanaz Seyedin, Arash Mohammadi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.10847">BAD: Bidirectional Auto-regressive Diffusion for Text-to-Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autoregressive models excel in modeling sequential dependencies by enforcing causal constraints, yet they struggle to capture complex bidirectional patterns due to their unidirectional nature. In contrast, mask-based models leverage bidirectional context, enabling richer dependency modeling. However, they often assume token independence during prediction, which undermines the modeling of sequential dependencies. Additionally, the corruption of sequences through masking or absorption can introduce unnatural distortions, complicating the learning process. To address these issues, we propose Bidirectional Autoregressive Diffusion (BAD), a novel approach that unifies the strengths of autoregressive and mask-based generative models. BAD utilizes a permutation-based corruption technique that preserves the natural sequence structure while enforcing causal dependencies through randomized ordering, enabling the effective capture of both sequential and bidirectional relationships. Comprehensive experiments show that BAD outperforms autoregressive and mask-based models in text-to-motion generation, suggesting a novel pre-training strategy for sequence modeling. The codebase for BAD is available on https://github.com/RohollahHS/BAD.
<div id='section'>Paperid: <span id='pid'>106, <a href='https://arxiv.org/pdf/2409.09177.pdf' target='_blank'>https://arxiv.org/pdf/2409.09177.pdf</a></span>   <span><a href='https://github.com/rd20karim/Synch-Transformer' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Karim Radouane, Sylvie Ranwez, Julien Lagarde, Andon Tchechmedjiev
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.09177">Transformer with Controlled Attention for Synchronous Motion Captioning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we address a challenging task, synchronous motion captioning, that aim to generate a language description synchronized with human motion sequences. This task pertains to numerous applications, such as aligned sign language transcription, unsupervised action segmentation and temporal grounding. Our method introduces mechanisms to control self- and cross-attention distributions of the Transformer, allowing interpretability and time-aligned text generation. We achieve this through masking strategies and structuring losses that push the model to maximize attention only on the most important frames contributing to the generation of a motion word. These constraints aim to prevent undesired mixing of information in attention maps and to provide a monotonic attention distribution across tokens. Thus, the cross attentions of tokens are used for progressive text generation in synchronization with human motion sequences. We demonstrate the superior performance of our approach through evaluation on the two available benchmark datasets, KIT-ML and HumanML3D. As visual evaluation is essential for this task, we provide a comprehensive set of animated visual illustrations in the code repository: https://github.com/rd20karim/Synch-Transformer.
<div id='section'>Paperid: <span id='pid'>107, <a href='https://arxiv.org/pdf/2409.00774.pdf' target='_blank'>https://arxiv.org/pdf/2409.00774.pdf</a></span>   <span><a href='https://github.com/intelligolabs/SITUATE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Luigi Capogrosso, Andrea Toaiari, Andrea Avogaro, Uzair Khan, Aditya Jivoji, Franco Fummi, Marco Cristani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.00774">SITUATE: Indoor Human Trajectory Prediction through Geometric Features and Self-Supervised Vision Representation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Patterns of human motion in outdoor and indoor environments are substantially different due to the scope of the environment and the typical intentions of people therein. While outdoor trajectory forecasting has received significant attention, indoor forecasting is still an underexplored research area. This paper proposes SITUATE, a novel approach to cope with indoor human trajectory prediction by leveraging equivariant and invariant geometric features and a self-supervised vision representation. The geometric learning modules model the intrinsic symmetries and human movements inherent in indoor spaces. This concept becomes particularly important because self-loops at various scales and rapid direction changes often characterize indoor trajectories. On the other hand, the vision representation module is used to acquire spatial-semantic information about the environment to predict users' future locations more accurately. We evaluate our method through comprehensive experiments on the two most famous indoor trajectory forecasting datasets, i.e., THÃR and Supermarket, obtaining state-of-the-art performance. Furthermore, we also achieve competitive results in outdoor scenarios, showing that indoor-oriented forecasting models generalize better than outdoor-oriented ones. The source code is available at https://github.com/intelligolabs/SITUATE.
<div id='section'>Paperid: <span id='pid'>108, <a href='https://arxiv.org/pdf/2409.00487.pdf' target='_blank'>https://arxiv.org/pdf/2409.00487.pdf</a></span>   <span><a href='https://github.com/Xavier-Lin/TrackSSM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bin Hu, Run Luo, Zelin Liu, Cheng Wang, Wenyu Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.00487">TrackSSM: A General Motion Predictor by State-Space Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Temporal motion modeling has always been a key component in multiple object tracking (MOT) which can ensure smooth trajectory movement and provide accurate positional information to enhance association precision. However, current motion models struggle to be both efficient and effective across different application scenarios. To this end, we propose TrackSSM inspired by the recently popular state space models (SSM), a unified encoder-decoder motion framework that uses data-dependent state space model to perform temporal motion of trajectories. Specifically, we propose Flow-SSM, a module that utilizes the position and motion information from historical trajectories to guide the temporal state transition of object bounding boxes. Based on Flow-SSM, we design a flow decoder. It is composed of a cascaded motion decoding module employing Flow-SSM, which can use the encoded flow information to complete the temporal position prediction of trajectories. Additionally, we propose a Step-by-Step Linear (S$^2$L) training strategy. By performing linear interpolation between the positions of the object in the previous frame and the current frame, we construct the pseudo labels of step-by-step linear training, ensuring that the trajectory flow information can better guide the object bounding box in completing temporal transitions. TrackSSM utilizes a simple Mamba-Block to build a motion encoder for historical trajectories, forming a temporal motion model with an encoder-decoder structure in conjunction with the flow decoder. TrackSSM is applicable to various tracking scenarios and achieves excellent tracking performance across multiple benchmarks, further extending the potential of SSM-like temporal motion models in multi-object tracking tasks. Code and models are publicly available at \url{https://github.com/Xavier-Lin/TrackSSM}.
<div id='section'>Paperid: <span id='pid'>109, <a href='https://arxiv.org/pdf/2408.12885.pdf' target='_blank'>https://arxiv.org/pdf/2408.12885.pdf</a></span>   <span><a href='https://github.com/Gloria2tt/T3M.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenshuo Peng, Kaipeng Zhang, Sai Qian Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.12885">T3M: Text Guided 3D Human Motion Synthesis from Speech</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Speech-driven 3D motion synthesis seeks to create lifelike animations based on human speech, with potential uses in virtual reality, gaming, and the film production. Existing approaches reply solely on speech audio for motion generation, leading to inaccurate and inflexible synthesis results. To mitigate this problem, we introduce a novel text-guided 3D human motion synthesis method, termed \textit{T3M}. Unlike traditional approaches, T3M allows precise control over motion synthesis via textual input, enhancing the degree of diversity and user customization. The experiment results demonstrate that T3M can greatly outperform the state-of-the-art methods in both quantitative metrics and qualitative evaluations. We have publicly released our code at \href{https://github.com/Gloria2tt/T3M.git}{https://github.com/Gloria2tt/T3M.git}
<div id='section'>Paperid: <span id='pid'>110, <a href='https://arxiv.org/pdf/2408.03499.pdf' target='_blank'>https://arxiv.org/pdf/2408.03499.pdf</a></span>   <span><a href='https://github.com/volatileee/FacialPulse' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruiqi Wang, Jinyang Huang, Jie Zhang, Xin Liu, Xiang Zhang, Zhi Liu, Peng Zhao, Sigui Chen, Xiao Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.03499">FacialPulse: An Efficient RNN-based Depression Detection via Temporal Facial Landmarks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Depression is a prevalent mental health disorder that significantly impacts individuals' lives and well-being. Early detection and intervention are crucial for effective treatment and management of depression. Recently, there are many end-to-end deep learning methods leveraging the facial expression features for automatic depression detection. However, most current methods overlook the temporal dynamics of facial expressions. Although very recent 3DCNN methods remedy this gap, they introduce more computational cost due to the selection of CNN-based backbones and redundant facial features.
  To address the above limitations, by considering the timing correlation of facial expressions, we propose a novel framework called FacialPulse, which recognizes depression with high accuracy and speed. By harnessing the bidirectional nature and proficiently addressing long-term dependencies, the Facial Motion Modeling Module (FMMM) is designed in FacialPulse to fully capture temporal features. Since the proposed FMMM has parallel processing capabilities and has the gate mechanism to mitigate gradient vanishing, this module can also significantly boost the training speed.
  Besides, to effectively use facial landmarks to replace original images to decrease information redundancy, a Facial Landmark Calibration Module (FLCM) is designed to eliminate facial landmark errors to further improve recognition accuracy. Extensive experiments on the AVEC2014 dataset and MMDA dataset (a depression dataset) demonstrate the superiority of FacialPulse on recognition accuracy and speed, with the average MAE (Mean Absolute Error) decreased by 21% compared to baselines, and the recognition speed increased by 100% compared to state-of-the-art methods. Codes are released at https://github.com/volatileee/FacialPulse.
<div id='section'>Paperid: <span id='pid'>111, <a href='https://arxiv.org/pdf/2408.02091.pdf' target='_blank'>https://arxiv.org/pdf/2408.02091.pdf</a></span>   <span><a href='https://github.com/JunyuShi02/PMG-MRL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Junyu Shi, Baoxuan Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.02091">Past Movements-Guided Motion Representation Learning for Human Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion prediction based on 3D skeleton is a significant challenge in computer vision, primarily focusing on the effective representation of motion. In this paper, we propose a self-supervised learning framework designed to enhance motion representation. This framework consists of two stages: first, the network is pretrained through the self-reconstruction of past sequences, and the guided reconstruction of future sequences based on past movements. We design a velocity-based mask strategy to focus on the joints with large-scale moving. Subsequently, the pretrained network undergoes finetuning for specific tasks. Self-reconstruction, guided by patterns of past motion, substantially improves the model's ability to represent the spatiotemporal relationships among joints but also captures the latent relationships between past and future sequences. This capability is crucial for motion prediction tasks that solely depend on historical motion data. By employing this straightforward yet effective training paradigm, our method outperforms existing \textit{state-of-the-art} methods, reducing the average prediction errors by 8.8\% across Human3.6M, 3DPW, and AMASS datasets. The code is available at https://github.com/JunyuShi02/PMG-MRL.
<div id='section'>Paperid: <span id='pid'>112, <a href='https://arxiv.org/pdf/2407.19564.pdf' target='_blank'>https://arxiv.org/pdf/2407.19564.pdf</a></span>   <span><a href='https://github.com/csjfwang/Forecast-PEFT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jifeng Wang, Kaouther Messaoud, Yuejiang Liu, Juergen Gall, Alexandre Alahi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.19564">Forecast-PEFT: Parameter-Efficient Fine-Tuning for Pre-trained Motion Forecasting Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent progress in motion forecasting has been substantially driven by self-supervised pre-training. However, adapting pre-trained models for specific downstream tasks, especially motion prediction, through extensive fine-tuning is often inefficient. This inefficiency arises because motion prediction closely aligns with the masked pre-training tasks, and traditional full fine-tuning methods fail to fully leverage this alignment. To address this, we introduce Forecast-PEFT, a fine-tuning strategy that freezes the majority of the model's parameters, focusing adjustments on newly introduced prompts and adapters. This approach not only preserves the pre-learned representations but also significantly reduces the number of parameters that need retraining, thereby enhancing efficiency. This tailored strategy, supplemented by our method's capability to efficiently adapt to different datasets, enhances model efficiency and ensures robust performance across datasets without the need for extensive retraining. Our experiments show that Forecast-PEFT outperforms traditional full fine-tuning methods in motion prediction tasks, achieving higher accuracy with only 17% of the trainable parameters typically required. Moreover, our comprehensive adaptation, Forecast-FT, further improves prediction performance, evidencing up to a 9.6% enhancement over conventional baseline methods. Code will be available at https://github.com/csjfwang/Forecast-PEFT.
<div id='section'>Paperid: <span id='pid'>113, <a href='https://arxiv.org/pdf/2407.19244.pdf' target='_blank'>https://arxiv.org/pdf/2407.19244.pdf</a></span>   <span><a href='https://github.com/ph-w2000/SDM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Penghui Wen, Kun Hu, Dong Yuan, Zhiyuan Ning, Changyang Li, Zhiyong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.19244">Radio Frequency Signal based Human Silhouette Segmentation: A Sequential Diffusion Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Radio frequency (RF) signals have been proved to be flexible for human silhouette segmentation (HSS) under complex environments. Existing studies are mainly based on a one-shot approach, which lacks a coherent projection ability from the RF domain. Additionally, the spatio-temporal patterns have not been fully explored for human motion dynamics in HSS. Therefore, we propose a two-stage Sequential Diffusion Model (SDM) to progressively synthesize high-quality segmentation jointly with the considerations on motion dynamics. Cross-view transformation blocks are devised to guide the diffusion model in a multi-scale manner for comprehensively characterizing human related patterns in an individual frame such as directional projection from signal planes. Moreover, spatio-temporal blocks are devised to fine-tune the frame-level model to incorporate spatio-temporal contexts and motion dynamics, enhancing the consistency of the segmentation maps. Comprehensive experiments on a public benchmark -- HIBER demonstrate the state-of-the-art performance of our method with an IoU 0.732. Our code is available at https://github.com/ph-w2000/SDM.
<div id='section'>Paperid: <span id='pid'>114, <a href='https://arxiv.org/pdf/2407.11915.pdf' target='_blank'>https://arxiv.org/pdf/2407.11915.pdf</a></span>   <span><a href='https://github.com/dingdingding60/Humanoids2024HRI' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bosong Ding, Murat Kirtay, Giacomo Spigler
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.11915">Imitation of human motion achieves natural head movements for humanoid robots in an active-speaker detection task</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Head movements are crucial for social human-human interaction. They can transmit important cues (e.g., joint attention, speaker detection) that cannot be achieved with verbal interaction alone. This advantage also holds for human-robot interaction. Even though modeling human motions through generative AI models has become an active research area within robotics in recent years, the use of these methods for producing head movements in human-robot interaction remains underexplored. In this work, we employed a generative AI pipeline to produce human-like head movements for a Nao humanoid robot. In addition, we tested the system on a real-time active-speaker tracking task in a group conversation setting. Overall, the results show that the Nao robot successfully imitates human head movements in a natural manner while actively tracking the speakers during the conversation. Code and data from this study are available at https://github.com/dingdingding60/Humanoids2024HRI
<div id='section'>Paperid: <span id='pid'>115, <a href='https://arxiv.org/pdf/2407.11494.pdf' target='_blank'>https://arxiv.org/pdf/2407.11494.pdf</a></span>   <span><a href='https://github.com/GuoweiXu368/SLD-HMP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Guowei Xu, Jiale Tao, Wen Li, Lixin Duan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.11494">Learning Semantic Latent Directions for Accurate and Controllable Human Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the realm of stochastic human motion prediction (SHMP), researchers have often turned to generative models like GANS, VAEs and diffusion models. However, most previous approaches have struggled to accurately predict motions that are both realistic and coherent with past motion due to a lack of guidance on the latent distribution. In this paper, we introduce Semantic Latent Directions (SLD) as a solution to this challenge, aiming to constrain the latent space to learn meaningful motion semantics and enhance the accuracy of SHMP. SLD defines a series of orthogonal latent directions and represents the hypothesis of future motion as a linear combination of these directions. By creating such an information bottleneck, SLD excels in capturing meaningful motion semantics, thereby improving the precision of motion predictions. Moreover, SLD offers controllable prediction capabilities by adjusting the coefficients of the latent directions during the inference phase. Expanding on SLD, we introduce a set of motion queries to enhance the diversity of predictions. By aligning these motion queries with the SLD space, SLD is further promoted to more accurate and coherent motion predictions. Through extensive experiments conducted on widely used benchmarks, we showcase the superiority of our method in accurately predicting motions while maintaining a balance of realism and diversity. Our code and pretrained models are available at https://github.com/GuoweiXu368/SLD-HMP.
<div id='section'>Paperid: <span id='pid'>116, <a href='https://arxiv.org/pdf/2407.11420.pdf' target='_blank'>https://arxiv.org/pdf/2407.11420.pdf</a></span>   <span><a href='https://github.com/Unsigned-Long/iKalibr' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuolong Chen, Xingxing Li, Shengyu Li, Yuxuan Zhou, Xiaoteng Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.11420">iKalibr: Unified Targetless Spatiotemporal Calibration for Resilient Integrated Inertial Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The integrated inertial system, typically integrating an IMU and an exteroceptive sensor such as radar, LiDAR, and camera, has been widely accepted and applied in modern robotic applications for ego-motion estimation, motion control, or autonomous exploration. To improve system accuracy, robustness, and further usability, both multiple and various sensors are generally resiliently integrated, which benefits the system performance regarding failure tolerance, perception capability, and environment compatibility. For such systems, accurate and consistent spatiotemporal calibration is required to maintain a unique spatiotemporal framework for multi-sensor fusion. Considering most existing calibration methods (i) are generally oriented to specific integrated inertial systems, (ii) often only focus on spatial determination, (iii) usually require artificial targets, lacking convenience and usability, we propose iKalibr: a unified targetless spatiotemporal calibration framework for resilient integrated inertial systems, which overcomes the above issues, and enables both accurate and consistent calibration. Altogether four commonly employed sensors are supported in iKalibr currently, namely IMU, radar, LiDAR, and camera. The proposed method starts with a rigorous and efficient dynamic initialization, where all parameters in the estimator would be accurately recovered. Subsequently, several continuous-time batch optimizations are conducted to refine the initialized parameters toward better states. Sufficient real-world experiments were conducted to verify the feasibility and evaluate the calibration performance of iKalibr. The results demonstrate that iKalibr can achieve accurate resilient spatiotemporal calibration. We open-source our implementations at (https://github.com/Unsigned-Long/iKalibr) to benefit the research community.
<div id='section'>Paperid: <span id='pid'>117, <a href='https://arxiv.org/pdf/2407.09797.pdf' target='_blank'>https://arxiv.org/pdf/2407.09797.pdf</a></span>   <span><a href='https://github.com/HanLingsgjk/CSCV' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Han Ling, Quansen Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.09797">ScaleFlow++: Robust and Accurate Estimation of 3D Motion from Video</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Perceiving and understanding 3D motion is a core technology in fields such as autonomous driving, robots, and motion prediction. This paper proposes a 3D motion perception method called ScaleFlow++ that is easy to generalize. With just a pair of RGB images, ScaleFlow++ can robustly estimate optical flow and motion-in-depth (MID). Most existing methods directly regress MID from two RGB frames or optical flow, resulting in inaccurate and unstable results. Our key insight is cross-scale matching, which extracts deep motion clues by matching objects in pairs of images at different scales. Unlike previous methods, ScaleFlow++ integrates optical flow and MID estimation into a unified architecture, estimating optical flow and MID end-to-end based on feature matching. Moreover, we also proposed modules such as global initialization network, global iterative optimizer, and hybrid training pipeline to integrate global motion information, reduce the number of iterations, and prevent overfitting during training. On KITTI, ScaleFlow++ achieved the best monocular scene flow estimation performance, reducing SF-all from 6.21 to 5.79. The evaluation of MID even surpasses RGBD-based methods. In addition, ScaleFlow++ has achieved stunning zero-shot generalization performance in both rigid and nonrigid scenes. Code is available at \url{https://github.com/HanLingsgjk/CSCV}.
<div id='section'>Paperid: <span id='pid'>118, <a href='https://arxiv.org/pdf/2407.05238.pdf' target='_blank'>https://arxiv.org/pdf/2407.05238.pdf</a></span>   <span><a href='https://github.com/haooozi/P2P' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiahao Nie, Fei Xie, Sifan Zhou, Xueyi Zhou, Dong-Kyu Chae, Zhiwei He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.05238">P2P: Part-to-Part Motion Cues Guide a Strong Tracking Framework for LiDAR Point Clouds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D single object tracking (SOT) methods based on appearance matching has long suffered from insufficient appearance information incurred by incomplete, textureless and semantically deficient LiDAR point clouds. While motion paradigm exploits motion cues instead of appearance matching for tracking, it incurs complex multi-stage processing and segmentation module. In this paper, we first provide in-depth explorations on motion paradigm, which proves that (\textbf{i}) it is feasible to directly infer target relative motion from point clouds across consecutive frames; (\textbf{ii}) fine-grained information comparison between consecutive point clouds facilitates target motion modeling. We thereby propose to perform part-to-part motion modeling for consecutive point clouds and introduce a novel tracking framework, termed \textbf{P2P}. The novel framework fuses each corresponding part information between consecutive point clouds, effectively exploring detailed information changes and thus modeling accurate target-related motion cues. Following this framework, we present P2P-point and P2P-voxel models, incorporating implicit and explicit part-to-part motion modeling by point- and voxel-based representation, respectively. Without bells and whistles, P2P-voxel sets a new state-of-the-art performance ($\sim$\textbf{89\%}, \textbf{72\%} and \textbf{63\%} precision on KITTI, NuScenes and Waymo Open Dataset, respectively). Moreover, under the same point-based representation, P2P-point outperforms the previous motion tracker M$^2$Track by \textbf{3.3\%} and \textbf{6.7\%} on the KITTI and NuScenes, while running at a considerably high speed of \textbf{107 Fps} on a single RTX3090 GPU. The source code and pre-trained models are available at https://github.com/haooozi/P2P.
<div id='section'>Paperid: <span id='pid'>119, <a href='https://arxiv.org/pdf/2407.02129.pdf' target='_blank'>https://arxiv.org/pdf/2407.02129.pdf</a></span>   <span><a href='https://github.com/MIV-XJTU/ReliaAvatar' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bo Qian, Zhenhuan Wei, Jiashuo Li, Xing Wei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.02129">ReliaAvatar: A Robust Real-Time Avatar Animator with Integrated Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Efficiently estimating the full-body pose with minimal wearable devices presents a worthwhile research direction. Despite significant advancements in this field, most current research neglects to explore full-body avatar estimation under low-quality signal conditions, which is prevalent in practical usage. To bridge this gap, we summarize three scenarios that may be encountered in real-world applications: standard scenario, instantaneous data-loss scenario, and prolonged data-loss scenario, and propose a new evaluation benchmark. The solution we propose to address data-loss scenarios is integrating the full-body avatar pose estimation problem with motion prediction. Specifically, we present \textit{ReliaAvatar}, a real-time, \textbf{relia}ble \textbf{avatar} animator equipped with predictive modeling capabilities employing a dual-path architecture. ReliaAvatar operates effectively, with an impressive performance rate of 109 frames per second (fps). Extensive comparative evaluations on widely recognized benchmark datasets demonstrate Relia\-Avatar's superior performance in both standard and low data-quality conditions. The code is available at \url{https://github.com/MIV-XJTU/ReliaAvatar}.
<div id='section'>Paperid: <span id='pid'>120, <a href='https://arxiv.org/pdf/2406.06508.pdf' target='_blank'>https://arxiv.org/pdf/2406.06508.pdf</a></span>   <span><a href='https://github.com/MonkeySeeDoCG/MoMo-code' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sigal Raab, Inbar Gat, Nathan Sala, Guy Tevet, Rotem Shalev-Arkushin, Ohad Fried, Amit H. Bermano, Daniel Cohen-Or
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.06508">Monkey See, Monkey Do: Harnessing Self-attention in Motion Diffusion for Zero-shot Motion Transfer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Given the remarkable results of motion synthesis with diffusion models, a natural question arises: how can we effectively leverage these models for motion editing? Existing diffusion-based motion editing methods overlook the profound potential of the prior embedded within the weights of pre-trained models, which enables manipulating the latent feature space; hence, they primarily center on handling the motion space. In this work, we explore the attention mechanism of pre-trained motion diffusion models. We uncover the roles and interactions of attention elements in capturing and representing intricate human motion patterns, and carefully integrate these elements to transfer a leader motion to a follower one while maintaining the nuanced characteristics of the follower, resulting in zero-shot motion transfer. Editing features associated with selected motions allows us to confront a challenge observed in prior motion diffusion approaches, which use general directives (e.g., text, music) for editing, ultimately failing to convey subtle nuances effectively. Our work is inspired by how a monkey closely imitates what it sees while maintaining its unique motion patterns; hence we call it Monkey See, Monkey Do, and dub it MoMo. Employing our technique enables accomplishing tasks such as synthesizing out-of-distribution motions, style transfer, and spatial editing. Furthermore, diffusion inversion is seldom employed for motions; as a result, editing efforts focus on generated motions, limiting the editability of real ones. MoMo harnesses motion inversion, extending its application to both real and generated motions. Experimental results show the advantage of our approach over the current art. In particular, unlike methods tailored for specific applications through training, our approach is applied at inference time, requiring no training. Our webpage is at https://monkeyseedocg.github.io.
<div id='section'>Paperid: <span id='pid'>121, <a href='https://arxiv.org/pdf/2406.04765.pdf' target='_blank'>https://arxiv.org/pdf/2406.04765.pdf</a></span>   <span><a href='https://github.com/tianyuan168326/VideoSemanticCompression-Pytorch' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuan Tian, Guo Lu, Guangtao Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.04765">SMC++: Masked Learning of Unsupervised Video Semantic Compression</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Most video compression methods focus on human visual perception, neglecting semantic preservation. This leads to severe semantic loss during the compression, hampering downstream video analysis tasks. In this paper, we propose a Masked Video Modeling (MVM)-powered compression framework that particularly preserves video semantics, by jointly mining and compressing the semantics in a self-supervised manner. While MVM is proficient at learning generalizable semantics through the masked patch prediction task, it may also encode non-semantic information like trivial textural details, wasting bitcost and bringing semantic noises. To suppress this, we explicitly regularize the non-semantic entropy of the compressed video in the MVM token space. The proposed framework is instantiated as a simple Semantic-Mining-then-Compression (SMC) model. Furthermore, we extend SMC as an advanced SMC++ model from several aspects. First, we equip it with a masked motion prediction objective, leading to better temporal semantic learning ability. Second, we introduce a Transformer-based compression module, to improve the semantic compression efficacy. Considering that directly mining the complex redundancy among heterogeneous features in different coding stages is non-trivial, we introduce a compact blueprint semantic representation to align these features into a similar form, fully unleashing the power of the Transformer-based compression module. Extensive results demonstrate the proposed SMC and SMC++ models show remarkable superiority over previous traditional, learnable, and perceptual quality-oriented video codecs, on three video analysis tasks and seven datasets. \textit{Codes and model are available at: \url{https://github.com/tianyuan168326/VideoSemanticCompression-Pytorch}.
<div id='section'>Paperid: <span id='pid'>122, <a href='https://arxiv.org/pdf/2406.04649.pdf' target='_blank'>https://arxiv.org/pdf/2406.04649.pdf</a></span>   <span><a href='https://github.com/Inowlzy/SMART.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zengyuan Lai, Jiarui Yang, Songpengcheng Xia, Qi Wu, Zhen Sun, Wenxian Yu, Ling Pei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.04649">SMART: Scene-motion-aware human action recognition framework for mental disorder group</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Patients with mental disorders often exhibit risky abnormal actions, such as climbing walls or hitting windows, necessitating intelligent video behavior monitoring for smart healthcare with the rising Internet of Things (IoT) technology. However, the development of vision-based Human Action Recognition (HAR) for these actions is hindered by the lack of specialized algorithms and datasets. In this paper, we innovatively propose to build a vision-based HAR dataset including abnormal actions often occurring in the mental disorder group and then introduce a novel Scene-Motion-aware Action Recognition Technology framework, named SMART, consisting of two technical modules. First, we propose a scene perception module to extract human motion trajectory and human-scene interaction features, which introduces additional scene information for a supplementary semantic representation of the above actions. Second, the multi-stage fusion module fuses the skeleton motion, motion trajectory, and human-scene interaction features, enhancing the semantic association between the skeleton motion and the above supplementary representation, thus generating a comprehensive representation with both human motion and scene information. The effectiveness of our proposed method has been validated on our self-collected HAR dataset (MentalHAD), achieving 94.9% and 93.1% accuracy in un-seen subjects and scenes and outperforming state-of-the-art approaches by 6.5% and 13.2%, respectively. The demonstrated subject- and scene- generalizability makes it possible for SMART's migration to practical deployment in smart healthcare systems for mental disorder patients in medical settings. The code and dataset will be released publicly for further research: https://github.com/Inowlzy/SMART.git.
<div id='section'>Paperid: <span id='pid'>123, <a href='https://arxiv.org/pdf/2405.20991.pdf' target='_blank'>https://arxiv.org/pdf/2405.20991.pdf</a></span>   <span><a href='https://github.com/KTH-RPL/Detect_VLM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi Yang, Qingwen Zhang, Kei Ikemura, Nazre Batool, John Folkesson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.20991">Hard Cases Detection in Motion Prediction by Vision-Language Foundation Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Addressing hard cases in autonomous driving, such as anomalous road users, extreme weather conditions, and complex traffic interactions, presents significant challenges. To ensure safety, it is crucial to detect and manage these scenarios effectively for autonomous driving systems. However, the rarity and high-risk nature of these cases demand extensive, diverse datasets for training robust models. Vision-Language Foundation Models (VLMs) have shown remarkable zero-shot capabilities as being trained on extensive datasets. This work explores the potential of VLMs in detecting hard cases in autonomous driving. We demonstrate the capability of VLMs such as GPT-4v in detecting hard cases in traffic participant motion prediction on both agent and scenario levels. We introduce a feasible pipeline where VLMs, fed with sequential image frames with designed prompts, effectively identify challenging agents or scenarios, which are verified by existing prediction models. Moreover, by taking advantage of this detection of hard cases by VLMs, we further improve the training efficiency of the existing motion prediction pipeline by performing data selection for the training samples suggested by GPT. We show the effectiveness and feasibility of our pipeline incorporating VLMs with state-of-the-art methods on NuScenes datasets. The code is accessible at https://github.com/KTH-RPL/Detect_VLM.
<div id='section'>Paperid: <span id='pid'>124, <a href='https://arxiv.org/pdf/2405.19620.pdf' target='_blank'>https://arxiv.org/pdf/2405.19620.pdf</a></span>   <span><a href='https://github.com/swc-17/SparseDrive' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenchao Sun, Xuewu Lin, Yining Shi, Chuang Zhang, Haoran Wu, Sifa Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.19620">SparseDrive: End-to-End Autonomous Driving via Sparse Scene Representation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The well-established modular autonomous driving system is decoupled into different standalone tasks, e.g. perception, prediction and planning, suffering from information loss and error accumulation across modules. In contrast, end-to-end paradigms unify multi-tasks into a fully differentiable framework, allowing for optimization in a planning-oriented spirit. Despite the great potential of end-to-end paradigms, both the performance and efficiency of existing methods are not satisfactory, particularly in terms of planning safety. We attribute this to the computationally expensive BEV (bird's eye view) features and the straightforward design for prediction and planning. To this end, we explore the sparse representation and review the task design for end-to-end autonomous driving, proposing a new paradigm named SparseDrive. Concretely, SparseDrive consists of a symmetric sparse perception module and a parallel motion planner. The sparse perception module unifies detection, tracking and online mapping with a symmetric model architecture, learning a fully sparse representation of the driving scene. For motion prediction and planning, we review the great similarity between these two tasks, leading to a parallel design for motion planner. Based on this parallel design, which models planning as a multi-modal problem, we propose a hierarchical planning selection strategy , which incorporates a collision-aware rescore module, to select a rational and safe trajectory as the final planning output. With such effective designs, SparseDrive surpasses previous state-of-the-arts by a large margin in performance of all tasks, while achieving much higher training and inference efficiency. Code will be avaliable at https://github.com/swc-17/SparseDrive for facilitating future research.
<div id='section'>Paperid: <span id='pid'>125, <a href='https://arxiv.org/pdf/2405.19183.pdf' target='_blank'>https://arxiv.org/pdf/2405.19183.pdf</a></span>   <span><a href='https://github.com/TruongKhang/cLODE;' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Khang Truong Giang, Yongjae Kim, Andrea Finazzi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.19183">Conditional Latent ODEs for Motion Prediction in Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper addresses imitation learning for motion prediction problem in autonomous driving, especially in multi-agent setting. Different from previous methods based on GAN, we present the conditional latent ordinary differential equation (cLODE) to leverage both the generative strength of conditional VAE and the continuous representation of neural ODE. Our network architecture is inspired from the Latent-ODE model. The experiment shows that our method outperform the baseline methods in the simulation of multi-agent driving and is very efficient in term of GPU memory consumption. Our code and docker image are publicly available: https://github.com/TruongKhang/cLODE; https://hub.docker.com/r/kim4375731/clode.
<div id='section'>Paperid: <span id='pid'>126, <a href='https://arxiv.org/pdf/2405.06088.pdf' target='_blank'>https://arxiv.org/pdf/2405.06088.pdf</a></span>   <span><a href='https://github.com/edshieh/motionprediction' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Edmund Shieh, Joshua Lee Franco, Kang Min Bae, Tej Lalvani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.06088">A Mixture of Experts Approach to 3D Human Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This project addresses the challenge of human motion prediction, a critical area for applications such as au- tonomous vehicle movement detection. Previous works have emphasized the need for low inference times to provide real time performance for applications like these. Our primary objective is to critically evaluate existing model ar- chitectures, identifying their advantages and opportunities for improvement by replicating the state-of-the-art (SOTA) Spatio-Temporal Transformer model as best as possible given computational con- straints. These models have surpassed the limitations of RNN-based models and have demonstrated the ability to generate plausible motion sequences over both short and long term horizons through the use of spatio-temporal rep- resentations. We also propose a novel architecture to ad- dress challenges of real time inference speed by incorpo- rating a Mixture of Experts (MoE) block within the Spatial- Temporal (ST) attention layer. The particular variation that is used is Soft MoE, a fully-differentiable sparse Transformer that has shown promising ability to enable larger model capacity at lower inference cost. We make out code publicly available at https://github.com/edshieh/motionprediction
<div id='section'>Paperid: <span id='pid'>127, <a href='https://arxiv.org/pdf/2405.04370.pdf' target='_blank'>https://arxiv.org/pdf/2405.04370.pdf</a></span>   <span><a href='https://github.com/IRMVLab/Diff-IP2D' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Junyi Ma, Jingyi Xu, Xieyuanli Chen, Hesheng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.04370">Diff-IP2D: Diffusion-Based Hand-Object Interaction Prediction on Egocentric Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding how humans would behave during hand-object interaction is vital for applications in service robot manipulation and extended reality. To achieve this, some recent works have been proposed to simultaneously forecast hand trajectories and object affordances on human egocentric videos. The joint prediction serves as a comprehensive representation of future hand-object interactions in 2D space, indicating potential human motion and motivation. However, the existing approaches mostly adopt the autoregressive paradigm for unidirectional prediction, which lacks mutual constraints within the holistic future sequence, and accumulates errors along the time axis. Meanwhile, these works basically overlook the effect of camera egomotion on first-person view predictions. To address these limitations, we propose a novel diffusion-based interaction prediction method, namely Diff-IP2D, to forecast future hand trajectories and object affordances concurrently in an iterative non-autoregressive manner. We transform the sequential 2D images into latent feature space and design a denoising diffusion model to predict future latent interaction features conditioned on past ones. Motion features are further integrated into the conditional denoising process to enable Diff-IP2D aware of the camera wearer's dynamics for more accurate interaction prediction. Extensive experiments demonstrate that our method significantly outperforms the state-of-the-art baselines on both the off-the-shelf metrics and our newly proposed evaluation protocol. This highlights the efficacy of leveraging a generative paradigm for 2D hand-object interaction prediction. The code of Diff-IP2D is released as open source at https://github.com/IRMVLab/Diff-IP2D.
<div id='section'>Paperid: <span id='pid'>128, <a href='https://arxiv.org/pdf/2405.03485.pdf' target='_blank'>https://arxiv.org/pdf/2405.03485.pdf</a></span>   <span><a href='https://github.com/L-Sun/LGTM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haowen Sun, Ruikun Zheng, Haibin Huang, Chongyang Ma, Hui Huang, Ruizhen Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.03485">LGTM: Local-to-Global Text-Driven Human Motion Diffusion Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we introduce LGTM, a novel Local-to-Global pipeline for Text-to-Motion generation. LGTM utilizes a diffusion-based architecture and aims to address the challenge of accurately translating textual descriptions into semantically coherent human motion in computer animation. Specifically, traditional methods often struggle with semantic discrepancies, particularly in aligning specific motions to the correct body parts. To address this issue, we propose a two-stage pipeline to overcome this challenge: it first employs large language models (LLMs) to decompose global motion descriptions into part-specific narratives, which are then processed by independent body-part motion encoders to ensure precise local semantic alignment. Finally, an attention-based full-body optimizer refines the motion generation results and guarantees the overall coherence. Our experiments demonstrate that LGTM gains significant improvements in generating locally accurate, semantically-aligned human motion, marking a notable advancement in text-to-motion applications. Code and data for this paper are available at https://github.com/L-Sun/LGTM
<div id='section'>Paperid: <span id='pid'>129, <a href='https://arxiv.org/pdf/2405.01434.pdf' target='_blank'>https://arxiv.org/pdf/2405.01434.pdf</a></span>   <span><a href='https://github.com/HVision-NKU/StoryDiffusion' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yupeng Zhou, Daquan Zhou, Ming-Ming Cheng, Jiashi Feng, Qibin Hou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.01434">StoryDiffusion: Consistent Self-Attention for Long-Range Image and Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>For recent diffusion-based generative models, maintaining consistent content across a series of generated images, especially those containing subjects and complex details, presents a significant challenge. In this paper, we propose a new way of self-attention calculation, termed Consistent Self-Attention, that significantly boosts the consistency between the generated images and augments prevalent pretrained diffusion-based text-to-image models in a zero-shot manner. To extend our method to long-range video generation, we further introduce a novel semantic space temporal motion prediction module, named Semantic Motion Predictor. It is trained to estimate the motion conditions between two provided images in the semantic spaces. This module converts the generated sequence of images into videos with smooth transitions and consistent subjects that are significantly more stable than the modules based on latent spaces only, especially in the context of long video generation. By merging these two novel components, our framework, referred to as StoryDiffusion, can describe a text-based story with consistent images or videos encompassing a rich variety of contents. The proposed StoryDiffusion encompasses pioneering explorations in visual story generation with the presentation of images and videos, which we hope could inspire more research from the aspect of architectural modifications. Our code is made publicly available at https://github.com/HVision-NKU/StoryDiffusion.
<div id='section'>Paperid: <span id='pid'>130, <a href='https://arxiv.org/pdf/2404.19541.pdf' target='_blank'>https://arxiv.org/pdf/2404.19541.pdf</a></span>   <span><a href='https://github.com/eth-siplab/UltraInertialPoser' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Rayan Armani, Changlin Qian, Jiaxi Jiang, Christian Holz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.19541">Ultra Inertial Poser: Scalable Motion Capture and Tracking from Sparse Inertial Sensors and Ultra-Wideband Ranging</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While camera-based capture systems remain the gold standard for recording human motion, learning-based tracking systems based on sparse wearable sensors are gaining popularity. Most commonly, they use inertial sensors, whose propensity for drift and jitter have so far limited tracking accuracy. In this paper, we propose Ultra Inertial Poser, a novel 3D full body pose estimation method that constrains drift and jitter in inertial tracking via inter-sensor distances. We estimate these distances across sparse sensor setups using a lightweight embedded tracker that augments inexpensive off-the-shelf 6D inertial measurement units with ultra-wideband radio-based ranging$-$dynamically and without the need for stationary reference anchors. Our method then fuses these inter-sensor distances with the 3D states estimated from each sensor Our graph-based machine learning model processes the 3D states and distances to estimate a person's 3D full body pose and translation. To train our model, we synthesize inertial measurements and distance estimates from the motion capture database AMASS. For evaluation, we contribute a novel motion dataset of 10 participants who performed 25 motion types, captured by 6 wearable IMU+UWB trackers and an optical motion capture system, totaling 200 minutes of synchronized sensor data (UIP-DB). Our extensive experiments show state-of-the-art performance for our method over PIP and TIP, reducing position error from $13.62$ to $10.65cm$ ($22\%$ better) and lowering jitter from $1.56$ to $0.055km/s^3$ (a reduction of $97\%$).
<div id='section'>Paperid: <span id='pid'>131, <a href='https://arxiv.org/pdf/2404.17269.pdf' target='_blank'>https://arxiv.org/pdf/2404.17269.pdf</a></span>   <span><a href='https://github.com/cztuda/semantic-feature-clustering' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Christoph Zelch, Jan Peters, Oskar von Stryk
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.17269">Clustering of Motion Trajectories by a Distance Measure Based on Semantic Features</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Clustering of motion trajectories is highly relevant for human-robot interactions as it allows the anticipation of human motions, fast reaction to those, as well as the recognition of explicit gestures. Further, it allows automated analysis of recorded motion data. Many clustering algorithms for trajectories build upon distance metrics that are based on pointwise Euclidean distances. However, our work indicates that focusing on salient characteristics is often sufficient. We present a novel distance measure for motion plans consisting of state and control trajectories that is based on a compressed representation built from their main features. This approach allows a flexible choice of feature classes relevant to the respective task. The distance measure is used in agglomerative hierarchical clustering. We compare our method with the widely used dynamic time warping algorithm on test sets of motion plans for the Furuta pendulum and the Manutec robot arm and on real-world data from a human motion dataset. The proposed method demonstrates slight advantages in clustering and strong advantages in runtime, especially for long trajectories.
<div id='section'>Paperid: <span id='pid'>132, <a href='https://arxiv.org/pdf/2404.13657.pdf' target='_blank'>https://arxiv.org/pdf/2404.13657.pdf</a></span>   <span><a href='https://github.com/eanson023/mlp' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sheng Yan, Mengyuan Liu, Yong Wang, Yang Liu, Chen Chen, Hong Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.13657">MLP: Motion Label Prior for Temporal Sentence Localization in Untrimmed 3D Human Motions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we address the unexplored question of temporal sentence localization in human motions (TSLM), aiming to locate a target moment from a 3D human motion that semantically corresponds to a text query. Considering that 3D human motions are captured using specialized motion capture devices, motions with only a few joints lack complex scene information like objects and lighting. Due to this character, motion data has low contextual richness and semantic ambiguity between frames, which limits the accuracy of predictions made by current video localization frameworks extended to TSLM to only a rough level. To refine this, we devise two novel label-prior-assisted training schemes: one embed prior knowledge of foreground and background to highlight the localization chances of target moments, and the other forces the originally rough predictions to overlap with the more accurate predictions obtained from the flipped start/end prior label sequences during recovery training. We show that injecting label-prior knowledge into the model is crucial for improving performance at high IoU. In our constructed TSLM benchmark, our model termed MLP achieves a recall of 44.13 at IoU@0.7 on the BABEL dataset and 71.17 on HumanML3D (Restore), outperforming prior works. Finally, we showcase the potential of our approach in corpus-level moment retrieval. Our source code is openly accessible at https://github.com/eanson023/mlp.
<div id='section'>Paperid: <span id='pid'>133, <a href='https://arxiv.org/pdf/2404.12867.pdf' target='_blank'>https://arxiv.org/pdf/2404.12867.pdf</a></span>   <span><a href='https://github.com/TabGuigui/FipTR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xingtai Gui, Tengteng Huang, Haonan Shao, Haotian Yao, Chi Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.12867">FipTR: A Simple yet Effective Transformer Framework for Future Instance Prediction in Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The future instance prediction from a Bird's Eye View(BEV) perspective is a vital component in autonomous driving, which involves future instance segmentation and instance motion prediction. Existing methods usually rely on a redundant and complex pipeline which requires multiple auxiliary outputs and post-processing procedures. Moreover, estimated errors on each of the auxiliary predictions will lead to degradation of the prediction performance. In this paper, we propose a simple yet effective fully end-to-end framework named Future Instance Prediction Transformer(FipTR), which views the task as BEV instance segmentation and prediction for future frames. We propose to adopt instance queries representing specific traffic participants to directly estimate the corresponding future occupied masks, and thus get rid of complex post-processing procedures. Besides, we devise a flow-aware BEV predictor for future BEV feature prediction composed of a flow-aware deformable attention that takes backward flow guiding the offset sampling. A novel future instance matching strategy is also proposed to further improve the temporal coherence. Extensive experiments demonstrate the superiority of FipTR and its effectiveness under different temporal BEV encoders. The code is available at https://github.com/TabGuigui/FipTR .
<div id='section'>Paperid: <span id='pid'>134, <a href='https://arxiv.org/pdf/2404.11327.pdf' target='_blank'>https://arxiv.org/pdf/2404.11327.pdf</a></span>   <span><a href='https://github.com/L-Scofano/SDA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Luca Scofano, Alessio Sampieri, Tommaso Campari, Valentino Sacco, Indro Spinelli, Lamberto Ballan, Fabio Galasso
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.11327">Following the Human Thread in Social Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The success of collaboration between humans and robots in shared environments relies on the robot's real-time adaptation to human motion. Specifically, in Social Navigation, the agent should be close enough to assist but ready to back up to let the human move freely, avoiding collisions. Human trajectories emerge as crucial cues in Social Navigation, but they are partially observable from the robot's egocentric view and computationally complex to process.
  We present the first Social Dynamics Adaptation model (SDA) based on the robot's state-action history to infer the social dynamics. We propose a two-stage Reinforcement Learning framework: the first learns to encode the human trajectories into social dynamics and learns a motion policy conditioned on this encoded information, the current status, and the previous action. Here, the trajectories are fully visible, i.e., assumed as privileged information. In the second stage, the trained policy operates without direct access to trajectories. Instead, the model infers the social dynamics solely from the history of previous actions and statuses in real-time. Tested on the novel Habitat 3.0 platform, SDA sets a novel state-of-the-art (SotA) performance in finding and following humans.
  The code can be found at https://github.com/L-Scofano/SDA.
<div id='section'>Paperid: <span id='pid'>135, <a href='https://arxiv.org/pdf/2404.10879.pdf' target='_blank'>https://arxiv.org/pdf/2404.10879.pdf</a></span>   <span><a href='https://github.com/TUMFTM/FlexMap_Fusion' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Maximilian Leitenstern, Florian Sauerbeck, Dominik Kulmer, Johannes Betz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.10879">FlexMap Fusion: Georeferencing and Automated Conflation of HD Maps with OpenStreetMap</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Today's software stacks for autonomous vehicles rely on HD maps to enable sufficient localization, accurate path planning, and reliable motion prediction. Recent developments have resulted in pipelines for the automated generation of HD maps to reduce manual efforts for creating and updating these HD maps. We present FlexMap Fusion, a methodology to automatically update and enhance existing HD vector maps using OpenStreetMap. Our approach is designed to enable the use of HD maps created from LiDAR and camera data within Autoware. The pipeline provides different functionalities: It provides the possibility to georeference both the point cloud map and the vector map using an RTK-corrected GNSS signal. Moreover, missing semantic attributes can be conflated from OpenStreetMap into the vector map. Differences between the HD map and OpenStreetMap are visualized for manual refinement by the user. In general, our findings indicate that our approach leads to reduced human labor during HD map generation, increases the scalability of the mapping pipeline, and improves the completeness and usability of the maps. The methodological choices may have resulted in limitations that arise especially at complex street structures, e.g., traffic islands. Therefore, more research is necessary to create efficient preprocessing algorithms and advancements in the dynamic adjustment of matching parameters. In order to build upon our work, our source code is available at https://github.com/TUMFTM/FlexMap_Fusion.
<div id='section'>Paperid: <span id='pid'>136, <a href='https://arxiv.org/pdf/2404.09445.pdf' target='_blank'>https://arxiv.org/pdf/2404.09445.pdf</a></span>   <span><a href='https://github.com/THU-LYJ-Lab/InstructMotion' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jenny Sheng, Matthieu Lin, Andrew Zhao, Kevin Pruvost, Yu-Hui Wen, Yangguang Li, Gao Huang, Yong-Jin Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.09445">Exploring Text-to-Motion Generation with Human Preference</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents an exploration of preference learning in text-to-motion generation. We find that current improvements in text-to-motion generation still rely on datasets requiring expert labelers with motion capture systems. Instead, learning from human preference data does not require motion capture systems; a labeler with no expertise simply compares two generated motions. This is particularly efficient because evaluating the model's output is easier than gathering the motion that performs a desired task (e.g. backflip). To pioneer the exploration of this paradigm, we annotate 3,528 preference pairs generated by MotionGPT, marking the first effort to investigate various algorithms for learning from preference data. In particular, our exploration highlights important design choices when using preference data. Additionally, our experimental results show that preference learning has the potential to greatly improve current text-to-motion generative models. Our code and dataset are publicly available at https://github.com/THU-LYJ-Lab/InstructMotion}{https://github.com/THU-LYJ-Lab/InstructMotion to further facilitate research in this area.
<div id='section'>Paperid: <span id='pid'>137, <a href='https://arxiv.org/pdf/2404.05218.pdf' target='_blank'>https://arxiv.org/pdf/2404.05218.pdf</a></span>   <span><a href='https://github.com/Jaewoo97/T2P' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jaewoo Jeong, Daehee Park, Kuk-Jin Yoon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.05218">Multi-agent Long-term 3D Human Pose Forecasting via Interaction-aware Trajectory Conditioning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human pose forecasting garners attention for its diverse applications. However, challenges in modeling the multi-modal nature of human motion and intricate interactions among agents persist, particularly with longer timescales and more agents. In this paper, we propose an interaction-aware trajectory-conditioned long-term multi-agent human pose forecasting model, utilizing a coarse-to-fine prediction approach: multi-modal global trajectories are initially forecasted, followed by respective local pose forecasts conditioned on each mode. In doing so, our Trajectory2Pose model introduces a graph-based agent-wise interaction module for a reciprocal forecast of local motion-conditioned global trajectory and trajectory-conditioned local pose. Our model effectively handles the multi-modality of human motion and the complexity of long-term multi-agent interactions, improving performance in complex environments. Furthermore, we address the lack of long-term (6s+) multi-agent (5+) datasets by constructing a new dataset from real-world images and 2D annotations, enabling a comprehensive evaluation of our proposed model. State-of-the-art prediction performance on both complex and simpler datasets confirms the generalized effectiveness of our method. The code is available at https://github.com/Jaewoo97/T2P.
<div id='section'>Paperid: <span id='pid'>138, <a href='https://arxiv.org/pdf/2404.03790.pdf' target='_blank'>https://arxiv.org/pdf/2404.03790.pdf</a></span>   <span><a href='https://github.com/stevens-armlab/uvms_bimanual_sim' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Justin Sitler, Srikarran Sowrirajan, Brendan Englot, Long Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.03790">A Bimanual Teleoperation Framework for Light Duty Underwater Vehicle-Manipulator Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In an effort to lower the barrier to entry in underwater manipulation, this paper presents an open-source, user-friendly framework for bimanual teleoperation of a light-duty underwater vehicle-manipulator system (UVMS). This framework allows for the control of the vehicle along with two manipulators and their end-effectors using two low-cost haptic devices.
  The UVMS kinematics are derived in order to create an independent resolved motion rate controller for each manipulator, which optimally controls the joint positions to achieve a desired end-effector pose. This desired pose is computed in real-time using a teleoperation controller developed to process the dual haptic device input from the user. A physics-based simulation environment is used to implement this framework for two example tasks as well as provide data for error analysis of user commands. The first task illustrates the functionality of the framework through motion control of the vehicle and manipulators using only the haptic devices. The second task is to grasp an object using both manipulators simultaneously, demonstrating precision and coordination using the framework. The framework code is available at https://github.com/stevens-armlab/uvms_bimanual_sim.
<div id='section'>Paperid: <span id='pid'>139, <a href='https://arxiv.org/pdf/2404.03789.pdf' target='_blank'>https://arxiv.org/pdf/2404.03789.pdf</a></span>   <span><a href='https://github.com/PurdueDigitalTwin/seneva' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Juanwu Lu, Can Cui, Yunsheng Ma, Aniket Bera, Ziran Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.03789">Quantifying Uncertainty in Motion Prediction with Variational Bayesian Mixture</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Safety and robustness are crucial factors in developing trustworthy autonomous vehicles. One essential aspect of addressing these factors is to equip vehicles with the capability to predict future trajectories for all moving objects in the surroundings and quantify prediction uncertainties. In this paper, we propose the Sequential Neural Variational Agent (SeNeVA), a generative model that describes the distribution of future trajectories for a single moving object. Our approach can distinguish Out-of-Distribution data while quantifying uncertainty and achieving competitive performance compared to state-of-the-art methods on the Argoverse 2 and INTERACTION datasets. Specifically, a 0.446 meters minimum Final Displacement Error, a 0.203 meters minimum Average Displacement Error, and a 5.35% Miss Rate are achieved on the INTERACTION test set. Extensive qualitative and quantitative analysis is also provided to evaluate the proposed model. Our open-source code is available at https://github.com/PurdueDigitalTwin/seneva.
<div id='section'>Paperid: <span id='pid'>140, <a href='https://arxiv.org/pdf/2404.03736.pdf' target='_blank'>https://arxiv.org/pdf/2404.03736.pdf</a></span>   <span><a href='https://github.com/JarrentWu1031/SC4D' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zijie Wu, Chaohui Yu, Yanqin Jiang, Chenjie Cao, Fan Wang, Xiang Bai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.03736">SC4D: Sparse-Controlled Video-to-4D Generation and Motion Transfer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in 2D/3D generative models enable the generation of dynamic 3D objects from a single-view video. Existing approaches utilize score distillation sampling to form the dynamic scene as dynamic NeRF or dense 3D Gaussians. However, these methods struggle to strike a balance among reference view alignment, spatio-temporal consistency, and motion fidelity under single-view conditions due to the implicit nature of NeRF or the intricate dense Gaussian motion prediction. To address these issues, this paper proposes an efficient, sparse-controlled video-to-4D framework named SC4D, that decouples motion and appearance to achieve superior video-to-4D generation. Moreover, we introduce Adaptive Gaussian (AG) initialization and Gaussian Alignment (GA) loss to mitigate shape degeneration issue, ensuring the fidelity of the learned motion and shape. Comprehensive experimental results demonstrate that our method surpasses existing methods in both quality and efficiency. In addition, facilitated by the disentangled modeling of motion and appearance of SC4D, we devise a novel application that seamlessly transfers the learned motion onto a diverse array of 4D entities according to textual descriptions.
<div id='section'>Paperid: <span id='pid'>141, <a href='https://arxiv.org/pdf/2403.18512.pdf' target='_blank'>https://arxiv.org/pdf/2403.18512.pdf</a></span>   <span><a href='https://github.com/qrzou/ParCo' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiran Zou, Shangyuan Yuan, Shian Du, Yu Wang, Chang Liu, Yi Xu, Jie Chen, Xiangyang Ji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.18512">ParCo: Part-Coordinating Text-to-Motion Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We study a challenging task: text-to-motion synthesis, aiming to generate motions that align with textual descriptions and exhibit coordinated movements. Currently, the part-based methods introduce part partition into the motion synthesis process to achieve finer-grained generation. However, these methods encounter challenges such as the lack of coordination between different part motions and difficulties for networks to understand part concepts. Moreover, introducing finer-grained part concepts poses computational complexity challenges. In this paper, we propose Part-Coordinating Text-to-Motion Synthesis (ParCo), endowed with enhanced capabilities for understanding part motions and communication among different part motion generators, ensuring a coordinated and fined-grained motion synthesis. Specifically, we discretize whole-body motion into multiple part motions to establish the prior concept of different parts. Afterward, we employ multiple lightweight generators designed to synthesize different part motions and coordinate them through our part coordination module. Our approach demonstrates superior performance on common benchmarks with economic computations, including HumanML3D and KIT-ML, providing substantial evidence of its effectiveness. Code is available at https://github.com/qrzou/ParCo .
<div id='section'>Paperid: <span id='pid'>142, <a href='https://arxiv.org/pdf/2403.17694.pdf' target='_blank'>https://arxiv.org/pdf/2403.17694.pdf</a></span>   <span><a href='https://github.com/scutzzj/AniPortrait' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Huawei Wei, Zejun Yang, Zhisheng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.17694">AniPortrait: Audio-Driven Synthesis of Photorealistic Portrait Animation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this study, we propose AniPortrait, a novel framework for generating high-quality animation driven by audio and a reference portrait image. Our methodology is divided into two stages. Initially, we extract 3D intermediate representations from audio and project them into a sequence of 2D facial landmarks. Subsequently, we employ a robust diffusion model, coupled with a motion module, to convert the landmark sequence into photorealistic and temporally consistent portrait animation. Experimental results demonstrate the superiority of AniPortrait in terms of facial naturalness, pose diversity, and visual quality, thereby offering an enhanced perceptual experience. Moreover, our methodology exhibits considerable potential in terms of flexibility and controllability, which can be effectively applied in areas such as facial motion editing or face reenactment. We release code and model weights at https://github.com/scutzzj/AniPortrait
<div id='section'>Paperid: <span id='pid'>143, <a href='https://arxiv.org/pdf/2403.14173.pdf' target='_blank'>https://arxiv.org/pdf/2403.14173.pdf</a></span>   <span><a href='https://github.com/kafeiyin00/HCTO' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianping Li, Shenghai Yuan, Muqing Cao, Thien-Minh Nguyen, Kun Cao, Lihua Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.14173">HCTO: Optimality-Aware LiDAR Inertial Odometry with Hybrid Continuous Time Optimization for Compact Wearable Mapping System</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Compact wearable mapping system (WMS) has gained significant attention due to their convenience in various applications. Specifically, it provides an efficient way to collect prior maps for 3D structure inspection and robot-based "last-mile delivery" in complex environments. However, vibrations in human motion and the uneven distribution of point cloud features in complex environments often lead to rapid drift, which is a prevalent issue when applying existing LiDAR Inertial Odometry (LIO) methods on low-cost WMS. To address these limitations, we propose a novel LIO for WMSs based on Hybrid Continuous Time Optimization (HCTO) considering the optimality of Lidar correspondences. First, HCTO recognizes patterns in human motion (high-frequency part, low-frequency part, and constant velocity part) by analyzing raw IMU measurements. Second, HCTO constructs hybrid IMU factors according to different motion states, which enables robust and accurate estimation against vibration-induced noise in the IMU measurements. Third, the best point correspondences are selected using optimal design to achieve real-time performance and better odometry accuracy. We conduct experiments on head-mounted WMS datasets to evaluate the performance of our system, demonstrating significant advantages over state-of-the-art methods. Video recordings of experiments can be found on the project page of HCTO: \href{https://github.com/kafeiyin00/HCTO}{https://github.com/kafeiyin00/HCTO}.
<div id='section'>Paperid: <span id='pid'>144, <a href='https://arxiv.org/pdf/2403.14104.pdf' target='_blank'>https://arxiv.org/pdf/2403.14104.pdf</a></span>   <span><a href='https://github.com/Motionpre/Adaptive-Salient-Loss-SAGGB' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhihao Wang, Yulin Zhou, Ningyu Zhang, Xiaosong Yang, Jun Xiao, Zhao Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.14104">Existence Is Chaos: Enhancing 3D Human Motion Prediction with Uncertainty Consideration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion prediction is consisting in forecasting future body poses from historically observed sequences. It is a longstanding challenge due to motion's complex dynamics and uncertainty. Existing methods focus on building up complicated neural networks to model the motion dynamics. The predicted results are required to be strictly similar to the training samples with L2 loss in current training pipeline. However, little attention has been paid to the uncertainty property which is crucial to the prediction task. We argue that the recorded motion in training data could be an observation of possible future, rather than a predetermined result. In addition, existing works calculate the predicted error on each future frame equally during training, while recent work indicated that different frames could play different roles. In this work, a novel computationally efficient encoder-decoder model with uncertainty consideration is proposed, which could learn proper characteristics for future frames by a dynamic function. Experimental results on benchmark datasets demonstrate that our uncertainty consideration approach has obvious advantages both in quantity and quality. Moreover, the proposed method could produce motion sequences with much better quality that avoids the intractable shaking artefacts. We believe our work could provide a novel perspective to consider the uncertainty quality for the general motion prediction task and encourage the studies in this field. The code will be available in https://github.com/Motionpre/Adaptive-Salient-Loss-SAGGB.
<div id='section'>Paperid: <span id='pid'>145, <a href='https://arxiv.org/pdf/2403.13518.pdf' target='_blank'>https://arxiv.org/pdf/2403.13518.pdf</a></span>   <span><a href='https://github.com/KunhangL/finemotiondiffuse' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kunhang Li, Yansong Feng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.13518">Motion Generation from Fine-grained Textual Descriptions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The task of text2motion is to generate human motion sequences from given textual descriptions, where the model explores diverse mappings from natural language instructions to human body movements. While most existing works are confined to coarse-grained motion descriptions, e.g., "A man squats.", fine-grained descriptions specifying movements of relevant body parts are barely explored. Models trained with coarse-grained texts may not be able to learn mappings from fine-grained motion-related words to motion primitives, resulting in the failure to generate motions from unseen descriptions. In this paper, we build a large-scale language-motion dataset specializing in fine-grained textual descriptions, FineHumanML3D, by feeding GPT-3.5-turbo with step-by-step instructions with pseudo-code compulsory checks. Accordingly, we design a new text2motion model, FineMotionDiffuse, making full use of fine-grained textual information. Our quantitative evaluation shows that FineMotionDiffuse trained on FineHumanML3D improves FID by a large margin of 0.38, compared with competitive baselines. According to the qualitative evaluation and case study, our model outperforms MotionDiffuse in generating spatially or chronologically composite motions, by learning the implicit mappings from fine-grained descriptions to the corresponding basic motions. We release our data at https://github.com/KunhangL/finemotiondiffuse.
<div id='section'>Paperid: <span id='pid'>146, <a href='https://arxiv.org/pdf/2403.12670.pdf' target='_blank'>https://arxiv.org/pdf/2403.12670.pdf</a></span>   <span><a href='https://github.com/library87/OpenRoboExp' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Boren Li, Hang Li, Hangxin Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.12670">Driving Animatronic Robot Facial Expression From Speech</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Animatronic robots hold the promise of enabling natural human-robot interaction through lifelike facial expressions. However, generating realistic, speech-synchronized robot expressions poses significant challenges due to the complexities of facial biomechanics and the need for responsive motion synthesis. This paper introduces a novel, skinning-centric approach to drive animatronic robot facial expressions from speech input. At its core, the proposed approach employs linear blend skinning (LBS) as a unifying representation, guiding innovations in both embodiment design and motion synthesis. LBS informs the actuation topology, facilitates human expression retargeting, and enables efficient speech-driven facial motion generation. This approach demonstrates the capability to produce highly realistic facial expressions on an animatronic face in real-time at over 4000 fps on a single Nvidia RTX 4090, significantly advancing robots' ability to replicate nuanced human expressions for natural interaction. To foster further research and development in this field, the code has been made publicly available at: \url{https://github.com/library87/OpenRoboExp}.
<div id='section'>Paperid: <span id='pid'>147, <a href='https://arxiv.org/pdf/2403.11492.pdf' target='_blank'>https://arxiv.org/pdf/2403.11492.pdf</a></span>   <span><a href='https://github.com/opendilab/SmartRefine/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yang Zhou, Hao Shao, Letian Wang, Steven L. Waslander, Hongsheng Li, Yu Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.11492">SmartRefine: A Scenario-Adaptive Refinement Framework for Efficient Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Predicting the future motion of surrounding agents is essential for autonomous vehicles (AVs) to operate safely in dynamic, human-robot-mixed environments. Context information, such as road maps and surrounding agents' states, provides crucial geometric and semantic information for motion behavior prediction. To this end, recent works explore two-stage prediction frameworks where coarse trajectories are first proposed, and then used to select critical context information for trajectory refinement. However, they either incur a large amount of computation or bring limited improvement, if not both. In this paper, we introduce a novel scenario-adaptive refinement strategy, named SmartRefine, to refine prediction with minimal additional computation. Specifically, SmartRefine can comprehensively adapt refinement configurations based on each scenario's properties, and smartly chooses the number of refinement iterations by introducing a quality score to measure the prediction quality and remaining refinement potential of each scenario. SmartRefine is designed as a generic and flexible approach that can be seamlessly integrated into most state-of-the-art motion prediction models. Experiments on Argoverse (1 & 2) show that our method consistently improves the prediction accuracy of multiple state-of-the-art prediction models. Specifically, by adding SmartRefine to QCNet, we outperform all published ensemble-free works on the Argoverse 2 leaderboard (single agent track) at submission. Comprehensive studies are also conducted to ablate design choices and explore the mechanism behind multi-iteration refinement. Codes are available at https://github.com/opendilab/SmartRefine/
<div id='section'>Paperid: <span id='pid'>148, <a href='https://arxiv.org/pdf/2403.11057.pdf' target='_blank'>https://arxiv.org/pdf/2403.11057.pdf</a></span>   <span><a href='https://github.com/AIR-DISCOVER/LLM-Augmented-MTR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoji Zheng, Lixiu Wu, Zhijie Yan, Yuanrong Tang, Hao Zhao, Chen Zhong, Bokui Chen, Jiangtao Gong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.11057">Large Language Models Powered Context-aware Motion Prediction in Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion prediction is among the most fundamental tasks in autonomous driving. Traditional methods of motion forecasting primarily encode vector information of maps and historical trajectory data of traffic participants, lacking a comprehensive understanding of overall traffic semantics, which in turn affects the performance of prediction tasks. In this paper, we utilized Large Language Models (LLMs) to enhance the global traffic context understanding for motion prediction tasks. We first conducted systematic prompt engineering, visualizing complex traffic environments and historical trajectory information of traffic participants into image prompts -- Transportation Context Map (TC-Map), accompanied by corresponding text prompts. Through this approach, we obtained rich traffic context information from the LLM. By integrating this information into the motion prediction model, we demonstrate that such context can enhance the accuracy of motion predictions. Furthermore, considering the cost associated with LLMs, we propose a cost-effective deployment strategy: enhancing the accuracy of motion prediction tasks at scale with 0.7\% LLM-augmented datasets. Our research offers valuable insights into enhancing the understanding of traffic scenes of LLMs and the motion prediction performance of autonomous driving. The source code is available at \url{https://github.com/AIR-DISCOVER/LLM-Augmented-MTR} and \url{https://aistudio.baidu.com/projectdetail/7809548}.
<div id='section'>Paperid: <span id='pid'>149, <a href='https://arxiv.org/pdf/2403.07420.pdf' target='_blank'>https://arxiv.org/pdf/2403.07420.pdf</a></span>   <span><a href='https://github.com/showlab/DragAnything' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Weijia Wu, Zhuang Li, Yuchao Gu, Rui Zhao, Yefei He, David Junhao Zhang, Mike Zheng Shou, Yan Li, Tingting Gao, Di Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.07420">DragAnything: Motion Control for Anything using Entity Representation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce DragAnything, which utilizes a entity representation to achieve motion control for any object in controllable video generation. Comparison to existing motion control methods, DragAnything offers several advantages. Firstly, trajectory-based is more userfriendly for interaction, when acquiring other guidance signals (e.g., masks, depth maps) is labor-intensive. Users only need to draw a line (trajectory) during interaction. Secondly, our entity representation serves as an open-domain embedding capable of representing any object, enabling the control of motion for diverse entities, including background. Lastly, our entity representation allows simultaneous and distinct motion control for multiple objects. Extensive experiments demonstrate that our DragAnything achieves state-of-the-art performance for FVD, FID, and User Study, particularly in terms of object motion control, where our method surpasses the previous methods (e.g., DragNUWA) by 26% in human voting.
<div id='section'>Paperid: <span id='pid'>150, <a href='https://arxiv.org/pdf/2403.05489.pdf' target='_blank'>https://arxiv.org/pdf/2403.05489.pdf</a></span>   <span><a href='https://github.com/kit-mrt/future-motion' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Royden Wagner, Omer Sahin Tas, Marvin Klemp, Carlos Fernandez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.05489">JointMotion: Joint Self-Supervision for Joint Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present JointMotion, a self-supervised pre-training method for joint motion prediction in self-driving vehicles. Our method jointly optimizes a scene-level objective connecting motion and environments, and an instance-level objective to refine learned representations. Scene-level representations are learned via non-contrastive similarity learning of past motion sequences and environment context. At the instance level, we use masked autoencoding to refine multimodal polyline representations. We complement this with an adaptive pre-training decoder that enables JointMotion to generalize across different environment representations, fusion mechanisms, and dataset characteristics. Notably, our method reduces the joint final displacement error of Wayformer, HPTR, and Scene Transformer models by 3\%, 8\%, and 12\%, respectively; and enables transfer learning between the Waymo Open Motion and the Argoverse 2 Motion Forecasting datasets. Code: https://github.com/kit-mrt/future-motion
<div id='section'>Paperid: <span id='pid'>151, <a href='https://arxiv.org/pdf/2402.19237.pdf' target='_blank'>https://arxiv.org/pdf/2402.19237.pdf</a></span>   <span><a href='https://github.com/QualityMinds/cistgcn' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Edgar Medina, Leyong Loh, Namrata Gurung, Kyung Hun Oh, Niels Heller
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.19237">Context-based Interpretable Spatio-Temporal Graph Convolutional Network for Human Motion Forecasting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion prediction is still an open problem extremely important for autonomous driving and safety applications. Due to the complex spatiotemporal relation of motion sequences, this remains a challenging problem not only for movement prediction but also to perform a preliminary interpretation of the joint connections. In this work, we present a Context-based Interpretable Spatio-Temporal Graph Convolutional Network (CIST-GCN), as an efficient 3D human pose forecasting model based on GCNs that encompasses specific layers, aiding model interpretability and providing information that might be useful when analyzing motion distribution and body behavior. Our architecture extracts meaningful information from pose sequences, aggregates displacements and accelerations into the input model, and finally predicts the output displacements. Extensive experiments on Human 3.6M, AMASS, 3DPW, and ExPI datasets demonstrate that CIST-GCN outperforms previous methods in human motion prediction and robustness. Since the idea of enhancing interpretability for motion prediction has its merits, we showcase experiments towards it and provide preliminary evaluations of such insights here. available code: https://github.com/QualityMinds/cistgcn
<div id='section'>Paperid: <span id='pid'>152, <a href='https://arxiv.org/pdf/2402.11502.pdf' target='_blank'>https://arxiv.org/pdf/2402.11502.pdf</a></span>   <span><a href='https://github.com/wzzheng/GenAD' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/wzzheng/GenAD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenzhao Zheng, Ruiqi Song, Xianda Guo, Chenming Zhang, Long Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.11502">GenAD: Generative End-to-End Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Directly producing planning results from raw sensors has been a long-desired solution for autonomous driving and has attracted increasing attention recently. Most existing end-to-end autonomous driving methods factorize this problem into perception, motion prediction, and planning. However, we argue that the conventional progressive pipeline still cannot comprehensively model the entire traffic evolution process, e.g., the future interaction between the ego car and other traffic participants and the structural trajectory prior. In this paper, we explore a new paradigm for end-to-end autonomous driving, where the key is to predict how the ego car and the surroundings evolve given past scenes. We propose GenAD, a generative framework that casts autonomous driving into a generative modeling problem. We propose an instance-centric scene tokenizer that first transforms the surrounding scenes into map-aware instance tokens. We then employ a variational autoencoder to learn the future trajectory distribution in a structural latent space for trajectory prior modeling. We further adopt a temporal model to capture the agent and ego movements in the latent space to generate more effective future trajectories. GenAD finally simultaneously performs motion prediction and planning by sampling distributions in the learned structural latent space conditioned on the instance tokens and using the learned temporal model to generate futures. Extensive experiments on the widely used nuScenes benchmark show that the proposed GenAD achieves state-of-the-art performance on vision-centric end-to-end autonomous driving with high efficiency. Code: https://github.com/wzzheng/GenAD.
<div id='section'>Paperid: <span id='pid'>153, <a href='https://arxiv.org/pdf/2402.02519.pdf' target='_blank'>https://arxiv.org/pdf/2402.02519.pdf</a></span>   <span><a href='https://github.com/HKUST-Aerial-Robotics/SIMPL' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/HKUST-Aerial-Robotics/SIMPL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lu Zhang, Peiliang Li, Sikang Liu, Shaojie Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.02519">SIMPL: A Simple and Efficient Multi-agent Motion Prediction Baseline for Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a Simple and effIcient Motion Prediction baseLine (SIMPL) for autonomous vehicles. Unlike conventional agent-centric methods with high accuracy but repetitive computations and scene-centric methods with compromised accuracy and generalizability, SIMPL delivers real-time, accurate motion predictions for all relevant traffic participants. To achieve improvements in both accuracy and inference speed, we propose a compact and efficient global feature fusion module that performs directed message passing in a symmetric manner, enabling the network to forecast future motion for all road users in a single feed-forward pass and mitigating accuracy loss caused by viewpoint shifting. Additionally, we investigate the continuous trajectory parameterization using Bernstein basis polynomials in trajectory decoding, allowing evaluations of states and their higher-order derivatives at any desired time point, which is valuable for downstream planning tasks. As a strong baseline, SIMPL exhibits highly competitive performance on Argoverse 1 & 2 motion forecasting benchmarks compared with other state-of-the-art methods. Furthermore, its lightweight design and low inference latency make SIMPL highly extensible and promising for real-world onboard deployment. We open-source the code at https://github.com/HKUST-Aerial-Robotics/SIMPL.
<div id='section'>Paperid: <span id='pid'>154, <a href='https://arxiv.org/pdf/2401.11037.pdf' target='_blank'>https://arxiv.org/pdf/2401.11037.pdf</a></span>   <span><a href='https://github.com/MinkaiXu/egno' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Minkai Xu, Jiaqi Han, Aaron Lou, Jean Kossaifi, Arvind Ramanathan, Kamyar Azizzadenesheli, Jure Leskovec, Stefano Ermon, Anima Anandkumar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.11037">Equivariant Graph Neural Operator for Modeling 3D Dynamics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modeling the complex three-dimensional (3D) dynamics of relational systems is an important problem in the natural sciences, with applications ranging from molecular simulations to particle mechanics. Machine learning methods have achieved good success by learning graph neural networks to model spatial interactions. However, these approaches do not faithfully capture temporal correlations since they only model next-step predictions. In this work, we propose Equivariant Graph Neural Operator (EGNO), a novel and principled method that directly models dynamics as trajectories instead of just next-step prediction. Different from existing methods, EGNO explicitly learns the temporal evolution of 3D dynamics where we formulate the dynamics as a function over time and learn neural operators to approximate it. To capture the temporal correlations while keeping the intrinsic SE(3)-equivariance, we develop equivariant temporal convolutions parameterized in the Fourier space and build EGNO by stacking the Fourier layers over equivariant networks. EGNO is the first operator learning framework that is capable of modeling solution dynamics functions over time while retaining 3D equivariance. Comprehensive experiments in multiple domains, including particle simulations, human motion capture, and molecular dynamics, demonstrate the significantly superior performance of EGNO against existing methods, thanks to the equivariant temporal modeling. Our code is available at https://github.com/MinkaiXu/egno.
<div id='section'>Paperid: <span id='pid'>155, <a href='https://arxiv.org/pdf/2401.08398.pdf' target='_blank'>https://arxiv.org/pdf/2401.08398.pdf</a></span>   <span><a href='https://github.com/grignarder/high-quality-blendshape-generation' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xin Ming, Jiawei Li, Jingwang Ling, Libo Zhang, Feng Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.08398">High-Quality Mesh Blendshape Generation from Face Videos via Neural Inverse Rendering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Readily editable mesh blendshapes have been widely used in animation pipelines, while recent advancements in neural geometry and appearance representations have enabled high-quality inverse rendering. Building upon these observations, we introduce a novel technique that reconstructs mesh-based blendshape rigs from single or sparse multi-view videos, leveraging state-of-the-art neural inverse rendering. We begin by constructing a deformation representation that parameterizes vertex displacements into differential coordinates with tetrahedral connections, allowing for high-quality vertex deformation on high-resolution meshes. By constructing a set of semantic regulations in this representation, we achieve joint optimization of blendshapes and expression coefficients. Furthermore, to enable a user-friendly multi-view setup with unsynchronized cameras, we propose a neural regressor to model time-varying motion parameters. This approach implicitly considers the time difference across multiple cameras, enhancing the accuracy of motion modeling. Experiments demonstrate that, with the flexible input of single or sparse multi-view videos, we reconstruct personalized high-fidelity blendshapes. These blendshapes are both geometrically and semantically accurate, and they are compatible with industrial animation pipelines. Code and data are available at https://github.com/grignarder/high-quality-blendshape-generation.
<div id='section'>Paperid: <span id='pid'>156, <a href='https://arxiv.org/pdf/2401.02142.pdf' target='_blank'>https://arxiv.org/pdf/2401.02142.pdf</a></span>   <span><a href='https://github.com/Xuehao-Gao/GUESS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuehao Gao, Yang Yang, Zhenyu Xie, Shaoyi Du, Zhongqian Sun, Yang Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.02142">GUESS:GradUally Enriching SyntheSis for Text-Driven Human Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose a novel cascaded diffusion-based generative framework for text-driven human motion synthesis, which exploits a strategy named GradUally Enriching SyntheSis (GUESS as its abbreviation). The strategy sets up generation objectives by grouping body joints of detailed skeletons in close semantic proximity together and then replacing each of such joint group with a single body-part node. Such an operation recursively abstracts a human pose to coarser and coarser skeletons at multiple granularity levels. With gradually increasing the abstraction level, human motion becomes more and more concise and stable, significantly benefiting the cross-modal motion synthesis task. The whole text-driven human motion synthesis problem is then divided into multiple abstraction levels and solved with a multi-stage generation framework with a cascaded latent diffusion model: an initial generator first generates the coarsest human motion guess from a given text description; then, a series of successive generators gradually enrich the motion details based on the textual description and the previous synthesized results. Notably, we further integrate GUESS with the proposed dynamic multi-condition fusion mechanism to dynamically balance the cooperative effects of the given textual condition and synthesized coarse motion prompt in different generation stages. Extensive experiments on large-scale datasets verify that GUESS outperforms existing state-of-the-art methods by large margins in terms of accuracy, realisticness, and diversity. Code is available at https://github.com/Xuehao-Gao/GUESS.
<div id='section'>Paperid: <span id='pid'>157, <a href='https://arxiv.org/pdf/2312.16221.pdf' target='_blank'>https://arxiv.org/pdf/2312.16221.pdf</a></span>   <span><a href='https://github.com/take2rohit/stride' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Rohit Lal, Saketh Bachu, Yash Garg, Arindam Dutta, Calvin-Khang Ta, Dripta S. Raychaudhuri, Hannah Dela Cruz, M. Salman Asif, Amit K. Roy-Chowdhury
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.16221">STRIDE: Single-video based Temporally Continuous Occlusion-Robust 3D Pose Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The capability to accurately estimate 3D human poses is crucial for diverse fields such as action recognition, gait recognition, and virtual/augmented reality. However, a persistent and significant challenge within this field is the accurate prediction of human poses under conditions of severe occlusion. Traditional image-based estimators struggle with heavy occlusions due to a lack of temporal context, resulting in inconsistent predictions. While video-based models benefit from processing temporal data, they encounter limitations when faced with prolonged occlusions that extend over multiple frames. This challenge arises because these models struggle to generalize beyond their training datasets, and the variety of occlusions is hard to capture in the training data. Addressing these challenges, we propose STRIDE (Single-video based TempoRally contInuous Occlusion-Robust 3D Pose Estimation), a novel Test-Time Training (TTT) approach to fit a human motion prior for each video. This approach specifically handles occlusions that were not encountered during the model's training. By employing STRIDE, we can refine a sequence of noisy initial pose estimates into accurate, temporally coherent poses during test time, effectively overcoming the limitations of prior methods. Our framework demonstrates flexibility by being model-agnostic, allowing us to use any off-the-shelf 3D pose estimation method for improving robustness and temporal consistency. We validate STRIDE's efficacy through comprehensive experiments on challenging datasets like Occluded Human3.6M, Human3.6M, and OCMotion, where it not only outperforms existing single-image and video-based pose estimation models but also showcases superior handling of substantial occlusions, achieving fast, robust, accurate, and temporally consistent 3D pose estimates. Code is made publicly available at https://github.com/take2rohit/stride
<div id='section'>Paperid: <span id='pid'>158, <a href='https://arxiv.org/pdf/2312.14937.pdf' target='_blank'>https://arxiv.org/pdf/2312.14937.pdf</a></span>   <span><a href='https://github.com/yihua7/SC-GS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi-Hua Huang, Yang-Tian Sun, Ziyi Yang, Xiaoyang Lyu, Yan-Pei Cao, Xiaojuan Qi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.14937">SC-GS: Sparse-Controlled Gaussian Splatting for Editable Dynamic Scenes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Novel view synthesis for dynamic scenes is still a challenging problem in computer vision and graphics. Recently, Gaussian splatting has emerged as a robust technique to represent static scenes and enable high-quality and real-time novel view synthesis. Building upon this technique, we propose a new representation that explicitly decomposes the motion and appearance of dynamic scenes into sparse control points and dense Gaussians, respectively. Our key idea is to use sparse control points, significantly fewer in number than the Gaussians, to learn compact 6 DoF transformation bases, which can be locally interpolated through learned interpolation weights to yield the motion field of 3D Gaussians. We employ a deformation MLP to predict time-varying 6 DoF transformations for each control point, which reduces learning complexities, enhances learning abilities, and facilitates obtaining temporal and spatial coherent motion patterns. Then, we jointly learn the 3D Gaussians, the canonical space locations of control points, and the deformation MLP to reconstruct the appearance, geometry, and dynamics of 3D scenes. During learning, the location and number of control points are adaptively adjusted to accommodate varying motion complexities in different regions, and an ARAP loss following the principle of as rigid as possible is developed to enforce spatial continuity and local rigidity of learned motions. Finally, thanks to the explicit sparse motion representation and its decomposition from appearance, our method can enable user-controlled motion editing while retaining high-fidelity appearances. Extensive experiments demonstrate that our approach outperforms existing approaches on novel view synthesis with a high rendering speed and enables novel appearance-preserved motion editing applications. Project page: https://yihua7.github.io/SC-GS-web/
<div id='section'>Paperid: <span id='pid'>159, <a href='https://arxiv.org/pdf/2312.11972.pdf' target='_blank'>https://arxiv.org/pdf/2312.11972.pdf</a></span>   <span><a href='https://github.com/Dingpx/EAI' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Pengxiang Ding, Qiongjie Cui, Min Zhang, Mengyuan Liu, Haofan Wang, Donglin Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.11972">Expressive Forecasting of 3D Whole-body Human Motions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion forecasting, with the goal of estimating future human behavior over a period of time, is a fundamental task in many real-world applications. However, existing works typically concentrate on predicting the major joints of the human body without considering the delicate movements of the human hands. In practical applications, hand gesture plays an important role in human communication with the real world, and expresses the primary intention of human beings. In this work, we are the first to formulate a whole-body human pose forecasting task, which jointly predicts the future body and hand activities. Correspondingly, we propose a novel Encoding-Alignment-Interaction (EAI) framework that aims to predict both coarse (body joints) and fine-grained (gestures) activities collaboratively, enabling expressive and cross-facilitated forecasting of 3D whole-body human motions. Specifically, our model involves two key constituents: cross-context alignment (XCA) and cross-context interaction (XCI). Considering the heterogeneous information within the whole-body, XCA aims to align the latent features of various human components, while XCI focuses on effectively capturing the context interaction among the human components. We conduct extensive experiments on a newly-introduced large-scale benchmark and achieve state-of-the-art performance. The code is public for research purposes at https://github.com/Dingpx/EAI.
<div id='section'>Paperid: <span id='pid'>160, <a href='https://arxiv.org/pdf/2312.09501.pdf' target='_blank'>https://arxiv.org/pdf/2312.09501.pdf</a></span>   <span><a href='https://github.com/Longzhong-Lin/EDA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Longzhong Lin, Xuewu Lin, Tianwei Lin, Lichao Huang, Rong Xiong, Yue Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.09501">EDA: Evolving and Distinct Anchors for Multimodal Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion prediction is a crucial task in autonomous driving, and one of its major challenges lands in the multimodality of future behaviors. Many successful works have utilized mixture models which require identification of positive mixture components, and correspondingly fall into two main lines: prediction-based and anchor-based matching. The prediction clustering phenomenon in prediction-based matching makes it difficult to pick representative trajectories for downstream tasks, while the anchor-based matching suffers from a limited regression capability. In this paper, we introduce a novel paradigm, named Evolving and Distinct Anchors (EDA), to define the positive and negative components for multimodal motion prediction based on mixture models. We enable anchors to evolve and redistribute themselves under specific scenes for an enlarged regression capacity. Furthermore, we select distinct anchors before matching them with the ground truth, which results in impressive scoring performance. Our approach enhances all metrics compared to the baseline MTR, particularly with a notable relative reduction of 13.5% in Miss Rate, resulting in state-of-the-art performance on the Waymo Open Motion Dataset. Code is available at https://github.com/Longzhong-Lin/EDA.
<div id='section'>Paperid: <span id='pid'>161, <a href='https://arxiv.org/pdf/2312.08009.pdf' target='_blank'>https://arxiv.org/pdf/2312.08009.pdf</a></span>   <span><a href='https://github.com/kwwcv/SSMP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kewei Wang, Yizheng Wu, Zhiyu Pan, Xingyi Li, Ke Xian, Zhe Wang, Zhiguo Cao, Guosheng Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.08009">Semi-Supervised Class-Agnostic Motion Prediction with Pseudo Label Regeneration and BEVMix</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Class-agnostic motion prediction methods aim to comprehend motion within open-world scenarios, holding significance for autonomous driving systems. However, training a high-performance model in a fully-supervised manner always requires substantial amounts of manually annotated data, which can be both expensive and time-consuming to obtain. To address this challenge, our study explores the potential of semi-supervised learning (SSL) for class-agnostic motion prediction. Our SSL framework adopts a consistency-based self-training paradigm, enabling the model to learn from unlabeled data by generating pseudo labels through test-time inference. To improve the quality of pseudo labels, we propose a novel motion selection and re-generation module. This module effectively selects reliable pseudo labels and re-generates unreliable ones. Furthermore, we propose two data augmentation strategies: temporal sampling and BEVMix. These strategies facilitate consistency regularization in SSL. Experiments conducted on nuScenes demonstrate that our SSL method can surpass the self-supervised approach by a large margin by utilizing only a tiny fraction of labeled data. Furthermore, our method exhibits comparable performance to weakly and some fully supervised methods. These results highlight the ability of our method to strike a favorable balance between annotation costs and performance. Code will be available at https://github.com/kwwcv/SSMP.
<div id='section'>Paperid: <span id='pid'>162, <a href='https://arxiv.org/pdf/2312.04152.pdf' target='_blank'>https://arxiv.org/pdf/2312.04152.pdf</a></span>   <span><a href='https://github.com/VUT-HFUT/EulerMormer' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fei Wang, Dan Guo, Kun Li, Meng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.04152">EulerMormer: Robust Eulerian Motion Magnification via Dynamic Filtering within Transformer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video Motion Magnification (VMM) aims to break the resolution limit of human visual perception capability and reveal the imperceptible minor motion that contains valuable information in the macroscopic domain. However, challenges arise in this task due to photon noise inevitably introduced by photographic devices and spatial inconsistency in amplification, leading to flickering artifacts in static fields and motion blur and distortion in dynamic fields in the video. Existing methods focus on explicit motion modeling without emphasizing prioritized denoising during the motion magnification process. This paper proposes a novel dynamic filtering strategy to achieve static-dynamic field adaptive denoising. Specifically, based on Eulerian theory, we separate texture and shape to extract motion representation through inter-frame shape differences, expecting to leverage these subdivided features to solve this task finely. Then, we introduce a novel dynamic filter that eliminates noise cues and preserves critical features in the motion magnification and amplification generation phases. Overall, our unified framework, EulerMormer, is a pioneering effort to first equip with Transformer in learning-based VMM. The core of the dynamic filter lies in a global dynamic sparse cross-covariance attention mechanism that explicitly removes noise while preserving vital information, coupled with a multi-scale dual-path gating mechanism that selectively regulates the dependence on different frequency features to reduce spatial attenuation and complement motion boundaries. We demonstrate extensive experiments that EulerMormer achieves more robust video motion magnification from the Eulerian perspective, significantly outperforming state-of-the-art methods. The source code is available at https://github.com/VUT-HFUT/EulerMormer.
<div id='section'>Paperid: <span id='pid'>163, <a href='https://arxiv.org/pdf/2312.03703.pdf' target='_blank'>https://arxiv.org/pdf/2312.03703.pdf</a></span>   <span><a href='https://github.com/fanglaosi/Skeleton-in-Context' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinshun Wang, Zhongbin Fang, Xia Li, Xiangtai Li, Mengyuan Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.03703">Skeleton-in-Context: Unified Skeleton Sequence Modeling with In-Context Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In-context learning provides a new perspective for multi-task modeling for vision and NLP. Under this setting, the model can perceive tasks from prompts and accomplish them without any extra task-specific head predictions or model fine-tuning. However, Skeleton sequence modeling via in-context learning remains unexplored. Directly applying existing in-context models from other areas onto skeleton sequences fails due to the inter-frame and cross-task pose similarity that makes it outstandingly hard to perceive the task correctly from a subtle context. To address this challenge, we propose Skeleton-in-Context (SiC), an effective framework for in-context skeleton sequence modeling. Our SiC is able to handle multiple skeleton-based tasks simultaneously after a single training process and accomplish each task from context according to the given prompt. It can further generalize to new, unseen tasks according to customized prompts. To facilitate context perception, we additionally propose a task-unified prompt, which adaptively learns tasks of different natures, such as partial joint-level generation, sequence-level prediction, or 2D-to-3D motion prediction. We conduct extensive experiments to evaluate the effectiveness of our SiC on multiple tasks, including motion prediction, pose estimation, joint completion, and future pose estimation. We also evaluate its generalization capability on unseen tasks such as motion-in-between. These experiments show that our model achieves state-of-the-art multi-task performance and even outperforms single-task methods on certain tasks.
<div id='section'>Paperid: <span id='pid'>164, <a href='https://arxiv.org/pdf/2312.01283.pdf' target='_blank'>https://arxiv.org/pdf/2312.01283.pdf</a></span>   <span><a href='https://github.com/fcntes/IndoorDepth' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chao Fan, Zhenyu Yin, Yue Li, Feiqing Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.01283">Deeper into Self-Supervised Monocular Indoor Depth Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Monocular depth estimation using Convolutional Neural Networks (CNNs) has shown impressive performance in outdoor driving scenes. However, self-supervised learning of indoor depth from monocular sequences is quite challenging for researchers because of the following two main reasons. One is the large areas of low-texture regions and the other is the complex ego-motion on indoor training datasets. In this work, our proposed method, named IndoorDepth, consists of two innovations. In particular, we first propose a novel photometric loss with improved structural similarity (SSIM) function to tackle the challenge from low-texture regions. Moreover, in order to further mitigate the issue of inaccurate ego-motion prediction, multiple photometric losses at different stages are used to train a deeper pose network with two residual pose blocks. Subsequent ablation study can validate the effectiveness of each new idea. Experiments on the NYUv2 benchmark demonstrate that our IndoorDepth outperforms the previous state-of-the-art methods by a large margin. In addition, we also validate the generalization ability of our method on ScanNet dataset. Code is availabe at https://github.com/fcntes/IndoorDepth.
<div id='section'>Paperid: <span id='pid'>165, <a href='https://arxiv.org/pdf/2311.15864.pdf' target='_blank'>https://arxiv.org/pdf/2311.15864.pdf</a></span>   <span><a href='https://github.com/zhenzhiwang/intercontrol' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenzhi Wang, Jingbo Wang, Yixuan Li, Dahua Lin, Bo Dai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.15864">InterControl: Zero-shot Human Interaction Generation by Controlling Every Joint</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-conditioned motion synthesis has made remarkable progress with the emergence of diffusion models. However, the majority of these motion diffusion models are primarily designed for a single character and overlook multi-human interactions. In our approach, we strive to explore this problem by synthesizing human motion with interactions for a group of characters of any size in a zero-shot manner. The key aspect of our approach is the adaptation of human-wise interactions as pairs of human joints that can be either in contact or separated by a desired distance. In contrast to existing methods that necessitate training motion generation models on multi-human motion datasets with a fixed number of characters, our approach inherently possesses the flexibility to model human interactions involving an arbitrary number of individuals, thereby transcending the limitations imposed by the training data. We introduce a novel controllable motion generation method, InterControl, to encourage the synthesized motions maintaining the desired distance between joint pairs. It consists of a motion controller and an inverse kinematics guidance module that realistically and accurately aligns the joints of synthesized characters to the desired location. Furthermore, we demonstrate that the distance between joint pairs for human-wise interactions can be generated using an off-the-shelf Large Language Model (LLM). Experimental results highlight the capability of our framework to generate interactions with multiple human characters and its potential to work with off-the-shelf physics-based character simulators. Code is available at https://github.com/zhenzhiwang/intercontrol
<div id='section'>Paperid: <span id='pid'>166, <a href='https://arxiv.org/pdf/2311.01015.pdf' target='_blank'>https://arxiv.org/pdf/2311.01015.pdf</a></span>   <span><a href='https://github.com/jpthu17/GraphMotion' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Peng Jin, Yang Wu, Yanbo Fan, Zhongqian Sun, Yang Wei, Li Yuan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.01015">Act As You Wish: Fine-Grained Control of Motion Diffusion Model with Hierarchical Semantic Graphs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Most text-driven human motion generation methods employ sequential modeling approaches, e.g., transformer, to extract sentence-level text representations automatically and implicitly for human motion synthesis. However, these compact text representations may overemphasize the action names at the expense of other important properties and lack fine-grained details to guide the synthesis of subtly distinct motion. In this paper, we propose hierarchical semantic graphs for fine-grained control over motion generation. Specifically, we disentangle motion descriptions into hierarchical semantic graphs including three levels of motions, actions, and specifics. Such global-to-local structures facilitate a comprehensive understanding of motion description and fine-grained control of motion generation. Correspondingly, to leverage the coarse-to-fine topology of hierarchical semantic graphs, we decompose the text-to-motion diffusion process into three semantic levels, which correspond to capturing the overall motion, local actions, and action specifics. Extensive experiments on two benchmark human motion datasets, including HumanML3D and KIT, with superior performances, justify the efficacy of our method. More encouragingly, by modifying the edge weights of hierarchical semantic graphs, our method can continuously refine the generated motion, which may have a far-reaching impact on the community. Code and pre-training weights are available at https://github.com/jpthu17/GraphMotion.
<div id='section'>Paperid: <span id='pid'>167, <a href='https://arxiv.org/pdf/2310.17403.pdf' target='_blank'>https://arxiv.org/pdf/2310.17403.pdf</a></span>   <span><a href='https://github.com/cv-stuttgart/DetectionDefenses' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Erik Scheurer, Jenny Schmalfuss, Alexander Lis, AndrÃ©s Bruhn
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.17403">Detection Defenses: An Empty Promise against Adversarial Patch Attacks on Optical Flow</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Adversarial patches undermine the reliability of optical flow predictions when placed in arbitrary scene locations. Therefore, they pose a realistic threat to real-world motion detection and its downstream applications. Potential remedies are defense strategies that detect and remove adversarial patches, but their influence on the underlying motion prediction has not been investigated. In this paper, we thoroughly examine the currently available detect-and-remove defenses ILP and LGS for a wide selection of state-of-the-art optical flow methods, and illuminate their side effects on the quality and robustness of the final flow predictions. In particular, we implement defense-aware attacks to investigate whether current defenses are able to withstand attacks that take the defense mechanism into account. Our experiments yield two surprising results: Detect-and-remove defenses do not only lower the optical flow quality on benign scenes, in doing so, they also harm the robustness under patch attacks for all tested optical flow methods except FlowNetC. As currently employed detect-and-remove defenses fail to deliver the promised adversarial robustness for optical flow, they evoke a false sense of security. The code is available at https://github.com/cv-stuttgart/DetectionDefenses.
<div id='section'>Paperid: <span id='pid'>168, <a href='https://arxiv.org/pdf/2310.12970.pdf' target='_blank'>https://arxiv.org/pdf/2310.12970.pdf</a></span>   <span><a href='https://github.com/zhejz/HPTR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhejun Zhang, Alexander Liniger, Christos Sakaridis, Fisher Yu, Luc Van Gool
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.12970">Real-Time Motion Prediction via Heterogeneous Polyline Transformer with Relative Pose Encoding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The real-world deployment of an autonomous driving system requires its components to run on-board and in real-time, including the motion prediction module that predicts the future trajectories of surrounding traffic participants. Existing agent-centric methods have demonstrated outstanding performance on public benchmarks. However, they suffer from high computational overhead and poor scalability as the number of agents to be predicted increases. To address this problem, we introduce the K-nearest neighbor attention with relative pose encoding (KNARPE), a novel attention mechanism allowing the pairwise-relative representation to be used by Transformers. Then, based on KNARPE we present the Heterogeneous Polyline Transformer with Relative pose encoding (HPTR), a hierarchical framework enabling asynchronous token update during the online inference. By sharing contexts among agents and reusing the unchanged contexts, our approach is as efficient as scene-centric methods, while performing on par with state-of-the-art agent-centric methods. Experiments on Waymo and Argoverse-2 datasets show that HPTR achieves superior performance among end-to-end methods that do not apply expensive post-processing or model ensembling. The code is available at https://github.com/zhejz/HPTR.
<div id='section'>Paperid: <span id='pid'>169, <a href='https://arxiv.org/pdf/2310.08114.pdf' target='_blank'>https://arxiv.org/pdf/2310.08114.pdf</a></span>   <span><a href='https://github.com/TUMFTM/FusionTracking' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Phillip Karle, Felix Fent, Sebastian Huch, Florian Sauerbeck, Markus Lienkamp
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.08114">Multi-Modal Sensor Fusion and Object Tracking for Autonomous Racing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reliable detection and tracking of surrounding objects are indispensable for comprehensive motion prediction and planning of autonomous vehicles. Due to the limitations of individual sensors, the fusion of multiple sensor modalities is required to improve the overall detection capabilities. Additionally, robust motion tracking is essential for reducing the effect of sensor noise and improving state estimation accuracy. The reliability of the autonomous vehicle software becomes even more relevant in complex, adversarial high-speed scenarios at the vehicle handling limits in autonomous racing. In this paper, we present a modular multi-modal sensor fusion and tracking method for high-speed applications. The method is based on the Extended Kalman Filter (EKF) and is capable of fusing heterogeneous detection inputs to track surrounding objects consistently. A novel delay compensation approach enables to reduce the influence of the perception software latency and to output an updated object list. It is the first fusion and tracking method validated in high-speed real-world scenarios at the Indy Autonomous Challenge 2021 and the Autonomous Challenge at CES (AC@CES) 2022, proving its robustness and computational efficiency on embedded systems. It does not require any labeled data and achieves position tracking residuals below 0.1 m. The related code is available as open-source software at https://github.com/TUMFTM/FusionTracking.
<div id='section'>Paperid: <span id='pid'>170, <a href='https://arxiv.org/pdf/2310.08080.pdf' target='_blank'>https://arxiv.org/pdf/2310.08080.pdf</a></span>   <span><a href='https://github.com/ZywooSimple/RT-SRTS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Miao Zhu, Qiming Fu, Bo Liu, Mengxi Zhang, Bojian Li, Xiaoyan Luo, Fugen Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.08080">RT-SRTS: Angle-Agnostic Real-Time Simultaneous 3D Reconstruction and Tumor Segmentation from Single X-Ray Projection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Radiotherapy is one of the primary treatment methods for tumors, but the organ movement caused by respiration limits its accuracy. Recently, 3D imaging from a single X-ray projection has received extensive attention as a promising approach to address this issue. However, current methods can only reconstruct 3D images without directly locating the tumor and are only validated for fixed-angle imaging, which fails to fully meet the requirements of motion control in radiotherapy. In this study, a novel imaging method RT-SRTS is proposed which integrates 3D imaging and tumor segmentation into one network based on multi-task learning (MTL) and achieves real-time simultaneous 3D reconstruction and tumor segmentation from a single X-ray projection at any angle. Furthermore, the attention enhanced calibrator (AEC) and uncertain-region elaboration (URE) modules have been proposed to aid feature extraction and improve segmentation accuracy. The proposed method was evaluated on fifteen patient cases and compared with three state-of-the-art methods. It not only delivers superior 3D reconstruction but also demonstrates commendable tumor segmentation results. Simultaneous reconstruction and segmentation can be completed in approximately 70 ms, significantly faster than the required time threshold for real-time tumor tracking. The efficacies of both AEC and URE have also been validated in ablation studies. The code of work is available at https://github.com/ZywooSimple/RT-SRTS.
<div id='section'>Paperid: <span id='pid'>171, <a href='https://arxiv.org/pdf/2310.07916.pdf' target='_blank'>https://arxiv.org/pdf/2310.07916.pdf</a></span>   <span><a href='https://github.com/Cenbylin/DAP-NeRF' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ancheng Lin, Yusheng Xiang, Jun Li, Mukesh Prasad
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.07916">Dynamic Appearance Particle Neural Radiance Field</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Neural Radiance Fields (NeRFs) have shown great potential in modeling 3D scenes. Dynamic NeRFs extend this model by capturing time-varying elements, typically using deformation fields. The existing dynamic NeRFs employ a similar Eulerian representation for both light radiance and deformation fields. This leads to a close coupling of appearance and motion and lacks a physical interpretation. In this work, we propose Dynamic Appearance Particle Neural Radiance Field (DAP-NeRF), which introduces particle-based representation to model the motions of visual elements in a dynamic 3D scene. DAP-NeRF consists of the superposition of a static field and a dynamic field. The dynamic field is quantized as a collection of appearance particles, which carries the visual information of a small dynamic element in the scene and is equipped with a motion model. All components, including the static field, the visual features and the motion models of particles, are learned from monocular videos without any prior geometric knowledge of the scene. We develop an efficient computational framework for the particle-based model. We also construct a new dataset to evaluate motion modeling. Experimental results show that DAP-NeRF is an effective technique to capture not only the appearance but also the physically meaningful motions in a 3D dynamic scene. Code is available at: https://github.com/Cenbylin/DAP-NeRF.
<div id='section'>Paperid: <span id='pid'>172, <a href='https://arxiv.org/pdf/2310.07324.pdf' target='_blank'>https://arxiv.org/pdf/2310.07324.pdf</a></span>   <span><a href='https://github.com/rd20karim/M2T-Interpretable' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Karim Radouane, Julien Lagarde, Sylvie Ranwez, Andon Tchechmedjiev
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.07324">Guided Attention for Interpretable Motion Captioning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diverse and extensive work has recently been conducted on text-conditioned human motion generation. However, progress in the reverse direction, motion captioning, has seen less comparable advancement. In this paper, we introduce a novel architecture design that enhances text generation quality by emphasizing interpretability through spatio-temporal and adaptive attention mechanisms. To encourage human-like reasoning, we propose methods for guiding attention during training, emphasizing relevant skeleton areas over time and distinguishing motion-related words. We discuss and quantify our model's interpretability using relevant histograms and density distributions. Furthermore, we leverage interpretability to derive fine-grained information about human motion, including action localization, body part identification, and the distinction of motion-related words. Finally, we discuss the transferability of our approaches to other tasks. Our experiments demonstrate that attention guidance leads to interpretable captioning while enhancing performance compared to higher parameter-count, non-interpretable state-of-the-art systems. The code is available at: https://github.com/rd20karim/M2T-Interpretable.
<div id='section'>Paperid: <span id='pid'>173, <a href='https://arxiv.org/pdf/2309.10948.pdf' target='_blank'>https://arxiv.org/pdf/2309.10948.pdf</a></span>   <span><a href='https://github.com/Amir-Samadi/VVF-TP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>MReza Alipour Sormoli, Amir Samadi, Sajjad Mozaffari, Konstantinos Koufos, Mehrdad Dianati, Roger Woodman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.10948">A Novel Deep Neural Network for Trajectory Prediction in Automated Vehicles Using Velocity Vector Field</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Anticipating the motion of other road users is crucial for automated driving systems (ADS), as it enables safe and informed downstream decision-making and motion planning. Unfortunately, contemporary learning-based approaches for motion prediction exhibit significant performance degradation as the prediction horizon increases or the observation window decreases. This paper proposes a novel technique for trajectory prediction that combines a data-driven learning-based method with a velocity vector field (VVF) generated from a nature-inspired concept, i.e., fluid flow dynamics. In this work, the vector field is incorporated as an additional input to a convolutional-recurrent deep neural network to help predict the most likely future trajectories given a sequence of bird's eye view scene representations. The performance of the proposed model is compared with state-of-the-art methods on the HighD dataset demonstrating that the VVF inclusion improves the prediction accuracy for both short and long-term (5~sec) time horizons. It is also shown that the accuracy remains consistent with decreasing observation windows which alleviates the requirement of a long history of past observations for accurate trajectory prediction. Source codes are available at: https://github.com/Amir-Samadi/VVF-TP.
<div id='section'>Paperid: <span id='pid'>174, <a href='https://arxiv.org/pdf/2309.08947.pdf' target='_blank'>https://arxiv.org/pdf/2309.08947.pdf</a></span>   <span><a href='https://github.com/L-Scofano/STAG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Luca Scofano, Alessio Sampieri, Elisabeth Schiele, Edoardo De Matteis, Laura Leal-TaixÃ©, Fabio Galasso
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.08947">Staged Contact-Aware Global Human Motion Forecasting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scene-aware global human motion forecasting is critical for manifold applications, including virtual reality, robotics, and sports. The task combines human trajectory and pose forecasting within the provided scene context, which represents a significant challenge.
  So far, only Mao et al. NeurIPS'22 have addressed scene-aware global motion, cascading the prediction of future scene contact points and the global motion estimation. They perform the latter as the end-to-end forecasting of future trajectories and poses. However, end-to-end contrasts with the coarse-to-fine nature of the task and it results in lower performance, as we demonstrate here empirically.
  We propose a STAGed contact-aware global human motion forecasting STAG, a novel three-stage pipeline for predicting global human motion in a 3D environment. We first consider the scene and the respective human interaction as contact points. Secondly, we model the human trajectory forecasting within the scene, predicting the coarse motion of the human body as a whole. The third and last stage matches a plausible fine human joint motion to complement the trajectory considering the estimated contacts.
  Compared to the state-of-the-art (SoA), STAG achieves a 1.8% and 16.2% overall improvement in pose and trajectory prediction, respectively, on the scene-aware GTA-IM dataset. A comprehensive ablation study confirms the advantages of staged modeling over end-to-end approaches. Furthermore, we establish the significance of a newly proposed temporal counter called the "time-to-go", which tells how long it is before reaching scene contact and endpoints. Notably, STAG showcases its ability to generalize to datasets lacking a scene and achieves a new state-of-the-art performance on CMU-Mocap, without leveraging any social cues. Our code is released at: https://github.com/L-Scofano/STAG
<div id='section'>Paperid: <span id='pid'>175, <a href='https://arxiv.org/pdf/2309.03387.pdf' target='_blank'>https://arxiv.org/pdf/2309.03387.pdf</a></span>   <span><a href='https://github.com/Cram3r95/mapfe4mp' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Carlos GÃ³mez-HuÃ©lamo, Marcos V. Conde, Rafael Barea, Manuel OcaÃ±a, Luis M. Bergasa
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.03387">Efficient Baselines for Motion Prediction in Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion Prediction (MP) of multiple surroundings agents is a crucial task in arbitrarily complex environments, from simple robots to Autonomous Driving Stacks (ADS). Current techniques tackle this problem using end-to-end pipelines, where the input data is usually a rendered top-view of the physical information and the past trajectories of the most relevant agents; leveraging this information is a must to obtain optimal performance. In that sense, a reliable ADS must produce reasonable predictions on time. However, despite many approaches use simple ConvNets and LSTMs to obtain the social latent features, State-Of-The-Art (SOTA) models might be too complex for real-time applications when using both sources of information (map and past trajectories) as well as little interpretable, specially considering the physical information. Moreover, the performance of such models highly depends on the number of available inputs for each particular traffic scenario, which are expensive to obtain, particularly, annotated High-Definition (HD) maps.
  In this work, we propose several efficient baselines for the well-known Argoverse 1 Motion Forecasting Benchmark. We aim to develop compact models using SOTA techniques for MP, including attention mechanisms and GNNs. Our lightweight models use standard social information and interpretable map information such as points from the driveable area and plausible centerlines by means of a novel preprocessing step based on kinematic constraints, in opposition to black-box CNN-based or too-complex graphs methods for map encoding, to generate plausible multimodal trajectories achieving up-to-pair accuracy with less operations and parameters than other SOTA methods. Our code is publicly available at https://github.com/Cram3r95/mapfe4mp .
<div id='section'>Paperid: <span id='pid'>176, <a href='https://arxiv.org/pdf/2309.00796.pdf' target='_blank'>https://arxiv.org/pdf/2309.00796.pdf</a></span>   <span><a href='https://github.com/ZcyMonkey/AttT2M' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chongyang Zhong, Lei Hu, Zihao Zhang, Shihong Xia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.00796">AttT2M: Text-Driven Human Motion Generation with Multi-Perspective Attention Mechanism</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating 3D human motion based on textual descriptions has been a research focus in recent years. It requires the generated motion to be diverse, natural, and conform to the textual description. Due to the complex spatio-temporal nature of human motion and the difficulty in learning the cross-modal relationship between text and motion, text-driven motion generation is still a challenging problem. To address these issues, we propose \textbf{AttT2M}, a two-stage method with multi-perspective attention mechanism: \textbf{body-part attention} and \textbf{global-local motion-text attention}. The former focuses on the motion embedding perspective, which means introducing a body-part spatio-temporal encoder into VQ-VAE to learn a more expressive discrete latent space. The latter is from the cross-modal perspective, which is used to learn the sentence-level and word-level motion-text cross-modal relationship. The text-driven motion is finally generated with a generative transformer. Extensive experiments conducted on HumanML3D and KIT-ML demonstrate that our method outperforms the current state-of-the-art works in terms of qualitative and quantitative evaluation, and achieve fine-grained synthesis and action2motion. Our code is in https://github.com/ZcyMonkey/AttT2M
<div id='section'>Paperid: <span id='pid'>177, <a href='https://arxiv.org/pdf/2308.16801.pdf' target='_blank'>https://arxiv.org/pdf/2308.16801.pdf</a></span>   <span><a href='https://github.com/MohsenZand/ResChunk' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohsen Zand, Ali Etemad, Michael Greenspan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.16801">Multiscale Residual Learning of Graph Convolutional Sequence Chunks for Human Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A new method is proposed for human motion prediction by learning temporal and spatial dependencies. Recently, multiscale graphs have been developed to model the human body at higher abstraction levels, resulting in more stable motion prediction. Current methods however predetermine scale levels and combine spatially proximal joints to generate coarser scales based on human priors, even though movement patterns in different motion sequences vary and do not fully comply with a fixed graph of spatially connected joints. Another problem with graph convolutional methods is mode collapse, in which predicted poses converge around a mean pose with no discernible movements, particularly in long-term predictions. To tackle these issues, we propose ResChunk, an end-to-end network which explores dynamically correlated body components based on the pairwise relationships between all joints in individual sequences. ResChunk is trained to learn the residuals between target sequence chunks in an autoregressive manner to enforce the temporal connectivities between consecutive chunks. It is hence a sequence-to-sequence prediction network which considers dynamic spatio-temporal features of sequences at multiple levels. Our experiments on two challenging benchmark datasets, CMU Mocap and Human3.6M, demonstrate that our proposed method is able to effectively model the sequence information for motion prediction and outperform other techniques to set a new state-of-the-art. Our code is available at https://github.com/MohsenZand/ResChunk.
<div id='section'>Paperid: <span id='pid'>178, <a href='https://arxiv.org/pdf/2308.11875.pdf' target='_blank'>https://arxiv.org/pdf/2308.11875.pdf</a></span>   <span><a href='https://github.com/LeoZhiheng/MTM-Tracker.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiheng Li, Yu Lin, Yubo Cui, Shuo Li, Zheng Fang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.11875">Motion-to-Matching: A Mixed Paradigm for 3D Single Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D single object tracking with LiDAR points is an important task in the computer vision field. Previous methods usually adopt the matching-based or motion-centric paradigms to estimate the current target status. However, the former is sensitive to the similar distractors and the sparseness of point cloud due to relying on appearance matching, while the latter usually focuses on short-term motion clues (eg. two frames) and ignores the long-term motion pattern of target. To address these issues, we propose a mixed paradigm with two stages, named MTM-Tracker, which combines motion modeling with feature matching into a single network. Specifically, in the first stage, we exploit the continuous historical boxes as motion prior and propose an encoder-decoder structure to locate target coarsely. Then, in the second stage, we introduce a feature interaction module to extract motion-aware features from consecutive point clouds and match them to refine target movement as well as regress other target states. Extensive experiments validate that our paradigm achieves competitive performance on large-scale datasets (70.9% in KITTI and 51.70% in NuScenes). The code will be open soon at https://github.com/LeoZhiheng/MTM-Tracker.git.
<div id='section'>Paperid: <span id='pid'>179, <a href='https://arxiv.org/pdf/2308.10305.pdf' target='_blank'>https://arxiv.org/pdf/2308.10305.pdf</a></span>   <span><a href='https://github.com/kasvii/PMCE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yingxuan You, Hong Liu, Ti Wang, Wenhao Li, Runwei Ding, Xia Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.10305">Co-Evolution of Pose and Mesh for 3D Human Body Estimation from Video</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite significant progress in single image-based 3D human mesh recovery, accurately and smoothly recovering 3D human motion from a video remains challenging. Existing video-based methods generally recover human mesh by estimating the complex pose and shape parameters from coupled image features, whose high complexity and low representation ability often result in inconsistent pose motion and limited shape patterns. To alleviate this issue, we introduce 3D pose as the intermediary and propose a Pose and Mesh Co-Evolution network (PMCE) that decouples this task into two parts: 1) video-based 3D human pose estimation and 2) mesh vertices regression from the estimated 3D pose and temporal image feature. Specifically, we propose a two-stream encoder that estimates mid-frame 3D pose and extracts a temporal image feature from the input image sequence. In addition, we design a co-evolution decoder that performs pose and mesh interactions with the image-guided Adaptive Layer Normalization (AdaLN) to make pose and mesh fit the human body shape. Extensive experiments demonstrate that the proposed PMCE outperforms previous state-of-the-art methods in terms of both per-frame accuracy and temporal consistency on three benchmark datasets: 3DPW, Human3.6M, and MPI-INF-3DHP. Our code is available at https://github.com/kasvii/PMCE.
<div id='section'>Paperid: <span id='pid'>180, <a href='https://arxiv.org/pdf/2308.09910.pdf' target='_blank'>https://arxiv.org/pdf/2308.09910.pdf</a></span>   <span><a href='https://github.com/Me-Ditto/Physics-Guided-Mocap' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingyi Ju, Buzhen Huang, Chen Zhu, Zhihao Li, Yangang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.09910">Physics-Guided Human Motion Capture with Pose Probability Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Incorporating physics in human motion capture to avoid artifacts like floating, foot sliding, and ground penetration is a promising direction. Existing solutions always adopt kinematic results as reference motions, and the physics is treated as a post-processing module. However, due to the depth ambiguity, monocular motion capture inevitably suffers from noises, and the noisy reference often leads to failure for physics-based tracking. To address the obstacles, our key-idea is to employ physics as denoising guidance in the reverse diffusion process to reconstruct physically plausible human motion from a modeled pose probability distribution. Specifically, we first train a latent gaussian model that encodes the uncertainty of 2D-to-3D lifting to facilitate reverse diffusion. Then, a physics module is constructed to track the motion sampled from the distribution. The discrepancies between the tracked motion and image observation are used to provide explicit guidance for the reverse diffusion model to refine the motion. With several iterations, the physics-based tracking and kinematic denoising promote each other to generate a physically plausible human motion. Experimental results show that our method outperforms previous physics-based methods in both joint accuracy and success rate. More information can be found at \url{https://github.com/Me-Ditto/Physics-Guided-Mocap}.
<div id='section'>Paperid: <span id='pid'>181, <a href='https://arxiv.org/pdf/2308.09611.pdf' target='_blank'>https://arxiv.org/pdf/2308.09611.pdf</a></span>   <span><a href='https://github.com/yhZhai/ATOM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuanhao Zhai, Mingzhen Huang, Tianyu Luan, Lu Dong, Ifeoma Nwogu, Siwei Lyu, David Doermann, Junsong Yuan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.09611">Language-guided Human Motion Synthesis with Atomic Actions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Language-guided human motion synthesis has been a challenging task due to the inherent complexity and diversity of human behaviors. Previous methods face limitations in generalization to novel actions, often resulting in unrealistic or incoherent motion sequences. In this paper, we propose ATOM (ATomic mOtion Modeling) to mitigate this problem, by decomposing actions into atomic actions, and employing a curriculum learning strategy to learn atomic action composition. First, we disentangle complex human motions into a set of atomic actions during learning, and then assemble novel actions using the learned atomic actions, which offers better adaptability to new actions. Moreover, we introduce a curriculum learning training strategy that leverages masked motion modeling with a gradual increase in the mask ratio, and thus facilitates atomic action assembly. This approach mitigates the overfitting problem commonly encountered in previous methods while enforcing the model to learn better motion representations. We demonstrate the effectiveness of ATOM through extensive experiments, including text-to-motion and action-to-motion synthesis tasks. We further illustrate its superiority in synthesizing plausible and coherent text-guided human motion sequences.
<div id='section'>Paperid: <span id='pid'>182, <a href='https://arxiv.org/pdf/2308.08942.pdf' target='_blank'>https://arxiv.org/pdf/2308.08942.pdf</a></span>   <span><a href='https://github.com/MediaBrain-SJTU/AuxFormer' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenxin Xu, Robby T. Tan, Yuhong Tan, Siheng Chen, Xinchao Wang, Yanfeng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.08942">Auxiliary Tasks Benefit 3D Skeleton-based Human Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Exploring spatial-temporal dependencies from observed motions is one of the core challenges of human motion prediction. Previous methods mainly focus on dedicated network structures to model the spatial and temporal dependencies. This paper considers a new direction by introducing a model learning framework with auxiliary tasks. In our auxiliary tasks, partial body joints' coordinates are corrupted by either masking or adding noise and the goal is to recover corrupted coordinates depending on the rest coordinates. To work with auxiliary tasks, we propose a novel auxiliary-adapted transformer, which can handle incomplete, corrupted motion data and achieve coordinate recovery via capturing spatial-temporal dependencies. Through auxiliary tasks, the auxiliary-adapted transformer is promoted to capture more comprehensive spatial-temporal dependencies among body joints' coordinates, leading to better feature learning. Extensive experimental results have shown that our method outperforms state-of-the-art methods by remarkable margins of 7.2%, 3.7%, and 9.4% in terms of 3D mean per joint position error (MPJPE) on the Human3.6M, CMU Mocap, and 3DPW datasets, respectively. We also demonstrate that our method is more robust under data missing cases and noisy data cases. Code is available at https://github.com/MediaBrain-SJTU/AuxFormer.
<div id='section'>Paperid: <span id='pid'>183, <a href='https://arxiv.org/pdf/2308.07234.pdf' target='_blank'>https://arxiv.org/pdf/2308.07234.pdf</a></span>   <span><a href='https://github.com/chaytonmin/UniWorld' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chen Min, Dawei Zhao, Liang Xiao, Yiming Nie, Bin Dai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.07234">UniWorld: Autonomous Driving Pre-training via World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we draw inspiration from Alberto Elfes' pioneering work in 1989, where he introduced the concept of the occupancy grid as World Models for robots. We imbue the robot with a spatial-temporal world model, termed UniWorld, to perceive its surroundings and predict the future behavior of other participants. UniWorld involves initially predicting 4D geometric occupancy as the World Models for foundational stage and subsequently fine-tuning on downstream tasks. UniWorld can estimate missing information concerning the world state and predict plausible future states of the world. Besides, UniWorld's pre-training process is label-free, enabling the utilization of massive amounts of image-LiDAR pairs to build a Foundational Model.The proposed unified pre-training framework demonstrates promising results in key tasks such as motion prediction, multi-camera 3D object detection, and surrounding semantic scene completion. When compared to monocular pre-training methods on the nuScenes dataset, UniWorld shows a significant improvement of about 1.5% in IoU for motion prediction, 2.0% in mAP and 2.0% in NDS for multi-camera 3D object detection, as well as a 3% increase in mIoU for surrounding semantic scene completion. By adopting our unified pre-training method, a 25% reduction in 3D training annotation costs can be achieved, offering significant practical value for the implementation of real-world autonomous driving. Codes are publicly available at https://github.com/chaytonmin/UniWorld.
<div id='section'>Paperid: <span id='pid'>184, <a href='https://arxiv.org/pdf/2308.07092.pdf' target='_blank'>https://arxiv.org/pdf/2308.07092.pdf</a></span>   <span><a href='https://github.com/maoyunyao/MAMP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunyao Mao, Jiajun Deng, Wengang Zhou, Yao Fang, Wanli Ouyang, Houqiang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.07092">Masked Motion Predictors are Strong 3D Action Representation Learners</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In 3D human action recognition, limited supervised data makes it challenging to fully tap into the modeling potential of powerful networks such as transformers. As a result, researchers have been actively investigating effective self-supervised pre-training strategies. In this work, we show that instead of following the prevalent pretext task to perform masked self-component reconstruction in human joints, explicit contextual motion modeling is key to the success of learning effective feature representation for 3D action recognition. Formally, we propose the Masked Motion Prediction (MAMP) framework. To be specific, the proposed MAMP takes as input the masked spatio-temporal skeleton sequence and predicts the corresponding temporal motion of the masked human joints. Considering the high temporal redundancy of the skeleton sequence, in our MAMP, the motion information also acts as an empirical semantic richness prior that guide the masking process, promoting better attention to semantically rich temporal regions. Extensive experiments on NTU-60, NTU-120, and PKU-MMD datasets show that the proposed MAMP pre-training substantially improves the performance of the adopted vanilla transformer, achieving state-of-the-art results without bells and whistles. The source code of our MAMP is available at https://github.com/maoyunyao/MAMP.
<div id='section'>Paperid: <span id='pid'>185, <a href='https://arxiv.org/pdf/2308.06554.pdf' target='_blank'>https://arxiv.org/pdf/2308.06554.pdf</a></span>   <span><a href='https://github.com/hygenie1228/CycleAdapt_RELEASE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hyeongjin Nam, Daniel Sungho Jung, Yeonguk Oh, Kyoung Mu Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.06554">Cyclic Test-Time Adaptation on Monocular Video for 3D Human Mesh Reconstruction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite recent advances in 3D human mesh reconstruction, domain gap between training and test data is still a major challenge. Several prior works tackle the domain gap problem via test-time adaptation that fine-tunes a network relying on 2D evidence (e.g., 2D human keypoints) from test images. However, the high reliance on 2D evidence during adaptation causes two major issues. First, 2D evidence induces depth ambiguity, preventing the learning of accurate 3D human geometry. Second, 2D evidence is noisy or partially non-existent during test time, and such imperfect 2D evidence leads to erroneous adaptation. To overcome the above issues, we introduce CycleAdapt, which cyclically adapts two networks: a human mesh reconstruction network (HMRNet) and a human motion denoising network (MDNet), given a test video. In our framework, to alleviate high reliance on 2D evidence, we fully supervise HMRNet with generated 3D supervision targets by MDNet. Our cyclic adaptation scheme progressively elaborates the 3D supervision targets, which compensate for imperfect 2D evidence. As a result, our CycleAdapt achieves state-of-the-art performance compared to previous test-time adaptation methods. The codes are available at https://github.com/hygenie1228/CycleAdapt_RELEASE.
<div id='section'>Paperid: <span id='pid'>186, <a href='https://arxiv.org/pdf/2308.06137.pdf' target='_blank'>https://arxiv.org/pdf/2308.06137.pdf</a></span>   <span><a href='https://github.com/portal-cornell/Game-Theoretic-Forecasting-Planning' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kushal Kedia, Prithwish Dan, Sanjiban Choudhury
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.06137">A Game-Theoretic Framework for Joint Forecasting and Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Planning safe robot motions in the presence of humans requires reliable forecasts of future human motion. However, simply predicting the most likely motion from prior interactions does not guarantee safety. Such forecasts fail to model the long tail of possible events, which are rarely observed in limited datasets. On the other hand, planning for worst-case motions leads to overtly conservative behavior and a "frozen robot". Instead, we aim to learn forecasts that predict counterfactuals that humans guard against. We propose a novel game-theoretic framework for joint planning and forecasting with the payoff being the performance of the planner against the demonstrator, and present practical algorithms to train models in an end-to-end fashion. We demonstrate that our proposed algorithm results in safer plans in a crowd navigation simulator and real-world datasets of pedestrian motion. We release our code at https://github.com/portal-cornell/Game-Theoretic-Forecasting-Planning.
<div id='section'>Paperid: <span id='pid'>187, <a href='https://arxiv.org/pdf/2308.05298.pdf' target='_blank'>https://arxiv.org/pdf/2308.05298.pdf</a></span>   <span><a href='https://github.com/KHB1698/DC-GCT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongbo Kang, Yong Wang, Mengyuan Liu, Doudou Wu, Peng Liu, Wenming Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.05298">Double-chain Constraints for 3D Human Pose Estimation in Images and Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reconstructing 3D poses from 2D poses lacking depth information is particularly challenging due to the complexity and diversity of human motion. The key is to effectively model the spatial constraints between joints to leverage their inherent dependencies. Thus, we propose a novel model, called Double-chain Graph Convolutional Transformer (DC-GCT), to constrain the pose through a double-chain design consisting of local-to-global and global-to-local chains to obtain a complex representation more suitable for the current human pose. Specifically, we combine the advantages of GCN and Transformer and design a Local Constraint Module (LCM) based on GCN and a Global Constraint Module (GCM) based on self-attention mechanism as well as a Feature Interaction Module (FIM). The proposed method fully captures the multi-level dependencies between human body joints to optimize the modeling capability of the model. Moreover, we propose a method to use temporal information into the single-frame model by guiding the video sequence embedding through the joint embedding of the target frame, with negligible increase in computational cost. Experimental results demonstrate that DC-GCT achieves state-of-the-art performance on two challenging datasets (Human3.6M and MPI-INF-3DHP). Notably, our model achieves state-of-the-art performance on all action categories in the Human3.6M dataset using detected 2D poses from CPN, and our code is available at: https://github.com/KHB1698/DC-GCT.
<div id='section'>Paperid: <span id='pid'>188, <a href='https://arxiv.org/pdf/2308.04536.pdf' target='_blank'>https://arxiv.org/pdf/2308.04536.pdf</a></span>   <span><a href='https://github.com/Necolizer/Facial-Prior-Based-FOMM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi Zhang, Youjun Zhao, Yuhang Wen, Zixuan Tang, Xinhua Xu, Mengyuan Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.04536">Facial Prior Based First Order Motion Model for Micro-expression Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Spotting facial micro-expression from videos finds various potential applications in fields including clinical diagnosis and interrogation, meanwhile this task is still difficult due to the limited scale of training data. To solve this problem, this paper tries to formulate a new task called micro-expression generation and then presents a strong baseline which combines the first order motion model with facial prior knowledge. Given a target face, we intend to drive the face to generate micro-expression videos according to the motion patterns of source videos. Specifically, our new model involves three modules. First, we extract facial prior features from a region focusing module. Second, we estimate facial motion using key points and local affine transformations with a motion prediction module. Third, expression generation module is used to drive the target face to generate videos. We train our model on public CASME II, SAMM and SMIC datasets and then use the model to generate new micro-expression videos for evaluation. Our model achieves the first place in the Facial Micro-Expression Challenge 2021 (MEGC2021), where our superior performance is verified by three experts with Facial Action Coding System certification. Source code is provided in https://github.com/Necolizer/Facial-Prior-Based-FOMM.
<div id='section'>Paperid: <span id='pid'>189, <a href='https://arxiv.org/pdf/2308.01850.pdf' target='_blank'>https://arxiv.org/pdf/2308.01850.pdf</a></span>   <span><a href='https://github.com/yangzhao1230/PCMDM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhao Yang, Bing Su, Ji-Rong Wen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.01850">Synthesizing Long-Term Human Motions with Diffusion Models via Coherent Sampling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-to-motion generation has gained increasing attention, but most existing methods are limited to generating short-term motions that correspond to a single sentence describing a single action. However, when a text stream describes a sequence of continuous motions, the generated motions corresponding to each sentence may not be coherently linked. Existing long-term motion generation methods face two main issues. Firstly, they cannot directly generate coherent motions and require additional operations such as interpolation to process the generated actions. Secondly, they generate subsequent actions in an autoregressive manner without considering the influence of future actions on previous ones. To address these issues, we propose a novel approach that utilizes a past-conditioned diffusion model with two optional coherent sampling methods: Past Inpainting Sampling and Compositional Transition Sampling. Past Inpainting Sampling completes subsequent motions by treating previous motions as conditions, while Compositional Transition Sampling models the distribution of the transition as the composition of two adjacent motions guided by different text prompts. Our experimental results demonstrate that our proposed method is capable of generating compositional and coherent long-term 3D human motions controlled by a user-instructed long text stream. The code is available at \href{https://github.com/yangzhao1230/PCMDM}{https://github.com/yangzhao1230/PCMDM}.
<div id='section'>Paperid: <span id='pid'>190, <a href='https://arxiv.org/pdf/2307.02227.pdf' target='_blank'>https://arxiv.org/pdf/2307.02227.pdf</a></span>   <span><a href='https://github.com/sunlicai/MAE-DFER' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/sunlicai/MAE-DFER' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Licai Sun, Zheng Lian, Bin Liu, Jianhua Tao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.02227">MAE-DFER: Efficient Masked Autoencoder for Self-supervised Dynamic Facial Expression Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dynamic facial expression recognition (DFER) is essential to the development of intelligent and empathetic machines. Prior efforts in this field mainly fall into supervised learning paradigm, which is severely restricted by the limited labeled data in existing datasets. Inspired by recent unprecedented success of masked autoencoders (e.g., VideoMAE), this paper proposes MAE-DFER, a novel self-supervised method which leverages large-scale self-supervised pre-training on abundant unlabeled data to largely advance the development of DFER. Since the vanilla Vision Transformer (ViT) employed in VideoMAE requires substantial computation during fine-tuning, MAE-DFER develops an efficient local-global interaction Transformer (LGI-Former) as the encoder. Moreover, in addition to the standalone appearance content reconstruction in VideoMAE, MAE-DFER also introduces explicit temporal facial motion modeling to encourage LGI-Former to excavate both static appearance and dynamic motion information. Extensive experiments on six datasets show that MAE-DFER consistently outperforms state-of-the-art supervised methods by significant margins (e.g., +6.30\% UAR on DFEW and +8.34\% UAR on MAFW), verifying that it can learn powerful dynamic facial representations via large-scale self-supervised pre-training. Besides, it has comparable or even better performance than VideoMAE, while largely reducing the computational cost (about 38\% FLOPs). We believe MAE-DFER has paved a new way for the advancement of DFER and can inspire more relevant research in this field and even other related tasks. Codes and models are publicly available at https://github.com/sunlicai/MAE-DFER.
<div id='section'>Paperid: <span id='pid'>191, <a href='https://arxiv.org/pdf/2307.00818.pdf' target='_blank'>https://arxiv.org/pdf/2307.00818.pdf</a></span>   <span><a href='https://github.com/IDEA-Research/Motion-X' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jing Lin, Ailing Zeng, Shunlin Lu, Yuanhao Cai, Ruimao Zhang, Haoqian Wang, Lei Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.00818">Motion-X: A Large-scale 3D Expressive Whole-body Human Motion Dataset</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we present Motion-X, a large-scale 3D expressive whole-body motion dataset. Existing motion datasets predominantly contain body-only poses, lacking facial expressions, hand gestures, and fine-grained pose descriptions. Moreover, they are primarily collected from limited laboratory scenes with textual descriptions manually labeled, which greatly limits their scalability. To overcome these limitations, we develop a whole-body motion and text annotation pipeline, which can automatically annotate motion from either single- or multi-view videos and provide comprehensive semantic labels for each video and fine-grained whole-body pose descriptions for each frame. This pipeline is of high precision, cost-effective, and scalable for further research. Based on it, we construct Motion-X, which comprises 15.6M precise 3D whole-body pose annotations (i.e., SMPL-X) covering 81.1K motion sequences from massive scenes. Besides, Motion-X provides 15.6M frame-level whole-body pose descriptions and 81.1K sequence-level semantic labels. Comprehensive experiments demonstrate the accuracy of the annotation pipeline and the significant benefit of Motion-X in enhancing expressive, diverse, and natural motion generation, as well as 3D whole-body human mesh recovery.
<div id='section'>Paperid: <span id='pid'>192, <a href='https://arxiv.org/pdf/2306.17738.pdf' target='_blank'>https://arxiv.org/pdf/2306.17738.pdf</a></span>   <span><a href='https://github.com/hrii-iit/xsens_mvn_ros' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mattia Leonori, Marta Lorenzini, Luca Fortini, Juan M. Gandarias, Arash Ajoudani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.17738">The Bridge between Xsens Motion-Capture and Robot Operating System (ROS): Enabling Robots with Online 3D Human Motion Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This document introduces the bridge between the leading inertial motion-capture systems for 3D human tracking and the most used robotics software framework. 3D kinematic data provided by Xsens are translated into ROS messages to make them usable by robots and a Unified Robotics Description Format (URDF) model of the human kinematics is generated, which can be run and displayed in ROS 3D visualizer, RViz. The code to implement the to-ROS-bridge is a ROS package called xsens_mvn_ros and is available on GitHub at https://github.com/hrii-iit/xsens_mvn_ros The main documentation can be found at https://hrii-iit.github.io/xsens_mvn_ros/index.html
<div id='section'>Paperid: <span id='pid'>193, <a href='https://arxiv.org/pdf/2306.17010.pdf' target='_blank'>https://arxiv.org/pdf/2306.17010.pdf</a></span>   <span><a href='https://github.com/Toytiny/milliFlow' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/Toytiny/milliFlow' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fangqiang Ding, Zhen Luo, Peijun Zhao, Chris Xiaoxuan Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.17010">milliFlow: Scene Flow Estimation on mmWave Radar Point Cloud for Human Motion Sensing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion sensing plays a crucial role in smart systems for decision-making, user interaction, and personalized services. Extensive research that has been conducted is predominantly based on cameras, whose intrusive nature limits their use in smart home applications. To address this, mmWave radars have gained popularity due to their privacy-friendly features. In this work, we propose milliFlow, a novel deep learning approach to estimate scene flow as complementary motion information for mmWave point cloud, serving as an intermediate level of features and directly benefiting downstream human motion sensing tasks. Experimental results demonstrate the superior performance of our method when compared with the competing approaches. Furthermore, by incorporating scene flow information, we achieve remarkable improvements in human activity recognition and human parsing and support human body part tracking. Code and dataset are available at https://github.com/Toytiny/milliFlow.
<div id='section'>Paperid: <span id='pid'>194, <a href='https://arxiv.org/pdf/2306.16927.pdf' target='_blank'>https://arxiv.org/pdf/2306.16927.pdf</a></span>   <span><a href='https://github.com/OpenDriveLab/End-to-end-Autonomous-Driving' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Li Chen, Penghao Wu, Kashyap Chitta, Bernhard Jaeger, Andreas Geiger, Hongyang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.16927">End-to-end Autonomous Driving: Challenges and Frontiers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The autonomous driving community has witnessed a rapid growth in approaches that embrace an end-to-end algorithm framework, utilizing raw sensor input to generate vehicle motion plans, instead of concentrating on individual tasks such as detection and motion prediction. End-to-end systems, in comparison to modular pipelines, benefit from joint feature optimization for perception and planning. This field has flourished due to the availability of large-scale datasets, closed-loop evaluation, and the increasing need for autonomous driving algorithms to perform effectively in challenging scenarios. In this survey, we provide a comprehensive analysis of more than 270 papers, covering the motivation, roadmap, methodology, challenges, and future trends in end-to-end autonomous driving. We delve into several critical challenges, including multi-modality, interpretability, causal confusion, robustness, and world models, amongst others. Additionally, we discuss current advancements in foundation models and visual pre-training, as well as how to incorporate these techniques within the end-to-end driving framework. we maintain an active repository that contains up-to-date literature and open-source projects at https://github.com/OpenDriveLab/End-to-end-Autonomous-Driving.
<div id='section'>Paperid: <span id='pid'>195, <a href='https://arxiv.org/pdf/2306.16736.pdf' target='_blank'>https://arxiv.org/pdf/2306.16736.pdf</a></span>   <span><a href='https://github.com/xymsh/GraMMaR' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/xymsh/GraMMaR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sihan Ma, Qiong Cao, Hongwei Yi, Jing Zhang, Dacheng Tao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.16736">GraMMaR: Ground-aware Motion Model for 3D Human Motion Reconstruction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Demystifying complex human-ground interactions is essential for accurate and realistic 3D human motion reconstruction from RGB videos, as it ensures consistency between the humans and the ground plane. Prior methods have modeled human-ground interactions either implicitly or in a sparse manner, often resulting in unrealistic and incorrect motions when faced with noise and uncertainty. In contrast, our approach explicitly represents these interactions in a dense and continuous manner. To this end, we propose a novel Ground-aware Motion Model for 3D Human Motion Reconstruction, named GraMMaR, which jointly learns the distribution of transitions in both pose and interaction between every joint and ground plane at each time step of a motion sequence. It is trained to explicitly promote consistency between the motion and distance change towards the ground. After training, we establish a joint optimization strategy that utilizes GraMMaR as a dual-prior, regularizing the optimization towards the space of plausible ground-aware motions. This leads to realistic and coherent motion reconstruction, irrespective of the assumed or learned ground plane. Through extensive evaluation on the AMASS and AIST++ datasets, our model demonstrates good generalization and discriminating abilities in challenging cases including complex and ambiguous human-ground interactions. The code will be available at https://github.com/xymsh/GraMMaR.
<div id='section'>Paperid: <span id='pid'>196, <a href='https://arxiv.org/pdf/2306.14795.pdf' target='_blank'>https://arxiv.org/pdf/2306.14795.pdf</a></span>   <span><a href='https://github.com/OpenMotionLab/MotionGPT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Biao Jiang, Xin Chen, Wen Liu, Jingyi Yu, Gang Yu, Tao Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.14795">MotionGPT: Human Motion as a Foreign Language</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Though the advancement of pre-trained large language models unfolds, the exploration of building a unified model for language and other multi-modal data, such as motion, remains challenging and untouched so far. Fortunately, human motion displays a semantic coupling akin to human language, often perceived as a form of body language. By fusing language data with large-scale motion models, motion-language pre-training that can enhance the performance of motion-related tasks becomes feasible. Driven by this insight, we propose MotionGPT, a unified, versatile, and user-friendly motion-language model to handle multiple motion-relevant tasks. Specifically, we employ the discrete vector quantization for human motion and transfer 3D motion into motion tokens, similar to the generation process of word tokens. Building upon this "motion vocabulary", we perform language modeling on both motion and text in a unified manner, treating human motion as a specific language. Moreover, inspired by prompt learning, we pre-train MotionGPT with a mixture of motion-language data and fine-tune it on prompt-based question-and-answer tasks. Extensive experiments demonstrate that MotionGPT achieves state-of-the-art performances on multiple motion tasks including text-driven motion generation, motion captioning, motion prediction, and motion in-between.
<div id='section'>Paperid: <span id='pid'>197, <a href='https://arxiv.org/pdf/2306.11249.pdf' target='_blank'>https://arxiv.org/pdf/2306.11249.pdf</a></span>   <span><a href='https://github.com/chengtan9907/OpenSTL' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/chengtan9907/OpenSTL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Cheng Tan, Siyuan Li, Zhangyang Gao, Wenfei Guan, Zedong Wang, Zicheng Liu, Lirong Wu, Stan Z. Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.11249">OpenSTL: A Comprehensive Benchmark of Spatio-Temporal Predictive Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Spatio-temporal predictive learning is a learning paradigm that enables models to learn spatial and temporal patterns by predicting future frames from given past frames in an unsupervised manner. Despite remarkable progress in recent years, a lack of systematic understanding persists due to the diverse settings, complex implementation, and difficult reproducibility. Without standardization, comparisons can be unfair and insights inconclusive. To address this dilemma, we propose OpenSTL, a comprehensive benchmark for spatio-temporal predictive learning that categorizes prevalent approaches into recurrent-based and recurrent-free models. OpenSTL provides a modular and extensible framework implementing various state-of-the-art methods. We conduct standard evaluations on datasets across various domains, including synthetic moving object trajectory, human motion, driving scenes, traffic flow and weather forecasting. Based on our observations, we provide a detailed analysis of how model architecture and dataset properties affect spatio-temporal predictive learning performance. Surprisingly, we find that recurrent-free models achieve a good balance between efficiency and performance than recurrent models. Thus, we further extend the common MetaFormers to boost recurrent-free spatial-temporal predictive learning. We open-source the code and models at https://github.com/chengtan9907/OpenSTL.
<div id='section'>Paperid: <span id='pid'>198, <a href='https://arxiv.org/pdf/2306.10840.pdf' target='_blank'>https://arxiv.org/pdf/2306.10840.pdf</a></span>   <span><a href='https://github.com/kit-mrt/future-motion' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Royden Wagner, Omer Sahin Tas, Marvin Klemp, Carlos Fernandez, Christoph Stiller
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.10840">RedMotion: Motion Prediction via Redundancy Reduction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce RedMotion, a transformer model for motion prediction in self-driving vehicles that learns environment representations via redundancy reduction. Our first type of redundancy reduction is induced by an internal transformer decoder and reduces a variable-sized set of local road environment tokens, representing road graphs and agent data, to a fixed-sized global embedding. The second type of redundancy reduction is obtained by self-supervised learning and applies the redundancy reduction principle to embeddings generated from augmented views of road environments. Our experiments reveal that our representation learning approach outperforms PreTraM, Traj-MAE, and GraphDINO in a semi-supervised setting. Moreover, RedMotion achieves competitive results compared to HPTR or MTR++ in the Waymo Motion Prediction Challenge. Our open-source implementation is available at: https://github.com/kit-mrt/future-motion
<div id='section'>Paperid: <span id='pid'>199, <a href='https://arxiv.org/pdf/2306.10761.pdf' target='_blank'>https://arxiv.org/pdf/2306.10761.pdf</a></span>   <span><a href='https://github.com/EdwardLeeLPZ/PowerBEV' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Peizheng Li, Shuxiao Ding, Xieyuanli Chen, Niklas Hanselmann, Marius Cordts, Juergen Gall
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.10761">PowerBEV: A Powerful Yet Lightweight Framework for Instance Prediction in Bird's-Eye View</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurately perceiving instances and predicting their future motion are key tasks for autonomous vehicles, enabling them to navigate safely in complex urban traffic. While bird's-eye view (BEV) representations are commonplace in perception for autonomous driving, their potential in a motion prediction setting is less explored. Existing approaches for BEV instance prediction from surround cameras rely on a multi-task auto-regressive setup coupled with complex post-processing to predict future instances in a spatio-temporally consistent manner. In this paper, we depart from this paradigm and propose an efficient novel end-to-end framework named POWERBEV, which differs in several design choices aimed at reducing the inherent redundancy in previous methods. First, rather than predicting the future in an auto-regressive fashion, POWERBEV uses a parallel, multi-scale module built from lightweight 2D convolutional networks. Second, we show that segmentation and centripetal backward flow are sufficient for prediction, simplifying previous multi-task objectives by eliminating redundant output modalities. Building on this output representation, we propose a simple, flow warping-based post-processing approach which produces more stable instance associations across time. Through this lightweight yet powerful design, POWERBEV outperforms state-of-the-art baselines on the NuScenes Dataset and poses an alternative paradigm for BEV instance prediction. We made our code publicly available at: https://github.com/EdwardLeeLPZ/PowerBEV.
<div id='section'>Paperid: <span id='pid'>200, <a href='https://arxiv.org/pdf/2306.07576.pdf' target='_blank'>https://arxiv.org/pdf/2306.07576.pdf</a></span>   <span><a href='https://github.com/ActionR-Group/Stream-GCN,' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuheng Yang, Haipeng Chen, Zhenguang Liu, Yingda Lyu, Beibei Zhang, Shuang Wu, Zhibo Wang, Kui Ren
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.07576">Action Recognition with Multi-stream Motion Modeling and Mutual Information Maximization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Action recognition has long been a fundamental and intriguing problem in artificial intelligence. The task is challenging due to the high dimensionality nature of an action, as well as the subtle motion details to be considered. Current state-of-the-art approaches typically learn from articulated motion sequences in the straightforward 3D Euclidean space. However, the vanilla Euclidean space is not efficient for modeling important motion characteristics such as the joint-wise angular acceleration, which reveals the driving force behind the motion. Moreover, current methods typically attend to each channel equally and lack theoretical constrains on extracting task-relevant features from the input.
  In this paper, we seek to tackle these challenges from three aspects: (1) We propose to incorporate an acceleration representation, explicitly modeling the higher-order variations in motion. (2) We introduce a novel Stream-GCN network equipped with multi-stream components and channel attention, where different representations (i.e., streams) supplement each other towards a more precise action recognition while attention capitalizes on those important channels. (3) We explore feature-level supervision for maximizing the extraction of task-relevant information and formulate this into a mutual information loss. Empirically, our approach sets the new state-of-the-art performance on three benchmark datasets, NTU RGB+D, NTU RGB+D 120, and NW-UCLA. Our code is anonymously released at https://github.com/ActionR-Group/Stream-GCN, hoping to inspire the community.
<div id='section'>Paperid: <span id='pid'>201, <a href='https://arxiv.org/pdf/2305.15842.pdf' target='_blank'>https://arxiv.org/pdf/2305.15842.pdf</a></span>   <span><a href='https://github.com/mesnico/text-to-motion-retrieval' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Nicola Messina, Jan Sedmidubsky, Fabrizio Falchi, TomÃ¡Å¡ Rebok
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.15842">Text-to-Motion Retrieval: Towards Joint Understanding of Human Motion Data and Natural Language</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Due to recent advances in pose-estimation methods, human motion can be extracted from a common video in the form of 3D skeleton sequences. Despite wonderful application opportunities, effective and efficient content-based access to large volumes of such spatio-temporal skeleton data still remains a challenging problem. In this paper, we propose a novel content-based text-to-motion retrieval task, which aims at retrieving relevant motions based on a specified natural-language textual description. To define baselines for this uncharted task, we employ the BERT and CLIP language representations to encode the text modality and successful spatio-temporal models to encode the motion modality. We additionally introduce our transformer-based approach, called Motion Transformer (MoT), which employs divided space-time attention to effectively aggregate the different skeleton joints in space and time. Inspired by the recent progress in text-to-image/video matching, we experiment with two widely-adopted metric-learning loss functions. Finally, we set up a common evaluation protocol by defining qualitative metrics for assessing the quality of the retrieved motions, targeting the two recently-introduced KIT Motion-Language and HumanML3D datasets. The code for reproducing our results is available at https://github.com/mesnico/text-to-motion-retrieval.
<div id='section'>Paperid: <span id='pid'>202, <a href='https://arxiv.org/pdf/2305.12554.pdf' target='_blank'>https://arxiv.org/pdf/2305.12554.pdf</a></span>   <span><a href='https://github.com/jsun57/CoMusion/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiarui Sun, Girish Chowdhary
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.12554">CoMusion: Towards Consistent Stochastic Human Motion Prediction via Motion Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Stochastic Human Motion Prediction (HMP) aims to predict multiple possible future human pose sequences from observed ones. Most prior works learn motion distributions through encoding-decoding in the latent space, which does not preserve motion's spatial-temporal structure. While effective, these methods often require complex, multi-stage training and yield predictions that are inconsistent with the provided history and can be physically unrealistic. To address these issues, we propose CoMusion, a single-stage, end-to-end diffusion-based stochastic HMP framework. CoMusion is inspired from the insight that a smooth future pose initialization improves prediction performance, a strategy not previously utilized in stochastic models but evidenced in deterministic works. To generate such initialization, CoMusion's motion predictor starts with a Transformer-based network for initial reconstruction of corrupted motion. Then, a graph convolutional network (GCN) is employed to refine the prediction considering past observations in the discrete cosine transformation (DCT) space. Our method, facilitated by the Transformer-GCN module design and a proposed variance scheduler, excels in predicting accurate, realistic, and consistent motions, while maintaining appropriate diversity. Experimental results on benchmark datasets demonstrate that CoMusion surpasses prior methods across metrics, while demonstrating superior generation quality. Our Code is released at https://github.com/jsun57/CoMusion/ .
<div id='section'>Paperid: <span id='pid'>203, <a href='https://arxiv.org/pdf/2305.11094.pdf' target='_blank'>https://arxiv.org/pdf/2305.11094.pdf</a></span>   <span><a href='https://github.com/YoungSeng/QPGesture' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sicheng Yang, Zhiyong Wu, Minglei Li, Zhensong Zhang, Lei Hao, Weihong Bao, Haolin Zhuang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.11094">QPGesture: Quantization-Based and Phase-Guided Motion Matching for Natural Speech-Driven Gesture Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Speech-driven gesture generation is highly challenging due to the random jitters of human motion. In addition, there is an inherent asynchronous relationship between human speech and gestures. To tackle these challenges, we introduce a novel quantization-based and phase-guided motion-matching framework. Specifically, we first present a gesture VQ-VAE module to learn a codebook to summarize meaningful gesture units. With each code representing a unique gesture, random jittering problems are alleviated effectively. We then use Levenshtein distance to align diverse gestures with different speech. Levenshtein distance based on audio quantization as a similarity metric of corresponding speech of gestures helps match more appropriate gestures with speech, and solves the alignment problem of speech and gestures well. Moreover, we introduce phase to guide the optimal gesture matching based on the semantics of context or rhythm of audio. Phase guides when text-based or speech-based gestures should be performed to make the generated gestures more natural. Extensive experiments show that our method outperforms recent approaches on speech-driven gesture generation. Our code, database, pre-trained models, and demos are available at https://github.com/YoungSeng/QPGesture.
<div id='section'>Paperid: <span id='pid'>204, <a href='https://arxiv.org/pdf/2305.09381.pdf' target='_blank'>https://arxiv.org/pdf/2305.09381.pdf</a></span>   <span><a href='https://github.com/fluide1022/AMD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bo Han, Hao Peng, Minjing Dong, Yi Ren, Yixuan Shen, Chang Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.09381">AMD: Autoregressive Motion Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion generation aims to produce plausible human motion sequences according to various conditional inputs, such as text or audio. Despite the feasibility of existing methods in generating motion based on short prompts and simple motion patterns, they encounter difficulties when dealing with long prompts or complex motions. The challenges are two-fold: 1) the scarcity of human motion-captured data for long prompts and complex motions. 2) the high diversity of human motions in the temporal domain and the substantial divergence of distributions from conditional modalities, leading to a many-to-many mapping problem when generating motion with complex and long texts. In this work, we address these gaps by 1) elaborating the first dataset pairing long textual descriptions and 3D complex motions (HumanLong3D), and 2) proposing an autoregressive motion diffusion model (AMD). Specifically, AMD integrates the text prompt at the current timestep with the text prompt and action sequences at the previous timestep as conditional information to predict the current action sequences in an iterative manner. Furthermore, we present its generalization for X-to-Motion with "No Modality Left Behind", enabling the generation of high-definition and high-fidelity human motions based on user-defined modality input.
<div id='section'>Paperid: <span id='pid'>205, <a href='https://arxiv.org/pdf/2305.04469.pdf' target='_blank'>https://arxiv.org/pdf/2305.04469.pdf</a></span>   <span><a href='https://github.com/ZoneLikeWonderland/HACK-Model' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Longwen Zhang, Zijun Zhao, Xinzhou Cong, Qixuan Zhang, Shuqi Gu, Yuchong Gao, Rui Zheng, Wei Yang, Lan Xu, Jingyi Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.04469">HACK: Learning a Parametric Head and Neck Model for High-fidelity Animation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Significant advancements have been made in developing parametric models for digital humans, with various approaches concentrating on parts such as the human body, hand, or face. Nevertheless, connectors such as the neck have been overlooked in these models, with rich anatomical priors often unutilized. In this paper, we introduce HACK (Head-And-neCK), a novel parametric model for constructing the head and cervical region of digital humans. Our model seeks to disentangle the full spectrum of neck and larynx motions, facial expressions, and appearance variations, providing personalized and anatomically consistent controls, particularly for the neck regions. To build our HACK model, we acquire a comprehensive multi-modal dataset of the head and neck under various facial expressions. We employ a 3D ultrasound imaging scheme to extract the inner biomechanical structures, namely the precise 3D rotation information of the seven vertebrae of the cervical spine. We then adopt a multi-view photometric approach to capture the geometry and physically-based textures of diverse subjects, who exhibit a diverse range of static expressions as well as sequential head-and-neck movements. Using the multi-modal dataset, we train the parametric HACK model by separating the 3D head and neck depiction into various shape, pose, expression, and larynx blendshapes from the neutral expression and the rest skeletal pose. We adopt an anatomically-consistent skeletal design for the cervical region, and the expression is linked to facial action units for artist-friendly controls. HACK addresses the head and neck as a unified entity, offering more accurate and expressive controls, with a new level of realism, particularly for the neck regions. This approach has significant benefits for numerous applications and enables inter-correlation analysis between head and neck for fine-grained motion synthesis and transfer.
<div id='section'>Paperid: <span id='pid'>206, <a href='https://arxiv.org/pdf/2305.04195.pdf' target='_blank'>https://arxiv.org/pdf/2305.04195.pdf</a></span>   <span><a href='https://github.com/eanson023/rehamot' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sheng Yan, Yang Liu, Haoqiang Wang, Xin Du, Mengyuan Liu, Hong Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.04195">Cross-Modal Retrieval for Motion and Text via DopTriple Loss</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cross-modal retrieval of image-text and video-text is a prominent research area in computer vision and natural language processing. However, there has been insufficient attention given to cross-modal retrieval between human motion and text, despite its wide-ranging applicability. To address this gap, we utilize a concise yet effective dual-unimodal transformer encoder for tackling this task. Recognizing that overlapping atomic actions in different human motion sequences can lead to semantic conflicts between samples, we explore a novel triplet loss function called DropTriple Loss. This loss function discards false negative samples from the negative sample set and focuses on mining remaining genuinely hard negative samples for triplet training, thereby reducing violations they cause. We evaluate our model and approach on the HumanML3D and KIT Motion-Language datasets. On the latest HumanML3D dataset, we achieve a recall of 62.9% for motion retrieval and 71.5% for text retrieval (both based on R@10). The source code for our approach is publicly available at https://github.com/eanson023/rehamot.
<div id='section'>Paperid: <span id='pid'>207, <a href='https://arxiv.org/pdf/2305.03286.pdf' target='_blank'>https://arxiv.org/pdf/2305.03286.pdf</a></span>   <span><a href='https://github.com/xupei0610/CompositeMotion' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Pei Xu, Xiumin Shang, Victor Zordan, Ioannis Karamouzas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.03286">Composite Motion Learning with Task Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a deep learning method for composite and task-driven motion control for physically simulated characters. In contrast to existing data-driven approaches using reinforcement learning that imitate full-body motions, we learn decoupled motions for specific body parts from multiple reference motions simultaneously and directly by leveraging the use of multiple discriminators in a GAN-like setup. In this process, there is no need of any manual work to produce composite reference motions for learning. Instead, the control policy explores by itself how the composite motions can be combined automatically. We further account for multiple task-specific rewards and train a single, multi-objective control policy. To this end, we propose a novel framework for multi-objective learning that adaptively balances the learning of disparate motions from multiple sources and multiple goal-directed control objectives. In addition, as composite motions are typically augmentations of simpler behaviors, we introduce a sample-efficient method for training composite control policies in an incremental manner, where we reuse a pre-trained policy as the meta policy and train a cooperative policy that adapts the meta one for new composite tasks. We show the applicability of our approach on a variety of challenging multi-objective tasks involving both composite motion imitation and multiple goal-directed control.
<div id='section'>Paperid: <span id='pid'>208, <a href='https://arxiv.org/pdf/2304.13651.pdf' target='_blank'>https://arxiv.org/pdf/2304.13651.pdf</a></span>   <span><a href='https://github.com/ZitianTang/Thermal-IM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zitian Tang, Wenjie Ye, Wei-Chiu Ma, Hang Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.13651">What Happened 3 Seconds Ago? Inferring the Past with Thermal Imaging</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Inferring past human motion from RGB images is challenging due to the inherent uncertainty of the prediction problem. Thermal images, on the other hand, encode traces of past human-object interactions left in the environment via thermal radiation measurement. Based on this observation, we collect the first RGB-Thermal dataset for human motion analysis, dubbed Thermal-IM. Then we develop a three-stage neural network model for accurate past human pose estimation. Comprehensive experiments show that thermal cues significantly reduce the ambiguities of this task, and the proposed model achieves remarkable performance. The dataset is available at https://github.com/ZitianTang/Thermal-IM.
<div id='section'>Paperid: <span id='pid'>209, <a href='https://arxiv.org/pdf/2304.08483.pdf' target='_blank'>https://arxiv.org/pdf/2304.08483.pdf</a></span>   <span><a href='https://github.com/yumingj/Text2Performer' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuming Jiang, Shuai Yang, Tong Liang Koh, Wayne Wu, Chen Change Loy, Ziwei Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.08483">Text2Performer: Text-Driven Human Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-driven content creation has evolved to be a transformative technique that revolutionizes creativity. Here we study the task of text-driven human video generation, where a video sequence is synthesized from texts describing the appearance and motions of a target performer. Compared to general text-driven video generation, human-centric video generation requires maintaining the appearance of synthesized human while performing complex motions. In this work, we present Text2Performer to generate vivid human videos with articulated motions from texts. Text2Performer has two novel designs: 1) decomposed human representation and 2) diffusion-based motion sampler. First, we decompose the VQVAE latent space into human appearance and pose representation in an unsupervised manner by utilizing the nature of human videos. In this way, the appearance is well maintained along the generated frames. Then, we propose continuous VQ-diffuser to sample a sequence of pose embeddings. Unlike existing VQ-based methods that operate in the discrete space, continuous VQ-diffuser directly outputs the continuous pose embeddings for better motion modeling. Finally, motion-aware masking strategy is designed to mask the pose embeddings spatial-temporally to enhance the temporal coherence. Moreover, to facilitate the task of text-driven human video generation, we contribute a Fashion-Text2Video dataset with manually annotated action labels and text descriptions. Extensive experiments demonstrate that Text2Performer generates high-quality human videos (up to 512x256 resolution) with diverse appearances and flexible motions.
<div id='section'>Paperid: <span id='pid'>210, <a href='https://arxiv.org/pdf/2304.05116.pdf' target='_blank'>https://arxiv.org/pdf/2304.05116.pdf</a></span>   <span><a href='https://github.com/westny/mtp-go' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Theodor Westny, Joel Oskarsson, BjÃ¶rn Olofsson, Erik Frisk
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.05116">Evaluation of Differentially Constrained Motion Models for Graph-Based Trajectory Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Given their flexibility and encouraging performance, deep-learning models are becoming standard for motion prediction in autonomous driving. However, with great flexibility comes a lack of interpretability and possible violations of physical constraints. Accompanying these data-driven methods with differentially-constrained motion models to provide physically feasible trajectories is a promising future direction. The foundation for this work is a previously introduced graph-neural-network-based model, MTP-GO. The neural network learns to compute the inputs to an underlying motion model to provide physically feasible trajectories. This research investigates the performance of various motion models in combination with numerical solvers for the prediction task. The study shows that simpler models, such as low-order integrator models, are preferred over more complex, e.g., kinematic models, to achieve accurate predictions. Further, the numerical solver can have a substantial impact on performance, advising against commonly used first-order methods like Euler forward. Instead, a second-order method like Heun's can greatly improve predictions.
<div id='section'>Paperid: <span id='pid'>211, <a href='https://arxiv.org/pdf/2304.02154.pdf' target='_blank'>https://arxiv.org/pdf/2304.02154.pdf</a></span>   <span><a href='https://github.com/paragkhanna1/dataset' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Parag Khanna, MÃ¥rten BjÃ¶rkman, Christian Smith
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.02154">A Multimodal Data Set of Human Handovers with Design Implications for Human-Robot Handovers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Handovers are basic yet sophisticated motor tasks performed seamlessly by humans. They are among the most common activities in our daily lives and social environments. This makes mastering the art of handovers critical for a social and collaborative robot. In this work, we present an experimental study that involved human-human handovers by 13 pairs, i.e., 26 participants. We record and explore multiple features of handovers amongst humans aimed at inspiring handovers amongst humans and robots. With this work, we further create and publish a novel data set of 8672 handovers, bringing together human motion and the forces involved. We further analyze the effect of object weight and the role of visual sensory input in human-human handovers, as well as possible design implications for robots. As a proof of concept, the data set was used for creating a human-inspired data-driven strategy for robotic grip release in handovers, which was demonstrated to result in better robot to human handovers.
<div id='section'>Paperid: <span id='pid'>212, <a href='https://arxiv.org/pdf/2303.15334.pdf' target='_blank'>https://arxiv.org/pdf/2303.15334.pdf</a></span>   <span><a href='https://github.com/ifzhang/ByteTrack-V2' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifu Zhang, Xinggang Wang, Xiaoqing Ye, Wei Zhang, Jincheng Lu, Xiao Tan, Errui Ding, Peize Sun, Jingdong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.15334">ByteTrackV2: 2D and 3D Multi-Object Tracking by Associating Every Detection Box</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) aims at estimating bounding boxes and identities of objects across video frames. Detection boxes serve as the basis of both 2D and 3D MOT. The inevitable changing of detection scores leads to object missing after tracking. We propose a hierarchical data association strategy to mine the true objects in low-score detection boxes, which alleviates the problems of object missing and fragmented trajectories. The simple and generic data association strategy shows effectiveness under both 2D and 3D settings. In 3D scenarios, it is much easier for the tracker to predict object velocities in the world coordinate. We propose a complementary motion prediction strategy that incorporates the detected velocities with a Kalman filter to address the problem of abrupt motion and short-term disappearing. ByteTrackV2 leads the nuScenes 3D MOT leaderboard in both camera (56.4% AMOTA) and LiDAR (70.1% AMOTA) modalities. Furthermore, it is nonparametric and can be integrated with various detectors, making it appealing in real applications. The source code is released at https://github.com/ifzhang/ByteTrack-V2.
<div id='section'>Paperid: <span id='pid'>213, <a href='https://arxiv.org/pdf/2303.10876.pdf' target='_blank'>https://arxiv.org/pdf/2303.10876.pdf</a></span>   <span><a href='https://github.com/MediaBrain-SJTU/EqMotion' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenxin Xu, Robby T. Tan, Yuhong Tan, Siheng Chen, Yu Guang Wang, Xinchao Wang, Yanfeng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.10876">EqMotion: Equivariant Multi-agent Motion Prediction with Invariant Interaction Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning to predict agent motions with relationship reasoning is important for many applications. In motion prediction tasks, maintaining motion equivariance under Euclidean geometric transformations and invariance of agent interaction is a critical and fundamental principle. However, such equivariance and invariance properties are overlooked by most existing methods. To fill this gap, we propose EqMotion, an efficient equivariant motion prediction model with invariant interaction reasoning. To achieve motion equivariance, we propose an equivariant geometric feature learning module to learn a Euclidean transformable feature through dedicated designs of equivariant operations. To reason agent's interactions, we propose an invariant interaction reasoning module to achieve a more stable interaction modeling. To further promote more comprehensive motion features, we propose an invariant pattern feature learning module to learn an invariant pattern feature, which cooperates with the equivariant geometric feature to enhance network expressiveness. We conduct experiments for the proposed model on four distinct scenarios: particle dynamics, molecule dynamics, human skeleton motion prediction and pedestrian trajectory prediction. Experimental results show that our method is not only generally applicable, but also achieves state-of-the-art prediction performances on all the four tasks, improving by 24.0/30.1/8.6/9.2%. Code is available at https://github.com/MediaBrain-SJTU/EqMotion.
<div id='section'>Paperid: <span id='pid'>214, <a href='https://arxiv.org/pdf/2303.08658.pdf' target='_blank'>https://arxiv.org/pdf/2303.08658.pdf</a></span>   <span><a href='https://github.com/Kebii/R2ET' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaxu Zhang, Junwu Weng, Di Kang, Fang Zhao, Shaoli Huang, Xuefei Zhe, Linchao Bao, Ying Shan, Jue Wang, Zhigang Tu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.08658">Skinned Motion Retargeting with Residual Perception of Motion Semantics & Geometry</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A good motion retargeting cannot be reached without reasonable consideration of source-target differences on both the skeleton and shape geometry levels. In this work, we propose a novel Residual RETargeting network (R2ET) structure, which relies on two neural modification modules, to adjust the source motions to fit the target skeletons and shapes progressively. In particular, a skeleton-aware module is introduced to preserve the source motion semantics. A shape-aware module is designed to perceive the geometries of target characters to reduce interpenetration and contact-missing. Driven by our explored distance-based losses that explicitly model the motion semantics and geometry, these two modules can learn residual motion modifications on the source motion to generate plausible retargeted motion in a single inference without post-processing. To balance these two modifications, we further present a balancing gate to conduct linear interpolation between them. Extensive experiments on the public dataset Mixamo demonstrate that our R2ET achieves the state-of-the-art performance, and provides a good balance between the preservation of motion semantics as well as the attenuation of interpenetration and contact-missing. Code is available at https://github.com/Kebii/R2ET.
<div id='section'>Paperid: <span id='pid'>215, <a href='https://arxiv.org/pdf/2303.04116.pdf' target='_blank'>https://arxiv.org/pdf/2303.04116.pdf</a></span>   <span><a href='https://github.com/zhejz/TrafficBots' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhejun Zhang, Alexander Liniger, Dengxin Dai, Fisher Yu, Luc Van Gool
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.04116">TrafficBots: Towards World Models for Autonomous Driving Simulation and Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Data-driven simulation has become a favorable way to train and test autonomous driving algorithms. The idea of replacing the actual environment with a learned simulator has also been explored in model-based reinforcement learning in the context of world models. In this work, we show data-driven traffic simulation can be formulated as a world model. We present TrafficBots, a multi-agent policy built upon motion prediction and end-to-end driving, and based on TrafficBots we obtain a world model tailored for the planning module of autonomous vehicles. Existing data-driven traffic simulators are lacking configurability and scalability. To generate configurable behaviors, for each agent we introduce a destination as navigational information, and a time-invariant latent personality that specifies the behavioral style. To improve the scalability, we present a new scheme of positional encoding for angles, allowing all agents to share the same vectorized context and the use of an architecture based on dot-product attention. As a result, we can simulate all traffic participants seen in dense urban scenarios. Experiments on the Waymo open motion dataset show TrafficBots can simulate realistic multi-agent behaviors and achieve good performance on the motion prediction task.
<div id='section'>Paperid: <span id='pid'>216, <a href='https://arxiv.org/pdf/2302.11306.pdf' target='_blank'>https://arxiv.org/pdf/2302.11306.pdf</a></span>   <span><a href='https://github.com/KumapowerLIU/Human-MotionFormer' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongyu Liu, Xintong Han, Chengbin Jin, Lihui Qian, Huawei Wei, Zhe Lin, Faqiang Wang, Haoye Dong, Yibing Song, Jia Xu, Qifeng Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.11306">Human MotionFormer: Transferring Human Motions with Vision Transformers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion transfer aims to transfer motions from a target dynamic person to a source static one for motion synthesis. An accurate matching between the source person and the target motion in both large and subtle motion changes is vital for improving the transferred motion quality. In this paper, we propose Human MotionFormer, a hierarchical ViT framework that leverages global and local perceptions to capture large and subtle motion matching, respectively. It consists of two ViT encoders to extract input features (i.e., a target motion image and a source human image) and a ViT decoder with several cascaded blocks for feature matching and motion transfer. In each block, we set the target motion feature as Query and the source person as Key and Value, calculating the cross-attention maps to conduct a global feature matching. Further, we introduce a convolutional layer to improve the local perception after the global cross-attention computations. This matching process is implemented in both warping and generation branches to guide the motion transfer. During training, we propose a mutual learning loss to enable the co-supervision between warping and generation branches for better motion representations. Experiments show that our Human MotionFormer sets the new state-of-the-art performance both qualitatively and quantitatively. Project page: \url{https://github.com/KumapowerLIU/Human-MotionFormer}
<div id='section'>Paperid: <span id='pid'>217, <a href='https://arxiv.org/pdf/2302.04860.pdf' target='_blank'>https://arxiv.org/pdf/2302.04860.pdf</a></span>   <span><a href='https://github.com/Sirui-Xu/STARS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sirui Xu, Yu-Xiong Wang, Liang-Yan Gui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.04860">Diverse Human Motion Prediction Guided by Multi-Level Spatial-Temporal Anchors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Predicting diverse human motions given a sequence of historical poses has received increasing attention. Despite rapid progress, existing work captures the multi-modal nature of human motions primarily through likelihood-based sampling, where the mode collapse has been widely observed. In this paper, we propose a simple yet effective approach that disentangles randomly sampled codes with a deterministic learnable component named anchors to promote sample precision and diversity. Anchors are further factorized into spatial anchors and temporal anchors, which provide attractively interpretable control over spatial-temporal disparity. In principle, our spatial-temporal anchor-based sampling (STARS) can be applied to different motion predictors. Here we propose an interaction-enhanced spatial-temporal graph convolutional network (IE-STGCN) that encodes prior knowledge of human motions (e.g., spatial locality), and incorporate the anchors into it. Extensive experiments demonstrate that our approach outperforms state of the art in both stochastic and deterministic prediction, suggesting it as a unified framework for modeling human motions. Our code and pretrained models are available at https://github.com/Sirui-Xu/STARS.
<div id='section'>Paperid: <span id='pid'>218, <a href='https://arxiv.org/pdf/2301.05175.pdf' target='_blank'>https://arxiv.org/pdf/2301.05175.pdf</a></span>   <span><a href='https://github.com/dluvizon/scene-aware-3d-multi-human' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Diogo Luvizon, Marc Habermann, Vladislav Golyanik, Adam Kortylewski, Christian Theobalt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.05175">Scene-Aware 3D Multi-Human Motion Capture from a Single Camera</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we consider the problem of estimating the 3D position of multiple humans in a scene as well as their body shape and articulation from a single RGB video recorded with a static camera. In contrast to expensive marker-based or multi-view systems, our lightweight setup is ideal for private users as it enables an affordable 3D motion capture that is easy to install and does not require expert knowledge. To deal with this challenging setting, we leverage recent advances in computer vision using large-scale pre-trained models for a variety of modalities, including 2D body joints, joint angles, normalized disparity maps, and human segmentation masks. Thus, we introduce the first non-linear optimization-based approach that jointly solves for the absolute 3D position of each human, their articulated pose, their individual shapes as well as the scale of the scene. In particular, we estimate the scene depth and person unique scale from normalized disparity predictions using the 2D body joints and joint angles. Given the per-frame scene depth, we reconstruct a point-cloud of the static scene in 3D space. Finally, given the per-frame 3D estimates of the humans and scene point-cloud, we perform a space-time coherent optimization over the video to ensure temporal, spatial and physical plausibility. We evaluate our method on established multi-person 3D human pose benchmarks where we consistently outperform previous methods and we qualitatively demonstrate that our method is robust to in-the-wild conditions including challenging scenes with people of different sizes.
<div id='section'>Paperid: <span id='pid'>219, <a href='https://arxiv.org/pdf/2212.05897.pdf' target='_blank'>https://arxiv.org/pdf/2212.05897.pdf</a></span>   <span><a href='https://github.com/TaeryungLee/MultiAct_RELEASE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Taeryung Lee, Gyeongsik Moon, Kyoung Mu Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.05897">MultiAct: Long-Term 3D Human Motion Generation from Multiple Action Labels</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We tackle the problem of generating long-term 3D human motion from multiple action labels. Two main previous approaches, such as action- and motion-conditioned methods, have limitations to solve this problem. The action-conditioned methods generate a sequence of motion from a single action. Hence, it cannot generate long-term motions composed of multiple actions and transitions between actions. Meanwhile, the motion-conditioned methods generate future motions from initial motion. The generated future motions only depend on the past, so they are not controllable by the user's desired actions. We present MultiAct, the first framework to generate long-term 3D human motion from multiple action labels. MultiAct takes account of both action and motion conditions with a unified recurrent generation system. It repetitively takes the previous motion and action label; then, it generates a smooth transition and the motion of the given action. As a result, MultiAct produces realistic long-term motion controlled by the given sequence of multiple action labels. Codes are available here at https://github.com/TaeryungLee/MultiAct_RELEASE.
<div id='section'>Paperid: <span id='pid'>220, <a href='https://arxiv.org/pdf/2210.15929.pdf' target='_blank'>https://arxiv.org/pdf/2210.15929.pdf</a></span>   <span><a href='https://github.com/junfanlin/oohmg' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Junfan Lin, Jianlong Chang, Lingbo Liu, Guanbin Li, Liang Lin, Qi Tian, Chang Wen Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.15929">Being Comes from Not-being: Open-vocabulary Text-to-Motion Generation with Wordless Training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-to-motion generation is an emerging and challenging problem, which aims to synthesize motion with the same semantics as the input text. However, due to the lack of diverse labeled training data, most approaches either limit to specific types of text annotations or require online optimizations to cater to the texts during inference at the cost of efficiency and stability. In this paper, we investigate offline open-vocabulary text-to-motion generation in a zero-shot learning manner that neither requires paired training data nor extra online optimization to adapt for unseen texts. Inspired by the prompt learning in NLP, we pretrain a motion generator that learns to reconstruct the full motion from the masked motion. During inference, instead of changing the motion generator, our method reformulates the input text into a masked motion as the prompt for the motion generator to ``reconstruct'' the motion. In constructing the prompt, the unmasked poses of the prompt are synthesized by a text-to-pose generator. To supervise the optimization of the text-to-pose generator, we propose the first text-pose alignment model for measuring the alignment between texts and 3D poses. And to prevent the pose generator from overfitting to limited training texts, we further propose a novel wordless training mechanism that optimizes the text-to-pose generator without any training texts. The comprehensive experimental results show that our method obtains a significant improvement against the baseline methods. The code is available at https://github.com/junfanlin/oohmg.
<div id='section'>Paperid: <span id='pid'>221, <a href='https://arxiv.org/pdf/2209.13906.pdf' target='_blank'>https://arxiv.org/pdf/2209.13906.pdf</a></span>   <span><a href='https://github.com/robot-perception-group/SmartMocap' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Nitin Saini, Chun-hao P. Huang, Michael J. Black, Aamir Ahmad
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.13906">SmartMocap: Joint Estimation of Human and Camera Motion using Uncalibrated RGB Cameras</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Markerless human motion capture (mocap) from multiple RGB cameras is a widely studied problem. Existing methods either need calibrated cameras or calibrate them relative to a static camera, which acts as the reference frame for the mocap system. The calibration step has to be done a priori for every capture session, which is a tedious process, and re-calibration is required whenever cameras are intentionally or accidentally moved. In this paper, we propose a mocap method which uses multiple static and moving extrinsically uncalibrated RGB cameras. The key components of our method are as follows. First, since the cameras and the subject can move freely, we select the ground plane as a common reference to represent both the body and the camera motions unlike existing methods which represent bodies in the camera coordinate. Second, we learn a probability distribution of short human motion sequences ($\sim$1sec) relative to the ground plane and leverage it to disambiguate between the camera and human motion. Third, we use this distribution as a motion prior in a novel multi-stage optimization approach to fit the SMPL human body model and the camera poses to the human body keypoints on the images. Finally, we show that our method can work on a variety of datasets ranging from aerial cameras to smartphones. It also gives more accurate results compared to the state-of-the-art on the task of monocular human mocap with a static camera. Our code is available for research purposes on https://github.com/robot-perception-group/SmartMocap.
<div id='section'>Paperid: <span id='pid'>222, <a href='https://arxiv.org/pdf/2209.13508.pdf' target='_blank'>https://arxiv.org/pdf/2209.13508.pdf</a></span>   <span><a href='https://github.com/sshaoshuai/MTR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shaoshuai Shi, Li Jiang, Dengxin Dai, Bernt Schiele
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.13508">Motion Transformer with Global Intention Localization and Local Movement Refinement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Predicting multimodal future behavior of traffic participants is essential for robotic vehicles to make safe decisions. Existing works explore to directly predict future trajectories based on latent features or utilize dense goal candidates to identify agent's destinations, where the former strategy converges slowly since all motion modes are derived from the same feature while the latter strategy has efficiency issue since its performance highly relies on the density of goal candidates. In this paper, we propose Motion TRansformer (MTR) framework that models motion prediction as the joint optimization of global intention localization and local movement refinement. Instead of using goal candidates, MTR incorporates spatial intention priors by adopting a small set of learnable motion query pairs. Each motion query pair takes charge of trajectory prediction and refinement for a specific motion mode, which stabilizes the training process and facilitates better multimodal predictions. Experiments show that MTR achieves state-of-the-art performance on both the marginal and joint motion prediction challenges, ranking 1st on the leaderboards of Waymo Open Motion Dataset. The source code is available at https://github.com/sshaoshuai/MTR.
<div id='section'>Paperid: <span id='pid'>223, <a href='https://arxiv.org/pdf/2207.01794.pdf' target='_blank'>https://arxiv.org/pdf/2207.01794.pdf</a></span>   <span><a href='https://github.com/jhavl/dkt' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/jhavl/dkt' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jesse Haviland, Peter Corke
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2207.01794">Manipulator Differential Kinematics: Part 2: Acceleration and Advanced Applications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This is the second and final article on the tutorial on manipulator differential kinematics. In Part 1, we described a method of modelling kinematics using the elementary transform sequence (ETS), before formulating forward kinematics and the manipulator Jacobian. We then described some basic applications of the manipulator Jacobian including resolved-rate motion control (RRMC), inverse kinematics (IK), and some manipulator performance measures.
  In this article, we formulate the second-order differential kinematics, leading to a definition of manipulator Hessian. We then describe the differential kinematics' analytical forms, which are essential to dynamics applications. Subsequently, we provide a general formula for higher-order derivatives. The first application we consider is advanced velocity control. In this section, we extend resolved-rate motion control to perform sub-tasks while still achieving the goal before redefining the algorithm as a quadratic program to enable greater flexibility and additional constraints. We then take another look at numerical inverse kinematics with an emphasis on adding constraints. Finally, we analyse how the manipulator Hessian can help to escape singularities.
  We have provided Jupyter Notebooks to accompany each section within this tutorial. The Notebooks are written in Python code and use the Robotics Toolbox for Python, and the Swift Simulator to provide examples and implementations of algorithms. While not absolutely essential, for the most engaging and informative experience, we recommend working through the Jupyter Notebooks while reading this article. The Notebooks and setup instructions can be accessed at https://github.com/jhavl/dkt.
<div id='section'>Paperid: <span id='pid'>224, <a href='https://arxiv.org/pdf/2204.01297.pdf' target='_blank'>https://arxiv.org/pdf/2204.01297.pdf</a></span>   <span><a href='https://github.com/Jaakk0F/DSTD-GCN' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiajun Fu, Fuxing Yang, Yonghao Dang, Xiaoli Liu, Jianqin Yin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2204.01297">Learning Constrained Dynamic Correlations in Spatiotemporal Graphs for Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion prediction is challenging due to the complex spatiotemporal feature modeling. Among all methods, graph convolution networks (GCNs) are extensively utilized because of their superiority in explicit connection modeling. Within a GCN, the graph correlation adjacency matrix drives feature aggregation and is the key to extracting predictive motion features. State-of-the-art methods decompose the spatiotemporal correlation into spatial correlations for each frame and temporal correlations for each joint. Directly parameterizing these correlations introduces redundant parameters to represent common relations shared by all frames and all joints. Besides, the spatiotemporal graph adjacency matrix is the same for different motion samples and cannot reflect sample-wise correspondence variances. To overcome these two bottlenecks, we propose dynamic spatiotemporal decompose GC (DSTD-GC), which only takes 28.6% parameters of the state-of-the-art GC. The key of DSTD-GC is constrained dynamic correlation modeling, which explicitly parameterizes the common static constraints as a spatial/temporal vanilla adjacency matrix shared by all frames/joints and dynamically extracts correspondence variances for each frame/joint with an adjustment modeling function. For each sample, the common constrained adjacency matrices are fixed to represent generic motion patterns, while the extracted variances complete the matrices with specific pattern adjustments. Meanwhile, we mathematically reformulate GCs on spatiotemporal graphs into a unified form and find that DSTD-GC relaxes certain constraints of other GC, which contributes to a better representation capability. By combining DSTD-GC with prior knowledge, we propose a powerful spatiotemporal GCN called DSTD-GCN, which outperforms SOTA methods by $3.9\% \sim 8.7\%$ in prediction accuracy with $55.0\% \sim 96.9\%$ fewer parameters.
<div id='section'>Paperid: <span id='pid'>225, <a href='https://arxiv.org/pdf/2203.09116.pdf' target='_blank'>https://arxiv.org/pdf/2203.09116.pdf</a></span>   <span><a href='https://github.com/meaten/MotionAug' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Takahiro Maeda, Norimichi Ukita
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2203.09116">MotionAug: Augmentation with Physical Correction for Human Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a motion data augmentation scheme incorporating motion synthesis encouraging diversity and motion correction imposing physical plausibility. This motion synthesis consists of our modified Variational AutoEncoder (VAE) and Inverse Kinematics (IK). In this VAE, our proposed sampling-near-samples method generates various valid motions even with insufficient training motion data. Our IK-based motion synthesis method allows us to generate a variety of motions semi-automatically. Since these two schemes generate unrealistic artifacts in the synthesized motions, our motion correction rectifies them. This motion correction scheme consists of imitation learning with physics simulation and subsequent motion debiasing. For this imitation learning, we propose the PD-residual force that significantly accelerates the training process. Furthermore, our motion debiasing successfully offsets the motion bias induced by imitation learning to maximize the effect of augmentation. As a result, our method outperforms previous noise-based motion augmentation methods by a large margin on both Recurrent Neural Network-based and Graph Convolutional Network-based human motion prediction models. The code is available at https://github.com/meaten/MotionAug.
<div id='section'>Paperid: <span id='pid'>226, <a href='https://arxiv.org/pdf/2203.04232.pdf' target='_blank'>https://arxiv.org/pdf/2203.04232.pdf</a></span>   <span><a href='https://github.com/jimmy-dq/DMT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yan Xia, Qiangqiang Wu, Wei Li, Antoni B. Chan, Uwe Stilla
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2203.04232">A Lightweight and Detector-free 3D Single Object Tracker on Point Clouds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent works on 3D single object tracking treat the task as a target-specific 3D detection task, where an off-the-shelf 3D detector is commonly employed for the tracking. However, it is non-trivial to perform accurate target-specific detection since the point cloud of objects in raw LiDAR scans is usually sparse and incomplete. In this paper, we address this issue by explicitly leveraging temporal motion cues and propose DMT, a Detector-free Motion-prediction-based 3D Tracking network that completely removes the usage of complicated 3D detectors and is lighter, faster, and more accurate than previous trackers. Specifically, the motion prediction module is first introduced to estimate a potential target center of the current frame in a point-cloud-free manner. Then, an explicit voting module is proposed to directly regress the 3D box from the estimated target center. Extensive experiments on KITTI and NuScenes datasets demonstrate that our DMT can still achieve better performance (~10% improvement over the NuScenes dataset) and a faster tracking speed (i.e., 72 FPS) than state-of-the-art approaches without applying any complicated 3D detectors. Our code is released at \url{https://github.com/jimmy-dq/DMT}
<div id='section'>Paperid: <span id='pid'>227, <a href='https://arxiv.org/pdf/2109.14174.pdf' target='_blank'>https://arxiv.org/pdf/2109.14174.pdf</a></span>   <span><a href='https://github.com/IndigoPurple/TSAMT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yaping Zhao, Guanghan Li, Edmund Y. Lam
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2109.14174">Cross-Camera Human Motion Transfer by Time Series Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With advances in optical sensor technology, heterogeneous camera systems are increasingly used for high-resolution (HR) video acquisition and analysis. However, motion transfer across multiple cameras poses challenges. To address this, we propose an algorithm based on time series analysis that identifies motion seasonality and constructs an additive model to extract transferable patterns. Validated on real-world data, our algorithm demonstrates effectiveness and interpretability. Notably, it improves pose estimation in low-resolution videos by leveraging patterns derived from HR counterparts, enhancing practical utility. Code is available at: https://github.com/IndigoPurple/TSAMT
<div id='section'>Paperid: <span id='pid'>228, <a href='https://arxiv.org/pdf/2308.04808.pdf' target='_blank'>https://arxiv.org/pdf/2308.04808.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qingyao Xu, Weibo Mao, Jingze Gong, Chenxin Xu, Siheng Chen, Weidi Xie, Ya Zhang, Yanfeng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.04808">Joint-Relation Transformer for Multi-Person Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-person motion prediction is a challenging problem due to the dependency of motion on both individual past movements and interactions with other people. Transformer-based methods have shown promising results on this task, but they miss the explicit relation representation between joints, such as skeleton structure and pairwise distance, which is crucial for accurate interaction modeling. In this paper, we propose the Joint-Relation Transformer, which utilizes relation information to enhance interaction modeling and improve future motion prediction. Our relation information contains the relative distance and the intra-/inter-person physical constraints. To fuse relation and joint information, we design a novel joint-relation fusion layer with relation-aware attention to update both features. Additionally, we supervise the relation information by forecasting future distance. Experiments show that our method achieves a 13.4% improvement of 900ms VIM on 3DPW-SoMoF/RC and 17.8%/12.0% improvement of 3s MPJPE on CMU-Mpcap/MuPoTS-3D dataset.
<div id='section'>Paperid: <span id='pid'>229, <a href='https://arxiv.org/pdf/2410.05931.pdf' target='_blank'>https://arxiv.org/pdf/2410.05931.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuta Sahara, Akihiro Miki, Yoshimoto Ribayashi, Shunnosuke Yoshimura, Kento Kawaharazuka, Kei Okada, Masayuki Inaba
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.05931">Construction of Musculoskeletal Simulation for Shoulder Complex with Ligaments and Its Validation via Model Predictive Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The complex ways in which humans utilize their bodies in sports and martial arts are remarkable, and human motion analysis is one of the most effective tools for robot body design and control. On the other hand, motion analysis is not easy, and it is difficult to measure complex body motions in detail due to the influence of numerous muscles and soft tissues, mainly ligaments. In response, various musculoskeletal simulators have been developed and applied to motion analysis and robotics. However, none of them reproduce the ligaments but only the muscles, nor do they focus on the shoulder complex, including the clavicle and scapula, which is one of the most complex parts of the body. Therefore, in this study, a detailed simulation model of the shoulder complex including ligaments is constructed. The model will mimic not only the skeletal structure and muscle arrangement but also the ligament arrangement and maximum muscle strength. Through model predictive control based on the constructed simulation, we confirmed that the ligaments contribute to joint stabilization in the first movement and that the proper distribution of maximum muscle force contributes to the equalization of the load on each muscle, demonstrating the effectiveness of this simulation.
<div id='section'>Paperid: <span id='pid'>230, <a href='https://arxiv.org/pdf/2309.14225.pdf' target='_blank'>https://arxiv.org/pdf/2309.14225.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Annan Tang, Takuma Hiraoka, Naoki Hiraoka, Fan Shi, Kento Kawaharazuka, Kunio Kojima, Kei Okada, Masayuki Inaba
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.14225">HumanMimic: Learning Natural Locomotion and Transitions for Humanoid Robot via Wasserstein Adversarial Imitation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Transferring human motion skills to humanoid robots remains a significant challenge. In this study, we introduce a Wasserstein adversarial imitation learning system, allowing humanoid robots to replicate natural whole-body locomotion patterns and execute seamless transitions by mimicking human motions. First, we present a unified primitive-skeleton motion retargeting to mitigate morphological differences between arbitrary human demonstrators and humanoid robots. An adversarial critic component is integrated with Reinforcement Learning (RL) to guide the control policy to produce behaviors aligned with the data distribution of mixed reference motions. Additionally, we employ a specific Integral Probabilistic Metric (IPM), namely the Wasserstein-1 distance with a novel soft boundary constraint to stabilize the training process and prevent mode collapse. Our system is evaluated on a full-sized humanoid JAXON in the simulator. The resulting control policy demonstrates a wide range of locomotion patterns, including standing, push-recovery, squat walking, human-like straight-leg walking, and dynamic running. Notably, even in the absence of transition motions in the demonstration dataset, robots showcase an emerging ability to transit naturally between distinct locomotion patterns as desired speed changes.
<div id='section'>Paperid: <span id='pid'>231, <a href='https://arxiv.org/pdf/2410.06513.pdf' target='_blank'>https://arxiv.org/pdf/2410.06513.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoyang Liu, Yunyao Mao, Wengang Zhou, Houqiang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.06513">MotionRL: Align Text-to-Motion Generation to Human Preferences with Multi-Reward Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce MotionRL, the first approach to utilize Multi-Reward Reinforcement Learning (RL) for optimizing text-to-motion generation tasks and aligning them with human preferences. Previous works focused on improving numerical performance metrics on the given datasets, often neglecting the variability and subjectivity of human feedback. In contrast, our novel approach uses reinforcement learning to fine-tune the motion generator based on human preferences prior knowledge of the human perception model, allowing it to generate motions that better align human preferences. In addition, MotionRL introduces a novel multi-objective optimization strategy to approximate Pareto optimality between text adherence, motion quality, and human preferences. Extensive experiments and user studies demonstrate that MotionRL not only allows control over the generated results across different objectives but also significantly enhances performance across these metrics compared to other algorithms.
<div id='section'>Paperid: <span id='pid'>232, <a href='https://arxiv.org/pdf/2405.15541.pdf' target='_blank'>https://arxiv.org/pdf/2405.15541.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunyao Mao, Xiaoyang Liu, Wengang Zhou, Zhenbo Lu, Houqiang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.15541">Learning Generalizable Human Motion Generator with Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-driven human motion generation, as one of the vital tasks in computer-aided content creation, has recently attracted increasing attention. While pioneering research has largely focused on improving numerical performance metrics on given datasets, practical applications reveal a common challenge: existing methods often overfit specific motion expressions in the training data, hindering their ability to generalize to novel descriptions like unseen combinations of motions. This limitation restricts their broader applicability. We argue that the aforementioned problem primarily arises from the scarcity of available motion-text pairs, given the many-to-many nature of text-driven motion generation. To tackle this problem, we formulate text-to-motion generation as a Markov decision process and present \textbf{InstructMotion}, which incorporate the trail and error paradigm in reinforcement learning for generalizable human motion generation. Leveraging contrastive pre-trained text and motion encoders, we delve into optimizing reward design to enable InstructMotion to operate effectively on both paired data, enhancing global semantic level text-motion alignment, and synthetic text-only data, facilitating better generalization to novel prompts without the need for ground-truth motion supervision. Extensive experiments on prevalent benchmarks and also our synthesized unpaired dataset demonstrate that the proposed InstructMotion achieves outstanding performance both quantitatively and qualitatively.
<div id='section'>Paperid: <span id='pid'>233, <a href='https://arxiv.org/pdf/2306.10900.pdf' target='_blank'>https://arxiv.org/pdf/2306.10900.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yaqi Zhang, Di Huang, Bin Liu, Shixiang Tang, Yan Lu, Lu Chen, Lei Bai, Qi Chu, Nenghai Yu, Wanli Ouyang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.10900">MotionGPT: Finetuned LLMs Are General-Purpose Motion Generators</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating realistic human motion from given action descriptions has experienced significant advancements because of the emerging requirement of digital humans. While recent works have achieved impressive results in generating motion directly from textual action descriptions, they often support only a single modality of the control signal, which limits their application in the real digital human industry. This paper presents a Motion General-Purpose generaTor (MotionGPT) that can use multimodal control signals, e.g., text and single-frame poses, for generating consecutive human motions by treating multimodal signals as special input tokens in large language models (LLMs). Specifically, we first quantize multimodal control signals into discrete codes and then formulate them in a unified prompt instruction to ask the LLMs to generate the motion answer. Our MotionGPT demonstrates a unified human motion generation model with multimodal control signals by tuning a mere 0.4% of LLM parameters. To the best of our knowledge, MotionGPT is the first method to generate human motion by multimodal control signals, which we hope can shed light on this new direction. Visit our webpage at https://qiqiapink.github.io/MotionGPT/.
<div id='section'>Paperid: <span id='pid'>234, <a href='https://arxiv.org/pdf/2303.01685.pdf' target='_blank'>https://arxiv.org/pdf/2303.01685.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lintao Wang, Kun Hu, Lei Bai, Yu Ding, Wanli Ouyang, Zhiyong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.01685">Multi-Scale Control Signal-Aware Transformer for Motion Synthesis without Phase</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Synthesizing controllable motion for a character using deep learning has been a promising approach due to its potential to learn a compact model without laborious feature engineering. To produce dynamic motion from weak control signals such as desired paths, existing methods often require auxiliary information such as phases for alleviating motion ambiguity, which limits their generalisation capability. As past poses often contain useful auxiliary hints, in this paper, we propose a task-agnostic deep learning method, namely Multi-scale Control Signal-aware Transformer (MCS-T), with an attention based encoder-decoder architecture to discover the auxiliary information implicitly for synthesizing controllable motion without explicitly requiring auxiliary information such as phase. Specifically, an encoder is devised to adaptively formulate the motion patterns of a character's past poses with multi-scale skeletons, and a decoder driven by control signals to further synthesize and predict the character's state by paying context-specialised attention to the encoded past motion patterns. As a result, it helps alleviate the issues of low responsiveness and slow transition which often happen in conventional methods not using auxiliary information. Both qualitative and quantitative experimental results on an existing biped locomotion dataset, which involves diverse types of motion transitions, demonstrate the effectiveness of our method. In particular, MCS-T is able to successfully generate motions comparable to those generated by the methods using auxiliary information.
<div id='section'>Paperid: <span id='pid'>235, <a href='https://arxiv.org/pdf/2405.20325.pdf' target='_blank'>https://arxiv.org/pdf/2405.20325.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuyuan Tu, Qi Dai, Zihao Zhang, Sicheng Xie, Zhi-Qi Cheng, Chong Luo, Xintong Han, Zuxuan Wu, Yu-Gang Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.20325">MotionFollower: Editing Video Motion via Lightweight Score-Guided Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite impressive advancements in diffusion-based video editing models in altering video attributes, there has been limited exploration into modifying motion information while preserving the original protagonist's appearance and background. In this paper, we propose MotionFollower, a lightweight score-guided diffusion model for video motion editing. To introduce conditional controls to the denoising process, MotionFollower leverages two of our proposed lightweight signal controllers, one for poses and the other for appearances, both of which consist of convolution blocks without involving heavy attention calculations. Further, we design a score guidance principle based on a two-branch architecture, including the reconstruction and editing branches, which significantly enhance the modeling capability of texture details and complicated backgrounds. Concretely, we enforce several consistency regularizers and losses during the score estimation. The resulting gradients thus inject appropriate guidance to the intermediate latents, forcing the model to preserve the original background details and protagonists' appearances without interfering with the motion modification. Experiments demonstrate the competitive motion editing ability of MotionFollower qualitatively and quantitatively. Compared with MotionEditor, the most advanced motion editing model, MotionFollower achieves an approximately 80% reduction in GPU memory while delivering superior motion editing performance and exclusively supporting large camera movements and actions.
<div id='section'>Paperid: <span id='pid'>236, <a href='https://arxiv.org/pdf/2311.18830.pdf' target='_blank'>https://arxiv.org/pdf/2311.18830.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuyuan Tu, Qi Dai, Zhi-Qi Cheng, Han Hu, Xintong Han, Zuxuan Wu, Yu-Gang Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.18830">MotionEditor: Editing Video Motion via Content-Aware Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing diffusion-based video editing models have made gorgeous advances for editing attributes of a source video over time but struggle to manipulate the motion information while preserving the original protagonist's appearance and background. To address this, we propose MotionEditor, a diffusion model for video motion editing. MotionEditor incorporates a novel content-aware motion adapter into ControlNet to capture temporal motion correspondence. While ControlNet enables direct generation based on skeleton poses, it encounters challenges when modifying the source motion in the inverted noise due to contradictory signals between the noise (source) and the condition (reference). Our adapter complements ControlNet by involving source content to transfer adapted control signals seamlessly. Further, we build up a two-branch architecture (a reconstruction branch and an editing branch) with a high-fidelity attention injection mechanism facilitating branch interaction. This mechanism enables the editing branch to query the key and value from the reconstruction branch in a decoupled manner, making the editing branch retain the original background and protagonist appearance. We also propose a skeleton alignment algorithm to address the discrepancies in pose size and position. Experiments demonstrate the promising motion editing ability of MotionEditor, both qualitatively and quantitatively.
<div id='section'>Paperid: <span id='pid'>237, <a href='https://arxiv.org/pdf/2508.04228.pdf' target='_blank'>https://arxiv.org/pdf/2508.04228.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kangrui Cen, Baixuan Zhao, Yi Xin, Siqi Luo, Guangtao Zhai, Xiaohong Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.04228">LayerT2V: Interactive Multi-Object Trajectory Layering for Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Controlling object motion trajectories in Text-to-Video (T2V) generation is a challenging and relatively under-explored area, particularly in scenarios involving multiple moving objects. Most community models and datasets in the T2V domain are designed for single-object motion, limiting the performance of current generative models in multi-object tasks. Additionally, existing motion control methods in T2V either lack support for multi-object motion scenes or experience severe performance degradation when object trajectories intersect, primarily due to the semantic conflicts in colliding regions. To address these limitations, we introduce LayerT2V, the first approach for generating video by compositing background and foreground objects layer by layer. This layered generation enables flexible integration of multiple independent elements within a video, positioning each element on a distinct "layer" and thus facilitating coherent multi-object synthesis while enhancing control over the generation process. Extensive experiments demonstrate the superiority of LayerT2V in generating complex multi-object scenarios, showcasing 1.4x and 4.5x improvements in mIoU and AP50 metrics over state-of-the-art (SOTA) methods. Project page and code are available at https://kr-panghu.github.io/LayerT2V/ .
<div id='section'>Paperid: <span id='pid'>238, <a href='https://arxiv.org/pdf/2411.04079.pdf' target='_blank'>https://arxiv.org/pdf/2411.04079.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ke Fan, Jiangning Zhang, Ran Yi, Jingyu Gong, Yabiao Wang, Yating Wang, Xin Tan, Chengjie Wang, Lizhuang Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.04079">Textual Decomposition Then Sub-motion-space Scattering for Open-Vocabulary Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-to-motion generation is a crucial task in computer vision, which generates the target 3D motion by the given text. The existing annotated datasets are limited in scale, resulting in most existing methods overfitting to the small datasets and unable to generalize to the motions of the open domain. Some methods attempt to solve the open-vocabulary motion generation problem by aligning to the CLIP space or using the Pretrain-then-Finetuning paradigm. However, the current annotated dataset's limited scale only allows them to achieve mapping from sub-text-space to sub-motion-space, instead of mapping between full-text-space and full-motion-space (full mapping), which is the key to attaining open-vocabulary motion generation. To this end, this paper proposes to leverage the atomic motion (simple body part motions over a short time period) as an intermediate representation, and leverage two orderly coupled steps, i.e., Textual Decomposition and Sub-motion-space Scattering, to address the full mapping problem. For Textual Decomposition, we design a fine-grained description conversion algorithm, and combine it with the generalization ability of a large language model to convert any given motion text into atomic texts. Sub-motion-space Scattering learns the compositional process from atomic motions to the target motions, to make the learned sub-motion-space scattered to form the full-motion-space. For a given motion of the open domain, it transforms the extrapolation into interpolation and thereby significantly improves generalization. Our network, $DSO$-Net, combines textual $d$ecomposition and sub-motion-space $s$cattering to solve the $o$pen-vocabulary motion generation. Extensive experiments demonstrate that our DSO-Net achieves significant improvements over the state-of-the-art methods on open-vocabulary motion generation. Code is available at https://vankouf.github.io/DSONet/.
<div id='section'>Paperid: <span id='pid'>239, <a href='https://arxiv.org/pdf/2405.15763.pdf' target='_blank'>https://arxiv.org/pdf/2405.15763.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ke Fan, Junshu Tang, Weijian Cao, Ran Yi, Moran Li, Jingyu Gong, Jiangning Zhang, Yabiao Wang, Chengjie Wang, Lizhuang Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.15763">FreeMotion: A Unified Framework for Number-free Text-to-Motion Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-to-motion synthesis is a crucial task in computer vision. Existing methods are limited in their universality, as they are tailored for single-person or two-person scenarios and can not be applied to generate motions for more individuals. To achieve the number-free motion synthesis, this paper reconsiders motion generation and proposes to unify the single and multi-person motion by the conditional motion distribution. Furthermore, a generation module and an interaction module are designed for our FreeMotion framework to decouple the process of conditional motion generation and finally support the number-free motion synthesis. Besides, based on our framework, the current single-person motion spatial control method could be seamlessly integrated, achieving precise control of multi-person motion. Extensive experiments demonstrate the superior performance of our method and our capability to infer single and multi-human motions simultaneously.
<div id='section'>Paperid: <span id='pid'>240, <a href='https://arxiv.org/pdf/2405.12970.pdf' target='_blank'>https://arxiv.org/pdf/2405.12970.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yue Han, Junwei Zhu, Keke He, Xu Chen, Yanhao Ge, Wei Li, Xiangtai Li, Jiangning Zhang, Chengjie Wang, Yong Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.12970">Face Adapter for Pre-Trained Diffusion Models with Fine-Grained ID and Attribute Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current face reenactment and swapping methods mainly rely on GAN frameworks, but recent focus has shifted to pre-trained diffusion models for their superior generation capabilities. However, training these models is resource-intensive, and the results have not yet achieved satisfactory performance levels. To address this issue, we introduce Face-Adapter, an efficient and effective adapter designed for high-precision and high-fidelity face editing for pre-trained diffusion models. We observe that both face reenactment/swapping tasks essentially involve combinations of target structure, ID and attribute. We aim to sufficiently decouple the control of these factors to achieve both tasks in one model. Specifically, our method contains: 1) A Spatial Condition Generator that provides precise landmarks and background; 2) A Plug-and-play Identity Encoder that transfers face embeddings to the text space by a transformer decoder. 3) An Attribute Controller that integrates spatial conditions and detailed attributes. Face-Adapter achieves comparable or even superior performance in terms of motion control precision, ID retention capability, and generation quality compared to fully fine-tuned face reenactment/swapping models. Additionally, Face-Adapter seamlessly integrates with various StableDiffusion models.
<div id='section'>Paperid: <span id='pid'>241, <a href='https://arxiv.org/pdf/2402.04324.pdf' target='_blank'>https://arxiv.org/pdf/2402.04324.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weiming Ren, Huan Yang, Ge Zhang, Cong Wei, Xinrun Du, Wenhao Huang, Wenhu Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.04324">ConsistI2V: Enhancing Visual Consistency for Image-to-Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image-to-video (I2V) generation aims to use the initial frame (alongside a text prompt) to create a video sequence. A grand challenge in I2V generation is to maintain visual consistency throughout the video: existing methods often struggle to preserve the integrity of the subject, background, and style from the first frame, as well as ensure a fluid and logical progression within the video narrative. To mitigate these issues, we propose ConsistI2V, a diffusion-based method to enhance visual consistency for I2V generation. Specifically, we introduce (1) spatiotemporal attention over the first frame to maintain spatial and motion consistency, (2) noise initialization from the low-frequency band of the first frame to enhance layout consistency. These two approaches enable ConsistI2V to generate highly consistent videos. We also extend the proposed approaches to show their potential to improve consistency in auto-regressive long video generation and camera motion control. To verify the effectiveness of our method, we propose I2V-Bench, a comprehensive evaluation benchmark for I2V generation. Our automatic and human evaluation results demonstrate the superiority of ConsistI2V over existing methods.
<div id='section'>Paperid: <span id='pid'>242, <a href='https://arxiv.org/pdf/2206.03408.pdf' target='_blank'>https://arxiv.org/pdf/2206.03408.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaitao Meng, Qingqing Wu, Jie Xu, Wen Chen, Zhiyong Feng, Robert Schober, A. Lee Swindlehurst
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2206.03408">UAV-Enabled Integrated Sensing and Communication: Opportunities and Challenges</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unmanned aerial vehicle (UAV)-enabled integrated sensing and communication (ISAC) has attracted growing research interests in the context of sixth-generation (6G) wireless networks, in which UAVs will be exploited as aerial wireless platforms to provide better coverage and enhanced sensing and communication (S&C) services. However, due to the UAVs' size, weight, and power (SWAP) constraints, controllable mobility, and line-of-sight (LoS) air-ground channels, UAV-enabled ISAC introduces both new opportunities and challenges. This article provides an overview of UAV-enabled ISAC, and proposes various solutions for optimizing the S&C performance. In particular, we first introduce UAV-enabled joint S&C, and discuss UAV motion control, wireless resource allocation, and interference management for the cases of single and multiple UAVs. Then, we present two application scenarios for exploiting the synergy between S&C, namely sensing-assisted UAV communication and communication-assisted UAV sensing. Finally, we highlight several interesting research directions to guide and motivate future work.
<div id='section'>Paperid: <span id='pid'>243, <a href='https://arxiv.org/pdf/2509.14915.pdf' target='_blank'>https://arxiv.org/pdf/2509.14915.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shenghai Yuan, Jason Wai Hao Yee, Weixiang Guo, Zhongyuan Liu, Thien-Minh Nguyen, Lihua Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14915">PERAL: Perception-Aware Motion Control for Passive LiDAR Excitation in Spherical Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous mobile robots increasingly rely on LiDAR-IMU odometry for navigation and mapping, yet horizontally mounted LiDARs such as the MID360 capture few near-ground returns, limiting terrain awareness and degrading performance in feature-scarce environments. Prior solutions - static tilt, active rotation, or high-density sensors - either sacrifice horizontal perception or incur added actuators, cost, and power. We introduce PERAL, a perception-aware motion control framework for spherical robots that achieves passive LiDAR excitation without dedicated hardware. By modeling the coupling between internal differential-drive actuation and sensor attitude, PERAL superimposes bounded, non-periodic oscillations onto nominal goal- or trajectory-tracking commands, enriching vertical scan diversity while preserving navigation accuracy. Implemented on a compact spherical robot, PERAL is validated across laboratory, corridor, and tactical environments. Experiments demonstrate up to 96 percent map completeness, a 27 percent reduction in trajectory tracking error, and robust near-ground human detection, all at lower weight, power, and cost compared with static tilt, active rotation, and fixed horizontal baselines. The design and code will be open-sourced upon acceptance.
<div id='section'>Paperid: <span id='pid'>244, <a href='https://arxiv.org/pdf/2503.03774.pdf' target='_blank'>https://arxiv.org/pdf/2503.03774.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenmin Huang, Ce Hao, Wei Zhan, Jun Ma, Masayoshi Tomizuka
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.03774">Fair Play in the Fast Lane: Integrating Sportsmanship into Autonomous Racing Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous racing has gained significant attention as a platform for high-speed decision-making and motion control. While existing methods primarily focus on trajectory planning and overtaking strategies, the role of sportsmanship in ensuring fair competition remains largely unexplored. In human racing, rules such as the one-motion rule and the enough-space rule prevent dangerous and unsportsmanlike behavior. However, autonomous racing systems often lack mechanisms to enforce these principles, potentially leading to unsafe maneuvers. This paper introduces a bi-level game-theoretic framework to integrate sportsmanship (SPS) into versus racing. At the high level, we model racing intentions using a Stackelberg game, where Monte Carlo Tree Search (MCTS) is employed to derive optimal strategies. At the low level, vehicle interactions are formulated as a Generalized Nash Equilibrium Problem (GNEP), ensuring that all agents follow sportsmanship constraints while optimizing their trajectories. Simulation results demonstrate the effectiveness of the proposed approach in enforcing sportsmanship rules while maintaining competitive performance. We analyze different scenarios where attackers and defenders adhere to or disregard sportsmanship rules and show how knowledge of these constraints influences strategic decision-making. This work highlights the importance of balancing competition and fairness in autonomous racing and provides a foundation for developing ethical and safe AI-driven racing systems.
<div id='section'>Paperid: <span id='pid'>245, <a href='https://arxiv.org/pdf/2409.10032.pdf' target='_blank'>https://arxiv.org/pdf/2409.10032.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weiliang Tang, Jia-Hui Pan, Wei Zhan, Jianshu Zhou, Huaxiu Yao, Yun-Hui Liu, Masayoshi Tomizuka, Mingyu Ding, Chi-Wing Fu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.10032">Embodiment-Agnostic Action Planning via Object-Part Scene Flow</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Observing that the key for robotic action planning is to understand the target-object motion when its associated part is manipulated by the end effector, we propose to generate the 3D object-part scene flow and extract its transformations to solve the action trajectories for diverse embodiments. The advantage of our approach is that it derives the robot action explicitly from object motion prediction, yielding a more robust policy by understanding the object motions. Also, beyond policies trained on embodiment-centric data, our method is embodiment-agnostic, generalizable across diverse embodiments, and being able to learn from human demonstrations. Our method comprises three components: an object-part predictor to locate the part for the end effector to manipulate, an RGBD video generator to predict future RGBD videos, and a trajectory planner to extract embodiment-agnostic transformation sequences and solve the trajectory for diverse embodiments. Trained on videos even without trajectory data, our method still outperforms existing works significantly by 27.7% and 26.2% on the prevailing virtual environments MetaWorld and Franka-Kitchen, respectively. Furthermore, we conducted real-world experiments, showing that our policy, trained only with human demonstration, can be deployed to various embodiments.
<div id='section'>Paperid: <span id='pid'>246, <a href='https://arxiv.org/pdf/2403.06086.pdf' target='_blank'>https://arxiv.org/pdf/2403.06086.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Juanwu Lu, Wei Zhan, Masayoshi Tomizuka, Yeping Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.06086">Towards Generalizable and Interpretable Motion Prediction: A Deep Variational Bayes Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Estimating the potential behavior of the surrounding human-driven vehicles is crucial for the safety of autonomous vehicles in a mixed traffic flow. Recent state-of-the-art achieved accurate prediction using deep neural networks. However, these end-to-end models are usually black boxes with weak interpretability and generalizability. This paper proposes the Goal-based Neural Variational Agent (GNeVA), an interpretable generative model for motion prediction with robust generalizability to out-of-distribution cases. For interpretability, the model achieves target-driven motion prediction by estimating the spatial distribution of long-term destinations with a variational mixture of Gaussians. We identify a causal structure among maps and agents' histories and derive a variational posterior to enhance generalizability. Experiments on motion prediction datasets validate that the fitted model can be interpretable and generalizable and can achieve comparable performance to state-of-the-art results.
<div id='section'>Paperid: <span id='pid'>247, <a href='https://arxiv.org/pdf/2307.00040.pdf' target='_blank'>https://arxiv.org/pdf/2307.00040.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tan Wang, Linjie Li, Kevin Lin, Yuanhao Zhai, Chung-Ching Lin, Zhengyuan Yang, Hanwang Zhang, Zicheng Liu, Lijuan Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.00040">DisCo: Disentangled Control for Realistic Human Dance Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generative AI has made significant strides in computer vision, particularly in text-driven image/video synthesis (T2I/T2V). Despite the notable advancements, it remains challenging in human-centric content synthesis such as realistic dance generation. Current methodologies, primarily tailored for human motion transfer, encounter difficulties when confronted with real-world dance scenarios (e.g., social media dance), which require to generalize across a wide spectrum of poses and intricate human details. In this paper, we depart from the traditional paradigm of human motion transfer and emphasize two additional critical attributes for the synthesis of human dance content in social media contexts: (i) Generalizability: the model should be able to generalize beyond generic human viewpoints as well as unseen human subjects, backgrounds, and poses; (ii) Compositionality: it should allow for the seamless composition of seen/unseen subjects, backgrounds, and poses from different sources. To address these challenges, we introduce DISCO, which includes a novel model architecture with disentangled control to improve the compositionality of dance synthesis, and an effective human attribute pre-training for better generalizability to unseen humans. Extensive qualitative and quantitative results demonstrate that DisCc can generate high-quality human dance images and videos with diverse appearances and flexible motions. Code is available at https://disco-dance.github.io/.
<div id='section'>Paperid: <span id='pid'>248, <a href='https://arxiv.org/pdf/2505.16055.pdf' target='_blank'>https://arxiv.org/pdf/2505.16055.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Patanjali Maithani, Aliasghar Arab, Farshad Khorrami, Prashanth Krishnamurthy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.16055">Proactive Hierarchical Control Barrier Function-Based Safety Prioritization in Close Human-Robot Interaction Scenarios</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In collaborative human-robot environments, the unpredictable and dynamic nature of human motion can lead to situations where collisions become unavoidable. In such cases, it is essential for the robotic system to proactively mitigate potential harm through intelligent control strategies. This paper presents a hierarchical control framework based on Control Barrier Functions (CBFs) designed to ensure safe and adaptive operation of autonomous robotic manipulators during close-proximity human-robot interaction. The proposed method introduces a relaxation variable that enables real-time prioritization of safety constraints, allowing the robot to dynamically manage collision risks based on the criticality of different parts of the human body. A secondary constraint mechanism is incorporated to resolve infeasibility by increasing the priority of imminent threats. The framework is experimentally validated on a Franka Research 3 robot equipped with a ZED2i AI camera for real-time human pose and body detection. Experimental results confirm that the CBF-based controller, integrated with depth sensing, facilitates responsive and safe human-robot collaboration, while providing detailed risk analysis and maintaining robust performance in highly dynamic settings.
<div id='section'>Paperid: <span id='pid'>249, <a href='https://arxiv.org/pdf/2312.04561.pdf' target='_blank'>https://arxiv.org/pdf/2312.04561.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wen Wang, Kecheng Zheng, Qiuyu Wang, Hao Chen, Zifan Shi, Ceyuan Yang, Yujun Shen, Chunhua Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.04561">GenDeF: Learning Generative Deformation Field for Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We offer a new perspective on approaching the task of video generation. Instead of directly synthesizing a sequence of frames, we propose to render a video by warping one static image with a generative deformation field (GenDeF). Such a pipeline enjoys three appealing advantages. First, we can sufficiently reuse a well-trained image generator to synthesize the static image (also called canonical image), alleviating the difficulty in producing a video and thereby resulting in better visual quality. Second, we can easily convert a deformation field to optical flows, making it possible to apply explicit structural regularizations for motion modeling, leading to temporally consistent results. Third, the disentanglement between content and motion allows users to process a synthesized video through processing its corresponding static image without any tuning, facilitating many applications like video editing, keypoint tracking, and video segmentation. Both qualitative and quantitative results on three common video generation benchmarks demonstrate the superiority of our GenDeF method.
<div id='section'>Paperid: <span id='pid'>250, <a href='https://arxiv.org/pdf/2406.05338.pdf' target='_blank'>https://arxiv.org/pdf/2406.05338.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pengyang Ling, Jiazi Bu, Pan Zhang, Xiaoyi Dong, Yuhang Zang, Tong Wu, Huaian Chen, Jiaqi Wang, Yi Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.05338">MotionClone: Training-Free Motion Cloning for Controllable Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion-based controllable video generation offers the potential for creating captivating visual content. Existing methods typically necessitate model training to encode particular motion cues or incorporate fine-tuning to inject certain motion patterns, resulting in limited flexibility and generalization. In this work, we propose MotionClone, a training-free framework that enables motion cloning from reference videos to versatile motion-controlled video generation, including text-to-video and image-to-video. Based on the observation that the dominant components in temporal-attention maps drive motion synthesis, while the rest mainly capture noisy or very subtle motions, MotionClone utilizes sparse temporal attention weights as motion representations for motion guidance, facilitating diverse motion transfer across varying scenarios. Meanwhile, MotionClone allows for the direct extraction of motion representation through a single denoising step, bypassing the cumbersome inversion processes and thus promoting both efficiency and flexibility. Extensive experiments demonstrate that MotionClone exhibits proficiency in both global camera motion and local object motion, with notable superiority in terms of motion fidelity, textual alignment, and temporal consistency.
<div id='section'>Paperid: <span id='pid'>251, <a href='https://arxiv.org/pdf/2403.15709.pdf' target='_blank'>https://arxiv.org/pdf/2403.15709.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sihan Ma, Qiong Cao, Jing Zhang, Dacheng Tao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.15709">Contact-aware Human Motion Generation from Textual Descriptions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper addresses the problem of generating 3D interactive human motion from text. Given a textual description depicting the actions of different body parts in contact with static objects, we synthesize sequences of 3D body poses that are visually natural and physically plausible. Yet, this task poses a significant challenge due to the inadequate consideration of interactions by physical contacts in both motion and textual descriptions, leading to unnatural and implausible sequences. To tackle this challenge, we create a novel dataset named RICH-CAT, representing "Contact-Aware Texts" constructed from the RICH dataset. RICH-CAT comprises high-quality motion, accurate human-object contact labels, and detailed textual descriptions, encompassing over 8,500 motion-text pairs across 26 indoor/outdoor actions. Leveraging RICH-CAT, we propose a novel approach named CATMO for text-driven interactive human motion synthesis that explicitly integrates human body contacts as evidence. We employ two VQ-VAE models to encode motion and body contact sequences into distinct yet complementary latent spaces and an intertwined GPT for generating human motions and contacts in a mutually conditioned manner. Additionally, we introduce a pre-trained text encoder to learn textual embeddings that better discriminate among various contact types, allowing for more precise control over synthesized motions and contacts. Our experiments demonstrate the superior performance of our approach compared to existing text-to-motion methods, producing stable, contact-aware motion sequences. Code and data will be available for research purposes at https://xymsh.github.io/RICH-CAT/
<div id='section'>Paperid: <span id='pid'>252, <a href='https://arxiv.org/pdf/2408.17135.pdf' target='_blank'>https://arxiv.org/pdf/2408.17135.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yabiao Wang, Shuo Wang, Jiangning Zhang, Ke Fan, Jiafu Wu, Zhucun Xue, Yong Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.17135">TIMotion: Temporal and Interactive Framework for Efficient Human-Human Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human-human motion generation is essential for understanding humans as social beings. Current methods fall into two main categories: single-person-based methods and separate modeling-based methods. To delve into this field, we abstract the overall generation process into a general framework MetaMotion, which consists of two phases: temporal modeling and interaction mixing. For temporal modeling, the single-person-based methods concatenate two people into a single one directly, while the separate modeling-based methods skip the modeling of interaction sequences. The inadequate modeling described above resulted in sub-optimal performance and redundant model parameters. In this paper, we introduce TIMotion (Temporal and Interactive Modeling), an efficient and effective framework for human-human motion generation. Specifically, we first propose Causal Interactive Injection to model two separate sequences as a causal sequence leveraging the temporal and causal properties. Then we present Role-Evolving Scanning to adjust to the change in the active and passive roles throughout the interaction. Finally, to generate smoother and more rational motion, we design Localized Pattern Amplification to capture short-term motion patterns. Extensive experiments on InterHuman and InterX demonstrate that our method achieves superior performance. Project page: https://aigc-explorer.github.io/TIMotion-page/
<div id='section'>Paperid: <span id='pid'>253, <a href='https://arxiv.org/pdf/2404.15789.pdf' target='_blank'>https://arxiv.org/pdf/2404.15789.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Teng Hu, Jiangning Zhang, Ran Yi, Yating Wang, Hongrui Huang, Jieyu Weng, Yabiao Wang, Lizhuang Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.15789">MotionMaster: Training-free Camera Motion Transfer For Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The emergence of diffusion models has greatly propelled the progress in image and video generation. Recently, some efforts have been made in controllable video generation, including text-to-video generation and video motion control, among which camera motion control is an important topic. However, existing camera motion control methods rely on training a temporal camera module, and necessitate substantial computation resources due to the large amount of parameters in video generation models. Moreover, existing methods pre-define camera motion types during training, which limits their flexibility in camera control. Therefore, to reduce training costs and achieve flexible camera control, we propose COMD, a novel training-free video motion transfer model, which disentangles camera motions and object motions in source videos and transfers the extracted camera motions to new videos. We first propose a one-shot camera motion disentanglement method to extract camera motion from a single source video, which separates the moving objects from the background and estimates the camera motion in the moving objects region based on the motion in the background by solving a Poisson equation. Furthermore, we propose a few-shot camera motion disentanglement method to extract the common camera motion from multiple videos with similar camera motions, which employs a window-based clustering technique to extract the common features in temporal attention maps of multiple videos. Finally, we propose a motion combination method to combine different types of camera motions together, enabling our model a more controllable and flexible camera control. Extensive experiments demonstrate that our training-free approach can effectively decouple camera-object motion and apply the decoupled camera motion to a wide range of controllable video generation tasks, achieving flexible and diverse camera motion control.
<div id='section'>Paperid: <span id='pid'>254, <a href='https://arxiv.org/pdf/2311.12223.pdf' target='_blank'>https://arxiv.org/pdf/2311.12223.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shisheng Hu, Jie Gao, Xinyu Huang, Mushu Li, Kaige Qu, Conghao Zhou, Xuemin, Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.12223">Digital Twin-Based User-Centric Edge Continual Learning in Integrated Sensing and Communication</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose a digital twin (DT)-based user-centric approach for processing sensing data in an integrated sensing and communication (ISAC) system with high accuracy and efficient resource utilization. The considered scenario involves an ISAC device with a lightweight deep neural network (DNN) and a mobile edge computing (MEC) server with a large DNN. After collecting sensing data, the ISAC device either processes the data locally or uploads them to the server for higher-accuracy data processing. To cope with data drifts, the server updates the lightweight DNN when necessary, referred to as continual learning. Our objective is to minimize the long-term average computation cost of the MEC server by optimizing two decisions, i.e., sensing data offloading and sensing data selection for the DNN update. A DT of the ISAC device is constructed to predict the impact of potential decisions on the long-term computation cost of the server, based on which the decisions are made with closed-form formulas. Experiments on executing DNN-based human motion recognition tasks are conducted to demonstrate the outstanding performance of the proposed DT-based approach in computation cost minimization.
<div id='section'>Paperid: <span id='pid'>255, <a href='https://arxiv.org/pdf/2508.00362.pdf' target='_blank'>https://arxiv.org/pdf/2508.00362.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenghan Chen, Haodong Zhang, Dongqi Wang, Jiyu Yu, Haocheng Xu, Yue Wang, Rong Xiong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.00362">A Whole-Body Motion Imitation Framework from Human Data for Full-Size Humanoid Robot</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion imitation is a pivotal and effective approach for humanoid robots to achieve a more diverse range of complex and expressive movements, making their performances more human-like. However, the significant differences in kinematics and dynamics between humanoid robots and humans present a major challenge in accurately imitating motion while maintaining balance. In this paper, we propose a novel whole-body motion imitation framework for a full-size humanoid robot. The proposed method employs contact-aware whole-body motion retargeting to mimic human motion and provide initial values for reference trajectories, and the non-linear centroidal model predictive controller ensures the motion accuracy while maintaining balance and overcoming external disturbances in real time. The assistance of the whole-body controller allows for more precise torque control. Experiments have been conducted to imitate a variety of human motions both in simulation and in a real-world humanoid robot. These experiments demonstrate the capability of performing with accuracy and adaptability, which validates the effectiveness of our approach.
<div id='section'>Paperid: <span id='pid'>256, <a href='https://arxiv.org/pdf/2503.23284.pdf' target='_blank'>https://arxiv.org/pdf/2503.23284.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Feng-Lin Liu, Hongbo Fu, Xintao Wang, Weicai Ye, Pengfei Wan, Di Zhang, Lin Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.23284">SketchVideo: Sketch-based Video Generation and Editing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video generation and editing conditioned on text prompts or images have undergone significant advancements. However, challenges remain in accurately controlling global layout and geometry details solely by texts, and supporting motion control and local modification through images. In this paper, we aim to achieve sketch-based spatial and motion control for video generation and support fine-grained editing of real or synthetic videos. Based on the DiT video generation model, we propose a memory-efficient control structure with sketch control blocks that predict residual features of skipped DiT blocks. Sketches are drawn on one or two keyframes (at arbitrary time points) for easy interaction. To propagate such temporally sparse sketch conditions across all frames, we propose an inter-frame attention mechanism to analyze the relationship between the keyframes and each video frame. For sketch-based video editing, we design an additional video insertion module that maintains consistency between the newly edited content and the original video's spatial feature and dynamic motion. During inference, we use latent fusion for the accurate preservation of unedited regions. Extensive experiments demonstrate that our SketchVideo achieves superior performance in controllable video generation and editing.
<div id='section'>Paperid: <span id='pid'>257, <a href='https://arxiv.org/pdf/2503.09015.pdf' target='_blank'>https://arxiv.org/pdf/2503.09015.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haodong Zhang, Liang Zhang, Zhenghan Chen, Lu Chen, Yue Wang, Rong Xiong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.09015">Natural Humanoid Robot Locomotion with Generative Motion Prior</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Natural and lifelike locomotion remains a fundamental challenge for humanoid robots to interact with human society. However, previous methods either neglect motion naturalness or rely on unstable and ambiguous style rewards. In this paper, we propose a novel Generative Motion Prior (GMP) that provides fine-grained motion-level supervision for the task of natural humanoid robot locomotion. To leverage natural human motions, we first employ whole-body motion retargeting to effectively transfer them to the robot. Subsequently, we train a generative model offline to predict future natural reference motions for the robot based on a conditional variational auto-encoder. During policy training, the generative motion prior serves as a frozen online motion generator, delivering precise and comprehensive supervision at the trajectory level, including joint angles and keypoint positions. The generative motion prior significantly enhances training stability and improves interpretability by offering detailed and dense guidance throughout the learning process. Experimental results in both simulation and real-world environments demonstrate that our method achieves superior motion naturalness compared to existing approaches. Project page can be found at https://sites.google.com/view/humanoid-gmp
<div id='section'>Paperid: <span id='pid'>258, <a href='https://arxiv.org/pdf/2503.06499.pdf' target='_blank'>https://arxiv.org/pdf/2503.06499.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xukun Zhou, Fengxin Li, Ming Chen, Yan Zhou, Pengfei Wan, Di Zhang, Yeying Jin, Zhaoxin Fan, Hongyan Liu, Jun He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.06499">ExGes: Expressive Human Motion Retrieval and Modulation for Audio-Driven Gesture Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Audio-driven human gesture synthesis is a crucial task with broad applications in virtual avatars, human-computer interaction, and creative content generation. Despite notable progress, existing methods often produce gestures that are coarse, lack expressiveness, and fail to fully align with audio semantics. To address these challenges, we propose ExGes, a novel retrieval-enhanced diffusion framework with three key designs: (1) a Motion Base Construction, which builds a gesture library using training dataset; (2) a Motion Retrieval Module, employing constrative learning and momentum distillation for fine-grained reference poses retreiving; and (3) a Precision Control Module, integrating partial masking and stochastic masking to enable flexible and fine-grained control. Experimental evaluations on BEAT2 demonstrate that ExGes reduces FrÃ©chet Gesture Distance by 6.2\% and improves motion diversity by 5.3\% over EMAGE, with user studies revealing a 71.3\% preference for its naturalness and semantic relevance. Code will be released upon acceptance.
<div id='section'>Paperid: <span id='pid'>259, <a href='https://arxiv.org/pdf/2312.01964.pdf' target='_blank'>https://arxiv.org/pdf/2312.01964.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haodong Zhang, ZhiKe Chen, Haocheng Xu, Lei Hao, Xiaofei Wu, Songcen Xu, Zhensong Zhang, Yue Wang, Rong Xiong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.01964">Semantics-aware Motion Retargeting with Vision-Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Capturing and preserving motion semantics is essential to motion retargeting between animation characters. However, most of the previous works neglect the semantic information or rely on human-designed joint-level representations. Here, we present a novel Semantics-aware Motion reTargeting (SMT) method with the advantage of vision-language models to extract and maintain meaningful motion semantics. We utilize a differentiable module to render 3D motions. Then the high-level motion semantics are incorporated into the motion retargeting process by feeding the vision-language model with the rendered images and aligning the extracted semantic embeddings. To ensure the preservation of fine-grained motion details and high-level semantics, we adopt a two-stage pipeline consisting of skeleton-aware pre-training and fine-tuning with semantics and geometry constraints. Experimental results show the effectiveness of the proposed method in producing high-quality motion retargeting results while accurately preserving motion semantics.
<div id='section'>Paperid: <span id='pid'>260, <a href='https://arxiv.org/pdf/2407.17438.pdf' target='_blank'>https://arxiv.org/pdf/2407.17438.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenzhi Wang, Yixuan Li, Yanhong Zeng, Youqing Fang, Yuwei Guo, Wenran Liu, Jing Tan, Kai Chen, Tianfan Xue, Bo Dai, Dahua Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.17438">HumanVid: Demystifying Training Data for Camera-controllable Human Image Animation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human image animation involves generating videos from a character photo, allowing user control and unlocking the potential for video and movie production. While recent approaches yield impressive results using high-quality training data, the inaccessibility of these datasets hampers fair and transparent benchmarking. Moreover, these approaches prioritize 2D human motion and overlook the significance of camera motions in videos, leading to limited control and unstable video generation. To demystify the training data, we present HumanVid, the first large-scale high-quality dataset tailored for human image animation, which combines crafted real-world and synthetic data. For the real-world data, we compile a vast collection of real-world videos from the internet. We developed and applied careful filtering rules to ensure video quality, resulting in a curated collection of 20K high-resolution (1080P) human-centric videos. Human and camera motion annotation is accomplished using a 2D pose estimator and a SLAM-based method. To expand our synthetic dataset, we collected 10K 3D avatar assets and leveraged existing assets of body shapes, skin textures and clothings. Notably, we introduce a rule-based camera trajectory generation method, enabling the synthetic pipeline to incorporate diverse and precise camera motion annotation, which can rarely be found in real-world data. To verify the effectiveness of HumanVid, we establish a baseline model named CamAnimate, short for Camera-controllable Human Animation, that considers both human and camera motions as conditions. Through extensive experimentation, we demonstrate that such simple baseline training on our HumanVid achieves state-of-the-art performance in controlling both human pose and camera motions, setting a new benchmark. Demo, data and code could be found in the project website: https://humanvid.github.io/.
<div id='section'>Paperid: <span id='pid'>261, <a href='https://arxiv.org/pdf/2412.14018.pdf' target='_blank'>https://arxiv.org/pdf/2412.14018.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tong Chen, Shuya Yang, Junyi Wang, Long Bai, Hongliang Ren, Luping Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.14018">SurgSora: Object-Aware Diffusion Model for Controllable Surgical Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Surgical video generation can enhance medical education and research, but existing methods lack fine-grained motion control and realism. We introduce SurgSora, a framework that generates high-fidelity, motion-controllable surgical videos from a single input frame and user-specified motion cues. Unlike prior approaches that treat objects indiscriminately or rely on ground-truth segmentation masks, SurgSora leverages self-predicted object features and depth information to refine RGB appearance and optical flow for precise video synthesis. It consists of three key modules: (1) the Dual Semantic Injector, which extracts object-specific RGB-D features and segmentation cues to enhance spatial representations; (2) the Decoupled Flow Mapper, which fuses multi-scale optical flow with semantic features for realistic motion dynamics; and (3) the Trajectory Controller, which estimates sparse optical flow and enables user-guided object movement. By conditioning these enriched features within the Stable Video Diffusion, SurgSora achieves state-of-the-art visual authenticity and controllability in advancing surgical video synthesis, as demonstrated by extensive quantitative and qualitative comparisons. Our human evaluation in collaboration with expert surgeons further demonstrates the high realism of SurgSora-generated videos, highlighting the potential of our method for surgical training and education. Our project is available at https://surgsora.github.io/surgsora.github.io.
<div id='section'>Paperid: <span id='pid'>262, <a href='https://arxiv.org/pdf/2408.11801.pdf' target='_blank'>https://arxiv.org/pdf/2408.11801.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuzhou Huang, Yiran Qin, Shunlin Lu, Xintao Wang, Rui Huang, Ying Shan, Ruimao Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.11801">Story3D-Agent: Exploring 3D Storytelling Visualization with Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Traditional visual storytelling is complex, requiring specialized knowledge and substantial resources, yet often constrained by human creativity and creation precision. While Large Language Models (LLMs) enhance visual storytelling, current approaches often limit themselves to 2D visuals or oversimplify stories through motion synthesis and behavioral simulation, failing to create comprehensive, multi-dimensional narratives. To this end, we present Story3D-Agent, a pioneering approach that leverages the capabilities of LLMs to transform provided narratives into 3D-rendered visualizations. By integrating procedural modeling, our approach enables precise control over multi-character actions and motions, as well as diverse decorative elements, ensuring the long-range and dynamic 3D representation. Furthermore, our method supports narrative extension through logical reasoning, ensuring that generated content remains consistent with existing conditions. We have thoroughly evaluated our Story3D-Agent to validate its effectiveness, offering a basic framework to advance 3D story representation.
<div id='section'>Paperid: <span id='pid'>263, <a href='https://arxiv.org/pdf/2405.13865.pdf' target='_blank'>https://arxiv.org/pdf/2405.13865.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chong Mou, Mingdeng Cao, Xintao Wang, Zhaoyang Zhang, Ying Shan, Jian Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.13865">ReVideo: Remake a Video with Motion and Content Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite significant advancements in video generation and editing using diffusion models, achieving accurate and localized video editing remains a substantial challenge. Additionally, most existing video editing methods primarily focus on altering visual content, with limited research dedicated to motion editing. In this paper, we present a novel attempt to Remake a Video (ReVideo) which stands out from existing methods by allowing precise video editing in specific areas through the specification of both content and motion. Content editing is facilitated by modifying the first frame, while the trajectory-based motion control offers an intuitive user interaction experience. ReVideo addresses a new task involving the coupling and training imbalance between content and motion control. To tackle this, we develop a three-stage training strategy that progressively decouples these two aspects from coarse to fine. Furthermore, we propose a spatiotemporal adaptive fusion module to integrate content and motion control across various sampling steps and spatial locations. Extensive experiments demonstrate that our ReVideo has promising performance on several accurate video editing applications, i.e., (1) locally changing video content while keeping the motion constant, (2) keeping content unchanged and customizing new motion trajectories, (3) modifying both content and motion trajectories. Our method can also seamlessly extend these applications to multi-area editing without specific training, demonstrating its flexibility and robustness.
<div id='section'>Paperid: <span id='pid'>264, <a href='https://arxiv.org/pdf/2312.03793.pdf' target='_blank'>https://arxiv.org/pdf/2312.03793.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiwen Yu, Xiaodong Cun, Chenyang Qi, Yong Zhang, Xintao Wang, Ying Shan, Jian Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.03793">AnimateZero: Video Diffusion Models are Zero-Shot Image Animators</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large-scale text-to-video (T2V) diffusion models have great progress in recent years in terms of visual quality, motion and temporal consistency. However, the generation process is still a black box, where all attributes (e.g., appearance, motion) are learned and generated jointly without precise control ability other than rough text descriptions. Inspired by image animation which decouples the video as one specific appearance with the corresponding motion, we propose AnimateZero to unveil the pre-trained text-to-video diffusion model, i.e., AnimateDiff, and provide more precise appearance and motion control abilities for it. For appearance control, we borrow intermediate latents and their features from the text-to-image (T2I) generation for ensuring the generated first frame is equal to the given generated image. For temporal control, we replace the global temporal attention of the original T2V model with our proposed positional-corrected window attention to ensure other frames align with the first frame well. Empowered by the proposed methods, AnimateZero can successfully control the generating progress without further training. As a zero-shot image animator for given images, AnimateZero also enables multiple new applications, including interactive video generation and real image animation. The detailed experiments demonstrate the effectiveness of the proposed method in both T2V and related applications.
<div id='section'>Paperid: <span id='pid'>265, <a href='https://arxiv.org/pdf/2312.03641.pdf' target='_blank'>https://arxiv.org/pdf/2312.03641.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhouxia Wang, Ziyang Yuan, Xintao Wang, Tianshui Chen, Menghan Xia, Ping Luo, Ying Shan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.03641">MotionCtrl: A Unified and Flexible Motion Controller for Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motions in a video primarily consist of camera motion, induced by camera movement, and object motion, resulting from object movement. Accurate control of both camera and object motion is essential for video generation. However, existing works either mainly focus on one type of motion or do not clearly distinguish between the two, limiting their control capabilities and diversity. Therefore, this paper presents MotionCtrl, a unified and flexible motion controller for video generation designed to effectively and independently control camera and object motion. The architecture and training strategy of MotionCtrl are carefully devised, taking into account the inherent properties of camera motion, object motion, and imperfect training data. Compared to previous methods, MotionCtrl offers three main advantages: 1) It effectively and independently controls camera motion and object motion, enabling more fine-grained motion control and facilitating flexible and diverse combinations of both types of motion. 2) Its motion conditions are determined by camera poses and trajectories, which are appearance-free and minimally impact the appearance or shape of objects in generated videos. 3) It is a relatively generalizable model that can adapt to a wide array of camera poses and trajectories once trained. Extensive qualitative and quantitative experiments have been conducted to demonstrate the superiority of MotionCtrl over existing methods. Project Page: https://wzhouxiff.github.io/projects/MotionCtrl/
<div id='section'>Paperid: <span id='pid'>266, <a href='https://arxiv.org/pdf/2308.07749.pdf' target='_blank'>https://arxiv.org/pdf/2308.07749.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bosheng Qin, Wentao Ye, Qifan Yu, Siliang Tang, Yueting Zhuang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.07749">Dancing Avatar: Pose and Text-Guided Human Motion Videos Synthesis with Image Diffusion Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rising demand for creating lifelike avatars in the digital realm has led to an increased need for generating high-quality human videos guided by textual descriptions and poses. We propose Dancing Avatar, designed to fabricate human motion videos driven by poses and textual cues. Our approach employs a pretrained T2I diffusion model to generate each video frame in an autoregressive fashion. The crux of innovation lies in our adept utilization of the T2I diffusion model for producing video frames successively while preserving contextual relevance. We surmount the hurdles posed by maintaining human character and clothing consistency across varying poses, along with upholding the background's continuity amidst diverse human movements. To ensure consistent human appearances across the entire video, we devise an intra-frame alignment module. This module assimilates text-guided synthesized human character knowledge into the pretrained T2I diffusion model, synergizing insights from ChatGPT. For preserving background continuity, we put forth a background alignment pipeline, amalgamating insights from segment anything and image inpainting techniques. Furthermore, we propose an inter-frame alignment module that draws inspiration from an auto-regressive pipeline to augment temporal consistency between adjacent frames, where the preceding frame guides the synthesis process of the current frame. Comparisons with state-of-the-art methods demonstrate that Dancing Avatar exhibits the capacity to generate human videos with markedly superior quality, both in terms of human and background fidelity, as well as temporal coherence compared to existing state-of-the-art approaches.
<div id='section'>Paperid: <span id='pid'>267, <a href='https://arxiv.org/pdf/2311.16635.pdf' target='_blank'>https://arxiv.org/pdf/2311.16635.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sitong Su, Litao Guo, Lianli Gao, Hengtao Shen, Jingkuan Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.16635">MotionZero:Exploiting Motion Priors for Zero-shot Text-to-Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Zero-shot Text-to-Video synthesis generates videos based on prompts without any videos. Without motion information from videos, motion priors implied in prompts are vital guidance. For example, the prompt "airplane landing on the runway" indicates motion priors that the "airplane" moves downwards while the "runway" stays static. Whereas the motion priors are not fully exploited in previous approaches, thus leading to two nontrivial issues: 1) the motion variation pattern remains unaltered and prompt-agnostic for disregarding motion priors; 2) the motion control of different objects is inaccurate and entangled without considering the independent motion priors of different objects. To tackle the two issues, we propose a prompt-adaptive and disentangled motion control strategy coined as MotionZero, which derives motion priors from prompts of different objects by Large-Language-Models and accordingly applies motion control of different objects to corresponding regions in disentanglement. Furthermore, to facilitate videos with varying degrees of motion amplitude, we propose a Motion-Aware Attention scheme which adjusts attention among frames by motion amplitude. Extensive experiments demonstrate that our strategy could correctly control motion of different objects and support versatile applications including zero-shot video edit.
<div id='section'>Paperid: <span id='pid'>268, <a href='https://arxiv.org/pdf/2309.17390.pdf' target='_blank'>https://arxiv.org/pdf/2309.17390.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiang Guo, Jiadai Sun, Yuchao Dai, Guanying Chen, Xiaoqing Ye, Xiao Tan, Errui Ding, Yumeng Zhang, Jingdong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.17390">Forward Flow for Novel View Synthesis of Dynamic Scenes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper proposes a neural radiance field (NeRF) approach for novel view synthesis of dynamic scenes using forward warping. Existing methods often adopt a static NeRF to represent the canonical space, and render dynamic images at other time steps by mapping the sampled 3D points back to the canonical space with the learned backward flow field. However, this backward flow field is non-smooth and discontinuous, which is difficult to be fitted by commonly used smooth motion models. To address this problem, we propose to estimate the forward flow field and directly warp the canonical radiance field to other time steps. Such forward flow field is smooth and continuous within the object region, which benefits the motion model learning. To achieve this goal, we represent the canonical radiance field with voxel grids to enable efficient forward warping, and propose a differentiable warping process, including an average splatting operation and an inpaint network, to resolve the many-to-one and one-to-many mapping issues. Thorough experiments show that our method outperforms existing methods in both novel view rendering and motion modeling, demonstrating the effectiveness of our forward flow motion modeling. Project page: https://npucvr.github.io/ForwardFlowDNeRF
<div id='section'>Paperid: <span id='pid'>269, <a href='https://arxiv.org/pdf/2508.02106.pdf' target='_blank'>https://arxiv.org/pdf/2508.02106.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaiyang Ji, Ye Shi, Zichen Jin, Kangyi Chen, Lan Xu, Yuexin Ma, Jingyi Yu, Jingya Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02106">Towards Immersive Human-X Interaction: A Real-Time Framework for Physically Plausible Motion Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Real-time synthesis of physically plausible human interactions remains a critical challenge for immersive VR/AR systems and humanoid robotics. While existing methods demonstrate progress in kinematic motion generation, they often fail to address the fundamental tension between real-time responsiveness, physical feasibility, and safety requirements in dynamic human-machine interactions. We introduce Human-X, a novel framework designed to enable immersive and physically plausible human interactions across diverse entities, including human-avatar, human-humanoid, and human-robot systems. Unlike existing approaches that focus on post-hoc alignment or simplified physics, our method jointly predicts actions and reactions in real-time using an auto-regressive reaction diffusion planner, ensuring seamless synchronization and context-aware responses. To enhance physical realism and safety, we integrate an actor-aware motion tracking policy trained with reinforcement learning, which dynamically adapts to interaction partners' movements while avoiding artifacts like foot sliding and penetration. Extensive experiments on the Inter-X and InterHuman datasets demonstrate significant improvements in motion quality, interaction continuity, and physical plausibility over state-of-the-art methods. Our framework is validated in real-world applications, including virtual reality interface for human-robot interaction, showcasing its potential for advancing human-robot collaboration.
<div id='section'>Paperid: <span id='pid'>270, <a href='https://arxiv.org/pdf/2502.16175.pdf' target='_blank'>https://arxiv.org/pdf/2502.16175.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziwei Shan, Yaoyu He, Chengfeng Zhao, Jiashen Du, Jingyan Zhang, Qixuan Zhang, Jingyi Yu, Lan Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.16175">Mojito: LLM-Aided Motion Instructor with Jitter-Reduced Inertial Tokens</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human bodily movements convey critical insights into action intentions and cognitive processes, yet existing multimodal systems primarily focused on understanding human motion via language, vision, and audio, which struggle to capture the dynamic forces and torques inherent in 3D motion. Inertial measurement units (IMUs) present a promising alternative, offering lightweight, wearable, and privacy-conscious motion sensing. However, processing of streaming IMU data faces challenges such as wireless transmission instability, sensor noise, and drift, limiting their utility for long-term real-time motion capture (MoCap), and more importantly, online motion analysis. To address these challenges, we introduce Mojito, an intelligent motion agent that integrates inertial sensing with large language models (LLMs) for interactive motion capture and behavioral analysis.
<div id='section'>Paperid: <span id='pid'>271, <a href='https://arxiv.org/pdf/2502.02936.pdf' target='_blank'>https://arxiv.org/pdf/2502.02936.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junkun Jiang, Jie Chen, Ho Yin Au, Mingyuan Chen, Wei Xue, Yike Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.02936">Every Angle Is Worth A Second Glance: Mining Kinematic Skeletal Structures from Multi-view Joint Cloud</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-person motion capture over sparse angular observations is a challenging problem under interference from both self- and mutual-occlusions. Existing works produce accurate 2D joint detection, however, when these are triangulated and lifted into 3D, available solutions all struggle in selecting the most accurate candidates and associating them to the correct joint type and target identity. As such, in order to fully utilize all accurate 2D joint location information, we propose to independently triangulate between all same-typed 2D joints from all camera views regardless of their target ID, forming the Joint Cloud. Joint Cloud consist of both valid joints lifted from the same joint type and target ID, as well as falsely constructed ones that are from different 2D sources. These redundant and inaccurate candidates are processed over the proposed Joint Cloud Selection and Aggregation Transformer (JCSAT) involving three cascaded encoders which deeply explore the trajectile, skeletal structural, and view-dependent correlations among all 3D point candidates in the cross-embedding space. An Optimal Token Attention Path (OTAP) module is proposed which subsequently selects and aggregates informative features from these redundant observations for the final prediction of human motion. To demonstrate the effectiveness of JCSAT, we build and publish a new multi-person motion capture dataset BUMocap-X with complex interactions and severe occlusions. Comprehensive experiments over the newly presented as well as benchmark datasets validate the effectiveness of the proposed framework, which outperforms all existing state-of-the-art methods, especially under challenging occlusion scenarios.
<div id='section'>Paperid: <span id='pid'>272, <a href='https://arxiv.org/pdf/2409.08353.pdf' target='_blank'>https://arxiv.org/pdf/2409.08353.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuheng Jiang, Zhehao Shen, Yu Hong, Chengcheng Guo, Yize Wu, Yingliang Zhang, Jingyi Yu, Lan Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.08353">Robust Dual Gaussian Splatting for Immersive Human-centric Volumetric Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Volumetric video represents a transformative advancement in visual media, enabling users to freely navigate immersive virtual experiences and narrowing the gap between digital and real worlds. However, the need for extensive manual intervention to stabilize mesh sequences and the generation of excessively large assets in existing workflows impedes broader adoption. In this paper, we present a novel Gaussian-based approach, dubbed \textit{DualGS}, for real-time and high-fidelity playback of complex human performance with excellent compression ratios. Our key idea in DualGS is to separately represent motion and appearance using the corresponding skin and joint Gaussians. Such an explicit disentanglement can significantly reduce motion redundancy and enhance temporal coherence. We begin by initializing the DualGS and anchoring skin Gaussians to joint Gaussians at the first frame. Subsequently, we employ a coarse-to-fine training strategy for frame-by-frame human performance modeling. It includes a coarse alignment phase for overall motion prediction as well as a fine-grained optimization for robust tracking and high-fidelity rendering. To integrate volumetric video seamlessly into VR environments, we efficiently compress motion using entropy encoding and appearance using codec compression coupled with a persistent codebook. Our approach achieves a compression ratio of up to 120 times, only requiring approximately 350KB of storage per frame. We demonstrate the efficacy of our representation through photo-realistic, free-view experiences on VR headsets, enabling users to immersively watch musicians in performance and feel the rhythm of the notes at the performers' fingertips.
<div id='section'>Paperid: <span id='pid'>273, <a href='https://arxiv.org/pdf/2404.04890.pdf' target='_blank'>https://arxiv.org/pdf/2404.04890.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiangnan Tang, Jingya Wang, Kaiyang Ji, Lan Xu, Jingyi Yu, Ye Shi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.04890">A Unified Diffusion Framework for Scene-aware Human Motion Estimation from Sparse Signals</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Estimating full-body human motion via sparse tracking signals from head-mounted displays and hand controllers in 3D scenes is crucial to applications in AR/VR. One of the biggest challenges to this task is the one-to-many mapping from sparse observations to dense full-body motions, which endowed inherent ambiguities. To help resolve this ambiguous problem, we introduce a new framework to combine rich contextual information provided by scenes to benefit full-body motion tracking from sparse observations. To estimate plausible human motions given sparse tracking signals and 3D scenes, we develop $\text{S}^2$Fusion, a unified framework fusing \underline{S}cene and sparse \underline{S}ignals with a conditional dif\underline{Fusion} model. $\text{S}^2$Fusion first extracts the spatial-temporal relations residing in the sparse signals via a periodic autoencoder, and then produces time-alignment feature embedding as additional inputs. Subsequently, by drawing initial noisy motion from a pre-trained prior, $\text{S}^2$Fusion utilizes conditional diffusion to fuse scene geometry and sparse tracking signals to generate full-body scene-aware motions. The sampling procedure of $\text{S}^2$Fusion is further guided by a specially designed scene-penetration loss and phase-matching loss, which effectively regularizes the motion of the lower body even in the absence of any tracking signals, making the generated motion much more plausible and coherent. Extensive experimental results have demonstrated that our $\text{S}^2$Fusion outperforms the state-of-the-art in terms of estimation quality and smoothness.
<div id='section'>Paperid: <span id='pid'>274, <a href='https://arxiv.org/pdf/2403.11208.pdf' target='_blank'>https://arxiv.org/pdf/2403.11208.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qianyang Wu, Ye Shi, Xiaoshui Huang, Jingyi Yu, Lan Xu, Jingya Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.11208">THOR: Text to Human-Object Interaction Diffusion via Relation Intervention</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper addresses new methodologies to deal with the challenging task of generating dynamic Human-Object Interactions from textual descriptions (Text2HOI). While most existing works assume interactions with limited body parts or static objects, our task involves addressing the variation in human motion, the diversity of object shapes, and the semantic vagueness of object motion simultaneously. To tackle this, we propose a novel Text-guided Human-Object Interaction diffusion model with Relation Intervention (THOR). THOR is a cohesive diffusion model equipped with a relation intervention mechanism. In each diffusion step, we initiate text-guided human and object motion and then leverage human-object relations to intervene in object motion. This intervention enhances the spatial-temporal relations between humans and objects, with human-centric interaction representation providing additional guidance for synthesizing consistent motion from text. To achieve more reasonable and realistic results, interaction losses is introduced at different levels of motion granularity. Moreover, we construct Text-BEHAVE, a Text2HOI dataset that seamlessly integrates textual descriptions with the currently largest publicly available 3D HOI dataset. Both quantitative and qualitative experiments demonstrate the effectiveness of our proposed model.
<div id='section'>Paperid: <span id='pid'>275, <a href='https://arxiv.org/pdf/2402.17171.pdf' target='_blank'>https://arxiv.org/pdf/2402.17171.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiming Ren, Xiao Han, Chengfeng Zhao, Jingya Wang, Lan Xu, Jingyi Yu, Yuexin Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.17171">LiveHPS: LiDAR-based Scene-level Human Pose and Shape Estimation in Free Environment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>For human-centric large-scale scenes, fine-grained modeling for 3D human global pose and shape is significant for scene understanding and can benefit many real-world applications. In this paper, we present LiveHPS, a novel single-LiDAR-based approach for scene-level human pose and shape estimation without any limitation of light conditions and wearable devices. In particular, we design a distillation mechanism to mitigate the distribution-varying effect of LiDAR point clouds and exploit the temporal-spatial geometric and dynamic information existing in consecutive frames to solve the occlusion and noise disturbance. LiveHPS, with its efficient configuration and high-quality output, is well-suited for real-world applications. Moreover, we propose a huge human motion dataset, named FreeMotion, which is collected in various scenarios with diverse human poses, shapes and translations. It consists of multi-modal and multi-view acquisition data from calibrated and synchronized LiDARs, cameras, and IMUs. Extensive experiments on our new dataset and other public datasets demonstrate the SOTA performance and robustness of our approach. We will release our code and dataset soon.
<div id='section'>Paperid: <span id='pid'>276, <a href='https://arxiv.org/pdf/2312.08985.pdf' target='_blank'>https://arxiv.org/pdf/2312.08985.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Han Liang, Jiacheng Bao, Ruichi Zhang, Sihan Ren, Yuecheng Xu, Sibei Yang, Xin Chen, Jingyi Yu, Lan Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.08985">OMG: Towards Open-vocabulary Motion Generation via Mixture of Controllers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We have recently seen tremendous progress in realistic text-to-motion generation. Yet, the existing methods often fail or produce implausible motions with unseen text inputs, which limits the applications. In this paper, we present OMG, a novel framework, which enables compelling motion generation from zero-shot open-vocabulary text prompts. Our key idea is to carefully tailor the pretrain-then-finetune paradigm into the text-to-motion generation. At the pre-training stage, our model improves the generation ability by learning the rich out-of-domain inherent motion traits. To this end, we scale up a large unconditional diffusion model up to 1B parameters, so as to utilize the massive unlabeled motion data up to over 20M motion instances. At the subsequent fine-tuning stage, we introduce motion ControlNet, which incorporates text prompts as conditioning information, through a trainable copy of the pre-trained model and the proposed novel Mixture-of-Controllers (MoC) block. MoC block adaptively recognizes various ranges of the sub-motions with a cross-attention mechanism and processes them separately with the text-token-specific experts. Such a design effectively aligns the CLIP token embeddings of text prompts to various ranges of compact and expressive motion features. Extensive experiments demonstrate that our OMG achieves significant improvements over the state-of-the-art methods on zero-shot text-to-motion generation. Project page: https://tr3e.github.io/omg-page.
<div id='section'>Paperid: <span id='pid'>277, <a href='https://arxiv.org/pdf/2312.07937.pdf' target='_blank'>https://arxiv.org/pdf/2312.07937.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenqian Zhang, Molin Huang, Yuxuan Zhou, Juze Zhang, Jingyi Yu, Jingya Wang, Lan Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.07937">BOTH2Hands: Inferring 3D Hands from Both Text Prompts and Body Dynamics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The recently emerging text-to-motion advances have spired numerous attempts for convenient and interactive human motion generation. Yet, existing methods are largely limited to generating body motions only without considering the rich two-hand motions, let alone handling various conditions like body dynamics or texts. To break the data bottleneck, we propose BOTH57M, a novel multi-modal dataset for two-hand motion generation. Our dataset includes accurate motion tracking for the human body and hands and provides pair-wised finger-level hand annotations and body descriptions. We further provide a strong baseline method, BOTH2Hands, for the novel task: generating vivid two-hand motions from both implicit body dynamics and explicit text prompts. We first warm up two parallel body-to-hand and text-to-hand diffusion models and then utilize the cross-attention transformer for motion blending. Extensive experiments and cross-validations demonstrate the effectiveness of our approach and dataset for generating convincing two-hand motions from the hybrid body-and-textual conditions. Our dataset and code will be disseminated to the community for future research.
<div id='section'>Paperid: <span id='pid'>278, <a href='https://arxiv.org/pdf/2304.05684.pdf' target='_blank'>https://arxiv.org/pdf/2304.05684.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Han Liang, Wenqian Zhang, Wenxuan Li, Jingyi Yu, Lan Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.05684">InterGen: Diffusion-based Multi-human Motion Generation under Complex Interactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We have recently seen tremendous progress in diffusion advances for generating realistic human motions. Yet, they largely disregard the multi-human interactions. In this paper, we present InterGen, an effective diffusion-based approach that incorporates human-to-human interactions into the motion diffusion process, which enables layman users to customize high-quality two-person interaction motions, with only text guidance. We first contribute a multimodal dataset, named InterHuman. It consists of about 107M frames for diverse two-person interactions, with accurate skeletal motions and 23,337 natural language descriptions. For the algorithm side, we carefully tailor the motion diffusion model to our two-person interaction setting. To handle the symmetry of human identities during interactions, we propose two cooperative transformer-based denoisers that explicitly share weights, with a mutual attention mechanism to further connect the two denoising processes. Then, we propose a novel representation for motion input in our interaction diffusion model, which explicitly formulates the global relations between the two performers in the world frame. We further introduce two novel regularization terms to encode spatial relations, equipped with a corresponding damping scheme during the training of our interaction diffusion model. Extensive experiments validate the effectiveness and generalizability of InterGen. Notably, it can generate more diverse and compelling two-person motions than previous methods and enables various downstream applications for human interactions.
<div id='section'>Paperid: <span id='pid'>279, <a href='https://arxiv.org/pdf/2205.15410.pdf' target='_blank'>https://arxiv.org/pdf/2205.15410.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiming Ren, Chengfeng Zhao, Yannan He, Peishan Cong, Han Liang, Jingyi Yu, Lan Xu, Yuexin Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2205.15410">LiDAR-aid Inertial Poser: Large-scale Human Motion Capture by Sparse Inertial and LiDAR Sensors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a multi-sensor fusion method for capturing challenging 3D human motions with accurate consecutive local poses and global trajectories in large-scale scenarios, only using single LiDAR and 4 IMUs, which are set up conveniently and worn lightly. Specifically, to fully utilize the global geometry information captured by LiDAR and local dynamic motions captured by IMUs, we design a two-stage pose estimator in a coarse-to-fine manner, where point clouds provide the coarse body shape and IMU measurements optimize the local actions. Furthermore, considering the translation deviation caused by the view-dependent partial point cloud, we propose a pose-guided translation corrector. It predicts the offset between captured points and the real root locations, which makes the consecutive movements and trajectories more precise and natural. Moreover, we collect a LiDAR-IMU multi-modal mocap dataset, LIPD, with diverse human actions in long-range scenarios. Extensive quantitative and qualitative experiments on LIPD and other open datasets all demonstrate the capability of our approach for compelling motion capture in large-scale scenarios, which outperforms other methods by an obvious margin. We will release our code and captured dataset to stimulate future research.
<div id='section'>Paperid: <span id='pid'>280, <a href='https://arxiv.org/pdf/2202.05628.pdf' target='_blank'>https://arxiv.org/pdf/2202.05628.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haimin Luo, Teng Xu, Yuheng Jiang, Chenglin Zhou, Qiwei Qiu, Yingliang Zhang, Wei Yang, Lan Xu, Jingyi Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2202.05628">Artemis: Articulated Neural Pets with Appearance and Motion synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We, humans, are entering into a virtual era and indeed want to bring animals to the virtual world as well for companion. Yet, computer-generated (CGI) furry animals are limited by tedious off-line rendering, let alone interactive motion control. In this paper, we present ARTEMIS, a novel neural modeling and rendering pipeline for generating ARTiculated neural pets with appEarance and Motion synthesIS. Our ARTEMIS enables interactive motion control, real-time animation, and photo-realistic rendering of furry animals. The core of our ARTEMIS is a neural-generated (NGI) animal engine, which adopts an efficient octree-based representation for animal animation and fur rendering. The animation then becomes equivalent to voxel-level deformation based on explicit skeletal warping. We further use a fast octree indexing and efficient volumetric rendering scheme to generate appearance and density features maps. Finally, we propose a novel shading network to generate high-fidelity details of appearance and opacity under novel poses from appearance and density feature maps. For the motion control module in ARTEMIS, we combine state-of-the-art animal motion capture approach with recent neural character control scheme. We introduce an effective optimization scheme to reconstruct the skeletal motion of real animals captured by a multi-view RGB and Vicon camera array. We feed all the captured motion into a neural character control scheme to generate abstract control signals with motion styles. We further integrate ARTEMIS into existing engines that support VR headsets, providing an unprecedented immersive experience where a user can intimately interact with a variety of virtual animals with vivid movements and photo-realistic appearance. We make available our ARTEMIS model and dynamic furry animal dataset at https://haiminluo.github.io/publication/artemis/.
<div id='section'>Paperid: <span id='pid'>281, <a href='https://arxiv.org/pdf/2508.13013.pdf' target='_blank'>https://arxiv.org/pdf/2508.13013.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingqiao Xiu, Fangzhou Hong, Yicong Li, Mengze Li, Wentao Wang, Sirui Han, Liang Pan, Ziwei Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.13013">EgoTwin: Dreaming Body and View in First Person</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While exocentric video synthesis has achieved great progress, egocentric video generation remains largely underexplored, which requires modeling first-person view content along with camera motion patterns induced by the wearer's body movements. To bridge this gap, we introduce a novel task of joint egocentric video and human motion generation, characterized by two key challenges: 1) Viewpoint Alignment: the camera trajectory in the generated video must accurately align with the head trajectory derived from human motion; 2) Causal Interplay: the synthesized human motion must causally align with the observed visual dynamics across adjacent video frames. To address these challenges, we propose EgoTwin, a joint video-motion generation framework built on the diffusion transformer architecture. Specifically, EgoTwin introduces a head-centric motion representation that anchors the human motion to the head joint and incorporates a cybernetics-inspired interaction mechanism that explicitly captures the causal interplay between video and motion within attention operations. For comprehensive evaluation, we curate a large-scale real-world dataset of synchronized text-video-motion triplets and design novel metrics to assess video-motion consistency. Extensive experiments demonstrate the effectiveness of the EgoTwin framework.
<div id='section'>Paperid: <span id='pid'>282, <a href='https://arxiv.org/pdf/2507.11949.pdf' target='_blank'>https://arxiv.org/pdf/2507.11949.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuyang Xu, Zhiyang Dou, Mingyi Shi, Liang Pan, Leo Ho, Jingbo Wang, Yuan Liu, Cheng Lin, Yuexin Ma, Wenping Wang, Taku Komura
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.11949">MOSPA: Human Motion Generation Driven by Spatial Audio</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Enabling virtual humans to dynamically and realistically respond to diverse auditory stimuli remains a key challenge in character animation, demanding the integration of perceptual modeling and motion synthesis. Despite its significance, this task remains largely unexplored. Most previous works have primarily focused on mapping modalities like speech, audio, and music to generate human motion. As of yet, these models typically overlook the impact of spatial features encoded in spatial audio signals on human motion. To bridge this gap and enable high-quality modeling of human movements in response to spatial audio, we introduce the first comprehensive Spatial Audio-Driven Human Motion (SAM) dataset, which contains diverse and high-quality spatial audio and motion data. For benchmarking, we develop a simple yet effective diffusion-based generative framework for human MOtion generation driven by SPatial Audio, termed MOSPA, which faithfully captures the relationship between body motion and spatial audio through an effective fusion mechanism. Once trained, MOSPA could generate diverse realistic human motions conditioned on varying spatial audio inputs. We perform a thorough investigation of the proposed dataset and conduct extensive experiments for benchmarking, where our method achieves state-of-the-art performance on this task. Our model and dataset will be open-sourced upon acceptance. Please refer to our supplementary video for more details.
<div id='section'>Paperid: <span id='pid'>283, <a href='https://arxiv.org/pdf/2411.16964.pdf' target='_blank'>https://arxiv.org/pdf/2411.16964.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuming Feng, Zhiyang Dou, Ling-Hao Chen, Yuan Liu, Tianyu Li, Jingbo Wang, Zeyu Cao, Wenping Wang, Taku Komura, Lingjie Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.16964">MotionWavelet: Human Motion Prediction via Wavelet Manifold Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modeling temporal characteristics and the non-stationary dynamics of body movement plays a significant role in predicting human future motions. However, it is challenging to capture these features due to the subtle transitions involved in the complex human motions. This paper introduces MotionWavelet, a human motion prediction framework that utilizes Wavelet Transformation and studies human motion patterns in the spatial-frequency domain. In MotionWavelet, a Wavelet Diffusion Model (WDM) learns a Wavelet Manifold by applying Wavelet Transformation on the motion data therefore encoding the intricate spatial and temporal motion patterns. Once the Wavelet Manifold is built, WDM trains a diffusion model to generate human motions from Wavelet latent vectors. In addition to the WDM, MotionWavelet also presents a Wavelet Space Shaping Guidance mechanism to refine the denoising process to improve conformity with the manifold structure. WDM also develops Temporal Attention-Based Guidance to enhance prediction accuracy. Extensive experiments validate the effectiveness of MotionWavelet, demonstrating improved prediction accuracy and enhanced generalization across various benchmarks. Our code and models will be released upon acceptance.
<div id='section'>Paperid: <span id='pid'>284, <a href='https://arxiv.org/pdf/2312.17135.pdf' target='_blank'>https://arxiv.org/pdf/2312.17135.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiawei Ren, Mingyuan Zhang, Cunjun Yu, Xiao Ma, Liang Pan, Ziwei Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.17135">InsActor: Instruction-driven Physics-based Characters</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating animation of physics-based characters with intuitive control has long been a desirable task with numerous applications. However, generating physically simulated animations that reflect high-level human instructions remains a difficult problem due to the complexity of physical environments and the richness of human language. In this paper, we present InsActor, a principled generative framework that leverages recent advancements in diffusion-based human motion models to produce instruction-driven animations of physics-based characters. Our framework empowers InsActor to capture complex relationships between high-level human instructions and character motions by employing diffusion policies for flexibly conditioned motion planning. To overcome invalid states and infeasible state transitions in planned motions, InsActor discovers low-level skills and maps plans to latent skill sequences in a compact latent space. Extensive experiments demonstrate that InsActor achieves state-of-the-art results on various tasks, including instruction-driven motion generation and instruction-driven waypoint heading. Notably, the ability of InsActor to generate physically simulated animations using high-level human instructions makes it a valuable tool, particularly in executing long-horizon tasks with a rich set of instructions.
<div id='section'>Paperid: <span id='pid'>285, <a href='https://arxiv.org/pdf/2312.04547.pdf' target='_blank'>https://arxiv.org/pdf/2312.04547.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhongang Cai, Jianping Jiang, Zhongfei Qing, Xinying Guo, Mingyuan Zhang, Zhengyu Lin, Haiyi Mei, Chen Wei, Ruisi Wang, Wanqi Yin, Xiangyu Fan, Han Du, Liang Pan, Peng Gao, Zhitao Yang, Yang Gao, Jiaqi Li, Tianxiang Ren, Yukun Wei, Xiaogang Wang, Chen Change Loy, Lei Yang, Ziwei Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.04547">Digital Life Project: Autonomous 3D Characters with Social Intelligence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we present Digital Life Project, a framework utilizing language as the universal medium to build autonomous 3D characters, who are capable of engaging in social interactions and expressing with articulated body motions, thereby simulating life in a digital environment. Our framework comprises two primary components: 1) SocioMind: a meticulously crafted digital brain that models personalities with systematic few-shot exemplars, incorporates a reflection process based on psychology principles, and emulates autonomy by initiating dialogue topics; 2) MoMat-MoGen: a text-driven motion synthesis paradigm for controlling the character's digital body. It integrates motion matching, a proven industry technique to ensure motion quality, with cutting-edge advancements in motion generation for diversity. Extensive experiments demonstrate that each module achieves state-of-the-art performance in its respective domain. Collectively, they enable virtual characters to initiate and sustain dialogues autonomously, while evolving their socio-psychological states. Concurrently, these characters can perform contextually relevant bodily movements. Additionally, a motion captioning module further allows the virtual character to recognize and appropriately respond to human players' actions. Homepage: https://digital-life-project.com/
<div id='section'>Paperid: <span id='pid'>286, <a href='https://arxiv.org/pdf/2312.02256.pdf' target='_blank'>https://arxiv.org/pdf/2312.02256.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenyang Zhou, Zhiyang Dou, Zeyu Cao, Zhouyingcheng Liao, Jingbo Wang, Wenjia Wang, Yuan Liu, Taku Komura, Wenping Wang, Lingjie Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.02256">EMDM: Efficient Motion Diffusion Model for Fast and High-Quality Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce Efficient Motion Diffusion Model (EMDM) for fast and high-quality human motion generation. Current state-of-the-art generative diffusion models have produced impressive results but struggle to achieve fast generation without sacrificing quality. On the one hand, previous works, like motion latent diffusion, conduct diffusion within a latent space for efficiency, but learning such a latent space can be a non-trivial effort. On the other hand, accelerating generation by naively increasing the sampling step size, e.g., DDIM, often leads to quality degradation as it fails to approximate the complex denoising distribution. To address these issues, we propose EMDM, which captures the complex distribution during multiple sampling steps in the diffusion model, allowing for much fewer sampling steps and significant acceleration in generation. This is achieved by a conditional denoising diffusion GAN to capture multimodal data distributions among arbitrary (and potentially larger) step sizes conditioned on control signals, enabling fewer-step motion sampling with high fidelity and diversity. To minimize undesired motion artifacts, geometric losses are imposed during network learning. As a result, EMDM achieves real-time motion generation and significantly improves the efficiency of motion diffusion models compared to existing methods while achieving high-quality motion generation. Our code will be publicly available upon publication.
<div id='section'>Paperid: <span id='pid'>287, <a href='https://arxiv.org/pdf/2304.01116.pdf' target='_blank'>https://arxiv.org/pdf/2304.01116.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingyuan Zhang, Xinying Guo, Liang Pan, Zhongang Cai, Fangzhou Hong, Huirong Li, Lei Yang, Ziwei Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.01116">ReMoDiffuse: Retrieval-Augmented Motion Diffusion Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D human motion generation is crucial for creative industry. Recent advances rely on generative models with domain knowledge for text-driven motion generation, leading to substantial progress in capturing common motions. However, the performance on more diverse motions remains unsatisfactory. In this work, we propose ReMoDiffuse, a diffusion-model-based motion generation framework that integrates a retrieval mechanism to refine the denoising process. ReMoDiffuse enhances the generalizability and diversity of text-driven motion generation with three key designs: 1) Hybrid Retrieval finds appropriate references from the database in terms of both semantic and kinematic similarities. 2) Semantic-Modulated Transformer selectively absorbs retrieval knowledge, adapting to the difference between retrieved samples and the target motion sequence. 3) Condition Mixture better utilizes the retrieval database during inference, overcoming the scale sensitivity in classifier-free guidance. Extensive experiments demonstrate that ReMoDiffuse outperforms state-of-the-art methods by balancing both text-motion consistency and motion quality, especially for more diverse motion generation.
<div id='section'>Paperid: <span id='pid'>288, <a href='https://arxiv.org/pdf/2411.04399.pdf' target='_blank'>https://arxiv.org/pdf/2411.04399.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongsheng Wang, Zehui Feng, Tong Xiao, Genfan Yang, Shengyu Zhang, Fei Wu, Feng Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.04399">ProGraph: Temporally-alignable Probability Guided Graph Topological Modeling for 3D Human Reconstruction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current 3D human motion reconstruction methods from monocular videos rely on features within the current reconstruction window, leading to distortion and deformations in the human structure under local occlusions or blurriness in video frames. To estimate realistic 3D human mesh sequences based on incomplete features, we propose Temporally-alignable Probability Guided Graph Topological Modeling for 3D Human Reconstruction (ProGraph). For missing parts recovery, we exploit the explicit topological-aware probability distribution across the entire motion sequence. To restore the complete human, Graph Topological Modeling (GTM) learns the underlying topological structure, focusing on the relationships inherent in the individual parts. Next, to generate blurred motion parts, Temporal-alignable Probability Distribution (TPDist) utilizes the GTM to predict features based on distribution. This interactive mechanism facilitates motion consistency, allowing the restoration of human parts. Furthermore, Hierarchical Human Loss (HHLoss) constrains the probability distribution errors of inter-frame features during topological structure variation. Our Method achieves superior results than other SOTA methods in addressing occlusions and blurriness on 3DPW.
<div id='section'>Paperid: <span id='pid'>289, <a href='https://arxiv.org/pdf/2411.01805.pdf' target='_blank'>https://arxiv.org/pdf/2411.01805.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fuming You, Minghui Fang, Li Tang, Rongjie Huang, Yongqi Wang, Zhou Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.01805">MoMu-Diffusion: On Learning Long-Term Motion-Music Synchronization and Correspondence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion-to-music and music-to-motion have been studied separately, each attracting substantial research interest within their respective domains. The interaction between human motion and music is a reflection of advanced human intelligence, and establishing a unified relationship between them is particularly important. However, to date, there has been no work that considers them jointly to explore the modality alignment within. To bridge this gap, we propose a novel framework, termed MoMu-Diffusion, for long-term and synchronous motion-music generation. Firstly, to mitigate the huge computational costs raised by long sequences, we propose a novel Bidirectional Contrastive Rhythmic Variational Auto-Encoder (BiCoR-VAE) that extracts the modality-aligned latent representations for both motion and music inputs. Subsequently, leveraging the aligned latent spaces, we introduce a multi-modal Transformer-based diffusion model and a cross-guidance sampling strategy to enable various generation tasks, including cross-modal, multi-modal, and variable-length generation. Extensive experiments demonstrate that MoMu-Diffusion surpasses recent state-of-the-art methods both qualitatively and quantitatively, and can synthesize realistic, diverse, long-term, and beat-matched music or motion sequences. The generated samples and codes are available at https://momu-diffusion.github.io/
<div id='section'>Paperid: <span id='pid'>290, <a href='https://arxiv.org/pdf/2305.00787.pdf' target='_blank'>https://arxiv.org/pdf/2305.00787.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenhui Ye, Jinzheng He, Ziyue Jiang, Rongjie Huang, Jiawei Huang, Jinglin Liu, Yi Ren, Xiang Yin, Zejun Ma, Zhou Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.00787">GeneFace++: Generalized and Stable Real-Time Audio-Driven 3D Talking Face Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating talking person portraits with arbitrary speech audio is a crucial problem in the field of digital human and metaverse. A modern talking face generation method is expected to achieve the goals of generalized audio-lip synchronization, good video quality, and high system efficiency. Recently, neural radiance field (NeRF) has become a popular rendering technique in this field since it could achieve high-fidelity and 3D-consistent talking face generation with a few-minute-long training video. However, there still exist several challenges for NeRF-based methods: 1) as for the lip synchronization, it is hard to generate a long facial motion sequence of high temporal consistency and audio-lip accuracy; 2) as for the video quality, due to the limited data used to train the renderer, it is vulnerable to out-of-domain input condition and produce bad rendering results occasionally; 3) as for the system efficiency, the slow training and inference speed of the vanilla NeRF severely obstruct its usage in real-world applications. In this paper, we propose GeneFace++ to handle these challenges by 1) utilizing the pitch contour as an auxiliary feature and introducing a temporal loss in the facial motion prediction process; 2) proposing a landmark locally linear embedding method to regulate the outliers in the predicted motion sequence to avoid robustness issues; 3) designing a computationally efficient NeRF-based motion-to-video renderer to achieves fast training and real-time inference. With these settings, GeneFace++ becomes the first NeRF-based method that achieves stable and real-time talking face generation with generalized audio-lip synchronization. Extensive experiments show that our method outperforms state-of-the-art baselines in terms of subjective and objective evaluation. Video samples are available at https://genefaceplusplus.github.io .
<div id='section'>Paperid: <span id='pid'>291, <a href='https://arxiv.org/pdf/2507.06405.pdf' target='_blank'>https://arxiv.org/pdf/2507.06405.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lala Shakti Swarup Ray, Mengxi Liu, Deepika Gurung, Bo Zhou, Sungho Suh, Paul Lukowicz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06405">SImpHAR: Advancing impedance-based human activity recognition using 3D simulation and text-to-motion models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human Activity Recognition (HAR) with wearable sensors is essential for applications in healthcare, fitness, and human-computer interaction. Bio-impedance sensing offers unique advantages for fine-grained motion capture but remains underutilized due to the scarcity of labeled data. We introduce SImpHAR, a novel framework addressing this limitation through two core contributions. First, we propose a simulation pipeline that generates realistic bio-impedance signals from 3D human meshes using shortest-path estimation, soft-body physics, and text-to-motion generation serving as a digital twin for data augmentation. Second, we design a two-stage training strategy with decoupled approach that enables broader activity coverage without requiring label-aligned synthetic data. We evaluate SImpHAR on our collected ImpAct dataset and two public benchmarks, showing consistent improvements over state-of-the-art methods, with gains of up to 22.3% and 21.8%, in terms of accuracy and macro F1 score, respectively. Our results highlight the promise of simulation-driven augmentation and modular training for impedance-based HAR.
<div id='section'>Paperid: <span id='pid'>292, <a href='https://arxiv.org/pdf/2503.17978.pdf' target='_blank'>https://arxiv.org/pdf/2503.17978.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dominique Nshimyimana, Vitor Fortes Rey, Sungho Suh, Bo Zhou, Paul Lukowicz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.17978">PIM: Physics-Informed Multi-task Pre-training for Improving Inertial Sensor-Based Human Activity Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human activity recognition (HAR) with deep learning models relies on large amounts of labeled data, often challenging to obtain due to associated cost, time, and labor. Self-supervised learning (SSL) has emerged as an effective approach to leverage unlabeled data through pretext tasks, such as masked reconstruction and multitask learning with signal processing-based data augmentations, to pre-train encoder models. However, such methods are often derived from computer vision approaches that disregard physical mechanisms and constraints that govern wearable sensor data and the phenomena they reflect. In this paper, we propose a physics-informed multi-task pre-training (PIM) framework for IMU-based HAR. PIM generates pre-text tasks based on the understanding of basic physical aspects of human motion: including movement speed, angles of movement, and symmetry between sensor placements. Given a sensor signal, we calculate corresponding features using physics-based equations and use them as pretext tasks for SSL. This enables the model to capture fundamental physical characteristics of human activities, which is especially relevant for multi-sensor systems. Experimental evaluations on four HAR benchmark datasets demonstrate that the proposed method outperforms existing state-of-the-art methods, including data augmentation and masked reconstruction, in terms of accuracy and F1 score. We have observed gains of almost 10\% in macro f1 score and accuracy with only 2 to 8 labeled examples per class and up to 3% when there is no reduction in the amount of training data.
<div id='section'>Paperid: <span id='pid'>293, <a href='https://arxiv.org/pdf/2505.09074.pdf' target='_blank'>https://arxiv.org/pdf/2505.09074.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Letian Wang, Marc-Antoine Lavoie, Sandro Papais, Barza Nisar, Yuxiao Chen, Wenhao Ding, Boris Ivanovic, Hao Shao, Abulikemu Abuduweili, Evan Cook, Yang Zhou, Peter Karkus, Jiachen Li, Changliu Liu, Marco Pavone, Steven Waslander
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.09074">Trends in Motion Prediction Toward Deployable and Generalizable Autonomy: A Revisit and Perspectives</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion prediction, the anticipation of future agent states or scene evolution, is rooted in human cognition, bridging perception and decision-making. It enables intelligent systems, such as robots and self-driving cars, to act safely in dynamic, human-involved environments, and informs broader time-series reasoning challenges. With advances in methods, representations, and datasets, the field has seen rapid progress, reflected in quickly evolving benchmark results. Yet, when state-of-the-art methods are deployed in the real world, they often struggle to generalize to open-world conditions and fall short of deployment standards. This reveals a gap between research benchmarks, which are often idealized or ill-posed, and real-world complexity.
  To address this gap, this survey revisits the generalization and deployability of motion prediction models, with an emphasis on the applications of robotics, autonomous driving, and human motion. We first offer a comprehensive taxonomy of motion prediction methods, covering representations, modeling strategies, application domains, and evaluation protocols. We then study two key challenges: (1) how to push motion prediction models to be deployable to realistic deployment standards, where motion prediction does not act in a vacuum, but functions as one module of closed-loop autonomy stacks - it takes input from the localization and perception, and informs downstream planning and control. 2) how to generalize motion prediction models from limited seen scenarios/datasets to the open-world settings. Throughout the paper, we highlight critical open challenges to guide future work, aiming to recalibrate the community's efforts, fostering progress that is not only measurable but also meaningful for real-world applications. The project webpage corresponding to this paper can be found here https://trends-in-motion-prediction-2025.github.io/.
<div id='section'>Paperid: <span id='pid'>294, <a href='https://arxiv.org/pdf/2504.04338.pdf' target='_blank'>https://arxiv.org/pdf/2504.04338.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alexander Naumann, Xunjiang Gu, Tolga Dimlioglu, Mariusz Bojarski, Alperen Degirmenci, Alexander Popov, Devansh Bisla, Marco Pavone, Urs MÃ¼ller, Boris Ivanovic
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.04338">Data Scaling Laws for End-to-End Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous vehicle (AV) stacks have traditionally relied on decomposed approaches, with separate modules handling perception, prediction, and planning. However, this design introduces information loss during inter-module communication, increases computational overhead, and can lead to compounding errors. To address these challenges, recent works have proposed architectures that integrate all components into an end-to-end differentiable model, enabling holistic system optimization. This shift emphasizes data engineering over software integration, offering the potential to enhance system performance by simply scaling up training resources. In this work, we evaluate the performance of a simple end-to-end driving architecture on internal driving datasets ranging in size from 16 to 8192 hours with both open-loop metrics and closed-loop simulations. Specifically, we investigate how much additional training data is needed to achieve a target performance gain, e.g., a 5% improvement in motion prediction accuracy. By understanding the relationship between model performance and training dataset size, we aim to provide insights for data-driven decision-making in autonomous driving development.
<div id='section'>Paperid: <span id='pid'>295, <a href='https://arxiv.org/pdf/2503.03222.pdf' target='_blank'>https://arxiv.org/pdf/2503.03222.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhumei Wang, Zechen Hu, Ruoxi Guo, Huaijin Pi, Ziyong Feng, Sida Peng, Xiaowei Zhou, Mingtao Pei, Siyuan Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.03222">Mocap-2-to-3: Multi-view Lifting for Monocular Motion Recovery with 2D Pretraining</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recovering absolute human motion from monocular inputs is challenging due to two main issues. First, existing methods depend on 3D training data collected from limited environments, constraining out-of-distribution generalization. The second issue is the difficulty of estimating metric-scale poses from monocular input. To address these challenges, we introduce Mocap-2-to-3, a novel framework that performs multi-view lifting from monocular input by leveraging 2D data pre-training, enabling the reconstruction of metrically accurate 3D motions with absolute positions. To leverage abundant 2D data, we decompose complex 3D motion into multi-view syntheses. We first pretrain a single-view diffusion model on extensive 2D datasets, then fine-tune a multi-view model using public 3D data to enable view-consistent motion generation from monocular input, allowing the model to acquire action priors and diversity through 2D data. Furthermore, to recover absolute poses, we propose a novel human motion representation that decouples the learning of local pose and global movements, while encoding geometric priors of the ground to accelerate convergence. This enables progressive recovery of motion in absolute space during inference. Experimental results on in-the-wild benchmarks demonstrate that our method surpasses state-of-the-art approaches in both camera-space motion realism and world-grounded human positioning, while exhibiting superior generalization capability. Our code will be made publicly available.
<div id='section'>Paperid: <span id='pid'>296, <a href='https://arxiv.org/pdf/2503.00948.pdf' target='_blank'>https://arxiv.org/pdf/2503.00948.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jie Tian, Xiaoye Qu, Zhenyi Lu, Wei Wei, Sichen Liu, Yu Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.00948">Extrapolating and Decoupling Image-to-Video Generation Models: Motion Modeling is Easier Than You Think</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image-to-Video (I2V) generation aims to synthesize a video clip according to a given image and condition (e.g., text). The key challenge of this task lies in simultaneously generating natural motions while preserving the original appearance of the images. However, current I2V diffusion models (I2V-DMs) often produce videos with limited motion degrees or exhibit uncontrollable motion that conflicts with the textual condition. To address these limitations, we propose a novel Extrapolating and Decoupling framework, which introduces model merging techniques to the I2V domain for the first time. Specifically, our framework consists of three separate stages: (1) Starting with a base I2V-DM, we explicitly inject the textual condition into the temporal module using a lightweight, learnable adapter and fine-tune the integrated model to improve motion controllability. (2) We introduce a training-free extrapolation strategy to amplify the dynamic range of the motion, effectively reversing the fine-tuning process to enhance the motion degree significantly. (3) With the above two-stage models excelling in motion controllability and degree, we decouple the relevant parameters associated with each type of motion ability and inject them into the base I2V-DM. Since the I2V-DM handles different levels of motion controllability and dynamics at various denoising time steps, we adjust the motion-aware parameters accordingly over time. Extensive qualitative and quantitative experiments have been conducted to demonstrate the superiority of our framework over existing methods.
<div id='section'>Paperid: <span id='pid'>297, <a href='https://arxiv.org/pdf/2412.13111.pdf' target='_blank'>https://arxiv.org/pdf/2412.13111.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Huaijin Pi, Ruoxi Guo, Zehong Shen, Qing Shuai, Zechen Hu, Zhumei Wang, Yajiao Dong, Ruizhen Hu, Taku Komura, Sida Peng, Xiaowei Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.13111">Motion-2-to-3: Leveraging 2D Motion Data to Boost 3D Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-driven human motion synthesis is capturing significant attention for its ability to effortlessly generate intricate movements from abstract text cues, showcasing its potential for revolutionizing motion design not only in film narratives but also in virtual reality experiences and computer game development. Existing methods often rely on 3D motion capture data, which require special setups resulting in higher costs for data acquisition, ultimately limiting the diversity and scope of human motion. In contrast, 2D human videos offer a vast and accessible source of motion data, covering a wider range of styles and activities. In this paper, we explore leveraging 2D human motion extracted from videos as an alternative data source to improve text-driven 3D motion generation. Our approach introduces a novel framework that disentangles local joint motion from global movements, enabling efficient learning of local motion priors from 2D data. We first train a single-view 2D local motion generator on a large dataset of text-motion pairs. To enhance this model to synthesize 3D motion, we fine-tune the generator with 3D data, transforming it into a multi-view generator that predicts view-consistent local joint motion and root dynamics. Experiments on the HumanML3D dataset and novel text prompts demonstrate that our method efficiently utilizes 2D data, supporting realistic 3D human motion generation and broadening the range of motion types it supports. Our code will be made publicly available at https://zju3dv.github.io/Motion-2-to-3/.
<div id='section'>Paperid: <span id='pid'>298, <a href='https://arxiv.org/pdf/2409.06662.pdf' target='_blank'>https://arxiv.org/pdf/2409.06662.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zehong Shen, Huaijin Pi, Yan Xia, Zhi Cen, Sida Peng, Zechen Hu, Hujun Bao, Ruizhen Hu, Xiaowei Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.06662">World-Grounded Human Motion Recovery via Gravity-View Coordinates</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a novel method for recovering world-grounded human motion from monocular video. The main challenge lies in the ambiguity of defining the world coordinate system, which varies between sequences. Previous approaches attempt to alleviate this issue by predicting relative motion in an autoregressive manner, but are prone to accumulating errors. Instead, we propose estimating human poses in a novel Gravity-View (GV) coordinate system, which is defined by the world gravity and the camera view direction. The proposed GV system is naturally gravity-aligned and uniquely defined for each video frame, largely reducing the ambiguity of learning image-pose mapping. The estimated poses can be transformed back to the world coordinate system using camera rotations, forming a global motion sequence. Additionally, the per-frame estimation avoids error accumulation in the autoregressive methods. Experiments on in-the-wild benchmarks demonstrate that our method recovers more realistic motion in both the camera space and world-grounded settings, outperforming state-of-the-art methods in both accuracy and speed. The code is available at https://zju3dv.github.io/gvhmr/.
<div id='section'>Paperid: <span id='pid'>299, <a href='https://arxiv.org/pdf/2405.07784.pdf' target='_blank'>https://arxiv.org/pdf/2405.07784.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhi Cen, Huaijin Pi, Sida Peng, Zehong Shen, Minghui Yang, Shuai Zhu, Hujun Bao, Xiaowei Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.07784">Generating Human Motion in 3D Scenes from Text Descriptions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating human motions from textual descriptions has gained growing research interest due to its wide range of applications. However, only a few works consider human-scene interactions together with text conditions, which is crucial for visual and physical realism. This paper focuses on the task of generating human motions in 3D indoor scenes given text descriptions of the human-scene interactions. This task presents challenges due to the multi-modality nature of text, scene, and motion, as well as the need for spatial reasoning. To address these challenges, we propose a new approach that decomposes the complex problem into two more manageable sub-problems: (1) language grounding of the target object and (2) object-centric motion generation. For language grounding of the target object, we leverage the power of large language models. For motion generation, we design an object-centric scene representation for the generative model to focus on the target object, thereby reducing the scene complexity and facilitating the modeling of the relationship between human motions and the object. Experiments demonstrate the better motion quality of our approach compared to baselines and validate our design choices.
<div id='section'>Paperid: <span id='pid'>300, <a href='https://arxiv.org/pdf/2310.05885.pdf' target='_blank'>https://arxiv.org/pdf/2310.05885.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiyu Huang, Peter Karkus, Boris Ivanovic, Yuxiao Chen, Marco Pavone, Chen Lv
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.05885">DTPP: Differentiable Joint Conditional Prediction and Cost Evaluation for Tree Policy Planning in Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion prediction and cost evaluation are vital components in the decision-making system of autonomous vehicles. However, existing methods often ignore the importance of cost learning and treat them as separate modules. In this study, we employ a tree-structured policy planner and propose a differentiable joint training framework for both ego-conditioned prediction and cost models, resulting in a direct improvement of the final planning performance. For conditional prediction, we introduce a query-centric Transformer model that performs efficient ego-conditioned motion prediction. For planning cost, we propose a learnable context-aware cost function with latent interaction features, facilitating differentiable joint learning. We validate our proposed approach using the real-world nuPlan dataset and its associated planning test platform. Our framework not only matches state-of-the-art planning methods but outperforms other learning-based methods in planning quality, while operating more efficiently in terms of runtime. We show that joint training delivers significantly better performance than separate training of the two modules. Additionally, we find that tree-structured policy planning outperforms the conventional single-stage planning approach.
<div id='section'>Paperid: <span id='pid'>301, <a href='https://arxiv.org/pdf/2506.18680.pdf' target='_blank'>https://arxiv.org/pdf/2506.18680.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anindita Ghosh, Bing Zhou, Rishabh Dabral, Jian Wang, Vladislav Golyanik, Christian Theobalt, Philipp Slusallek, Chuan Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.18680">DuetGen: Music Driven Two-Person Dance Generation via Hierarchical Masked Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present DuetGen, a novel framework for generating interactive two-person dances from music. The key challenge of this task lies in the inherent complexities of two-person dance interactions, where the partners need to synchronize both with each other and with the music. Inspired by the recent advances in motion synthesis, we propose a two-stage solution: encoding two-person motions into discrete tokens and then generating these tokens from music. To effectively capture intricate interactions, we represent both dancers' motions as a unified whole to learn the necessary motion tokens, and adopt a coarse-to-fine learning strategy in both the stages. Our first stage utilizes a VQ-VAE that hierarchically separates high-level semantic features at a coarse temporal resolution from low-level details at a finer resolution, producing two discrete token sequences at different abstraction levels. Subsequently, in the second stage, two generative masked transformers learn to map music signals to these dance tokens: the first producing high-level semantic tokens, and the second, conditioned on music and these semantic tokens, producing the low-level tokens. We train both transformers to learn to predict randomly masked tokens within the sequence, enabling them to iteratively generate motion tokens by filling an empty token sequence during inference. Through the hierarchical masked modeling and dedicated interaction representation, DuetGen achieves the generation of synchronized and interactive two-person dances across various genres. Extensive experiments and user studies on a benchmark duet dance dataset demonstrate state-of-the-art performance of DuetGen in motion realism, music-dance alignment, and partner coordination.
<div id='section'>Paperid: <span id='pid'>302, <a href='https://arxiv.org/pdf/2505.04317.pdf' target='_blank'>https://arxiv.org/pdf/2505.04317.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruize Zhang, Sirui Xiang, Zelai Xu, Feng Gao, Shilong Ji, Wenhao Tang, Wenbo Ding, Chao Yu, Yu Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.04317">Mastering Multi-Drone Volleyball through Hierarchical Co-Self-Play Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we tackle the problem of learning to play 3v3 multi-drone volleyball, a new embodied competitive task that requires both high-level strategic coordination and low-level agile control. The task is turn-based, multi-agent, and physically grounded, posing significant challenges due to its long-horizon dependencies, tight inter-agent coupling, and the underactuated dynamics of quadrotors. To address this, we propose Hierarchical Co-Self-Play (HCSP), a hierarchical reinforcement learning framework that separates centralized high-level strategic decision-making from decentralized low-level motion control. We design a three-stage population-based training pipeline to enable both strategy and skill to emerge from scratch without expert demonstrations: (I) training diverse low-level skills, (II) learning high-level strategy via self-play with fixed low-level skills, and (III) joint fine-tuning through co-self-play. Experiments show that HCSP achieves superior performance, outperforming non-hierarchical self-play and rule-based hierarchical baselines with an average 82.9% win rate and a 71.5% win rate against the two-stage variant. Moreover, co-self-play leads to emergent team behaviors such as role switching and coordinated formations, demonstrating the effectiveness of our hierarchical design and training scheme. The project page is at https://sites.google.com/view/hi-co-self-play.
<div id='section'>Paperid: <span id='pid'>303, <a href='https://arxiv.org/pdf/2504.13582.pdf' target='_blank'>https://arxiv.org/pdf/2504.13582.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zongyuan Chen, Yan Xia, Jiayuan Liu, Jijia Liu, Wenhao Tang, Jiayu Chen, Feng Gao, Longfei Ma, Hongen Liao, Yu Wang, Chao Yu, Boyu Zhang, Fei Xing
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.13582">Hysteresis-Aware Neural Network Modeling and Whole-Body Reinforcement Learning Control of Soft Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Soft robots exhibit inherent compliance and safety, which makes them particularly suitable for applications requiring direct physical interaction with humans, such as surgical procedures. However, their nonlinear and hysteretic behavior, resulting from the properties of soft materials, presents substantial challenges for accurate modeling and control. In this study, we present a soft robotic system designed for surgical applications and propose a hysteresis-aware whole-body neural network model that accurately captures and predicts the soft robot's whole-body motion, including its hysteretic behavior. Building upon the high-precision dynamic model, we construct a highly parallel simulation environment for soft robot control and apply an on-policy reinforcement learning algorithm to efficiently train whole-body motion control strategies. Based on the trained control policy, we developed a soft robotic system for surgical applications and validated it through phantom-based laser ablation experiments in a physical environment. The results demonstrate that the hysteresis-aware modeling reduces the Mean Squared Error (MSE) by 84.95 percent compared to traditional modeling methods. The deployed control algorithm achieved a trajectory tracking error ranging from 0.126 to 0.250 mm on the real soft robot, highlighting its precision in real-world conditions. The proposed method showed strong performance in phantom-based surgical experiments and demonstrates its potential for complex scenarios, including future real-world clinical applications.
<div id='section'>Paperid: <span id='pid'>304, <a href='https://arxiv.org/pdf/2502.07869.pdf' target='_blank'>https://arxiv.org/pdf/2502.07869.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Christen Millerdurai, Hiroyasu Akada, Jian Wang, Diogo Luvizon, Alain Pagani, Didier Stricker, Christian Theobalt, Vladislav Golyanik
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.07869">EventEgo3D++: 3D Human Motion Capture from a Head-Mounted Event Camera</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Monocular egocentric 3D human motion capture remains a significant challenge, particularly under conditions of low lighting and fast movements, which are common in head-mounted device applications. Existing methods that rely on RGB cameras often fail under these conditions. To address these limitations, we introduce EventEgo3D++, the first approach that leverages a monocular event camera with a fisheye lens for 3D human motion capture. Event cameras excel in high-speed scenarios and varying illumination due to their high temporal resolution, providing reliable cues for accurate 3D human motion capture. EventEgo3D++ leverages the LNES representation of event streams to enable precise 3D reconstructions. We have also developed a mobile head-mounted device (HMD) prototype equipped with an event camera, capturing a comprehensive dataset that includes real event observations from both controlled studio environments and in-the-wild settings, in addition to a synthetic dataset. Additionally, to provide a more holistic dataset, we include allocentric RGB streams that offer different perspectives of the HMD wearer, along with their corresponding SMPL body model. Our experiments demonstrate that EventEgo3D++ achieves superior 3D accuracy and robustness compared to existing solutions, even in challenging conditions. Moreover, our method supports real-time 3D pose updates at a rate of 140Hz. This work is an extension of the EventEgo3D approach (CVPR 2024) and further advances the state of the art in egocentric 3D human motion capture. For more details, visit the project page at https://eventego3d.mpi-inf.mpg.de.
<div id='section'>Paperid: <span id='pid'>305, <a href='https://arxiv.org/pdf/2502.01932.pdf' target='_blank'>https://arxiv.org/pdf/2502.01932.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zelai Xu, Ruize Zhang, Chao Yu, Huining Yuan, Xiangmin Yi, Shilong Ji, Chuqi Wang, Wenhao Tang, Feng Gao, Wenbo Ding, Xinlei Chen, Yu Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.01932">VolleyBots: A Testbed for Multi-Drone Volleyball Game Combining Motion Control and Strategic Play</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robot sports, characterized by well-defined objectives, explicit rules, and dynamic interactions, present ideal scenarios for demonstrating embodied intelligence. In this paper, we present VolleyBots, a novel robot sports testbed where multiple drones cooperate and compete in the sport of volleyball under physical dynamics. VolleyBots integrates three features within a unified platform: competitive and cooperative gameplay, turn-based interaction structure, and agile 3D maneuvering. Competitive and cooperative gameplay challenges each drone to coordinate with its teammates while anticipating and countering opposing teams' tactics. Turn-based interaction demands precise timing, accurate state prediction, and management of long-horizon temporal dependencies. Agile 3D maneuvering requires rapid accelerations, sharp turns, and precise 3D positioning despite the quadrotor's underactuated dynamics. These intertwined features yield a complex problem combining motion control and strategic play, with no available expert demonstrations. We provide a comprehensive suite of tasks ranging from single-drone drills to multi-drone cooperative and competitive tasks, accompanied by baseline evaluations of representative multi-agent reinforcement learning (MARL) and game-theoretic algorithms. Simulation results show that on-policy reinforcement learning (RL) methods outperform off-policy methods in single-agent tasks, but both approaches struggle in complex tasks that combine motion control and strategic play. We additionally design a hierarchical policy which achieves a 69.5% percent win rate against the strongest baseline in the 3 vs 3 task, underscoring its potential as an effective solution for tackling the complex interplay between low-level control and high-level strategy. The project page is at https://sites.google.com/view/thu-volleybots.
<div id='section'>Paperid: <span id='pid'>306, <a href='https://arxiv.org/pdf/2404.08640.pdf' target='_blank'>https://arxiv.org/pdf/2404.08640.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Christen Millerdurai, Hiroyasu Akada, Jian Wang, Diogo Luvizon, Christian Theobalt, Vladislav Golyanik
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.08640">EventEgo3D: 3D Human Motion Capture from Egocentric Event Streams</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Monocular egocentric 3D human motion capture is a challenging and actively researched problem. Existing methods use synchronously operating visual sensors (e.g. RGB cameras) and often fail under low lighting and fast motions, which can be restricting in many applications involving head-mounted devices. In response to the existing limitations, this paper 1) introduces a new problem, i.e., 3D human motion capture from an egocentric monocular event camera with a fisheye lens, and 2) proposes the first approach to it called EventEgo3D (EE3D). Event streams have high temporal resolution and provide reliable cues for 3D human motion capture under high-speed human motions and rapidly changing illumination. The proposed EE3D framework is specifically tailored for learning with event streams in the LNES representation, enabling high 3D reconstruction accuracy. We also design a prototype of a mobile head-mounted device with an event camera and record a real dataset with event observations and the ground-truth 3D human poses (in addition to the synthetic dataset). Our EE3D demonstrates robustness and superior 3D accuracy compared to existing solutions across various challenging experiments while supporting real-time 3D pose update rates of 140Hz.
<div id='section'>Paperid: <span id='pid'>307, <a href='https://arxiv.org/pdf/2312.14929.pdf' target='_blank'>https://arxiv.org/pdf/2312.14929.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Soshi Shimada, Franziska Mueller, Jan Bednarik, Bardia Doosti, Bernd Bickel, Danhang Tang, Vladislav Golyanik, Jonathan Taylor, Christian Theobalt, Thabo Beeler
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.14929">MACS: Mass Conditioned 3D Hand and Object Motion Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The physical properties of an object, such as mass, significantly affect how we manipulate it with our hands. Surprisingly, this aspect has so far been neglected in prior work on 3D motion synthesis. To improve the naturalness of the synthesized 3D hand object motions, this work proposes MACS the first MAss Conditioned 3D hand and object motion Synthesis approach. Our approach is based on cascaded diffusion models and generates interactions that plausibly adjust based on the object mass and interaction type. MACS also accepts a manually drawn 3D object trajectory as input and synthesizes the natural 3D hand motions conditioned by the object mass. This flexibility enables MACS to be used for various downstream applications, such as generating synthetic training data for ML tasks, fast animation of hands for graphics workflows, and generating character interactions for computer games. We show experimentally that a small-scale dataset is sufficient for MACS to reasonably generalize across interpolated and extrapolated object masses unseen during the training. Furthermore, MACS shows moderate generalization to unseen objects, thanks to the mass-conditioned contact labels generated by our surface contact synthesis model ConNet. Our comprehensive user study confirms that the synthesized 3D hand-object interactions are highly plausible and realistic.
<div id='section'>Paperid: <span id='pid'>308, <a href='https://arxiv.org/pdf/2311.17057.pdf' target='_blank'>https://arxiv.org/pdf/2311.17057.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anindita Ghosh, Rishabh Dabral, Vladislav Golyanik, Christian Theobalt, Philipp Slusallek
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.17057">ReMoS: 3D Motion-Conditioned Reaction Synthesis for Two-Person Interactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current approaches for 3D human motion synthesis generate high quality animations of digital humans performing a wide variety of actions and gestures. However, a notable technological gap exists in addressing the complex dynamics of multi human interactions within this paradigm. In this work, we present ReMoS, a denoising diffusion based model that synthesizes full body reactive motion of a person in a two person interaction scenario. Given the motion of one person, we employ a combined spatio temporal cross attention mechanism to synthesize the reactive body and hand motion of the second person, thereby completing the interactions between the two. We demonstrate ReMoS across challenging two person scenarios such as pair dancing, Ninjutsu, kickboxing, and acrobatics, where one persons movements have complex and diverse influences on the other. We also contribute the ReMoCap dataset for two person interactions containing full body and finger motions. We evaluate ReMoS through multiple quantitative metrics, qualitative visualizations, and a user study, and also indicate usability in interactive motion editing applications.
<div id='section'>Paperid: <span id='pid'>309, <a href='https://arxiv.org/pdf/2308.12969.pdf' target='_blank'>https://arxiv.org/pdf/2308.12969.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wanyue Zhang, Rishabh Dabral, Thomas LeimkÃ¼hler, Vladislav Golyanik, Marc Habermann, Christian Theobalt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.12969">ROAM: Robust and Object-Aware Motion Generation Using Neural Pose Descriptors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing automatic approaches for 3D virtual character motion synthesis supporting scene interactions do not generalise well to new objects outside training distributions, even when trained on extensive motion capture datasets with diverse objects and annotated interactions. This paper addresses this limitation and shows that robustness and generalisation to novel scene objects in 3D object-aware character synthesis can be achieved by training a motion model with as few as one reference object. We leverage an implicit feature representation trained on object-only datasets, which encodes an SE(3)-equivariant descriptor field around the object. Given an unseen object and a reference pose-object pair, we optimise for the object-aware pose that is closest in the feature space to the reference pose. Finally, we use l-NSM, i.e., our motion generation model that is trained to seamlessly transition from locomotion to object interaction with the proposed bidirectional pose blending scheme. Through comprehensive numerical comparisons to state-of-the-art methods and in a user study, we demonstrate substantial improvements in 3D virtual character motion and interaction quality and robustness to scenarios with unseen objects. Our project page is available at https://vcai.mpi-inf.mpg.de/projects/ROAM/.
<div id='section'>Paperid: <span id='pid'>310, <a href='https://arxiv.org/pdf/2305.01599.pdf' target='_blank'>https://arxiv.org/pdf/2305.01599.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinyu Yi, Yuxiao Zhou, Marc Habermann, Vladislav Golyanik, Shaohua Pan, Christian Theobalt, Feng Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.01599">EgoLocate: Real-time Motion Capture, Localization, and Mapping with Sparse Body-mounted Sensors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human and environment sensing are two important topics in Computer Vision and Graphics. Human motion is often captured by inertial sensors, while the environment is mostly reconstructed using cameras. We integrate the two techniques together in EgoLocate, a system that simultaneously performs human motion capture (mocap), localization, and mapping in real time from sparse body-mounted sensors, including 6 inertial measurement units (IMUs) and a monocular phone camera. On one hand, inertial mocap suffers from large translation drift due to the lack of the global positioning signal. EgoLocate leverages image-based simultaneous localization and mapping (SLAM) techniques to locate the human in the reconstructed scene. On the other hand, SLAM often fails when the visual feature is poor. EgoLocate involves inertial mocap to provide a strong prior for the camera motion. Experiments show that localization, a key challenge for both two fields, is largely improved by our technique, compared with the state of the art of the two fields. Our codes are available for research at https://xinyu-yi.github.io/EgoLocate/.
<div id='section'>Paperid: <span id='pid'>311, <a href='https://arxiv.org/pdf/2212.07555.pdf' target='_blank'>https://arxiv.org/pdf/2212.07555.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anindita Ghosh, Rishabh Dabral, Vladislav Golyanik, Christian Theobalt, Philipp Slusallek
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.07555">IMos: Intent-Driven Full-Body Motion Synthesis for Human-Object Interactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Can we make virtual characters in a scene interact with their surrounding objects through simple instructions? Is it possible to synthesize such motion plausibly with a diverse set of objects and instructions? Inspired by these questions, we present the first framework to synthesize the full-body motion of virtual human characters performing specified actions with 3D objects placed within their reach. Our system takes textual instructions specifying the objects and the associated intentions of the virtual characters as input and outputs diverse sequences of full-body motions. This contrasts existing works, where full-body action synthesis methods generally do not consider object interactions, and human-object interaction methods focus mainly on synthesizing hand or finger movements for grasping objects. We accomplish our objective by designing an intent-driven fullbody motion generator, which uses a pair of decoupled conditional variational auto-regressors to learn the motion of the body parts in an autoregressive manner. We also optimize the 6-DoF pose of the objects such that they plausibly fit within the hands of the synthesized characters. We compare our proposed method with the existing methods of motion synthesis and establish a new and stronger state-of-the-art for the task of intent-driven motion synthesis.
<div id='section'>Paperid: <span id='pid'>312, <a href='https://arxiv.org/pdf/2212.04495.pdf' target='_blank'>https://arxiv.org/pdf/2212.04495.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rishabh Dabral, Muhammad Hamza Mughal, Vladislav Golyanik, Christian Theobalt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.04495">MoFusion: A Framework for Denoising-Diffusion-based Motion Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Conventional methods for human motion synthesis are either deterministic or struggle with the trade-off between motion diversity and motion quality. In response to these limitations, we introduce MoFusion, i.e., a new denoising-diffusion-based framework for high-quality conditional human motion synthesis that can generate long, temporally plausible, and semantically accurate motions based on a range of conditioning contexts (such as music and text). We also present ways to introduce well-known kinematic losses for motion plausibility within the motion diffusion framework through our scheduled weighting strategy. The learned latent space can be used for several interactive motion editing applications -- like inbetweening, seed conditioning, and text-based editing -- thus, providing crucial abilities for virtual character animation and robotics. Through comprehensive quantitative evaluations and a perceptual user study, we demonstrate the effectiveness of MoFusion compared to the state of the art on established benchmarks in the literature. We urge the reader to watch our supplementary video and visit https://vcai.mpi-inf.mpg.de/projects/MoFusion.
<div id='section'>Paperid: <span id='pid'>313, <a href='https://arxiv.org/pdf/2507.23305.pdf' target='_blank'>https://arxiv.org/pdf/2507.23305.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yixuan Dang, Qinyang Xu, Yu Zhang, Xiangtong Yao, Liding Zhang, Zhenshan Bing, Florian Roehrbein, Alois Knoll
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.23305">Whisker-based Active Tactile Perception for Contour Reconstruction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Perception using whisker-inspired tactile sensors currently faces a major challenge: the lack of active control in robots based on direct contact information from the whisker. To accurately reconstruct object contours, it is crucial for the whisker sensor to continuously follow and maintain an appropriate relative touch pose on the surface. This is especially important for localization based on tip contact, which has a low tolerance for sharp surfaces and must avoid slipping into tangential contact. In this paper, we first construct a magnetically transduced whisker sensor featuring a compact and robust suspension system composed of three flexible spiral arms. We develop a method that leverages a characterized whisker deflection profile to directly extract the tip contact position using gradient descent, with a Bayesian filter applied to reduce fluctuations. We then propose an active motion control policy to maintain the optimal relative pose of the whisker sensor against the object surface. A B-Spline curve is employed to predict the local surface curvature and determine the sensor orientation. Results demonstrate that our algorithm can effectively track objects and reconstruct contours with sub-millimeter accuracy. Finally, we validate the method in simulations and real-world experiments where a robot arm drives the whisker sensor to follow the surfaces of three different objects.
<div id='section'>Paperid: <span id='pid'>314, <a href='https://arxiv.org/pdf/2502.05996.pdf' target='_blank'>https://arxiv.org/pdf/2502.05996.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gaurav Shetty, Mahya Ramezani, Hamed Habibi, Holger Voos, Jose Luis Sanchez-Lopez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.05996">Motion Control in Multi-Rotor Aerial Robots Using Deep Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper investigates the application of Deep Reinforcement (DRL) Learning to address motion control challenges in drones for additive manufacturing (AM). Drone-based additive manufacturing promises flexible and autonomous material deposition in large-scale or hazardous environments. However, achieving robust real-time control of a multi-rotor aerial robot under varying payloads and potential disturbances remains challenging. Traditional controllers like PID often require frequent parameter re-tuning, limiting their applicability in dynamic scenarios. We propose a DRL framework that learns adaptable control policies for multi-rotor drones performing waypoint navigation in AM tasks. We compare Deep Deterministic Policy Gradient (DDPG) and Twin Delayed Deep Deterministic Policy Gradient (TD3) within a curriculum learning scheme designed to handle increasing complexity. Our experiments show TD3 consistently balances training stability, accuracy, and success, particularly when mass variability is introduced. These findings provide a scalable path toward robust, autonomous drone control in additive manufacturing.
<div id='section'>Paperid: <span id='pid'>315, <a href='https://arxiv.org/pdf/2405.17013.pdf' target='_blank'>https://arxiv.org/pdf/2405.17013.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qi Wu, Yubo Zhao, Yifan Wang, Xinhang Liu, Yu-Wing Tai, Chi-Keung Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.17013">Motion-Agent: A Conversational Framework for Human Motion Generation with LLMs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While previous approaches to 3D human motion generation have achieved notable success, they often rely on extensive training and are limited to specific tasks. To address these challenges, we introduce Motion-Agent, an efficient conversational framework designed for general human motion generation, editing, and understanding. Motion-Agent employs an open-source pre-trained language model to develop a generative agent, MotionLLM, that bridges the gap between motion and text. This is accomplished by encoding and quantizing motions into discrete tokens that align with the language model's vocabulary. With only 1--3\% of the model's parameters fine-tuned using adapters, MotionLLM delivers performance on par with diffusion models and other transformer-based methods trained from scratch. By integrating MotionLLM with GPT-4 without additional training, Motion-Agent is able to generate highly complex motion sequences through multi-turn conversations, a capability that previous models have struggled to achieve. Motion-Agent supports a wide range of motion-language tasks, offering versatile capabilities for generating and customizing human motion through interactive conversational exchanges. Project page: https://knoxzhao.github.io/Motion-Agent
<div id='section'>Paperid: <span id='pid'>316, <a href='https://arxiv.org/pdf/2312.05830.pdf' target='_blank'>https://arxiv.org/pdf/2312.05830.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunheng Li, Zhongyu Li, Shanghua Gao, Qilong Wang, Qibin Hou, Ming-Ming Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.05830">A Decoupled Spatio-Temporal Framework for Skeleton-based Action Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Effectively modeling discriminative spatio-temporal information is essential for segmenting activities in long action sequences. However, we observe that existing methods are limited in weak spatio-temporal modeling capability due to two forms of decoupled modeling: (i) cascaded interaction couples spatial and temporal modeling, which over-smooths motion modeling over the long sequence, and (ii) joint-shared temporal modeling adopts shared weights to model each joint, ignoring the distinct motion patterns of different joints. We propose a Decoupled Spatio-Temporal Framework (DeST) to address the above issues. Firstly, we decouple the cascaded spatio-temporal interaction to avoid stacking multiple spatio-temporal blocks, while achieving sufficient spatio-temporal interaction. Specifically, DeST performs once unified spatial modeling and divides the spatial features into different groups of subfeatures, which then adaptively interact with temporal features from different layers. Since the different sub-features contain distinct spatial semantics, the model could learn the optimal interaction pattern at each layer. Meanwhile, inspired by the fact that different joints move at different speeds, we propose joint-decoupled temporal modeling, which employs independent trainable weights to capture distinctive temporal features of each joint. On four large-scale benchmarks of different scenes, DeST significantly outperforms current state-of-the-art methods with less computational complexity.
<div id='section'>Paperid: <span id='pid'>317, <a href='https://arxiv.org/pdf/2503.09985.pdf' target='_blank'>https://arxiv.org/pdf/2503.09985.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiang Zhang, Jiahang Cao, Jingkai Sun, Yecheng Shao, Gang Han, Wen Zhao, Yijie Guo, Renjing Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.09985">ES-Parkour: Advanced Robot Parkour with Bio-inspired Event Camera and Spiking Neural Network</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, quadruped robotics has advanced significantly, particularly in perception and motion control via reinforcement learning, enabling complex motions in challenging environments. Visual sensors like depth cameras enhance stability and robustness but face limitations, such as low operating frequencies relative to joint control and sensitivity to lighting, which hinder outdoor deployment. Additionally, deep neural networks in sensor and control systems increase computational demands. To address these issues, we introduce spiking neural networks (SNNs) and event cameras to perform a challenging quadruped parkour task. Event cameras capture dynamic visual data, while SNNs efficiently process spike sequences, mimicking biological perception. Experimental results demonstrate that this approach significantly outperforms traditional models, achieving excellent parkour performance with just 11.7% of the energy consumption of an artificial neural network (ANN)-based model, yielding an 88.3% energy reduction. By integrating event cameras with SNNs, our work advances robotic reinforcement learning and opens new possibilities for applications in demanding environments.
<div id='section'>Paperid: <span id='pid'>318, <a href='https://arxiv.org/pdf/2503.08338.pdf' target='_blank'>https://arxiv.org/pdf/2503.08338.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingkai Sun, Qiang Zhang, Gang Han, Wen Zhao, Zhe Yong, Yan He, Jiaxu Wang, Jiahang Cao, Yijie Guo, Renjing Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.08338">Trinity: A Modular Humanoid Robot AI System</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, research on humanoid robots has garnered increasing attention. With breakthroughs in various types of artificial intelligence algorithms, embodied intelligence, exemplified by humanoid robots, has been highly anticipated. The advancements in reinforcement learning (RL) algorithms have significantly improved the motion control and generalization capabilities of humanoid robots. Simultaneously, the groundbreaking progress in large language models (LLM) and visual language models (VLM) has brought more possibilities and imagination to humanoid robots. LLM enables humanoid robots to understand complex tasks from language instructions and perform long-term task planning, while VLM greatly enhances the robots' understanding and interaction with their environment. This paper introduces \textcolor{magenta}{Trinity}, a novel AI system for humanoid robots that integrates RL, LLM, and VLM. By combining these technologies, Trinity enables efficient control of humanoid robots in complex environments. This innovative approach not only enhances the capabilities but also opens new avenues for future research and applications of humanoid robotics.
<div id='section'>Paperid: <span id='pid'>319, <a href='https://arxiv.org/pdf/2507.17406.pdf' target='_blank'>https://arxiv.org/pdf/2507.17406.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ayce Idil Aytekin, Chuqiao Li, Diogo Luvizon, Rishabh Dabral, Martin Oswald, Marc Habermann, Christian Theobalt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.17406">Physics-based Human Pose Estimation from a Single Moving RGB Camera</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Most monocular and physics-based human pose tracking methods, while achieving state-of-the-art results, suffer from artifacts when the scene does not have a strictly flat ground plane or when the camera is moving. Moreover, these methods are often evaluated on in-the-wild real world videos without ground-truth data or on synthetic datasets, which fail to model the real world light transport, camera motion, and pose-induced appearance and geometry changes. To tackle these two problems, we introduce MoviCam, the first non-synthetic dataset containing ground-truth camera trajectories of a dynamically moving monocular RGB camera, scene geometry, and 3D human motion with human-scene contact labels. Additionally, we propose PhysDynPose, a physics-based method that incorporates scene geometry and physical constraints for more accurate human motion tracking in case of camera motion and non-flat scenes. More precisely, we use a state-of-the-art kinematics estimator to obtain the human pose and a robust SLAM method to capture the dynamic camera trajectory, enabling the recovery of the human pose in the world frame. We then refine the kinematic pose estimate using our scene-aware physics optimizer. From our new benchmark, we found that even state-of-the-art methods struggle with this inherently challenging setting, i.e. a moving camera and non-planar environments, while our method robustly estimates both human and camera poses in world coordinates.
<div id='section'>Paperid: <span id='pid'>320, <a href='https://arxiv.org/pdf/2403.17936.pdf' target='_blank'>https://arxiv.org/pdf/2403.17936.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Muhammad Hamza Mughal, Rishabh Dabral, Ikhsanul Habibie, Lucia Donatelli, Marc Habermann, Christian Theobalt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.17936">ConvoFusion: Multi-Modal Conversational Diffusion for Co-Speech Gesture Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Gestures play a key role in human communication. Recent methods for co-speech gesture generation, while managing to generate beat-aligned motions, struggle generating gestures that are semantically aligned with the utterance. Compared to beat gestures that align naturally to the audio signal, semantically coherent gestures require modeling the complex interactions between the language and human motion, and can be controlled by focusing on certain words. Therefore, we present ConvoFusion, a diffusion-based approach for multi-modal gesture synthesis, which can not only generate gestures based on multi-modal speech inputs, but can also facilitate controllability in gesture synthesis. Our method proposes two guidance objectives that allow the users to modulate the impact of different conditioning modalities (e.g. audio vs text) as well as to choose certain words to be emphasized during gesturing. Our method is versatile in that it can be trained either for generating monologue gestures or even the conversational gestures. To further advance the research on multi-party interactive gestures, the DnD Group Gesture dataset is released, which contains 6 hours of gesture data showing 5 people interacting with one another. We compare our method with several recent works and demonstrate effectiveness of our method on a variety of tasks. We urge the reader to watch our supplementary video at our website.
<div id='section'>Paperid: <span id='pid'>321, <a href='https://arxiv.org/pdf/2403.05834.pdf' target='_blank'>https://arxiv.org/pdf/2403.05834.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiaochu Huang, Xu He, Boshi Tang, Haolin Zhuang, Liyang Chen, Shuochen Gao, Zhiyong Wu, Haozhi Huang, Helen Meng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.05834">Enhancing Expressiveness in Dance Generation via Integrating Frequency and Music Style Information</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dance generation, as a branch of human motion generation, has attracted increasing attention. Recently, a few works attempt to enhance dance expressiveness, which includes genre matching, beat alignment, and dance dynamics, from certain aspects. However, the enhancement is quite limited as they lack comprehensive consideration of the aforementioned three factors. In this paper, we propose ExpressiveBailando, a novel dance generation method designed to generate expressive dances, concurrently taking all three factors into account. Specifically, we mitigate the issue of speed homogenization by incorporating frequency information into VQ-VAE, thus improving dance dynamics. Additionally, we integrate music style information by extracting genre- and beat-related features with a pre-trained music model, hence achieving improvements in the other two factors. Extensive experimental results demonstrate that our proposed method can generate dances with high expressiveness and outperforms existing methods both qualitatively and quantitatively.
<div id='section'>Paperid: <span id='pid'>322, <a href='https://arxiv.org/pdf/2508.04681.pdf' target='_blank'>https://arxiv.org/pdf/2508.04681.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Liang Xu, Chengqun Yang, Zili Lin, Fei Xu, Yifan Liu, Congsheng Xu, Yiyi Zhang, Jie Qin, Xingdong Sheng, Yunhui Liu, Xin Jin, Yichao Yan, Wenjun Zeng, Xiaokang Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.04681">Perceiving and Acting in First-Person: A Dataset and Benchmark for Egocentric Human-Object-Human Interactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning action models from real-world human-centric interaction datasets is important towards building general-purpose intelligent assistants with efficiency. However, most existing datasets only offer specialist interaction category and ignore that AI assistants perceive and act based on first-person acquisition. We urge that both the generalist interaction knowledge and egocentric modality are indispensable. In this paper, we embed the manual-assisted task into a vision-language-action framework, where the assistant provides services to the instructor following egocentric vision and commands. With our hybrid RGB-MoCap system, pairs of assistants and instructors engage with multiple objects and the scene following GPT-generated scripts. Under this setting, we accomplish InterVLA, the first large-scale human-object-human interaction dataset with 11.4 hours and 1.2M frames of multimodal data, spanning 2 egocentric and 5 exocentric videos, accurate human/object motions and verbal commands. Furthermore, we establish novel benchmarks on egocentric human motion estimation, interaction synthesis, and interaction prediction with comprehensive analysis. We believe that our InterVLA testbed and the benchmarks will foster future works on building AI agents in the physical world.
<div id='section'>Paperid: <span id='pid'>323, <a href='https://arxiv.org/pdf/2408.16426.pdf' target='_blank'>https://arxiv.org/pdf/2408.16426.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiefeng Li, Ye Yuan, Davis Rempe, Haotian Zhang, Pavlo Molchanov, Cewu Lu, Jan Kautz, Umar Iqbal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.16426">COIN: Control-Inpainting Diffusion Prior for Human and Camera Motion Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Estimating global human motion from moving cameras is challenging due to the entanglement of human and camera motions. To mitigate the ambiguity, existing methods leverage learned human motion priors, which however often result in oversmoothed motions with misaligned 2D projections. To tackle this problem, we propose COIN, a control-inpainting motion diffusion prior that enables fine-grained control to disentangle human and camera motions. Although pre-trained motion diffusion models encode rich motion priors, we find it non-trivial to leverage such knowledge to guide global motion estimation from RGB videos. COIN introduces a novel control-inpainting score distillation sampling method to ensure well-aligned, consistent, and high-quality motion from the diffusion prior within a joint optimization framework. Furthermore, we introduce a new human-scene relation loss to alleviate the scale ambiguity by enforcing consistency among the humans, camera, and scene. Experiments on three challenging benchmarks demonstrate the effectiveness of COIN, which outperforms the state-of-the-art methods in terms of global human motion estimation and camera motion estimation. As an illustrative example, COIN outperforms the state-of-the-art method by 33% in world joint position error (W-MPJPE) on the RICH dataset.
<div id='section'>Paperid: <span id='pid'>324, <a href='https://arxiv.org/pdf/2310.13768.pdf' target='_blank'>https://arxiv.org/pdf/2310.13768.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Muhammed Kocabas, Ye Yuan, Pavlo Molchanov, Yunrong Guo, Michael J. Black, Otmar Hilliges, Jan Kautz, Umar Iqbal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.13768">PACE: Human and Camera Motion Estimation from in-the-wild Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a method to estimate human motion in a global scene from moving cameras. This is a highly challenging task due to the coupling of human and camera motions in the video. To address this problem, we propose a joint optimization framework that disentangles human and camera motions using both foreground human motion priors and background scene features. Unlike existing methods that use SLAM as initialization, we propose to tightly integrate SLAM and human motion priors in an optimization that is inspired by bundle adjustment. Specifically, we optimize human and camera motions to match both the observed human pose and scene features. This design combines the strengths of SLAM and motion priors, which leads to significant improvements in human and camera motion estimation. We additionally introduce a motion prior that is suitable for batch optimization, making our approach significantly more efficient than existing approaches. Finally, we propose a novel synthetic dataset that enables evaluating camera motion in addition to human motion from dynamic videos. Experiments on the synthetic and real-world RICH datasets demonstrate that our approach substantially outperforms prior art in recovering both human and camera motions.
<div id='section'>Paperid: <span id='pid'>325, <a href='https://arxiv.org/pdf/2309.08854.pdf' target='_blank'>https://arxiv.org/pdf/2309.08854.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiuyu Ren, Huan Yu, Jiajun Dai, Zhi Zheng, Jun Meng, Li Xu, Chao Xu, Fei Gao, Yanjun Cao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.08854">Intention-Aware Planner for Robust and Safe Aerial Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous target tracking with quadrotors has wide applications in many scenarios, such as cinematographic follow-up shooting or suspect chasing. Target motion prediction is necessary when designing the tracking planner. However, the widely used constant velocity or constant rotation assumption can not fully capture the dynamics of the target. The tracker may fail when the target happens to move aggressively, such as sudden turn or deceleration. In this paper, we propose an intention-aware planner by additionally considering the intention of the target to enhance safety and robustness in aerial tracking applications. Firstly, a designated intention prediction method is proposed, which combines a user-defined potential assessment function and a state observation function. A reachable region is generated to specifically evaluate the turning intentions. Then we design an intention-driven hybrid A* method to predict the future possible positions for the target. Finally, an intention-aware optimization approach is designed to generate a spatial-temporal optimal trajectory, allowing the tracker to perceive unexpected situations from the target. Benchmark comparisons and real-world experiments are conducted to validate the performance of our method.
<div id='section'>Paperid: <span id='pid'>326, <a href='https://arxiv.org/pdf/2505.18780.pdf' target='_blank'>https://arxiv.org/pdf/2505.18780.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yahao Fan, Tianxiang Gui, Kaiyang Ji, Shutong Ding, Chixuan Zhang, Jiayuan Gu, Jingyi Yu, Jingya Wang, Ye Shi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.18780">One Policy but Many Worlds: A Scalable Unified Policy for Versatile Humanoid Locomotion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humanoid locomotion faces a critical scalability challenge: traditional reinforcement learning (RL) methods require task-specific rewards and struggle to leverage growing datasets, even as more training terrains are introduced. We propose DreamPolicy, a unified framework that enables a single policy to master diverse terrains and generalize zero-shot to unseen scenarios by systematically integrating offline data and diffusion-driven motion synthesis. At its core, DreamPolicy introduces Humanoid Motion Imagery (HMI) - future state predictions synthesized through an autoregressive terrain-aware diffusion planner curated by aggregating rollouts from specialized policies across various distinct terrains. Unlike human motion datasets requiring laborious retargeting, our data directly captures humanoid kinematics, enabling the diffusion planner to synthesize "dreamed" trajectories that encode terrain-specific physical constraints. These trajectories act as dynamic objectives for our HMI-conditioned policy, bypassing manual reward engineering and enabling cross-terrain generalization. DreamPolicy addresses the scalability limitations of prior methods: while traditional RL fails to exploit growing datasets, our framework scales seamlessly with more offline data. As the dataset expands, the diffusion prior learns richer locomotion skills, which the policy leverages to master new terrains without retraining. Experiments demonstrate that DreamPolicy achieves average 90% success rates in training environments and an average of 20% higher success on unseen terrains than the prevalent method. It also generalizes to perturbed and composite scenarios where prior approaches collapse. By unifying offline data, diffusion-based trajectory synthesis, and policy optimization, DreamPolicy overcomes the "one task, one policy" bottleneck, establishing a paradigm for scalable, data-driven humanoid control.
<div id='section'>Paperid: <span id='pid'>327, <a href='https://arxiv.org/pdf/2503.08270.pdf' target='_blank'>https://arxiv.org/pdf/2503.08270.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chengjun Yu, Wei Zhai, Yuhang Yang, Yang Cao, Zheng-Jun Zha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.08270">HERO: Human Reaction Generation from Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human reaction generation represents a significant research domain for interactive AI, as humans constantly interact with their surroundings. Previous works focus mainly on synthesizing the reactive motion given a human motion sequence. This paradigm limits interaction categories to human-human interactions and ignores emotions that may influence reaction generation. In this work, we propose to generate 3D human reactions from RGB videos, which involves a wider range of interaction categories and naturally provides information about expressions that may reflect the subject's emotions. To cope with this task, we present HERO, a simple yet powerful framework for Human rEaction geneRation from videOs. HERO considers both global and frame-level local representations of the video to extract the interaction intention, and then uses the extracted interaction intention to guide the synthesis of the reaction. Besides, local visual representations are continuously injected into the model to maximize the exploitation of the dynamic properties inherent in videos. Furthermore, the ViMo dataset containing paired Video-Motion data is collected to support the task. In addition to human-human interactions, these video-motion pairs also cover animal-human interactions and scene-human interactions. Extensive experiments demonstrate the superiority of our methodology. The code and dataset will be publicly available at https://jackyu6.github.io/HERO.
<div id='section'>Paperid: <span id='pid'>328, <a href='https://arxiv.org/pdf/2411.00128.pdf' target='_blank'>https://arxiv.org/pdf/2411.00128.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>David Schneider, Simon ReiÃ, Marco Kugler, Alexander Jaus, Kunyu Peng, Susanne Sutschet, M. Saquib Sarfraz, Sven Matthiesen, Rainer Stiefelhagen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.00128">Muscles in Time: Learning to Understand Human Motion by Simulating Muscle Activations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Exploring the intricate dynamics between muscular and skeletal structures is pivotal for understanding human motion. This domain presents substantial challenges, primarily attributed to the intensive resources required for acquiring ground truth muscle activation data, resulting in a scarcity of datasets. In this work, we address this issue by establishing Muscles in Time (MinT), a large-scale synthetic muscle activation dataset. For the creation of MinT, we enriched existing motion capture datasets by incorporating muscle activation simulations derived from biomechanical human body models using the OpenSim platform, a common approach in biomechanics and human motion research. Starting from simple pose sequences, our pipeline enables us to extract detailed information about the timing of muscle activations within the human musculoskeletal system. Muscles in Time contains over nine hours of simulation data covering 227 subjects and 402 simulated muscle strands. We demonstrate the utility of this dataset by presenting results on neural network-based muscle activation estimation from human pose sequences with two different sequence-to-sequence architectures. Data and code are provided under https://simplexsigil.github.io/mint.
<div id='section'>Paperid: <span id='pid'>329, <a href='https://arxiv.org/pdf/2407.10625.pdf' target='_blank'>https://arxiv.org/pdf/2407.10625.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zijian He, Peixin Chen, Guangrun Wang, Guanbin Li, Philip H. S. Torr, Liang Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.10625">WildVidFit: Video Virtual Try-On in the Wild via Image-Based Controlled Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video virtual try-on aims to generate realistic sequences that maintain garment identity and adapt to a person's pose and body shape in source videos. Traditional image-based methods, relying on warping and blending, struggle with complex human movements and occlusions, limiting their effectiveness in video try-on applications. Moreover, video-based models require extensive, high-quality data and substantial computational resources. To tackle these issues, we reconceptualize video try-on as a process of generating videos conditioned on garment descriptions and human motion. Our solution, WildVidFit, employs image-based controlled diffusion models for a streamlined, one-stage approach. This model, conditioned on specific garments and individuals, is trained on still images rather than videos. It leverages diffusion guidance from pre-trained models including a video masked autoencoder for segment smoothness improvement and a self-supervised model for feature alignment of adjacent frame in the latent space. This integration markedly boosts the model's ability to maintain temporal coherence, enabling more effective video try-on within an image-based framework. Our experiments on the VITON-HD and DressCode datasets, along with tests on the VVT and TikTok datasets, demonstrate WildVidFit's capability to generate fluid and coherent videos. The project page website is at wildvidfit-project.github.io.
<div id='section'>Paperid: <span id='pid'>330, <a href='https://arxiv.org/pdf/2405.15325.pdf' target='_blank'>https://arxiv.org/pdf/2405.15325.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zijian Li, Yifan Shen, Kaitao Zheng, Ruichu Cai, Xiangchen Song, Mingming Gong, Zhengmao Zhu, Guangyi Chen, Kun Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.15325">On the Identification of Temporally Causal Representation with Instantaneous Dependence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Temporally causal representation learning aims to identify the latent causal process from time series observations, but most methods require the assumption that the latent causal processes do not have instantaneous relations. Although some recent methods achieve identifiability in the instantaneous causality case, they require either interventions on the latent variables or grouping of the observations, which are in general difficult to obtain in real-world scenarios. To fill this gap, we propose an \textbf{ID}entification framework for instantane\textbf{O}us \textbf{L}atent dynamics (\textbf{IDOL}) by imposing a sparse influence constraint that the latent causal processes have sparse time-delayed and instantaneous relations. Specifically, we establish identifiability results of the latent causal process based on sufficient variability and the sparse influence constraint by employing contextual information of time series data. Based on these theories, we incorporate a temporally variational inference architecture to estimate the latent variables and a gradient-based sparsity regularization to identify the latent causal process. Experimental results on simulation datasets illustrate that our method can identify the latent causal process. Furthermore, evaluations on multiple human motion forecasting benchmarks with instantaneous dependencies indicate the effectiveness of our method in real-world settings.
<div id='section'>Paperid: <span id='pid'>331, <a href='https://arxiv.org/pdf/2309.01372.pdf' target='_blank'>https://arxiv.org/pdf/2309.01372.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunhong Lou, Linchao Zhu, Yaxiong Wang, Xiaohan Wang, Yi Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.01372">DiverseMotion: Towards Diverse Human Motion Generation via Discrete Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present DiverseMotion, a new approach for synthesizing high-quality human motions conditioned on textual descriptions while preserving motion diversity.Despite the recent significant process in text-based human motion generation,existing methods often prioritize fitting training motions at the expense of action diversity. Consequently, striking a balance between motion quality and diversity remains an unresolved challenge. This problem is compounded by two key factors: 1) the lack of diversity in motion-caption pairs in existing benchmarks and 2) the unilateral and biased semantic understanding of the text prompt, focusing primarily on the verb component while neglecting the nuanced distinctions indicated by other words.In response to the first issue, we construct a large-scale Wild Motion-Caption dataset (WMC) to extend the restricted action boundary of existing well-annotated datasets, enabling the learning of diverse motions through a more extensive range of actions. To this end, a motion BLIP is trained upon a pretrained vision-language model, then we automatically generate diverse motion captions for the collected motion sequences. As a result, we finally build a dataset comprising 8,888 motions coupled with 141k text.To comprehensively understand the text command, we propose a Hierarchical Semantic Aggregation (HSA) module to capture the fine-grained semantics.Finally,we involve the above two designs into an effective Motion Discrete Diffusion (MDD) framework to strike a balance between motion quality and diversity. Extensive experiments on HumanML3D and KIT-ML show that our DiverseMotion achieves the state-of-the-art motion quality and competitive motion diversity. Dataset, code, and pretrained models will be released to reproduce all of our results.
<div id='section'>Paperid: <span id='pid'>332, <a href='https://arxiv.org/pdf/2304.04298.pdf' target='_blank'>https://arxiv.org/pdf/2304.04298.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guangyi Chen, Zhenhao Chen, Shunxing Fan, Kun Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.04298">Unsupervised Sampling Promoting for Stochastic Human Trajectory Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The indeterminate nature of human motion requires trajectory prediction systems to use a probabilistic model to formulate the multi-modality phenomenon and infer a finite set of future trajectories. However, the inference processes of most existing methods rely on Monte Carlo random sampling, which is insufficient to cover the realistic paths with finite samples, due to the long tail effect of the predicted distribution. To promote the sampling process of stochastic prediction, we propose a novel method, called BOsampler, to adaptively mine potential paths with Bayesian optimization in an unsupervised manner, as a sequential design strategy in which new prediction is dependent on the previously drawn samples. Specifically, we model the trajectory sampling as a Gaussian process and construct an acquisition function to measure the potential sampling value. This acquisition function applies the original distribution as prior and encourages exploring paths in the long-tail region. This sampling method can be integrated with existing stochastic predictive models without retraining. Experimental results on various baseline methods demonstrate the effectiveness of our method.
<div id='section'>Paperid: <span id='pid'>333, <a href='https://arxiv.org/pdf/2509.00403.pdf' target='_blank'>https://arxiv.org/pdf/2509.00403.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yushuo Chen, Ruizhi Shao, Youxin Pang, Hongwen Zhang, Xinyi Wu, Rihui Wu, Yebin Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.00403">DevilSight: Augmenting Monocular Human Avatar Reconstruction through a Virtual Perspective</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a novel framework to reconstruct human avatars from monocular videos. Recent approaches have struggled either to capture the fine-grained dynamic details from the input or to generate plausible details at novel viewpoints, which mainly stem from the limited representational capacity of the avatar model and insufficient observational data. To overcome these challenges, we propose to leverage the advanced video generative model, Human4DiT, to generate the human motions from alternative perspective as an additional supervision signal. This approach not only enriches the details in previously unseen regions but also effectively regularizes the avatar representation to mitigate artifacts. Furthermore, we introduce two complementary strategies to enhance video generation: To ensure consistent reproduction of human motion, we inject the physical identity into the model through video fine-tuning. For higher-resolution outputs with finer details, a patch-based denoising algorithm is employed. Experimental results demonstrate that our method outperforms recent state-of-the-art approaches and validate the effectiveness of our proposed strategies.
<div id='section'>Paperid: <span id='pid'>334, <a href='https://arxiv.org/pdf/2503.21847.pdf' target='_blank'>https://arxiv.org/pdf/2503.21847.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yong Xie, Yunlian Sun, Hongwen Zhang, Yebin Liu, Jinhui Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.21847">ReCoM: Realistic Co-Speech Motion Generation with Recurrent Embedded Transformer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present ReCoM, an efficient framework for generating high-fidelity and generalizable human body motions synchronized with speech. The core innovation lies in the Recurrent Embedded Transformer (RET), which integrates Dynamic Embedding Regularization (DER) into a Vision Transformer (ViT) core architecture to explicitly model co-speech motion dynamics. This architecture enables joint spatial-temporal dependency modeling, thereby enhancing gesture naturalness and fidelity through coherent motion synthesis. To enhance model robustness, we incorporate the proposed DER strategy, which equips the model with dual capabilities of noise resistance and cross-domain generalization, thereby improving the naturalness and fluency of zero-shot motion generation for unseen speech inputs. To mitigate inherent limitations of autoregressive inference, including error accumulation and limited self-correction, we propose an iterative reconstruction inference (IRI) strategy. IRI refines motion sequences via cyclic pose reconstruction, driven by two key components: (1) classifier-free guidance improves distribution alignment between generated and real gestures without auxiliary supervision, and (2) a temporal smoothing process eliminates abrupt inter-frame transitions while ensuring kinematic continuity. Extensive experiments on benchmark datasets validate ReCoM's effectiveness, achieving state-of-the-art performance across metrics. Notably, it reduces the FrÃ©chet Gesture Distance (FGD) from 18.70 to 2.48, demonstrating an 86.7% improvement in motion realism. Our project page is https://yong-xie-xy.github.io/ReCoM/.
<div id='section'>Paperid: <span id='pid'>335, <a href='https://arxiv.org/pdf/2411.04428.pdf' target='_blank'>https://arxiv.org/pdf/2411.04428.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuqi Zhao, Xinghao Zhu, Yuxin Chen, Chenran Li, Xiang Zhang, Mingyu Ding, Masayoshi Tomizuka
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.04428">DexH2R: Task-oriented Dexterous Manipulation from Human to Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dexterous manipulation is a critical aspect of human capability, enabling interaction with a wide variety of objects. Recent advancements in learning from human demonstrations and teleoperation have enabled progress for robots in such ability. However, these approaches either require complex data collection such as costly human effort for eye-robot contact, or suffer from poor generalization when faced with novel scenarios. To solve both challenges, we propose a framework, DexH2R, that combines human hand motion retargeting with a task-oriented residual action policy, improving task performance by bridging the embodiment gap between human and robotic dexterous hands. Specifically, DexH2R learns the residual policy directly from retargeted primitive actions and task-oriented rewards, eliminating the need for labor-intensive teleoperation systems. Moreover, we incorporate test-time guidance for novel scenarios by taking in desired trajectories of human hands and objects, allowing the dexterous hand to acquire new skills with high generalizability. Extensive experiments in both simulation and real-world environments demonstrate the effectiveness of our work, outperforming prior state-of-the-arts by 40% across various settings.
<div id='section'>Paperid: <span id='pid'>336, <a href='https://arxiv.org/pdf/2311.17460.pdf' target='_blank'>https://arxiv.org/pdf/2311.17460.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Yao, Hongwen Zhang, Yunlian Sun, Yebin Liu, Jinhui Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.17460">W-HMR: Monocular Human Mesh Recovery in World Space with Weak-Supervised Calibration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Previous methods for 3D human motion recovery from monocular images often fall short due to reliance on camera coordinates, leading to inaccuracies in real-world applications. The limited availability and diversity of focal length labels further exacerbate misalignment issues in reconstructed 3D human bodies. To address these challenges, we introduce W-HMR, a weak-supervised calibration method that predicts "reasonable" focal lengths based on body distortion information, eliminating the need for precise focal length labels. Our approach enhances 2D supervision precision and recovery accuracy. Additionally, we present the OrientCorrect module, which corrects body orientation for plausible reconstructions in world space, avoiding the error accumulation associated with inaccurate camera rotation predictions. Our contributions include a novel weak-supervised camera calibration technique, an effective orientation correction module, and a decoupling strategy that significantly improves the generalizability and accuracy of human motion recovery in both camera and world coordinates. The robustness of W-HMR is validated through extensive experiments on various datasets, showcasing its superiority over existing methods. Codes and demos have been made available on the project page https://yw0208.github.io/w-hmr/.
<div id='section'>Paperid: <span id='pid'>337, <a href='https://arxiv.org/pdf/2406.11253.pdf' target='_blank'>https://arxiv.org/pdf/2406.11253.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuan Wang, Zhao Wang, Junhao Gong, Di Huang, Tong He, Wanli Ouyang, Jile Jiao, Xuetao Feng, Qi Dou, Shixiang Tang, Dan Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.11253">Holistic-Motion2D: Scalable Whole-body Human Motion Generation in 2D Space</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we introduce a novel path to $\textit{general}$ human motion generation by focusing on 2D space. Traditional methods have primarily generated human motions in 3D, which, while detailed and realistic, are often limited by the scope of available 3D motion data in terms of both the size and the diversity. To address these limitations, we exploit extensive availability of 2D motion data. We present $\textbf{Holistic-Motion2D}$, the first comprehensive and large-scale benchmark for 2D whole-body motion generation, which includes over 1M in-the-wild motion sequences, each paired with high-quality whole-body/partial pose annotations and textual descriptions. Notably, Holistic-Motion2D is ten times larger than the previously largest 3D motion dataset. We also introduce a baseline method, featuring innovative $\textit{whole-body part-aware attention}$ and $\textit{confidence-aware modeling}$ techniques, tailored for 2D $\underline{\text T}$ext-driv$\underline{\text{EN}}$ whole-bo$\underline{\text D}$y motion gen$\underline{\text{ER}}$ation, namely $\textbf{Tender}$. Extensive experiments demonstrate the effectiveness of $\textbf{Holistic-Motion2D}$ and $\textbf{Tender}$ in generating expressive, diverse, and realistic human motions. We also highlight the utility of 2D motion for various downstream applications and its potential for lifting to 3D motion. The page link is: https://holistic-motion2d.github.io.
<div id='section'>Paperid: <span id='pid'>338, <a href='https://arxiv.org/pdf/2406.01867.pdf' target='_blank'>https://arxiv.org/pdf/2406.01867.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kengo Uchida, Takashi Shibuya, Yuhta Takida, Naoki Murata, Julian Tanke, Shusuke Takahashi, Yuki Mitsufuji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.01867">MoLA: Motion Generation and Editing with Latent Diffusion Enhanced by Adversarial Training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In text-to-motion generation, controllability as well as generation quality and speed has become increasingly critical. The controllability challenges include generating a motion of a length that matches the given textual description and editing the generated motions according to control signals, such as the start-end positions and the pelvis trajectory. In this paper, we propose MoLA, which provides fast, high-quality, variable-length motion generation and can also deal with multiple editing tasks in a single framework. Our approach revisits the motion representation used as inputs and outputs in the model, incorporating an activation variable to enable variable-length motion generation. Additionally, we integrate a variational autoencoder and a latent diffusion model, further enhanced through adversarial training, to achieve high-quality and fast generation. Moreover, we apply a training-free guided generation framework to achieve various editing tasks with motion control inputs. We quantitatively show the effectiveness of adversarial learning in text-to-motion generation, and demonstrate the applicability of our editing framework to multiple editing tasks in the motion domain.
<div id='section'>Paperid: <span id='pid'>339, <a href='https://arxiv.org/pdf/2307.12291.pdf' target='_blank'>https://arxiv.org/pdf/2307.12291.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiao Pan, Zongxin Yang, Jianxin Ma, Chang Zhou, Yi Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.12291">TransHuman: A Transformer-based Human Representation for Generalizable Neural Human Rendering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we focus on the task of generalizable neural human rendering which trains conditional Neural Radiance Fields (NeRF) from multi-view videos of different characters. To handle the dynamic human motion, previous methods have primarily used a SparseConvNet (SPC)-based human representation to process the painted SMPL. However, such SPC-based representation i) optimizes under the volatile observation space which leads to the pose-misalignment between training and inference stages, and ii) lacks the global relationships among human parts that is critical for handling the incomplete painted SMPL. Tackling these issues, we present a brand-new framework named TransHuman, which learns the painted SMPL under the canonical space and captures the global relationships between human parts with transformers. Specifically, TransHuman is mainly composed of Transformer-based Human Encoding (TransHE), Deformable Partial Radiance Fields (DPaRF), and Fine-grained Detail Integration (FDI). TransHE first processes the painted SMPL under the canonical space via transformers for capturing the global relationships between human parts. Then, DPaRF binds each output token with a deformable radiance field for encoding the query point under the observation space. Finally, the FDI is employed to further integrate fine-grained information from reference images. Extensive experiments on ZJU-MoCap and H36M show that our TransHuman achieves a significantly new state-of-the-art performance with high efficiency. Project page: https://pansanity666.github.io/TransHuman/
<div id='section'>Paperid: <span id='pid'>340, <a href='https://arxiv.org/pdf/2505.09827.pdf' target='_blank'>https://arxiv.org/pdf/2505.09827.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Julian Tanke, Takashi Shibuya, Kengo Uchida, Koichi Saito, Yuki Mitsufuji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.09827">Dyadic Mamba: Long-term Dyadic Human Motion Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating realistic dyadic human motion from text descriptions presents significant challenges, particularly for extended interactions that exceed typical training sequence lengths. While recent transformer-based approaches have shown promising results for short-term dyadic motion synthesis, they struggle with longer sequences due to inherent limitations in positional encoding schemes. In this paper, we introduce Dyadic Mamba, a novel approach that leverages State-Space Models (SSMs) to generate high-quality dyadic human motion of arbitrary length. Our method employs a simple yet effective architecture that facilitates information flow between individual motion sequences through concatenation, eliminating the need for complex cross-attention mechanisms. We demonstrate that Dyadic Mamba achieves competitive performance on standard short-term benchmarks while significantly outperforming transformer-based approaches on longer sequences. Additionally, we propose a new benchmark for evaluating long-term motion synthesis quality, providing a standardized framework for future research. Our results demonstrate that SSM-based architectures offer a promising direction for addressing the challenging task of long-term dyadic human motion synthesis from text descriptions.
<div id='section'>Paperid: <span id='pid'>341, <a href='https://arxiv.org/pdf/2410.13830.pdf' target='_blank'>https://arxiv.org/pdf/2410.13830.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yujie Wei, Shiwei Zhang, Hangjie Yuan, Xiang Wang, Haonan Qiu, Rui Zhao, Yutong Feng, Feng Liu, Zhizhong Huang, Jiaxin Ye, Yingya Zhang, Hongming Shan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.13830">DreamVideo-2: Zero-Shot Subject-Driven Video Customization with Precise Motion Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in customized video generation have enabled users to create videos tailored to both specific subjects and motion trajectories. However, existing methods often require complicated test-time fine-tuning and struggle with balancing subject learning and motion control, limiting their real-world applications. In this paper, we present DreamVideo-2, a zero-shot video customization framework capable of generating videos with a specific subject and motion trajectory, guided by a single image and a bounding box sequence, respectively, and without the need for test-time fine-tuning. Specifically, we introduce reference attention, which leverages the model's inherent capabilities for subject learning, and devise a mask-guided motion module to achieve precise motion control by fully utilizing the robust motion signal of box masks derived from bounding boxes. While these two components achieve their intended functions, we empirically observe that motion control tends to dominate over subject learning. To address this, we propose two key designs: 1) the masked reference attention, which integrates a blended latent mask modeling scheme into reference attention to enhance subject representations at the desired positions, and 2) a reweighted diffusion loss, which differentiates the contributions of regions inside and outside the bounding boxes to ensure a balance between subject and motion control. Extensive experimental results on a newly curated dataset demonstrate that DreamVideo-2 outperforms state-of-the-art methods in both subject customization and motion control. The dataset, code, and models will be made publicly available.
<div id='section'>Paperid: <span id='pid'>342, <a href='https://arxiv.org/pdf/2406.14558.pdf' target='_blank'>https://arxiv.org/pdf/2406.14558.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiawei Gao, Ziqin Wang, Zeqi Xiao, Jingbo Wang, Tai Wang, Jinkun Cao, Xiaolin Hu, Si Liu, Jifeng Dai, Jiangmiao Pang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.14558">CooHOI: Learning Cooperative Human-Object Interaction with Manipulated Object Dynamics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Enabling humanoid robots to clean rooms has long been a pursued dream within humanoid research communities. However, many tasks require multi-humanoid collaboration, such as carrying large and heavy furniture together. Given the scarcity of motion capture data on multi-humanoid collaboration and the efficiency challenges associated with multi-agent learning, these tasks cannot be straightforwardly addressed using training paradigms designed for single-agent scenarios. In this paper, we introduce Cooperative Human-Object Interaction (CooHOI), a framework designed to tackle the challenge of multi-humanoid object transportation problem through a two-phase learning paradigm: individual skill learning and subsequent policy transfer. First, a single humanoid character learns to interact with objects through imitation learning from human motion priors. Then, the humanoid learns to collaborate with others by considering the shared dynamics of the manipulated object using centralized training and decentralized execution (CTDE) multi-agent RL algorithms. When one agent interacts with the object, resulting in specific object dynamics changes, the other agents learn to respond appropriately, thereby achieving implicit communication and coordination between teammates. Unlike previous approaches that relied on tracking-based methods for multi-humanoid HOI, CooHOI is inherently efficient, does not depend on motion capture data of multi-humanoid interactions, and can be seamlessly extended to include more participants and a wide range of object types.
<div id='section'>Paperid: <span id='pid'>343, <a href='https://arxiv.org/pdf/2506.03753.pdf' target='_blank'>https://arxiv.org/pdf/2506.03753.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Caiyi Sun, Yujing Sun, Xiao Han, Zemin Yang, Jiawei Liu, Xinge Zhu, Siu Ming Yiu, Yuexin Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.03753">HUMOF: Human Motion Forecasting in Interactive Social Scenes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Complex scenes present significant challenges for predicting human behaviour due to the abundance of interaction information, such as human-human and humanenvironment interactions. These factors complicate the analysis and understanding of human behaviour, thereby increasing the uncertainty in forecasting human motions. Existing motion prediction methods thus struggle in these complex scenarios. In this paper, we propose an effective method for human motion forecasting in interactive scenes. To achieve a comprehensive representation of interactions, we design a hierarchical interaction feature representation so that high-level features capture the overall context of the interactions, while low-level features focus on fine-grained details. Besides, we propose a coarse-to-fine interaction reasoning module that leverages both spatial and frequency perspectives to efficiently utilize hierarchical features, thereby enhancing the accuracy of motion predictions. Our method achieves state-of-the-art performance across four public datasets. Code will be released when this paper is published.
<div id='section'>Paperid: <span id='pid'>344, <a href='https://arxiv.org/pdf/2502.14795.pdf' target='_blank'>https://arxiv.org/pdf/2502.14795.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pengxiang Ding, Jianfei Ma, Xinyang Tong, Binghong Zou, Xinxin Luo, Yiguo Fan, Ting Wang, Hongchao Lu, Panzhong Mo, Jinxin Liu, Yuefan Wang, Huaicheng Zhou, Wenshuo Feng, Jiacheng Liu, Siteng Huang, Donglin Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.14795">Humanoid-VLA: Towards Universal Humanoid Control with Visual Integration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper addresses the limitations of current humanoid robot control frameworks, which primarily rely on reactive mechanisms and lack autonomous interaction capabilities due to data scarcity. We propose Humanoid-VLA, a novel framework that integrates language understanding, egocentric scene perception, and motion control, enabling universal humanoid control. Humanoid-VLA begins with language-motion pre-alignment using non-egocentric human motion datasets paired with textual descriptions, allowing the model to learn universal motion patterns and action semantics. We then incorporate egocentric visual context through a parameter efficient video-conditioned fine-tuning, enabling context-aware motion generation. Furthermore, we introduce a self-supervised data augmentation strategy that automatically generates pseudoannotations directly derived from motion data. This process converts raw motion sequences into informative question-answer pairs, facilitating the effective use of large-scale unlabeled video data. Built upon whole-body control architectures, extensive experiments show that Humanoid-VLA achieves object interaction and environment exploration tasks with enhanced contextual awareness, demonstrating a more human-like capacity for adaptive and intelligent engagement.
<div id='section'>Paperid: <span id='pid'>345, <a href='https://arxiv.org/pdf/2411.16498.pdf' target='_blank'>https://arxiv.org/pdf/2411.16498.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>David Eduardo Moreno-VillamarÃ­n, Anna Hilsmann, Peter Eisert
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.16498">Multi-Resolution Generative Modeling of Human Motion from Limited Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a generative model that learns to synthesize human motion from limited training sequences. Our framework provides conditional generation and blending across multiple temporal resolutions. The model adeptly captures human motion patterns by integrating skeletal convolution layers and a multi-scale architecture. Our model contains a set of generative and adversarial networks, along with embedding modules, each tailored for generating motions at specific frame rates while exerting control over their content and details. Notably, our approach also extends to the synthesis of co-speech gestures, demonstrating its ability to generate synchronized gestures from speech inputs, even with limited paired data. Through direct synthesis of SMPL pose parameters, our approach avoids test-time adjustments to fit human body meshes. Experimental results showcase our model's ability to achieve extensive coverage of training examples, while generating diverse motions, as indicated by local and global diversity metrics.
<div id='section'>Paperid: <span id='pid'>346, <a href='https://arxiv.org/pdf/2411.15582.pdf' target='_blank'>https://arxiv.org/pdf/2411.15582.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaobao Wei, Qingpo Wuwu, Zhongyu Zhao, Zhuangzhe Wu, Nan Huang, Ming Lu, Ningning MA, Shanghang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.15582">EMD: Explicit Motion Modeling for High-Quality Street Gaussian Splatting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Photorealistic reconstruction of street scenes is essential for developing real-world simulators in autonomous driving. While recent methods based on 3D/4D Gaussian Splatting (GS) have demonstrated promising results, they still encounter challenges in complex street scenes due to the unpredictable motion of dynamic objects. Current methods typically decompose street scenes into static and dynamic objects, learning the Gaussians in either a supervised manner (e.g., w/ 3D bounding-box) or a self-supervised manner (e.g., w/o 3D bounding-box). However, these approaches do not effectively model the motions of dynamic objects (e.g., the motion speed of pedestrians is clearly different from that of vehicles), resulting in suboptimal scene decomposition. To address this, we propose Explicit Motion Decomposition (EMD), which models the motions of dynamic objects by introducing learnable motion embeddings to the Gaussians, enhancing the decomposition in street scenes. The proposed plug-and-play EMD module compensates for the lack of motion modeling in self-supervised street Gaussian splatting methods. We also introduce tailored training strategies to extend EMD to supervised approaches. Comprehensive experiments demonstrate the effectiveness of our method, achieving state-of-the-art novel view synthesis performance in self-supervised settings. The code is available at: https://qingpowuwu.github.io/emd.
<div id='section'>Paperid: <span id='pid'>347, <a href='https://arxiv.org/pdf/2403.13307.pdf' target='_blank'>https://arxiv.org/pdf/2403.13307.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Peishan Cong, Ziyi Wang, Zhiyang Dou, Yiming Ren, Wei Yin, Kai Cheng, Yujing Sun, Xiaoxiao Long, Xinge Zhu, Yuexin Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.13307">LaserHuman: Language-guided Scene-aware Human Motion Generation in Free Environment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Language-guided scene-aware human motion generation has great significance for entertainment and robotics. In response to the limitations of existing datasets, we introduce LaserHuman, a pioneering dataset engineered to revolutionize Scene-Text-to-Motion research. LaserHuman stands out with its inclusion of genuine human motions within 3D environments, unbounded free-form natural language descriptions, a blend of indoor and outdoor scenarios, and dynamic, ever-changing scenes. Diverse modalities of capture data and rich annotations present great opportunities for the research of conditional motion generation, and can also facilitate the development of real-life applications. Moreover, to generate semantically consistent and physically plausible human motions, we propose a multi-conditional diffusion model, which is simple but effective, achieving state-of-the-art performance on existing datasets.
<div id='section'>Paperid: <span id='pid'>348, <a href='https://arxiv.org/pdf/2312.04036.pdf' target='_blank'>https://arxiv.org/pdf/2312.04036.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weilin Wan, Yiming Huang, Shutong Wu, Taku Komura, Wenping Wang, Dinesh Jayaraman, Lingjie Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.04036">DiffusionPhase: Motion Diffusion in Frequency Domain</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this study, we introduce a learning-based method for generating high-quality human motion sequences from text descriptions (e.g., ``A person walks forward"). Existing techniques struggle with motion diversity and smooth transitions in generating arbitrary-length motion sequences, due to limited text-to-motion datasets and the pose representations used that often lack expressiveness or compactness. To address these issues, we propose the first method for text-conditioned human motion generation in the frequency domain of motions. We develop a network encoder that converts the motion space into a compact yet expressive parameterized phase space with high-frequency details encoded, capturing the local periodicity of motions in time and space with high accuracy. We also introduce a conditional diffusion model for predicting periodic motion parameters based on text descriptions and a start pose, efficiently achieving smooth transitions between motion sequences associated with different text descriptions. Experiments demonstrate that our approach outperforms current methods in generating a broader variety of high-quality motions, and synthesizing long sequences with natural transitions.
<div id='section'>Paperid: <span id='pid'>349, <a href='https://arxiv.org/pdf/2311.17135.pdf' target='_blank'>https://arxiv.org/pdf/2311.17135.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weilin Wan, Zhiyang Dou, Taku Komura, Wenping Wang, Dinesh Jayaraman, Lingjie Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.17135">TLControl: Trajectory and Language Control for Human Motion Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Controllable human motion synthesis is essential for applications in AR/VR, gaming and embodied AI. Existing methods often focus solely on either language or full trajectory control, lacking precision in synthesizing motions aligned with user-specified trajectories, especially for multi-joint control. To address these issues, we present TLControl, a novel method for realistic human motion synthesis, incorporating both low-level Trajectory and high-level Language semantics controls, through the integration of neural-based and optimization-based techniques. Specifically, we begin with training a VQ-VAE for a compact and well-structured latent motion space organized by body parts. We then propose a Masked Trajectories Transformer (MTT) for predicting a motion distribution conditioned on language and trajectory. Once trained, we use MTT to sample initial motion predictions given user-specified partial trajectories and text descriptions as conditioning. Finally, we introduce a test-time optimization to refine these coarse predictions for precise trajectory control, which offers flexibility by allowing users to specify various optimization goals and ensures high runtime efficiency. Comprehensive experiments show that TLControl significantly outperforms the state-of-the-art in trajectory accuracy and time efficiency, making it practical for interactive and high-quality animation generation.
<div id='section'>Paperid: <span id='pid'>350, <a href='https://arxiv.org/pdf/2304.12589.pdf' target='_blank'>https://arxiv.org/pdf/2304.12589.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiangze Jia, Hui Zhou, Xinge Zhu, Yandong Guo, Ji Zhang, Yuexin Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.12589">ContrastMotion: Self-supervised Scene Motion Learning for Large-Scale LiDAR Point Clouds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose a novel self-supervised motion estimator for LiDAR-based autonomous driving via BEV representation. Different from usually adopted self-supervised strategies for data-level structure consistency, we predict scene motion via feature-level consistency between pillars in consecutive frames, which can eliminate the effect caused by noise points and view-changing point clouds in dynamic scenes. Specifically, we propose \textit{Soft Discriminative Loss} that provides the network with more pseudo-supervised signals to learn discriminative and robust features in a contrastive learning manner. We also propose \textit{Gated Multi-frame Fusion} block that learns valid compensation between point cloud frames automatically to enhance feature extraction. Finally, \textit{pillar association} is proposed to predict pillar correspondence probabilities based on feature distance, and whereby further predicts scene motion. Extensive experiments show the effectiveness and superiority of our \textbf{ContrastMotion} on both scene flow and motion prediction tasks. The code is available soon.
<div id='section'>Paperid: <span id='pid'>351, <a href='https://arxiv.org/pdf/2506.09995.pdf' target='_blank'>https://arxiv.org/pdf/2506.09995.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuanpeng Tu, Hao Luo, Xi Chen, Xiang Bai, Fan Wang, Hengshuang Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.09995">PlayerOne: Egocentric World Simulator</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce PlayerOne, the first egocentric realistic world simulator, facilitating immersive and unrestricted exploration within vividly dynamic environments. Given an egocentric scene image from the user, PlayerOne can accurately construct the corresponding world and generate egocentric videos that are strictly aligned with the real scene human motion of the user captured by an exocentric camera. PlayerOne is trained in a coarse-to-fine pipeline that first performs pretraining on large-scale egocentric text-video pairs for coarse-level egocentric understanding, followed by finetuning on synchronous motion-video data extracted from egocentric-exocentric video datasets with our automatic construction pipeline. Besides, considering the varying importance of different components, we design a part-disentangled motion injection scheme, enabling precise control of part-level movements. In addition, we devise a joint reconstruction framework that progressively models both the 4D scene and video frames, ensuring scene consistency in the long-form video generation. Experimental results demonstrate its great generalization ability in precise control of varying human movements and worldconsistent modeling of diverse scenarios. It marks the first endeavor into egocentric real-world simulation and can pave the way for the community to delve into fresh frontiers of world modeling and its diverse applications.
<div id='section'>Paperid: <span id='pid'>352, <a href='https://arxiv.org/pdf/2505.20287.pdf' target='_blank'>https://arxiv.org/pdf/2505.20287.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhongwei Zhang, Fuchen Long, Zhaofan Qiu, Yingwei Pan, Wu Liu, Ting Yao, Tao Mei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.20287">MotionPro: A Precise Motion Controller for Image-to-Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Animating images with interactive motion control has garnered popularity for image-to-video (I2V) generation. Modern approaches typically rely on large Gaussian kernels to extend motion trajectories as condition without explicitly defining movement region, leading to coarse motion control and failing to disentangle object and camera moving. To alleviate these, we present MotionPro, a precise motion controller that novelly leverages region-wise trajectory and motion mask to regulate fine-grained motion synthesis and identify target motion category (i.e., object or camera moving), respectively. Technically, MotionPro first estimates the flow maps on each training video via a tracking model, and then samples the region-wise trajectories to simulate inference scenario. Instead of extending flow through large Gaussian kernels, our region-wise trajectory approach enables more precise control by directly utilizing trajectories within local regions, thereby effectively characterizing fine-grained movements. A motion mask is simultaneously derived from the predicted flow maps to capture the holistic motion dynamics of the movement regions. To pursue natural motion control, MotionPro further strengthens video denoising by incorporating both region-wise trajectories and motion mask through feature modulation. More remarkably, we meticulously construct a benchmark, i.e., MC-Bench, with 1.1K user-annotated image-trajectory pairs, for the evaluation of both fine-grained and object-level I2V motion control. Extensive experiments conducted on WebVid-10M and MC-Bench demonstrate the effectiveness of MotionPro. Please refer to our project page for more results: https://zhw-zhang.github.io/MotionPro-page/.
<div id='section'>Paperid: <span id='pid'>353, <a href='https://arxiv.org/pdf/2501.01427.pdf' target='_blank'>https://arxiv.org/pdf/2501.01427.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuanpeng Tu, Hao Luo, Xi Chen, Sihui Ji, Xiang Bai, Hengshuang Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.01427">VideoAnydoor: High-fidelity Video Object Insertion with Precise Motion Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite significant advancements in video generation, inserting a given object into videos remains a challenging task. The difficulty lies in preserving the appearance details of the reference object and accurately modeling coherent motions at the same time. In this paper, we propose VideoAnydoor, a zero-shot video object insertion framework with high-fidelity detail preservation and precise motion control. Starting from a text-to-video model, we utilize an ID extractor to inject the global identity and leverage a box sequence to control the overall motion. To preserve the detailed appearance and meanwhile support fine-grained motion control, we design a pixel warper. It takes the reference image with arbitrary key-points and the corresponding key-point trajectories as inputs. It warps the pixel details according to the trajectories and fuses the warped features with the diffusion U-Net, thus improving detail preservation and supporting users in manipulating the motion trajectories. In addition, we propose a training strategy involving both videos and static images with a weighted loss to enhance insertion quality. VideoAnydoor demonstrates significant superiority over existing methods and naturally supports various downstream applications (e.g., talking head generation, video virtual try-on, multi-region editing) without task-specific fine-tuning.
<div id='section'>Paperid: <span id='pid'>354, <a href='https://arxiv.org/pdf/2312.02928.pdf' target='_blank'>https://arxiv.org/pdf/2312.02928.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xi Chen, Zhiheng Liu, Mengting Chen, Yutong Feng, Yu Liu, Yujun Shen, Hengshuang Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.02928">LivePhoto: Real Image Animation with Text-guided Motion Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite the recent progress in text-to-video generation, existing studies usually overlook the issue that only spatial contents but not temporal motions in synthesized videos are under the control of text. Towards such a challenge, this work presents a practical system, named LivePhoto, which allows users to animate an image of their interest with text descriptions. We first establish a strong baseline that helps a well-learned text-to-image generator (i.e., Stable Diffusion) take an image as a further input. We then equip the improved generator with a motion module for temporal modeling and propose a carefully designed training pipeline to better link texts and motions. In particular, considering the facts that (1) text can only describe motions roughly (e.g., regardless of the moving speed) and (2) text may include both content and motion descriptions, we introduce a motion intensity estimation module as well as a text re-weighting module to reduce the ambiguity of text-to-motion mapping. Empirical evidence suggests that our approach is capable of well decoding motion-related textual instructions into videos, such as actions, camera movements, or even conjuring new contents from thin air (e.g., pouring water into an empty glass). Interestingly, thanks to the proposed intensity learning mechanism, our system offers users an additional control signal (i.e., the motion intensity) besides text for video customization.
<div id='section'>Paperid: <span id='pid'>355, <a href='https://arxiv.org/pdf/2301.06281.pdf' target='_blank'>https://arxiv.org/pdf/2301.06281.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Youxin Pang, Yong Zhang, Weize Quan, Yanbo Fan, Xiaodong Cun, Ying Shan, Dong-ming Yan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.06281">DPE: Disentanglement of Pose and Expression for General Video Portrait Editing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>One-shot video-driven talking face generation aims at producing a synthetic talking video by transferring the facial motion from a video to an arbitrary portrait image. Head pose and facial expression are always entangled in facial motion and transferred simultaneously. However, the entanglement sets up a barrier for these methods to be used in video portrait editing directly, where it may require to modify the expression only while maintaining the pose unchanged. One challenge of decoupling pose and expression is the lack of paired data, such as the same pose but different expressions. Only a few methods attempt to tackle this challenge with the feat of 3D Morphable Models (3DMMs) for explicit disentanglement. But 3DMMs are not accurate enough to capture facial details due to the limited number of Blenshapes, which has side effects on motion transfer. In this paper, we introduce a novel self-supervised disentanglement framework to decouple pose and expression without 3DMMs and paired data, which consists of a motion editing module, a pose generator, and an expression generator. The editing module projects faces into a latent space where pose motion and expression motion can be disentangled, and the pose or expression transfer can be performed in the latent space conveniently via addition. The two generators render the modified latent codes to images, respectively. Moreover, to guarantee the disentanglement, we propose a bidirectional cyclic training strategy with well-designed constraints. Evaluations demonstrate our method can control pose or expression independently and be used for general video editing.
<div id='section'>Paperid: <span id='pid'>356, <a href='https://arxiv.org/pdf/2508.08991.pdf' target='_blank'>https://arxiv.org/pdf/2508.08991.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zan Wang, Jingze Zhang, Yixin Chen, Baoxiong Jia, Wei Liang, Siyuan Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08991">Spatial-Temporal Multi-Scale Quantization for Flexible Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite significant advancements in human motion generation, current motion representations, typically formulated as discrete frame sequences, still face two critical limitations: (i) they fail to capture motion from a multi-scale perspective, limiting the capability in complex patterns modeling; (ii) they lack compositional flexibility, which is crucial for model's generalization in diverse generation tasks. To address these challenges, we introduce MSQ, a novel quantization method that compresses the motion sequence into multi-scale discrete tokens across spatial and temporal dimensions. MSQ employs distinct encoders to capture body parts at varying spatial granularities and temporally interpolates the encoded features into multiple scales before quantizing them into discrete tokens. Building on this representation, we establish a generative mask modeling model to effectively support motion editing, motion control, and conditional motion generation. Through quantitative and qualitative analysis, we show that our quantization method enables the seamless composition of motion tokens without requiring specialized design or re-training. Furthermore, extensive evaluations demonstrate that our approach outperforms existing baseline methods on various benchmarks.
<div id='section'>Paperid: <span id='pid'>357, <a href='https://arxiv.org/pdf/2504.05649.pdf' target='_blank'>https://arxiv.org/pdf/2504.05649.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yining Shi, Kun Jiang, Xin Zhao, Kangan Qian, Chuchu Xie, Tuopu Wen, Mengmeng Yang, Diange Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.05649">POD: Predictive Object Detection with Single-Frame FMCW LiDAR Point Cloud</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>LiDAR-based 3D object detection is a fundamental task in the field of autonomous driving. This paper explores the unique advantage of Frequency Modulated Continuous Wave (FMCW) LiDAR in autonomous perception. Given a single frame FMCW point cloud with radial velocity measurements, we expect that our object detector can detect the short-term future locations of objects using only the current frame sensor data and demonstrate a fast ability to respond to intermediate danger. To achieve this, we extend the standard object detection task to a novel task named predictive object detection (POD), which aims to predict the short-term future location and dimensions of objects based solely on current observations. Typically, a motion prediction task requires historical sensor information to process the temporal contexts of each object, while our detector's avoidance of multi-frame historical information enables a much faster response time to potential dangers. The core advantage of FMCW LiDAR lies in the radial velocity associated with every reflected point. We propose a novel POD framework, the core idea of which is to generate a virtual future point using a ray casting mechanism, create virtual two-frame point clouds with the current and virtual future frames, and encode these two-frame voxel features with a sparse 4D encoder. Subsequently, the 4D voxel features are separated by temporal indices and remapped into two Bird's Eye View (BEV) features: one decoded for standard current frame object detection and the other for future predictive object detection. Extensive experiments on our in-house dataset demonstrate the state-of-the-art standard and predictive detection performance of the proposed POD framework.
<div id='section'>Paperid: <span id='pid'>358, <a href='https://arxiv.org/pdf/2504.00432.pdf' target='_blank'>https://arxiv.org/pdf/2504.00432.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chong Li, Jingyang Huo, Weikang Gong, Yanwei Fu, Xiangyang Xue, Jianfeng Feng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.00432">DecoFuse: Decomposing and Fusing the "What", "Where", and "How" for Brain-Inspired fMRI-to-Video Decoding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Decoding visual experiences from brain activity is a significant challenge. Existing fMRI-to-video methods often focus on semantic content while overlooking spatial and motion information. However, these aspects are all essential and are processed through distinct pathways in the brain. Motivated by this, we propose DecoFuse, a novel brain-inspired framework for decoding videos from fMRI signals. It first decomposes the video into three components - semantic, spatial, and motion - then decodes each component separately before fusing them to reconstruct the video. This approach not only simplifies the complex task of video decoding by decomposing it into manageable sub-tasks, but also establishes a clearer connection between learned representations and their biological counterpart, as supported by ablation studies. Further, our experiments show significant improvements over previous state-of-the-art methods, achieving 82.4% accuracy for semantic classification, 70.6% accuracy in spatial consistency, a 0.212 cosine similarity for motion prediction, and 21.9% 50-way accuracy for video generation. Additionally, neural encoding analyses for semantic and spatial information align with the two-streams hypothesis, further validating the distinct roles of the ventral and dorsal pathways. Overall, DecoFuse provides a strong and biologically plausible framework for fMRI-to-video decoding. Project page: https://chongjg.github.io/DecoFuse/.
<div id='section'>Paperid: <span id='pid'>359, <a href='https://arxiv.org/pdf/2503.07367.pdf' target='_blank'>https://arxiv.org/pdf/2503.07367.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kangan Qian, Jinyu Miao, Ziang Luo, Zheng Fu, and Jinchen Li, Yining Shi, Yunlong Wang, Kun Jiang, Mengmeng Yang, Diange Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.07367">LEGO-Motion: Learning-Enhanced Grids with Occupancy Instance Modeling for Class-Agnostic Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate and reliable spatial and motion information plays a pivotal role in autonomous driving systems. However, object-level perception models struggle with handling open scenario categories and lack precise intrinsic geometry. On the other hand, occupancy-based class-agnostic methods excel in representing scenes but fail to ensure physics consistency and ignore the importance of interactions between traffic participants, hindering the model's ability to learn accurate and reliable motion. In this paper, we introduce a novel occupancy-instance modeling framework for class-agnostic motion prediction tasks, named LEGO-Motion, which incorporates instance features into Bird's Eye View (BEV) space. Our model comprises (1) a BEV encoder, (2) an Interaction-Augmented Instance Encoder, and (3) an Instance-Enhanced BEV Encoder, improving both interaction relationships and physics consistency within the model, thereby ensuring a more accurate and robust understanding of the environment. Extensive experiments on the nuScenes dataset demonstrate that our method achieves state-of-the-art performance, outperforming existing approaches. Furthermore, the effectiveness of our framework is validated on the advanced FMCW LiDAR benchmark, showcasing its practical applicability and generalization capabilities. The code will be made publicly available to facilitate further research.
<div id='section'>Paperid: <span id='pid'>360, <a href='https://arxiv.org/pdf/2410.10780.pdf' target='_blank'>https://arxiv.org/pdf/2410.10780.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ekkasit Pinyoanuntapong, Muhammad Usama Saleem, Korrawe Karunratanakul, Pu Wang, Hongfei Xue, Chen Chen, Chuan Guo, Junli Cao, Jian Ren, Sergey Tulyakov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.10780">MaskControl: Spatio-Temporal Control for Masked Motion Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in motion diffusion models have enabled spatially controllable text-to-motion generation. However, these models struggle to achieve high-precision control while maintaining high-quality motion generation. To address these challenges, we propose MaskControl, the first approach to introduce controllability to the generative masked motion model. Our approach introduces two key innovations. First, \textit{Logits Regularizer} implicitly perturbs logits at training time to align the distribution of motion tokens with the controlled joint positions, while regularizing the categorical token prediction to ensure high-fidelity generation. Second, \textit{Logit Optimization} explicitly optimizes the predicted logits during inference time, directly reshaping the token distribution that forces the generated motion to accurately align with the controlled joint positions. Moreover, we introduce \textit{Differentiable Expectation Sampling (DES)} to combat the non-differential distribution sampling process encountered by logits regularizer and optimization. Extensive experiments demonstrate that MaskControl outperforms state-of-the-art methods, achieving superior motion quality (FID decreases by ~77\%) and higher control precision (average error 0.91 vs. 1.08). Additionally, MaskControl enables diverse applications, including any-joint-any-frame control, body-part timeline control, and zero-shot objective control. Video visualization can be found at https://www.ekkasit.com/ControlMM-page/
<div id='section'>Paperid: <span id='pid'>361, <a href='https://arxiv.org/pdf/2403.18036.pdf' target='_blank'>https://arxiv.org/pdf/2403.18036.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zan Wang, Yixin Chen, Baoxiong Jia, Puhao Li, Jinlu Zhang, Jingze Zhang, Tengyu Liu, Yixin Zhu, Wei Liang, Siyuan Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.18036">Move as You Say, Interact as You Can: Language-guided Human Motion Generation with Scene Affordance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite significant advancements in text-to-motion synthesis, generating language-guided human motion within 3D environments poses substantial challenges. These challenges stem primarily from (i) the absence of powerful generative models capable of jointly modeling natural language, 3D scenes, and human motion, and (ii) the generative models' intensive data requirements contrasted with the scarcity of comprehensive, high-quality, language-scene-motion datasets. To tackle these issues, we introduce a novel two-stage framework that employs scene affordance as an intermediate representation, effectively linking 3D scene grounding and conditional motion generation. Our framework comprises an Affordance Diffusion Model (ADM) for predicting explicit affordance map and an Affordance-to-Motion Diffusion Model (AMDM) for generating plausible human motions. By leveraging scene affordance maps, our method overcomes the difficulty in generating human motion under multimodal condition signals, especially when training with limited data lacking extensive language-scene-motion pairs. Our extensive experiments demonstrate that our approach consistently outperforms all baselines on established benchmarks, including HumanML3D and HUMANISE. Additionally, we validate our model's exceptional generalization capabilities on a specially curated evaluation set featuring previously unseen descriptions and scenes.
<div id='section'>Paperid: <span id='pid'>362, <a href='https://arxiv.org/pdf/2401.02916.pdf' target='_blank'>https://arxiv.org/pdf/2401.02916.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxin Yang, Pengfei Zhu, Mengshi Qi, Huadong Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.02916">Uncovering the human motion pattern: Pattern Memory-based Diffusion Model for Trajectory Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human trajectory forecasting is a critical challenge in fields such as robotics and autonomous driving. Due to the inherent uncertainty of human actions and intentions in real-world scenarios, various unexpected occurrences may arise. To uncover latent motion patterns in human behavior, we introduce a novel memory-based method, named Motion Pattern Priors Memory Network. Our method involves constructing a memory bank derived from clustered prior knowledge of motion patterns observed in the training set trajectories. We introduce an addressing mechanism to retrieve the matched pattern and the potential target distributions for each prediction from the memory bank, which enables the identification and retrieval of natural motion patterns exhibited by agents, subsequently using the target priors memory token to guide the diffusion model to generate predictions. Extensive experiments validate the effectiveness of our approach, achieving state-of-the-art trajectory prediction accuracy. The code will be made publicly available.
<div id='section'>Paperid: <span id='pid'>363, <a href='https://arxiv.org/pdf/2303.05970.pdf' target='_blank'>https://arxiv.org/pdf/2303.05970.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chunrui Han, Jinrong Yang, Jianjian Sun, Zheng Ge, Runpei Dong, Hongyu Zhou, Weixin Mao, Yuang Peng, Xiangyu Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.05970">Exploring Recurrent Long-term Temporal Fusion for Multi-view 3D Perception</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Long-term temporal fusion is a crucial but often overlooked technique in camera-based Bird's-Eye-View (BEV) 3D perception. Existing methods are mostly in a parallel manner. While parallel fusion can benefit from long-term information, it suffers from increasing computational and memory overheads as the fusion window size grows. Alternatively, BEVFormer adopts a recurrent fusion pipeline so that history information can be efficiently integrated, yet it fails to benefit from longer temporal frames. In this paper, we explore an embarrassingly simple long-term recurrent fusion strategy built upon the LSS-based methods and find it already able to enjoy the merits from both sides, i.e., rich long-term information and efficient fusion pipeline. A temporal embedding module is further proposed to improve the model's robustness against occasionally missed frames in practical scenarios. We name this simple but effective fusing pipeline VideoBEV. Experimental results on the nuScenes benchmark show that VideoBEV obtains strong performance on various camera-based 3D perception tasks, including object detection (55.4\% mAP and 62.9\% NDS), segmentation (48.6\% vehicle mIoU), tracking (54.8\% AMOTA), and motion prediction (0.80m minADE and 0.463 EPA).
<div id='section'>Paperid: <span id='pid'>364, <a href='https://arxiv.org/pdf/2301.06052.pdf' target='_blank'>https://arxiv.org/pdf/2301.06052.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianrong Zhang, Yangsong Zhang, Xiaodong Cun, Shaoli Huang, Yong Zhang, Hongwei Zhao, Hongtao Lu, Xi Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.06052">T2M-GPT: Generating Human Motion from Textual Descriptions with Discrete Representations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we investigate a simple and must-known conditional generative framework based on Vector Quantised-Variational AutoEncoder (VQ-VAE) and Generative Pre-trained Transformer (GPT) for human motion generation from textural descriptions. We show that a simple CNN-based VQ-VAE with commonly used training recipes (EMA and Code Reset) allows us to obtain high-quality discrete representations. For GPT, we incorporate a simple corruption strategy during the training to alleviate training-testing discrepancy. Despite its simplicity, our T2M-GPT shows better performance than competitive approaches, including recent diffusion-based approaches. For example, on HumanML3D, which is currently the largest dataset, we achieve comparable performance on the consistency between text and generated motion (R-Precision), but with FID 0.116 largely outperforming MotionDiffuse of 0.630. Additionally, we conduct analyses on HumanML3D and observe that the dataset size is a limitation of our approach. Our work suggests that VQ-VAE still remains a competitive approach for human motion generation.
<div id='section'>Paperid: <span id='pid'>365, <a href='https://arxiv.org/pdf/2507.08307.pdf' target='_blank'>https://arxiv.org/pdf/2507.08307.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kui Jiang, Shiyu Liu, Junjun Jiang, Hongxun Yao, Xiaopeng Fan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.08307">M2DAO-Talker: Harmonizing Multi-granular Motion Decoupling and Alternating Optimization for Talking-head Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Audio-driven talking head generation holds significant potential for film production. While existing 3D methods have advanced motion modeling and content synthesis, they often produce rendering artifacts, such as motion blur, temporal jitter, and local penetration, due to limitations in representing stable, fine-grained motion fields. Through systematic analysis, we reformulate talking head generation into a unified framework comprising three steps: video preprocessing, motion representation, and rendering reconstruction. This framework underpins our proposed M2DAO-Talker, which addresses current limitations via multi-granular motion decoupling and alternating optimization. Specifically, we devise a novel 2D portrait preprocessing pipeline to extract frame-wise deformation control conditions (motion region segmentation masks, and camera parameters) to facilitate motion representation. To ameliorate motion modeling, we elaborate a multi-granular motion decoupling strategy, which independently models non-rigid (oral and facial) and rigid (head) motions for improved reconstruction accuracy. Meanwhile, a motion consistency constraint is developed to ensure head-torso kinematic consistency, thereby mitigating penetration artifacts caused by motion aliasing. In addition, an alternating optimization strategy is designed to iteratively refine facial and oral motion parameters, enabling more realistic video generation. Experiments across multiple datasets show that M2DAO-Talker achieves state-of-the-art performance, with the 2.43 dB PSNR improvement in generation quality and 0.64 gain in user-evaluated video realness versus TalkingGaussian while with 150 FPS inference speed. Our project homepage is https://m2dao-talker.github.io/M2DAO-Talk.github.io.
<div id='section'>Paperid: <span id='pid'>366, <a href='https://arxiv.org/pdf/2505.22977.pdf' target='_blank'>https://arxiv.org/pdf/2505.22977.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuolin Xu, Siming Zheng, Ziyi Wang, HC Yu, Jinwei Chen, Huaqi Zhang, Bo Li, Peng-Tao Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.22977">HyperMotion: DiT-Based Pose-Guided Human Image Animation of Complex Motions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in diffusion models have significantly improved conditional video generation, particularly in the pose-guided human image animation task. Although existing methods are capable of generating high-fidelity and time-consistent animation sequences in regular motions and static scenes, there are still obvious limitations when facing complex human body motions (Hypermotion) that contain highly dynamic, non-standard motions, and the lack of a high-quality benchmark for evaluation of complex human motion animations. To address this challenge, we introduce the \textbf{Open-HyperMotionX Dataset} and \textbf{HyperMotionX Bench}, which provide high-quality human pose annotations and curated video clips for evaluating and improving pose-guided human image animation models under complex human motion conditions. Furthermore, we propose a simple yet powerful DiT-based video generation baseline and design spatial low-frequency enhanced RoPE, a novel module that selectively enhances low-frequency spatial feature modeling by introducing learnable frequency scaling. Our method significantly improves structural stability and appearance consistency in highly dynamic human motion sequences. Extensive experiments demonstrate the effectiveness of our dataset and proposed approach in advancing the generation quality of complex human motion image animations. Code and dataset will be made publicly available.
<div id='section'>Paperid: <span id='pid'>367, <a href='https://arxiv.org/pdf/2505.21325.pdf' target='_blank'>https://arxiv.org/pdf/2505.21325.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guangyuan Li, Siming Zheng, Hao Zhang, Jinwei Chen, Junsheng Luan, Binkai Ou, Lei Zhao, Bo Li, Peng-Tao Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.21325">MagicTryOn: Harnessing Diffusion Transformer for Garment-Preserving Video Virtual Try-on</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video Virtual Try-On (VVT) aims to simulate the natural appearance of garments across consecutive video frames, capturing their dynamic variations and interactions with human body motion. However, current VVT methods still face challenges in terms of spatiotemporal consistency and garment content preservation. First, they use diffusion models based on the U-Net, which are limited in their expressive capability and struggle to reconstruct complex details. Second, they adopt a separative modeling approach for spatial and temporal attention, which hinders the effective capture of structural relationships and dynamic consistency across frames. Third, their expression of garment details remains insufficient, affecting the realism and stability of the overall synthesized results, especially during human motion. To address the above challenges, we propose MagicTryOn, a video virtual try-on framework built upon the large-scale video diffusion Transformer. We replace the U-Net architecture with a diffusion Transformer and combine full self-attention to jointly model the spatiotemporal consistency of videos. We design a coarse-to-fine garment preservation strategy. The coarse strategy integrates garment tokens during the embedding stage, while the fine strategy incorporates multiple garment-based conditions, such as semantics, textures, and contour lines during the denoising stage. Moreover, we introduce a mask-aware loss to further optimize garment region fidelity. Extensive experiments on both image and video try-on datasets demonstrate that our method outperforms existing SOTA methods in comprehensive evaluations and generalizes to in-the-wild scenarios.
<div id='section'>Paperid: <span id='pid'>368, <a href='https://arxiv.org/pdf/2505.21325.pdf' target='_blank'>https://arxiv.org/pdf/2505.21325.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guangyuan Li, Siming Zheng, Hao Zhang, Jinwei Chen, Junsheng Luan, Binkai Ou, Lei Zhao, Bo Li, Peng-Tao Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.21325">MagicTryOn: Harnessing Diffusion Transformer for Garment-Preserving Video Virtual Try-on</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video Virtual Try-On (VVT) aims to synthesize garments that appear natural across consecutive video frames, capturing both their dynamics and interactions with human motion. Despite recent progress, existing VVT methods still suffer from inadequate garment fidelity and limited spatiotemporal consistency. The reasons are: (1) under-exploitation of garment information, with limited garment cues being injected, resulting in weaker fine-detail fidelity; and (2) a lack of spatiotemporal modeling, which hampers cross-frame identity consistency and causes temporal jitter and appearance drift. In this paper, we present MagicTryOn, a diffusion-transformer based framework for garment-preserving video virtual try-on. To preserve fine-grained garment details, we propose a fine-grained garment-preservation strategy that disentangles garment cues and injects these decomposed priors into the denoising process. To improve temporal garment consistency and suppress jitter, we introduce a garment-aware spatiotemporal rotary positional embedding (RoPE) that extends RoPE within full self-attention, using spatiotemporal relative positions to modulate garment tokens. We further impose a mask-aware loss during training to enhance fidelity within garment regions. Moreover, we adopt distribution-matching distillation to compress the sampling trajectory to four steps, enabling real-time inference without degrading garment fidelity. Extensive quantitative and qualitative experiments demonstrate that MagicTryOn outperforms existing methods, delivering superior garment-detail fidelity and temporal stability in unconstrained settings.
<div id='section'>Paperid: <span id='pid'>369, <a href='https://arxiv.org/pdf/2412.07721.pdf' target='_blank'>https://arxiv.org/pdf/2412.07721.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhouxia Wang, Yushi Lan, Shangchen Zhou, Chen Change Loy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.07721">ObjCtrl-2.5D: Training-free Object Control with Camera Poses</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study aims to achieve more precise and versatile object control in image-to-video (I2V) generation. Current methods typically represent the spatial movement of target objects with 2D trajectories, which often fail to capture user intention and frequently produce unnatural results. To enhance control, we present ObjCtrl-2.5D, a training-free object control approach that uses a 3D trajectory, extended from a 2D trajectory with depth information, as a control signal. By modeling object movement as camera movement, ObjCtrl-2.5D represents the 3D trajectory as a sequence of camera poses, enabling object motion control using an existing camera motion control I2V generation model (CMC-I2V) without training. To adapt the CMC-I2V model originally designed for global motion control to handle local object motion, we introduce a module to isolate the target object from the background, enabling independent local control. In addition, we devise an effective way to achieve more accurate object control by sharing low-frequency warped latent within the object's region across frames. Extensive experiments demonstrate that ObjCtrl-2.5D significantly improves object control accuracy compared to training-free methods and offers more diverse control capabilities than training-based approaches using 2D trajectories, enabling complex effects like object rotation. Code and results are available at https://wzhouxiff.github.io/projects/ObjCtrl-2.5D/.
<div id='section'>Paperid: <span id='pid'>370, <a href='https://arxiv.org/pdf/2412.02261.pdf' target='_blank'>https://arxiv.org/pdf/2412.02261.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingyu Gong, Chong Zhang, Fengqi Liu, Ke Fan, Qianyu Zhou, Xin Tan, Zhizhong Zhang, Yuan Xie, Lizhuang Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.02261">Diffusion Implicit Policy for Unpaired Scene-aware Motion Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion generation is a long-standing problem, and scene-aware motion synthesis has been widely researched recently due to its numerous applications. Prevailing methods rely heavily on paired motion-scene data whose quantity is limited. Meanwhile, it is difficult to generalize to diverse scenes when trained only on a few specific ones. Thus, we propose a unified framework, termed Diffusion Implicit Policy (DIP), for scene-aware motion synthesis, where paired motion-scene data are no longer necessary. In this framework, we disentangle human-scene interaction from motion synthesis during training and then introduce an interaction-based implicit policy into motion diffusion during inference. Synthesized motion can be derived through iterative diffusion denoising and implicit policy optimization, thus motion naturalness and interaction plausibility can be maintained simultaneously. The proposed implicit policy optimizes the intermediate noised motion in a GAN Inversion manner to maintain motion continuity and control keyframe poses though the ControlNet branch and motion inpainting. For long-term motion synthesis, we introduce motion blending for stable transitions between multiple sub-tasks, where motions are fused in rotation power space and translation linear space. The proposed method is evaluated on synthesized scenes with ShapeNet furniture, and real scenes from PROX and Replica. Results show that our framework presents better motion naturalness and interaction plausibility than cutting-edge methods. This also indicates the feasibility of utilizing the DIP for motion synthesis in more general tasks and versatile scenes. https://jingyugong.github.io/DiffusionImplicitPolicy/
<div id='section'>Paperid: <span id='pid'>371, <a href='https://arxiv.org/pdf/2406.17758.pdf' target='_blank'>https://arxiv.org/pdf/2406.17758.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianzong Wu, Xiangtai Li, Yanhong Zeng, Jiangning Zhang, Qianyu Zhou, Yining Li, Yunhai Tong, Kai Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.17758">MotionBooth: Motion-Aware Customized Text-to-Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we present MotionBooth, an innovative framework designed for animating customized subjects with precise control over both object and camera movements. By leveraging a few images of a specific object, we efficiently fine-tune a text-to-video model to capture the object's shape and attributes accurately. Our approach presents subject region loss and video preservation loss to enhance the subject's learning performance, along with a subject token cross-attention loss to integrate the customized subject with motion control signals. Additionally, we propose training-free techniques for managing subject and camera motions during inference. In particular, we utilize cross-attention map manipulation to govern subject motion and introduce a novel latent shift module for camera movement control as well. MotionBooth excels in preserving the appearance of subjects while simultaneously controlling the motions in generated videos. Extensive quantitative and qualitative evaluations demonstrate the superiority and effectiveness of our method. Our project page is at https://jianzongwu.github.io/projects/motionbooth
<div id='section'>Paperid: <span id='pid'>372, <a href='https://arxiv.org/pdf/2404.17031.pdf' target='_blank'>https://arxiv.org/pdf/2404.17031.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Wang, Jiayou Qin, Xiwen Chen, Ashish Bastola, John Suchanek, Zihao Gong, Abolfazl Razi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.17031">Motor Focus: Fast Ego-Motion Prediction for Assistive Visual Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Assistive visual navigation systems for visually impaired individuals have become increasingly popular thanks to the rise of mobile computing. Most of these devices work by translating visual information into voice commands. In complex scenarios where multiple objects are present, it is imperative to prioritize object detection and provide immediate notifications for key entities in specific directions. This brings the need for identifying the observer's motion direction (ego-motion) by merely processing visual information, which is the key contribution of this paper. Specifically, we introduce Motor Focus, a lightweight image-based framework that predicts the ego-motion - the humans (and humanoid machines) movement intentions based on their visual feeds, while filtering out camera motion without any camera calibration. To this end, we implement an optical flow-based pixel-wise temporal analysis method to compensate for the camera motion with a Gaussian aggregation to smooth out the movement prediction area. Subsequently, to evaluate the performance, we collect a dataset including 50 clips of pedestrian scenes in 5 different scenarios. We tested this framework with classical feature detectors such as SIFT and ORB to show the comparison. Our framework demonstrates its superiority in speed (> 40FPS), accuracy (MAE = 60pixels), and robustness (SNR = 23dB), confirming its potential to enhance the usability of vision-based assistive navigation tools in complex environments.
<div id='section'>Paperid: <span id='pid'>373, <a href='https://arxiv.org/pdf/2403.01740.pdf' target='_blank'>https://arxiv.org/pdf/2403.01740.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingyu Gong, Min Wang, Wentao Liu, Chen Qian, Zhizhong Zhang, Yuan Xie, Lizhuang Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.01740">DEMOS: Dynamic Environment Motion Synthesis in 3D Scenes via Local Spherical-BEV Perception</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion synthesis in real-world 3D scenes has recently attracted much attention. However, the static environment assumption made by most current methods usually cannot be satisfied especially for real-time motion synthesis in scanned point cloud scenes, if multiple dynamic objects exist, e.g., moving persons or vehicles. To handle this problem, we propose the first Dynamic Environment MOtion Synthesis framework (DEMOS) to predict future motion instantly according to the current scene, and use it to dynamically update the latent motion for final motion synthesis. Concretely, we propose a Spherical-BEV perception method to extract local scene features that are specifically designed for instant scene-aware motion prediction. Then, we design a time-variant motion blending to fuse the new predicted motions into the latent motion, and the final motion is derived from the updated latent motions, benefitting both from motion-prior and iterative methods. We unify the data format of two prevailing datasets, PROX and GTA-IM, and take them for motion synthesis evaluation in 3D scenes. We also assess the effectiveness of the proposed method in dynamic environments from GTA-IM and Semantic3D to check the responsiveness. The results show our method outperforms previous works significantly and has great performance in handling dynamic environments.
<div id='section'>Paperid: <span id='pid'>374, <a href='https://arxiv.org/pdf/2306.17588.pdf' target='_blank'>https://arxiv.org/pdf/2306.17588.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Savvas Papaioannou, Panayiotis Kolios, Theocharis Theocharides, Christos G. Panayiotou, Marios M. Polycarpou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.17588">Unscented Optimal Control for 3D Coverage Planning with an Autonomous UAV Agent</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a novel probabilistically robust controller for the guidance of an unmanned aerial vehicle (UAV) in coverage planning missions, which can simultaneously optimize both the UAV's motion, and camera control inputs for the 3D coverage of a given object of interest. Specifically, the coverage planning problem is formulated in this work as an optimal control problem with logical constraints to enable the UAV agent to jointly: a) select a series of discrete camera field-of-view states which satisfy a set of coverage constraints, and b) optimize its motion control inputs according to a specified mission objective. We show how this hybrid optimal control problem can be solved with standard optimization tools by converting the logical expressions in the constraints into equality/inequality constraints involving only continuous variables. Finally, probabilistic robustness is achieved by integrating the unscented transformation to the proposed controller, thus enabling the design of robust open-loop coverage plans which take into account the future posterior distribution of the UAV's state inside the planning horizon.
<div id='section'>Paperid: <span id='pid'>375, <a href='https://arxiv.org/pdf/2506.15483.pdf' target='_blank'>https://arxiv.org/pdf/2506.15483.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shujia Li, Haiyu Zhang, Xinyuan Chen, Yaohui Wang, Yutong Ban
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.15483">GenHOI: Generalizing Text-driven 4D Human-Object Interaction Synthesis for Unseen Objects</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While diffusion models and large-scale motion datasets have advanced text-driven human motion synthesis, extending these advances to 4D human-object interaction (HOI) remains challenging, mainly due to the limited availability of large-scale 4D HOI datasets. In our study, we introduce GenHOI, a novel two-stage framework aimed at achieving two key objectives: 1) generalization to unseen objects and 2) the synthesis of high-fidelity 4D HOI sequences. In the initial stage of our framework, we employ an Object-AnchorNet to reconstruct sparse 3D HOI keyframes for unseen objects, learning solely from 3D HOI datasets, thereby mitigating the dependence on large-scale 4D HOI datasets. Subsequently, we introduce a Contact-Aware Diffusion Model (ContactDM) in the second stage to seamlessly interpolate sparse 3D HOI keyframes into densely temporally coherent 4D HOI sequences. To enhance the quality of generated 4D HOI sequences, we propose a novel Contact-Aware Encoder within ContactDM to extract human-object contact patterns and a novel Contact-Aware HOI Attention to effectively integrate the contact signals into diffusion models. Experimental results show that we achieve state-of-the-art results on the publicly available OMOMO and 3D-FUTURE datasets, demonstrating strong generalization abilities to unseen objects, while enabling high-fidelity 4D HOI generation.
<div id='section'>Paperid: <span id='pid'>376, <a href='https://arxiv.org/pdf/2506.01941.pdf' target='_blank'>https://arxiv.org/pdf/2506.01941.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Longyan Wu, Checheng Yu, Jieji Ren, Li Chen, Ran Huang, Guoying Gu, Hongyang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.01941">FreeTacMan: Robot-free Visuo-Tactile Data Collection System for Contact-rich Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Enabling robots with contact-rich manipulation remains a pivotal challenge in robot learning, which is substantially hindered by the data collection gap, including its inefficiency and limited sensor setup. While prior work has explored handheld paradigms, their rod-based mechanical structures remain rigid and unintuitive, providing limited tactile feedback and posing challenges for human operators. Motivated by the dexterity and force feedback of human motion, we propose FreeTacMan, a human-centric and robot-free data collection system for accurate and efficient robot manipulation. Concretely, we design a wearable data collection device with dual visuo-tactile grippers, which can be worn by human fingers for intuitive and natural control. A high-precision optical tracking system is introduced to capture end-effector poses, while synchronizing visual and tactile feedback simultaneously. FreeTacMan achieves multiple improvements in data collection performance compared to prior works, and enables effective policy learning for contact-rich manipulation tasks with the help of the visuo-tactile information. We will release the work to facilitate reproducibility and accelerate research in visuo-tactile manipulation.
<div id='section'>Paperid: <span id='pid'>377, <a href='https://arxiv.org/pdf/2506.01941.pdf' target='_blank'>https://arxiv.org/pdf/2506.01941.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Longyan Wu, Checheng Yu, Jieji Ren, Li Chen, Yufei Jiang, Ran Huang, Guoying Gu, Hongyang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.01941">FreeTacMan: Robot-free Visuo-Tactile Data Collection System for Contact-rich Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Enabling robots with contact-rich manipulation remains a pivotal challenge in robot learning, which is substantially hindered by the data collection gap, including its inefficiency and limited sensor setup. While prior work has explored handheld paradigms, their rod-based mechanical structures remain rigid and unintuitive, providing limited tactile feedback and posing challenges for human operators. Motivated by the dexterity and force feedback of human motion, we propose FreeTacMan, a human-centric and robot-free data collection system for accurate and efficient robot manipulation. Concretely, we design a wearable gripper with dual visuo-tactile sensors for data collection, which can be worn by human fingers for intuitive control. A high-precision optical tracking system is introduced to capture end-effector poses while synchronizing visual and tactile feedback simultaneously. We leverage FreeTacMan to collect a large-scale multimodal dataset, comprising over 3000k paired visual-tactile images with end-effector poses, 10k demonstration trajectories across 50 diverse contact-rich manipulation tasks. FreeTacMan achieves multiple improvements in data collection performance compared to prior works, and enables effective policy learning for contact-rich manipulation tasks with self-collected dataset. The full suite of hardware specifications and the dataset will be released to facilitate reproducibility and support research in visuo-tactile manipulation.
<div id='section'>Paperid: <span id='pid'>378, <a href='https://arxiv.org/pdf/2410.02141.pdf' target='_blank'>https://arxiv.org/pdf/2410.02141.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiqun Duan, Qiang Zhang, Jinzhao Zhou, Jingkai Sun, Xiaowei Jiang, Jiahang Cao, Jiaxu Wang, Yiqian Yang, Wen Zhao, Gang Han, Yijie Guo, Chin-Teng Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.02141">E2H: A Two-Stage Non-Invasive Neural Signal Driven Humanoid Robotic Whole-Body Control Framework</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in humanoid robotics, including the integration of hierarchical reinforcement learning-based control and the utilization of LLM planning, have significantly enhanced the ability of robots to perform complex tasks. In contrast to the highly developed humanoid robots, the human factors involved remain relatively unexplored. Directly controlling humanoid robots with the brain has already appeared in many science fiction novels, such as Pacific Rim and Gundam. In this work, we present E2H (EEG-to-Humanoid), an innovative framework that pioneers the control of humanoid robots using high-frequency non-invasive neural signals. As the none-invasive signal quality remains low in decoding precise spatial trajectory, we decompose the E2H framework in an innovative two-stage formation: 1) decoding neural signals (EEG) into semantic motion keywords, 2) utilizing LLM facilitated motion generation with a precise motion imitation control policy to realize humanoid robotics control. The method of directly driving robots with brainwave commands offers a novel approach to human-machine collaboration, especially in situations where verbal commands are impractical, such as in cases of speech impairments, space exploration, or underwater exploration, unlocking significant potential. E2H offers an exciting glimpse into the future, holding immense potential for human-computer interaction.
<div id='section'>Paperid: <span id='pid'>379, <a href='https://arxiv.org/pdf/2407.05679.pdf' target='_blank'>https://arxiv.org/pdf/2407.05679.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yumeng Zhang, Shi Gong, Kaixin Xiong, Xiaoqing Ye, Xiaofan Li, Xiao Tan, Fan Wang, Jizhou Huang, Hua Wu, Haifeng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.05679">BEVWorld: A Multimodal World Simulator for Autonomous Driving via Scene-Level BEV Latents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>World models have attracted increasing attention in autonomous driving for their ability to forecast potential future scenarios. In this paper, we propose BEVWorld, a novel framework that transforms multimodal sensor inputs into a unified and compact Bird's Eye View (BEV) latent space for holistic environment modeling. The proposed world model consists of two main components: a multi-modal tokenizer and a latent BEV sequence diffusion model. The multi-modal tokenizer first encodes heterogeneous sensory data, and its decoder reconstructs the latent BEV tokens into LiDAR and surround-view image observations via ray-casting rendering in a self-supervised manner. This enables joint modeling and bidirectional encoding-decoding of panoramic imagery and point cloud data within a shared spatial representation. On top of this, the latent BEV sequence diffusion model performs temporally consistent forecasting of future scenes, conditioned on high-level action tokens, enabling scene-level reasoning over time. Extensive experiments demonstrate the effectiveness of BEVWorld on autonomous driving benchmarks, showcasing its capability in realistic future scene generation and its benefits for downstream tasks such as perception and motion prediction.
<div id='section'>Paperid: <span id='pid'>380, <a href='https://arxiv.org/pdf/2305.11394.pdf' target='_blank'>https://arxiv.org/pdf/2305.11394.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tharindu Fernando, Harshala Gammulle, Sridha Sridharan, Simon Denman, Clinton Fookes
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.11394">Remembering What Is Important: A Factorised Multi-Head Retrieval and Auxiliary Memory Stabilisation Scheme for Human Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humans exhibit complex motions that vary depending on the task that they are performing, the interactions they engage in, as well as subject-specific preferences. Therefore, forecasting future poses based on the history of the previous motions is a challenging task. This paper presents an innovative auxiliary-memory-powered deep neural network framework for the improved modelling of historical knowledge. Specifically, we disentangle subject-specific, task-specific, and other auxiliary information from the observed pose sequences and utilise these factorised features to query the memory. A novel Multi-Head knowledge retrieval scheme leverages these factorised feature embeddings to perform multiple querying operations over the historical observations captured within the auxiliary memory. Moreover, our proposed dynamic masking strategy makes this feature disentanglement process dynamic. Two novel loss functions are introduced to encourage diversity within the auxiliary memory while ensuring the stability of the memory contents, such that it can locate and store salient information that can aid the long-term prediction of future motion, irrespective of data imbalances or the diversity of the input data distribution. With extensive experiments conducted on two public benchmarks, Human3.6M and CMU-Mocap, we demonstrate that these design choices collectively allow the proposed approach to outperform the current state-of-the-art methods by significant margins: $>$ 17\% on the Human3.6M dataset and $>$ 9\% on the CMU-Mocap dataset.
<div id='section'>Paperid: <span id='pid'>381, <a href='https://arxiv.org/pdf/2508.10522.pdf' target='_blank'>https://arxiv.org/pdf/2508.10522.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Quang Nguyen, Nhat Le, Baoru Huang, Minh Nhat Vu, Chengcheng Tang, Van Nguyen, Ngan Le, Thieu Vo, Anh Nguyen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10522">EgoMusic-driven Human Dance Motion Estimation with Skeleton Mamba</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Estimating human dance motion is a challenging task with various industrial applications. Recently, many efforts have focused on predicting human dance motion using either egocentric video or music as input. However, the task of jointly estimating human motion from both egocentric video and music remains largely unexplored. In this paper, we aim to develop a new method that predicts human dance motion from both egocentric video and music. In practice, the egocentric view often obscures much of the body, making accurate full-pose estimation challenging. Additionally, incorporating music requires the generated head and body movements to align well with both visual and musical inputs. We first introduce EgoAIST++, a new large-scale dataset that combines both egocentric views and music with more than 36 hours of dancing motion. Drawing on the success of diffusion models and Mamba on modeling sequences, we develop an EgoMusic Motion Network with a core Skeleton Mamba that explicitly captures the skeleton structure of the human body. We illustrate that our approach is theoretically supportive. Intensive experiments show that our method clearly outperforms state-of-the-art approaches and generalizes effectively to real-world data.
<div id='section'>Paperid: <span id='pid'>382, <a href='https://arxiv.org/pdf/2505.22944.pdf' target='_blank'>https://arxiv.org/pdf/2505.22944.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Angtian Wang, Haibin Huang, Jacob Zhiyuan Fang, Yiding Yang, Chongyang Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.22944">ATI: Any Trajectory Instruction for Controllable Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a unified framework for motion control in video generation that seamlessly integrates camera movement, object-level translation, and fine-grained local motion using trajectory-based inputs. In contrast to prior methods that address these motion types through separate modules or task-specific designs, our approach offers a cohesive solution by projecting user-defined trajectories into the latent space of pre-trained image-to-video generation models via a lightweight motion injector. Users can specify keypoints and their motion paths to control localized deformations, entire object motion, virtual camera dynamics, or combinations of these. The injected trajectory signals guide the generative process to produce temporally consistent and semantically aligned motion sequences. Our framework demonstrates superior performance across multiple video motion control tasks, including stylized motion effects (e.g., motion brushes), dynamic viewpoint changes, and precise local motion manipulation. Experiments show that our method provides significantly better controllability and visual quality compared to prior approaches and commercial solutions, while remaining broadly compatible with various state-of-the-art video generation backbones. Project page: https://anytraj.github.io/.
<div id='section'>Paperid: <span id='pid'>383, <a href='https://arxiv.org/pdf/2501.13335.pdf' target='_blank'>https://arxiv.org/pdf/2501.13335.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xianrui Luo, Juewen Peng, Zhongang Cai, Lei Yang, Fan Yang, Zhiguo Cao, Guosheng Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.13335">Deblur-Avatar: Animatable Avatars from Motion-Blurred Monocular Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a novel framework for modeling high-fidelity, animatable 3D human avatars from motion-blurred monocular video inputs. Motion blur is prevalent in real-world dynamic video capture, especially due to human movements in 3D human avatar modeling. Existing methods either (1) assume sharp image inputs, failing to address the detail loss introduced by motion blur, or (2) mainly consider blur by camera movements, neglecting the human motion blur which is more common in animatable avatars. Our proposed approach integrates a human movement-based motion blur model into 3D Gaussian Splatting (3DGS). By explicitly modeling human motion trajectories during exposure time, we jointly optimize the trajectories and 3D Gaussians to reconstruct sharp, high-quality human avatars. We employ a pose-dependent fusion mechanism to distinguish moving body regions, optimizing both blurred and sharp areas effectively. Extensive experiments on synthetic and real-world datasets demonstrate that our method significantly outperforms existing methods in rendering quality and quantitative metrics, producing sharp avatar reconstructions and enabling real-time rendering under challenging motion blur conditions.
<div id='section'>Paperid: <span id='pid'>384, <a href='https://arxiv.org/pdf/2409.18127.pdf' target='_blank'>https://arxiv.org/pdf/2409.18127.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fangzhou Hong, Vladimir Guzov, Hyo Jin Kim, Yuting Ye, Richard Newcombe, Ziwei Liu, Lingni Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.18127">EgoLM: Multi-Modal Language Model of Egocentric Motions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As the prevalence of wearable devices, learning egocentric motions becomes essential to develop contextual AI. In this work, we present EgoLM, a versatile framework that tracks and understands egocentric motions from multi-modal inputs, e.g., egocentric videos and motion sensors. EgoLM exploits rich contexts for the disambiguation of egomotion tracking and understanding, which are ill-posed under single modality conditions. To facilitate the versatile and multi-modal framework, our key insight is to model the joint distribution of egocentric motions and natural languages using large language models (LLM). Multi-modal sensor inputs are encoded and projected to the joint latent space of language models, and used to prompt motion generation or text generation for egomotion tracking or understanding, respectively. Extensive experiments on large-scale multi-modal human motion dataset validate the effectiveness of EgoLM as a generalist model for universal egocentric learning.
<div id='section'>Paperid: <span id='pid'>385, <a href='https://arxiv.org/pdf/2406.09905.pdf' target='_blank'>https://arxiv.org/pdf/2406.09905.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lingni Ma, Yuting Ye, Fangzhou Hong, Vladimir Guzov, Yifeng Jiang, Rowan Postyeni, Luis Pesqueira, Alexander Gamino, Vijay Baiyya, Hyo Jin Kim, Kevin Bailey, David Soriano Fosas, C. Karen Liu, Ziwei Liu, Jakob Engel, Renzo De Nardi, Richard Newcombe
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.09905">Nymeria: A Massive Collection of Multimodal Egocentric Daily Motion in the Wild</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce Nymeria - a large-scale, diverse, richly annotated human motion dataset collected in the wild with multiple multimodal egocentric devices. The dataset comes with a) full-body ground-truth motion; b) multiple multimodal egocentric data from Project Aria devices with videos, eye tracking, IMUs and etc; and c) a third-person perspective by an additional observer. All devices are precisely synchronized and localized in on metric 3D world. We derive hierarchical protocol to add in-context language descriptions of human motion, from fine-grain motion narration, to simplified atomic action and high-level activity summarization. To the best of our knowledge, Nymeria dataset is the world's largest collection of human motion in the wild; first of its kind to provide synchronized and localized multi-device multimodal egocentric data; and the world's largest motion-language dataset. It provides 300 hours of daily activities from 264 participants across 50 locations, total travelling distance over 399Km. The language descriptions contain 301.5K sentences in 8.64M words from a vocabulary size of 6545. To demonstrate the potential of the dataset, we evaluate several SOTA algorithms for egocentric body tracking, motion synthesis, and action recognition. Data and code are open-sourced for research (c.f. https://www.projectaria.com/datasets/nymeria).
<div id='section'>Paperid: <span id='pid'>386, <a href='https://arxiv.org/pdf/2405.04496.pdf' target='_blank'>https://arxiv.org/pdf/2405.04496.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi Zuo, Lingling Li, Licheng Jiao, Fang Liu, Xu Liu, Wenping Ma, Shuyuan Yang, Yuwei Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.04496">Edit-Your-Motion: Space-Time Diffusion Decoupling Learning for Video Motion Editing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing diffusion-based methods have achieved impressive results in human motion editing. However, these methods often exhibit significant ghosting and body distortion in unseen in-the-wild cases. In this paper, we introduce Edit-Your-Motion, a video motion editing method that tackles these challenges through one-shot fine-tuning on unseen cases. Specifically, firstly, we utilized DDIM inversion to initialize the noise, preserving the appearance of the source video and designed a lightweight motion attention adapter module to enhance motion fidelity. DDIM inversion aims to obtain the implicit representations by estimating the prediction noise from the source video, which serves as a starting point for the sampling process, ensuring the appearance consistency between the source and edited videos. The Motion Attention Module (MA) enhances the model's motion editing ability by resolving the conflict between the skeleton features and the appearance features. Secondly, to effectively decouple motion and appearance of source video, we design a spatio-temporal two-stage learning strategy (STL). In the first stage, we focus on learning temporal features of human motion and propose recurrent causal attention (RCA) to ensure consistency between video frames. In the second stage, we shift focus on learning the appearance features of the source video. With Edit-Your-Motion, users can edit the motion of humans in the source video, creating more engaging and diverse content. Extensive qualitative and quantitative experiments, along with user preference studies, show that Edit-Your-Motion outperforms other methods.
<div id='section'>Paperid: <span id='pid'>387, <a href='https://arxiv.org/pdf/2404.01284.pdf' target='_blank'>https://arxiv.org/pdf/2404.01284.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingyuan Zhang, Daisheng Jin, Chenyang Gu, Fangzhou Hong, Zhongang Cai, Jingfang Huang, Chongzhi Zhang, Xinying Guo, Lei Yang, Ying He, Ziwei Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.01284">Large Motion Model for Unified Multi-Modal Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion generation, a cornerstone technique in animation and video production, has widespread applications in various tasks like text-to-motion and music-to-dance. Previous works focus on developing specialist models tailored for each task without scalability. In this work, we present Large Motion Model (LMM), a motion-centric, multi-modal framework that unifies mainstream motion generation tasks into a generalist model. A unified motion model is appealing since it can leverage a wide range of motion data to achieve broad generalization beyond a single task. However, it is also challenging due to the heterogeneous nature of substantially different motion data and tasks. LMM tackles these challenges from three principled aspects: 1) Data: We consolidate datasets with different modalities, formats and tasks into a comprehensive yet unified motion generation dataset, MotionVerse, comprising 10 tasks, 16 datasets, a total of 320k sequences, and 100 million frames. 2) Architecture: We design an articulated attention mechanism ArtAttention that incorporates body part-aware modeling into Diffusion Transformer backbone. 3) Pre-Training: We propose a novel pre-training strategy for LMM, which employs variable frame rates and masking forms, to better exploit knowledge from diverse training data. Extensive experiments demonstrate that our generalist LMM achieves competitive performance across various standard motion generation tasks over state-of-the-art specialist models. Notably, LMM exhibits strong generalization capabilities and emerging properties across many unseen tasks. Additionally, our ablation studies reveal valuable insights about training and scaling up large motion models for future research.
<div id='section'>Paperid: <span id='pid'>388, <a href='https://arxiv.org/pdf/2404.01225.pdf' target='_blank'>https://arxiv.org/pdf/2404.01225.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tao Hu, Fangzhou Hong, Ziwei Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.01225">SurMo: Surface-based 4D Motion Modeling for Dynamic Human Rendering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dynamic human rendering from video sequences has achieved remarkable progress by formulating the rendering as a mapping from static poses to human images. However, existing methods focus on the human appearance reconstruction of every single frame while the temporal motion relations are not fully explored. In this paper, we propose a new 4D motion modeling paradigm, SurMo, that jointly models the temporal dynamics and human appearances in a unified framework with three key designs: 1) Surface-based motion encoding that models 4D human motions with an efficient compact surface-based triplane. It encodes both spatial and temporal motion relations on the dense surface manifold of a statistical body template, which inherits body topology priors for generalizable novel view synthesis with sparse training observations. 2) Physical motion decoding that is designed to encourage physical motion learning by decoding the motion triplane features at timestep t to predict both spatial derivatives and temporal derivatives at the next timestep t+1 in the training stage. 3) 4D appearance decoding that renders the motion triplanes into images by an efficient volumetric surface-conditioned renderer that focuses on the rendering of body surfaces with motion learning conditioning. Extensive experiments validate the state-of-the-art performance of our new paradigm and illustrate the expressiveness of surface-based motion triplanes for rendering high-fidelity view-consistent humans with fast motions and even motion-dependent shadows. Our project page is at: https://taohuumd.github.io/projects/SurMo/
<div id='section'>Paperid: <span id='pid'>389, <a href='https://arxiv.org/pdf/2401.14159.pdf' target='_blank'>https://arxiv.org/pdf/2401.14159.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang Chen, Feng Yan, Zhaoyang Zeng, Hao Zhang, Feng Li, Jie Yang, Hongyang Li, Qing Jiang, Lei Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.14159">Grounded SAM: Assembling Open-World Models for Diverse Visual Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce Grounded SAM, which uses Grounding DINO as an open-set object detector to combine with the segment anything model (SAM). This integration enables the detection and segmentation of any regions based on arbitrary text inputs and opens a door to connecting various vision models. As shown in Fig.1, a wide range of vision tasks can be achieved by using the versatile Grounded SAM pipeline. For example, an automatic annotation pipeline based solely on input images can be realized by incorporating models such as BLIP and Recognize Anything. Additionally, incorporating Stable-Diffusion allows for controllable image editing, while the integration of OSX facilitates promptable 3D human motion analysis. Grounded SAM also shows superior performance on open-vocabulary benchmarks, achieving 48.7 mean AP on SegInW (Segmentation in the wild) zero-shot benchmark with the combination of Grounding DINO-Base and SAM-Huge models.
<div id='section'>Paperid: <span id='pid'>390, <a href='https://arxiv.org/pdf/2312.15004.pdf' target='_blank'>https://arxiv.org/pdf/2312.15004.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingyuan Zhang, Huirong Li, Zhongang Cai, Jiawei Ren, Lei Yang, Ziwei Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.15004">FineMoGen: Fine-Grained Spatio-Temporal Motion Generation and Editing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-driven motion generation has achieved substantial progress with the emergence of diffusion models. However, existing methods still struggle to generate complex motion sequences that correspond to fine-grained descriptions, depicting detailed and accurate spatio-temporal actions. This lack of fine controllability limits the usage of motion generation to a larger audience. To tackle these challenges, we present FineMoGen, a diffusion-based motion generation and editing framework that can synthesize fine-grained motions, with spatial-temporal composition to the user instructions. Specifically, FineMoGen builds upon diffusion model with a novel transformer architecture dubbed Spatio-Temporal Mixture Attention (SAMI). SAMI optimizes the generation of the global attention template from two perspectives: 1) explicitly modeling the constraints of spatio-temporal composition; and 2) utilizing sparsely-activated mixture-of-experts to adaptively extract fine-grained features. To facilitate a large-scale study on this new fine-grained motion generation task, we contribute the HuMMan-MoGen dataset, which consists of 2,968 videos and 102,336 fine-grained spatio-temporal descriptions. Extensive experiments validate that FineMoGen exhibits superior motion generation quality over state-of-the-art methods. Notably, FineMoGen further enables zero-shot motion editing capabilities with the aid of modern large language models (LLM), which faithfully manipulates motion sequences with fine-grained instructions. Project Page: https://mingyuan-zhang.github.io/projects/FineMoGen.html
<div id='section'>Paperid: <span id='pid'>391, <a href='https://arxiv.org/pdf/2311.07446.pdf' target='_blank'>https://arxiv.org/pdf/2311.07446.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhongfei Qing, Zhongang Cai, Zhitao Yang, Lei Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.07446">Story-to-Motion: Synthesizing Infinite and Controllable Character Animation from Long Text</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating natural human motion from a story has the potential to transform the landscape of animation, gaming, and film industries. A new and challenging task, Story-to-Motion, arises when characters are required to move to various locations and perform specific motions based on a long text description. This task demands a fusion of low-level control (trajectories) and high-level control (motion semantics). Previous works in character control and text-to-motion have addressed related aspects, yet a comprehensive solution remains elusive: character control methods do not handle text description, whereas text-to-motion methods lack position constraints and often produce unstable motions. In light of these limitations, we propose a novel system that generates controllable, infinitely long motions and trajectories aligned with the input text. (1) We leverage contemporary Large Language Models to act as a text-driven motion scheduler to extract a series of (text, position, duration) pairs from long text. (2) We develop a text-driven motion retrieval scheme that incorporates motion matching with motion semantic and trajectory constraints. (3) We design a progressive mask transformer that addresses common artifacts in the transition motion such as unnatural pose and foot sliding. Beyond its pioneering role as the first comprehensive solution for Story-to-Motion, our system undergoes evaluation across three distinct sub-tasks: trajectory following, temporal action composition, and motion blending, where it outperforms previous state-of-the-art motion synthesis methods across the board. Homepage: https://story2motion.github.io/.
<div id='section'>Paperid: <span id='pid'>392, <a href='https://arxiv.org/pdf/2310.15948.pdf' target='_blank'>https://arxiv.org/pdf/2310.15948.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>An Vuong, Minh Nhat Vu, Toan Tien Nguyen, Baoru Huang, Dzung Nguyen, Thieu Vo, Anh Nguyen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.15948">Language-driven Scene Synthesis using Multi-conditional Diffusion Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scene synthesis is a challenging problem with several industrial applications. Recently, substantial efforts have been directed to synthesize the scene using human motions, room layouts, or spatial graphs as the input. However, few studies have addressed this problem from multiple modalities, especially combining text prompts. In this paper, we propose a language-driven scene synthesis task, which is a new task that integrates text prompts, human motion, and existing objects for scene synthesis. Unlike other single-condition synthesis tasks, our problem involves multiple conditions and requires a strategy for processing and encoding them into a unified space. To address the challenge, we present a multi-conditional diffusion model, which differs from the implicit unification approach of other diffusion literature by explicitly predicting the guiding points for the original data distribution. We demonstrate that our approach is theoretically supportive. The intensive experiment results illustrate that our method outperforms state-of-the-art benchmarks and enables natural scene editing applications. The source code and dataset can be accessed at https://lang-scene-synth.github.io/.
<div id='section'>Paperid: <span id='pid'>393, <a href='https://arxiv.org/pdf/2303.17774.pdf' target='_blank'>https://arxiv.org/pdf/2303.17774.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gengxin Liu, Qian Sun, Haibin Huang, Chongyang Ma, Yulan Guo, Li Yi, Hui Huang, Ruizhen Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.17774">Semi-Weakly Supervised Object Kinematic Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Given a 3D object, kinematic motion prediction aims to identify the mobile parts as well as the corresponding motion parameters. Due to the large variations in both topological structure and geometric details of 3D objects, this remains a challenging task and the lack of large scale labeled data also constrain the performance of deep learning based approaches. In this paper, we tackle the task of object kinematic motion prediction problem in a semi-weakly supervised manner. Our key observations are two-fold. First, although 3D dataset with fully annotated motion labels is limited, there are existing datasets and methods for object part semantic segmentation at large scale. Second, semantic part segmentation and mobile part segmentation is not always consistent but it is possible to detect the mobile parts from the underlying 3D structure. Towards this end, we propose a graph neural network to learn the map between hierarchical part-level segmentation and mobile parts parameters, which are further refined based on geometric alignment. This network can be first trained on PartNet-Mobility dataset with fully labeled mobility information and then applied on PartNet dataset with fine-grained and hierarchical part-level segmentation. The network predictions yield a large scale of 3D objects with pseudo labeled mobility information and can further be used for weakly-supervised learning with pre-existing segmentation. Our experiments show there are significant performance boosts with the augmented data for previous method designed for kinematic motion prediction on 3D partial scans.
<div id='section'>Paperid: <span id='pid'>394, <a href='https://arxiv.org/pdf/2509.20322.pdf' target='_blank'>https://arxiv.org/pdf/2509.20322.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shaofeng Yin, Yanjie Ze, Hong-Xing Yu, C. Karen Liu, Jiajun Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20322">VisualMimic: Visual Humanoid Loco-Manipulation via Motion Tracking and Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humanoid loco-manipulation in unstructured environments demands tight integration of egocentric perception and whole-body control. However, existing approaches either depend on external motion capture systems or fail to generalize across diverse tasks. We introduce VisualMimic, a visual sim-to-real framework that unifies egocentric vision with hierarchical whole-body control for humanoid robots. VisualMimic combines a task-agnostic low-level keypoint tracker -- trained from human motion data via a teacher-student scheme -- with a task-specific high-level policy that generates keypoint commands from visual and proprioceptive input. To ensure stable training, we inject noise into the low-level policy and clip high-level actions using human motion statistics. VisualMimic enables zero-shot transfer of visuomotor policies trained in simulation to real humanoid robots, accomplishing a wide range of loco-manipulation tasks such as box lifting, pushing, football dribbling, and kicking. Beyond controlled laboratory settings, our policies also generalize robustly to outdoor environments. Videos are available at: https://visualmimic.github.io .
<div id='section'>Paperid: <span id='pid'>395, <a href='https://arxiv.org/pdf/2507.04547.pdf' target='_blank'>https://arxiv.org/pdf/2507.04547.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xin You, Runze Yang, Chuyan Zhang, Zhongliang Jiang, Jie Yang, Nassir Navab
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.04547">FB-Diff: Fourier Basis-guided Diffusion for Temporal Interpolation of 4D Medical Imaging</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The temporal interpolation task for 4D medical imaging, plays a crucial role in clinical practice of respiratory motion modeling. Following the simplified linear-motion hypothesis, existing approaches adopt optical flow-based models to interpolate intermediate frames. However, realistic respiratory motions should be nonlinear and quasi-periodic with specific frequencies. Intuited by this property, we resolve the temporal interpolation task from the frequency perspective, and propose a Fourier basis-guided Diffusion model, termed FB-Diff. Specifically, due to the regular motion discipline of respiration, physiological motion priors are introduced to describe general characteristics of temporal data distributions. Then a Fourier motion operator is elaborately devised to extract Fourier bases by incorporating physiological motion priors and case-specific spectral information in the feature space of Variational Autoencoder. Well-learned Fourier bases can better simulate respiratory motions with motion patterns of specific frequencies. Conditioned on starting and ending frames, the diffusion model further leverages well-learned Fourier bases via the basis interaction operator, which promotes the temporal interpolation task in a generative manner. Extensive results demonstrate that FB-Diff achieves state-of-the-art (SOTA) perceptual performance with better temporal consistency while maintaining promising reconstruction metrics. Codes are available.
<div id='section'>Paperid: <span id='pid'>396, <a href='https://arxiv.org/pdf/2412.18600.pdf' target='_blank'>https://arxiv.org/pdf/2412.18600.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongjie Li, Hong-Xing Yu, Jiaman Li, Jiajun Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.18600">ZeroHSI: Zero-Shot 4D Human-Scene Interaction by Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human-scene interaction (HSI) generation is crucial for applications in embodied AI, virtual reality, and robotics. Yet, existing methods cannot synthesize interactions in unseen environments such as in-the-wild scenes or reconstructed scenes, as they rely on paired 3D scenes and captured human motion data for training, which are unavailable for unseen environments. We present ZeroHSI, a novel approach that enables zero-shot 4D human-scene interaction synthesis, eliminating the need for training on any MoCap data. Our key insight is to distill human-scene interactions from state-of-the-art video generation models, which have been trained on vast amounts of natural human movements and interactions, and use differentiable rendering to reconstruct human-scene interactions. ZeroHSI can synthesize realistic human motions in both static scenes and environments with dynamic objects, without requiring any ground-truth motion data. We evaluate ZeroHSI on a curated dataset of different types of various indoor and outdoor scenes with different interaction prompts, demonstrating its ability to generate diverse and contextually appropriate human-scene interactions.
<div id='section'>Paperid: <span id='pid'>397, <a href='https://arxiv.org/pdf/2404.13026.pdf' target='_blank'>https://arxiv.org/pdf/2404.13026.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianyuan Zhang, Hong-Xing Yu, Rundi Wu, Brandon Y. Feng, Changxi Zheng, Noah Snavely, Jiajun Wu, William T. Freeman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.13026">PhysDreamer: Physics-Based Interaction with 3D Objects via Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Realistic object interactions are crucial for creating immersive virtual experiences, yet synthesizing realistic 3D object dynamics in response to novel interactions remains a significant challenge. Unlike unconditional or text-conditioned dynamics generation, action-conditioned dynamics requires perceiving the physical material properties of objects and grounding the 3D motion prediction on these properties, such as object stiffness. However, estimating physical material properties is an open problem due to the lack of material ground-truth data, as measuring these properties for real objects is highly difficult. We present PhysDreamer, a physics-based approach that endows static 3D objects with interactive dynamics by leveraging the object dynamics priors learned by video generation models. By distilling these priors, PhysDreamer enables the synthesis of realistic object responses to novel interactions, such as external forces or agent manipulations. We demonstrate our approach on diverse examples of elastic objects and evaluate the realism of the synthesized interactions through a user study. PhysDreamer takes a step towards more engaging and realistic virtual experiences by enabling static 3D objects to dynamically respond to interactive stimuli in a physically plausible manner. See our project page at https://physdreamer.github.io/.
<div id='section'>Paperid: <span id='pid'>398, <a href='https://arxiv.org/pdf/2404.09967.pdf' target='_blank'>https://arxiv.org/pdf/2404.09967.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Han Lin, Jaemin Cho, Abhay Zala, Mohit Bansal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.09967">Ctrl-Adapter: An Efficient and Versatile Framework for Adapting Diverse Controls to Any Diffusion Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>ControlNets are widely used for adding spatial control to text-to-image diffusion models with different conditions, such as depth maps, scribbles/sketches, and human poses. However, when it comes to controllable video generation, ControlNets cannot be directly integrated into new backbones due to feature space mismatches, and training ControlNets for new backbones can be a significant burden for many users. Furthermore, applying ControlNets independently to different frames cannot effectively maintain object temporal consistency. To address these challenges, we introduce Ctrl-Adapter, an efficient and versatile framework that adds diverse controls to any image/video diffusion model through the adaptation of pretrained ControlNets. Ctrl-Adapter offers strong and diverse capabilities, including image and video control, sparse-frame video control, fine-grained patch-level multi-condition control (via an MoE router), zero-shot adaptation to unseen conditions, and supports a variety of downstream tasks beyond spatial control, including video editing, video style transfer, and text-guided motion control. With six diverse U-Net/DiT-based image/video diffusion models (SDXL, PixArt-$Î±$, I2VGen-XL, SVD, Latte, Hotshot-XL), Ctrl-Adapter matches the performance of pretrained ControlNets on COCO and achieves the state-of-the-art on DAVIS 2017 with significantly lower computation (< 10 GPU hours).
<div id='section'>Paperid: <span id='pid'>399, <a href='https://arxiv.org/pdf/2401.02539.pdf' target='_blank'>https://arxiv.org/pdf/2401.02539.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dianye Huang, Chenguang Yang, Mingchuan Zhou, Angelos Karlas, Nassir Navab, Zhongliang Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.02539">Robot-Assisted Deep Venous Thrombosis Ultrasound Examination using Virtual Fixture</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep Venous Thrombosis (DVT) is a common vascular disease with blood clots inside deep veins, which may block blood flow or even cause a life-threatening pulmonary embolism. A typical exam for DVT using ultrasound (US) imaging is by pressing the target vein until its lumen is fully compressed. However, the compression exam is highly operator-dependent. To alleviate intra- and inter-variations, we present a robotic US system with a novel hybrid force motion control scheme ensuring position and force tracking accuracy, and soft landing of the probe onto the target surface. In addition, a path-based virtual fixture is proposed to realize easy human-robot interaction for repeat compression operation at the lesion location. To ensure the biometric measurements obtained in different examinations are comparable, the 6D scanning path is determined in a coarse-to-fine manner using both an external RGBD camera and US images. The RGBD camera is first used to extract a rough scanning path on the object. Then, the segmented vascular lumen from US images are used to optimize the scanning path to ensure the visibility of the target object. To generate a continuous scan path for developing virtual fixtures, an arc-length based path fitting model considering both position and orientation is proposed. Finally, the whole system is evaluated on a human-like arm phantom with an uneven surface.
<div id='section'>Paperid: <span id='pid'>400, <a href='https://arxiv.org/pdf/2306.10518.pdf' target='_blank'>https://arxiv.org/pdf/2306.10518.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shusheng Xu, Huaijie Wang, Jiaxuan Gao, Yutao Ouyang, Chao Yu, Yi Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.10518">LAGOON: Language-Guided Motion Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We aim to control a robot to physically behave in the real world following any high-level language command like "cartwheel" or "kick". Although human motion datasets exist, this task remains particularly challenging since generative models can produce physically unrealistic motions, which will be more severe for robots due to different body structures and physical properties. Deploying such a motion to a physical robot can cause even greater difficulties due to the sim2real gap. We develop LAnguage-Guided mOtion cONtrol (LAGOON), a multi-phase reinforcement learning (RL) method to generate physically realistic robot motions under language commands. LAGOON first leverages a pretrained model to generate a human motion from a language command. Then an RL phase trains a control policy in simulation to mimic the generated human motion. Finally, with domain randomization, our learned policy can be deployed to a quadrupedal robot, leading to a quadrupedal robot that can take diverse behaviors in the real world under natural language commands
<div id='section'>Paperid: <span id='pid'>401, <a href='https://arxiv.org/pdf/2509.24209.pdf' target='_blank'>https://arxiv.org/pdf/2509.24209.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yingdong Hu, Yisheng He, Jinnan Chen, Weihao Yuan, Kejie Qiu, Zehong Lin, Siyu Zhu, Zilong Dong, Jun Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24209">Forge4D: Feed-Forward 4D Human Reconstruction and Interpolation from Uncalibrated Sparse-view Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Instant reconstruction of dynamic 3D humans from uncalibrated sparse-view videos is critical for numerous downstream applications. Existing methods, however, are either limited by the slow reconstruction speeds or incapable of generating novel-time representations. To address these challenges, we propose Forge4D, a feed-forward 4D human reconstruction and interpolation model that efficiently reconstructs temporally aligned representations from uncalibrated sparse-view videos, enabling both novel view and novel time synthesis. Our model simplifies the 4D reconstruction and interpolation problem as a joint task of streaming 3D Gaussian reconstruction and dense motion prediction. For the task of streaming 3D Gaussian reconstruction, we first reconstruct static 3D Gaussians from uncalibrated sparse-view images and then introduce learnable state tokens to enforce temporal consistency in a memory-friendly manner by interactively updating shared information across different timestamps. For novel time synthesis, we design a novel motion prediction module to predict dense motions for each 3D Gaussian between two adjacent frames, coupled with an occlusion-aware Gaussian fusion process to interpolate 3D Gaussians at arbitrary timestamps. To overcome the lack of the ground truth for dense motion supervision, we formulate dense motion prediction as a dense point matching task and introduce a self-supervised retargeting loss to optimize this module. An additional occlusion-aware optical flow loss is introduced to ensure motion consistency with plausible human movement, providing stronger regularization. Extensive experiments demonstrate the effectiveness of our model on both in-domain and out-of-domain datasets. Project page and code at: https://zhenliuzju.github.io/huyingdong/Forge4D.
<div id='section'>Paperid: <span id='pid'>402, <a href='https://arxiv.org/pdf/2509.10678.pdf' target='_blank'>https://arxiv.org/pdf/2509.10678.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiahao Luo, Chaoyang Wang, Michael Vasilkovsky, Vladislav Shakhrai, Di Liu, Peiye Zhuang, Sergey Tulyakov, Peter Wonka, Hsin-Ying Lee, James Davis, Jian Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.10678">T2Bs: Text-to-Character Blendshapes via Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present T2Bs, a framework for generating high-quality, animatable character head morphable models from text by combining static text-to-3D generation with video diffusion. Text-to-3D models produce detailed static geometry but lack motion synthesis, while video diffusion models generate motion with temporal and multi-view geometric inconsistencies. T2Bs bridges this gap by leveraging deformable 3D Gaussian splatting to align static 3D assets with video outputs. By constraining motion with static geometry and employing a view-dependent deformation MLP, T2Bs (i) outperforms existing 4D generation methods in accuracy and expressiveness while reducing video artifacts and view inconsistencies, and (ii) reconstructs smooth, coherent, fully registered 3D geometries designed to scale for building morphable models with diverse, realistic facial motions. This enables synthesizing expressive, animatable character heads that surpass current 4D generation techniques.
<div id='section'>Paperid: <span id='pid'>403, <a href='https://arxiv.org/pdf/2509.10678.pdf' target='_blank'>https://arxiv.org/pdf/2509.10678.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiahao Luo, Chaoyang Wang, Michael Vasilkovsky, Vladislav Shakhrai, Di Liu, Peiye Zhuang, Sergey Tulyakov, Peter Wonka, Hsin-Ying Lee, James Davis, Jian Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.10678">T2Bs: Text-to-Character Blendshapes via Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present T2Bs, a framework for generating high-quality, animatable character head morphable models from text by combining static text-to-3D generation with video diffusion. Text-to-3D models produce detailed static geometry but lack motion synthesis, while video diffusion models generate motion with temporal and multi-view geometric inconsistencies. T2Bs bridges this gap by leveraging deformable 3D Gaussian splatting to align static 3D assets with video outputs. By constraining motion with static geometry and employing a view-dependent deformation MLP, T2Bs (i) outperforms existing 4D generation methods in accuracy and expressiveness while reducing video artifacts and view inconsistencies, and (ii) reconstructs smooth, coherent, fully registered 3D geometries designed to scale for building morphable models with diverse, realistic facial motions. This enables synthesizing expressive, animatable character heads that surpass current 4D generation techniques.
<div id='section'>Paperid: <span id='pid'>404, <a href='https://arxiv.org/pdf/2505.10810.pdf' target='_blank'>https://arxiv.org/pdf/2505.10810.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gabriel Maldonado, Armin Danesh Pazho, Ghazal Alinezhad Noghre, Vinit Katariya, Hamed Tabkhi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.10810">MoCLIP: Motion-Aware Fine-Tuning and Distillation of CLIP for Human Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion generation is essential for fields such as animation, robotics, and virtual reality, requiring models that effectively capture motion dynamics from text descriptions. Existing approaches often rely on Contrastive Language-Image Pretraining (CLIP)-based text encoders, but their training on text-image pairs constrains their ability to understand temporal and kinematic structures inherent in motion and motion generation. This work introduces MoCLIP, a fine-tuned CLIP model with an additional motion encoding head, trained on motion sequences using contrastive learning and tethering loss. By explicitly incorporating motion-aware representations, MoCLIP enhances motion fidelity while remaining compatible with existing CLIP-based pipelines and seamlessly integrating into various CLIP-based methods. Experiments demonstrate that MoCLIP improves Top-1, Top-2, and Top-3 accuracy while maintaining competitive FID, leading to improved text-to-motion alignment results. These results highlight MoCLIP's versatility and effectiveness, establishing it as a robust framework for enhancing motion generation.
<div id='section'>Paperid: <span id='pid'>405, <a href='https://arxiv.org/pdf/2505.04917.pdf' target='_blank'>https://arxiv.org/pdf/2505.04917.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenxu Peng, Chenxu Wang, Minrui Zou, Danyang Li, Zhengpeng Yang, Yimian Dai, Ming-Ming Cheng, Xiang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.04917">A Simple Detector with Frame Dynamics is a Strong Tracker</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Infrared object tracking plays a crucial role in Anti-Unmanned Aerial Vehicle (Anti-UAV) applications. Existing trackers often depend on cropped template regions and have limited motion modeling capabilities, which pose challenges when dealing with tiny targets. To address this, we propose a simple yet effective infrared tiny-object tracker that enhances tracking performance by integrating global detection and motion-aware learning with temporal priors. Our method is based on object detection and achieves significant improvements through two key innovations. First, we introduce frame dynamics, leveraging frame difference and optical flow to encode both prior target features and motion characteristics at the input level, enabling the model to better distinguish the target from background clutter. Second, we propose a trajectory constraint filtering strategy in the post-processing stage, utilizing spatio-temporal priors to suppress false positives and enhance tracking robustness. Extensive experiments show that our method consistently outperforms existing approaches across multiple metrics in challenging infrared UAV tracking scenarios. Notably, we achieve state-of-the-art performance in the 4th Anti-UAV Challenge, securing 1st place in Track 1 and 2nd place in Track 2.
<div id='section'>Paperid: <span id='pid'>406, <a href='https://arxiv.org/pdf/2503.20724.pdf' target='_blank'>https://arxiv.org/pdf/2503.20724.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nan Jiang, Hongjie Li, Ziye Yuan, Zimo He, Yixin Chen, Tengyu Liu, Yixin Zhu, Siyuan Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.20724">Dynamic Motion Blending for Versatile Motion Editing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-guided motion editing enables high-level semantic control and iterative modifications beyond traditional keyframe animation. Existing methods rely on limited pre-collected training triplets, which severely hinders their versatility in diverse editing scenarios. We introduce MotionCutMix, an online data augmentation technique that dynamically generates training triplets by blending body part motions based on input text. While MotionCutMix effectively expands the training distribution, the compositional nature introduces increased randomness and potential body part incoordination. To model such a rich distribution, we present MotionReFit, an auto-regressive diffusion model with a motion coordinator. The auto-regressive architecture facilitates learning by decomposing long sequences, while the motion coordinator mitigates the artifacts of motion composition. Our method handles both spatial and temporal motion edits directly from high-level human instructions, without relying on additional specifications or Large Language Models. Through extensive experiments, we show that MotionReFit achieves state-of-the-art performance in text-guided motion editing.
<div id='section'>Paperid: <span id='pid'>407, <a href='https://arxiv.org/pdf/2502.05432.pdf' target='_blank'>https://arxiv.org/pdf/2502.05432.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohammadreza Baharani, Ghazal Alinezhad Noghre, Armin Danesh Pazho, Gabriel Maldonado, Hamed Tabkhi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.05432">MoFM: A Large-Scale Human Motion Foundation Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Foundation Models (FM) have increasingly drawn the attention of researchers due to their scalability and generalization across diverse tasks. Inspired by the success of FMs and the principles that have driven advancements in Large Language Models (LLMs), we introduce MoFM as a novel Motion Foundation Model. MoFM is designed for the semantic understanding of complex human motions in both time and space. To facilitate large-scale training, MotionBook, a comprehensive human motion dictionary of discretized motions is designed and employed. MotionBook utilizes Thermal Cubes to capture spatio-temporal motion heatmaps, applying principles from discrete variational models to encode human movements into discrete units for a more efficient and scalable representation. MoFM, trained on a large corpus of motion data, provides a foundational backbone adaptable to diverse downstream tasks, supporting paradigms such as one-shot, unsupervised, and supervised tasks. This versatility makes MoFM well-suited for a wide range of motion-based applications.
<div id='section'>Paperid: <span id='pid'>408, <a href='https://arxiv.org/pdf/2501.04782.pdf' target='_blank'>https://arxiv.org/pdf/2501.04782.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andrew Bond, Jui-Hsien Wang, Long Mai, Erkut Erdem, Aykut Erdem
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.04782">GaussianVideo: Efficient Video Representation via Hierarchical Gaussian Splatting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Efficient neural representations for dynamic video scenes are critical for applications ranging from video compression to interactive simulations. Yet, existing methods often face challenges related to high memory usage, lengthy training times, and temporal consistency. To address these issues, we introduce a novel neural video representation that combines 3D Gaussian splatting with continuous camera motion modeling. By leveraging Neural ODEs, our approach learns smooth camera trajectories while maintaining an explicit 3D scene representation through Gaussians. Additionally, we introduce a spatiotemporal hierarchical learning strategy, progressively refining spatial and temporal features to enhance reconstruction quality and accelerate convergence. This memory-efficient approach achieves high-quality rendering at impressive speeds. Experimental results show that our hierarchical learning, combined with robust camera motion modeling, captures complex dynamic scenes with strong temporal consistency, achieving state-of-the-art performance across diverse video datasets in both high- and low-motion scenarios.
<div id='section'>Paperid: <span id='pid'>409, <a href='https://arxiv.org/pdf/2501.04325.pdf' target='_blank'>https://arxiv.org/pdf/2501.04325.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhi-Lin Huang, Yixuan Liu, Chujun Qin, Zhongdao Wang, Dong Zhou, Dong Li, Emad Barsoum
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.04325">Edit as You See: Image-guided Video Editing via Masked Motion Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in diffusion models have significantly facilitated text-guided video editing. However, there is a relative scarcity of research on image-guided video editing, a method that empowers users to edit videos by merely indicating a target object in the initial frame and providing an RGB image as reference, without relying on the text prompts. In this paper, we propose a novel Image-guided Video Editing Diffusion model, termed IVEDiff for the image-guided video editing. IVEDiff is built on top of image editing models, and is equipped with learnable motion modules to maintain the temporal consistency of edited video. Inspired by self-supervised learning concepts, we introduce a masked motion modeling fine-tuning strategy that empowers the motion module's capabilities for capturing inter-frame motion dynamics, while preserving the capabilities for intra-frame semantic correlations modeling of the base image editing model. Moreover, an optical-flow-guided motion reference network is proposed to ensure the accurate propagation of information between edited video frames, alleviating the misleading effects of invalid information. We also construct a benchmark to facilitate further research. The comprehensive experiments demonstrate that our method is able to generate temporally smooth edited videos while robustly dealing with various editing objects with high quality.
<div id='section'>Paperid: <span id='pid'>410, <a href='https://arxiv.org/pdf/2410.07093.pdf' target='_blank'>https://arxiv.org/pdf/2410.07093.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhe Li, Weihao Yuan, Yisheng He, Lingteng Qiu, Shenhao Zhu, Xiaodong Gu, Weichao Shen, Yuan Dong, Zilong Dong, Laurence T. Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.07093">LaMP: Language-Motion Pretraining for Motion Generation, Retrieval, and Captioning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Language plays a vital role in the realm of human motion. Existing methods have largely depended on CLIP text embeddings for motion generation, yet they fall short in effectively aligning language and motion due to CLIP's pretraining on static image-text pairs. This work introduces LaMP, a novel Language-Motion Pretraining model, which transitions from a language-vision to a more suitable language-motion latent space. It addresses key limitations by generating motion-informative text embeddings, significantly enhancing the relevance and semantics of generated motion sequences. With LaMP, we advance three key tasks: text-to-motion generation, motion-text retrieval, and motion captioning through aligned language-motion representation learning. For generation, we utilize LaMP to provide the text condition instead of CLIP, and an autoregressive masked prediction is designed to achieve mask modeling without rank collapse in transformers. For retrieval, motion features from LaMP's motion transformer interact with query tokens to retrieve text features from the text transformer, and vice versa. For captioning, we finetune a large language model with the language-informative motion features to develop a strong motion captioning model. In addition, we introduce the LaMP-BertScore metric to assess the alignment of generated motions with textual descriptions. Extensive experimental results on multiple datasets demonstrate substantial improvements over previous methods across all three tasks. The code of our method will be made public.
<div id='section'>Paperid: <span id='pid'>411, <a href='https://arxiv.org/pdf/2408.15185.pdf' target='_blank'>https://arxiv.org/pdf/2408.15185.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ghazal Alinezhad Noghre, Armin Danesh Pazho, Hamed Tabkhi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.15185">Human-Centric Video Anomaly Detection Through Spatio-Temporal Pose Tokenization and Transformer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video Anomaly Detection (VAD) presents a significant challenge in computer vision, particularly due to the unpredictable and infrequent nature of anomalous events, coupled with the diverse and dynamic environments in which they occur. Human-centric VAD, a specialized area within this domain, faces additional complexities, including variations in human behavior, potential biases in data, and substantial privacy concerns related to human subjects. These issues complicate the development of models that are both robust and generalizable. To address these challenges, recent advancements have focused on pose-based VAD, which leverages human pose as a high-level feature to mitigate privacy concerns, reduce appearance biases, and minimize background interference. In this paper, we introduce SPARTA, a novel transformer-based architecture designed specifically for human-centric pose-based VAD. SPARTA introduces an innovative Spatio-Temporal Pose and Relative Pose (ST-PRP) tokenization method that produces an enriched representation of human motion over time. This approach ensures that the transformer's attention mechanism captures both spatial and temporal patterns simultaneously, rather than focusing on only one aspect. The addition of the relative pose further emphasizes subtle deviations from normal human movements. The architecture's core, a novel Unified Encoder Twin Decoders (UETD) transformer, significantly improves the detection of anomalous behaviors in video data. Extensive evaluations across multiple benchmark datasets demonstrate that SPARTA consistently outperforms existing methods, establishing a new state-of-the-art in pose-based VAD.
<div id='section'>Paperid: <span id='pid'>412, <a href='https://arxiv.org/pdf/2403.08629.pdf' target='_blank'>https://arxiv.org/pdf/2403.08629.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nan Jiang, Zhiyuan Zhang, Hongjie Li, Xiaoxuan Ma, Zan Wang, Yixin Chen, Tengyu Liu, Yixin Zhu, Siyuan Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.08629">Scaling Up Dynamic Human-Scene Interaction Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Confronting the challenges of data scarcity and advanced motion synthesis in human-scene interaction modeling, we introduce the TRUMANS dataset alongside a novel HSI motion synthesis method. TRUMANS stands as the most comprehensive motion-captured HSI dataset currently available, encompassing over 15 hours of human interactions across 100 indoor scenes. It intricately captures whole-body human motions and part-level object dynamics, focusing on the realism of contact. This dataset is further scaled up by transforming physical environments into exact virtual models and applying extensive augmentations to appearance and motion for both humans and objects while maintaining interaction fidelity. Utilizing TRUMANS, we devise a diffusion-based autoregressive model that efficiently generates HSI sequences of any length, taking into account both scene context and intended actions. In experiments, our approach shows remarkable zero-shot generalizability on a range of 3D scene datasets (e.g., PROX, Replica, ScanNet, ScanNet++), producing motions that closely mimic original motion-captured sequences, as confirmed by quantitative experiments and human studies.
<div id='section'>Paperid: <span id='pid'>413, <a href='https://arxiv.org/pdf/2302.00503.pdf' target='_blank'>https://arxiv.org/pdf/2302.00503.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Savvas Papaioannou, Andrew Markham, Niki Trigoni
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.00503">Tracking People in Highly Dynamic Industrial Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To date, the majority of positioning systems have been designed to operate within environments that have long-term stable macro-structure with potential small-scale dynamics. These assumptions allow the existing positioning systems to produce and utilize stable maps. However, in highly dynamic industrial settings these assumptions are no longer valid and the task of tracking people is more challenging due to the rapid large-scale changes in structure. In this paper we propose a novel positioning system for tracking people in highly dynamic industrial environments, such as construction sites. The proposed system leverages the existing CCTV camera infrastructure found in many industrial settings along with radio and inertial sensors within each worker's mobile phone to accurately track multiple people. This multi-target multi-sensor tracking framework also allows our system to use cross-modality training in order to deal with the environment dynamics. In particular, we show how our system uses cross-modality training in order to automatically keep track environmental changes (i.e. new walls) by utilizing occlusion maps. In addition, we show how these maps can be used in conjunction with social forces to accurately predict human motion and increase the tracking accuracy. We have conducted extensive real-world experiments in a construction site showing significant accuracy improvement via cross-modality training and the use of social forces.
<div id='section'>Paperid: <span id='pid'>414, <a href='https://arxiv.org/pdf/2212.10621.pdf' target='_blank'>https://arxiv.org/pdf/2212.10621.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nan Jiang, Tengyu Liu, Zhexuan Cao, Jieming Cui, Zhiyuan zhang, Yixin Chen, He Wang, Yixin Zhu, Siyuan Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.10621">Full-Body Articulated Human-Object Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fine-grained capturing of 3D HOI boosts human activity understanding and facilitates downstream visual tasks, including action recognition, holistic scene reconstruction, and human motion synthesis. Despite its significance, existing works mostly assume that humans interact with rigid objects using only a few body parts, limiting their scope. In this paper, we address the challenging problem of f-AHOI, wherein the whole human bodies interact with articulated objects, whose parts are connected by movable joints. We present CHAIRS, a large-scale motion-captured f-AHOI dataset, consisting of 16.2 hours of versatile interactions between 46 participants and 81 articulated and rigid sittable objects. CHAIRS provides 3D meshes of both humans and articulated objects during the entire interactive process, as well as realistic and physically plausible full-body interactions. We show the value of CHAIRS with object pose estimation. By learning the geometrical relationships in HOI, we devise the very first model that leverage human pose estimation to tackle the estimation of articulated object poses and shapes during whole-body interactions. Given an image and an estimated human pose, our model first reconstructs the pose and shape of the object, then optimizes the reconstruction according to a learned interaction prior. Under both evaluation settings (e.g., with or without the knowledge of objects' geometries/structures), our model significantly outperforms baselines. We hope CHAIRS will promote the community towards finer-grained interaction understanding. We will make the data/code publicly available.
<div id='section'>Paperid: <span id='pid'>415, <a href='https://arxiv.org/pdf/2506.12851.pdf' target='_blank'>https://arxiv.org/pdf/2506.12851.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weiji Xie, Jinrui Han, Jiakun Zheng, Huanyu Li, Xinzhe Liu, Jiyuan Shi, Weinan Zhang, Chenjia Bai, Xuelong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.12851">KungfuBot: Physics-Based Humanoid Whole-Body Control for Learning Highly-Dynamic Skills</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humanoid robots are promising to acquire various skills by imitating human behaviors. However, existing algorithms are only capable of tracking smooth, low-speed human motions, even with delicate reward and curriculum design. This paper presents a physics-based humanoid control framework, aiming to master highly-dynamic human behaviors such as Kungfu and dancing through multi-steps motion processing and adaptive motion tracking. For motion processing, we design a pipeline to extract, filter out, correct, and retarget motions, while ensuring compliance with physical constraints to the maximum extent. For motion imitation, we formulate a bi-level optimization problem to dynamically adjust the tracking accuracy tolerance based on the current tracking error, creating an adaptive curriculum mechanism. We further construct an asymmetric actor-critic framework for policy training. In experiments, we train whole-body control policies to imitate a set of highly-dynamic motions. Our method achieves significantly lower tracking errors than existing approaches and is successfully deployed on the Unitree G1 robot, demonstrating stable and expressive behaviors. The project page is https://kungfu-bot.github.io.
<div id='section'>Paperid: <span id='pid'>416, <a href='https://arxiv.org/pdf/2506.10353.pdf' target='_blank'>https://arxiv.org/pdf/2506.10353.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Runqi Ouyang, Haoyun Li, Zhenyuan Zhang, Xiaofeng Wang, Zheng Zhu, Guan Huang, Xingang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.10353">Motion-R1: Chain-of-Thought Reasoning and Reinforcement Learning for Human Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in large language models, especially in natural language understanding and reasoning, have opened new possibilities for text-to-motion generation. Although existing approaches have made notable progress in semantic alignment and motion synthesis, they often rely on end-to-end mapping strategies that fail to capture deep linguistic structures and logical reasoning. Consequently, generated motions tend to lack controllability, consistency, and diversity. To address these limitations, we propose Motion-R1, a unified motion-language modeling framework that integrates a Chain-of-Thought mechanism. By explicitly decomposing complex textual instructions into logically structured action paths, Motion-R1 provides high-level semantic guidance for motion generation, significantly enhancing the model's ability to interpret and execute multi-step, long-horizon, and compositionally rich commands. To train our model, we adopt Group Relative Policy Optimization, a reinforcement learning algorithm designed for large models, which leverages motion quality feedback to optimize reasoning chains and motion synthesis jointly. Extensive experiments across multiple benchmark datasets demonstrate that Motion-R1 achieves competitive or superior performance compared to state-of-the-art methods, particularly in scenarios requiring nuanced semantic understanding and long-term temporal coherence. The code, model and data will be publicly available.
<div id='section'>Paperid: <span id='pid'>417, <a href='https://arxiv.org/pdf/2506.08840.pdf' target='_blank'>https://arxiv.org/pdf/2506.08840.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dewei Wang, Xinmiao Wang, Xinzhe Liu, Jiyuan Shi, Yingnan Zhao, Chenjia Bai, Xuelong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.08840">MoRE: Mixture of Residual Experts for Humanoid Lifelike Gaits Learning on Complex Terrains</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humanoid robots have demonstrated robust locomotion capabilities using Reinforcement Learning (RL)-based approaches. Further, to obtain human-like behaviors, existing methods integrate human motion-tracking or motion prior in the RL framework. However, these methods are limited in flat terrains with proprioception only, restricting their abilities to traverse challenging terrains with human-like gaits. In this work, we propose a novel framework using a mixture of latent residual experts with multi-discriminators to train an RL policy, which is capable of traversing complex terrains in controllable lifelike gaits with exteroception. Our two-stage training pipeline first teaches the policy to traverse complex terrains using a depth camera, and then enables gait-commanded switching between human-like gait patterns. We also design gait rewards to adjust human-like behaviors like robot base height. Simulation and real-world experiments demonstrate that our framework exhibits exceptional performance in traversing complex terrains, and achieves seamless transitions between multiple human-like gait patterns.
<div id='section'>Paperid: <span id='pid'>418, <a href='https://arxiv.org/pdf/2505.16524.pdf' target='_blank'>https://arxiv.org/pdf/2505.16524.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Huitong Yang, Zhuoxiao Chen, Fengyi Zhang, Zi Huang, Yadan Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.16524">CodeMerge: Codebook-Guided Model Merging for Robust Test-Time Adaptation in Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Maintaining robust 3D perception under dynamic and unpredictable test-time conditions remains a critical challenge for autonomous driving systems. Existing test-time adaptation (TTA) methods often fail in high-variance tasks like 3D object detection due to unstable optimization and sharp minima. While recent model merging strategies based on linear mode connectivity (LMC) offer improved stability by interpolating between fine-tuned checkpoints, they are computationally expensive, requiring repeated checkpoint access and multiple forward passes. In this paper, we introduce CodeMerge, a lightweight and scalable model merging framework that bypasses these limitations by operating in a compact latent space. Instead of loading full models, CodeMerge represents each checkpoint with a low-dimensional fingerprint derived from the source model's penultimate features and constructs a key-value codebook. We compute merging coefficients using ridge leverage scores on these fingerprints, enabling efficient model composition without compromising adaptation quality. Our method achieves strong performance across challenging benchmarks, improving end-to-end 3D detection 14.9% NDS on nuScenes-C and LiDAR-based detection by over 7.6% mAP on nuScenes-to-KITTI, while benefiting downstream tasks such as online mapping, motion prediction and planning even without training. Code and pretrained models are released in the supplementary material.
<div id='section'>Paperid: <span id='pid'>419, <a href='https://arxiv.org/pdf/2503.24026.pdf' target='_blank'>https://arxiv.org/pdf/2503.24026.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Boyuan Wang, Xiaofeng Wang, Chaojun Ni, Guosheng Zhao, Zhiqin Yang, Zheng Zhu, Muyang Zhang, Yukun Zhou, Xinze Chen, Guan Huang, Lihong Liu, Xingang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.24026">HumanDreamer: Generating Controllable Human-Motion Videos via Decoupled Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human-motion video generation has been a challenging task, primarily due to the difficulty inherent in learning human body movements. While some approaches have attempted to drive human-centric video generation explicitly through pose control, these methods typically rely on poses derived from existing videos, thereby lacking flexibility. To address this, we propose HumanDreamer, a decoupled human video generation framework that first generates diverse poses from text prompts and then leverages these poses to generate human-motion videos. Specifically, we propose MotionVid, the largest dataset for human-motion pose generation. Based on the dataset, we present MotionDiT, which is trained to generate structured human-motion poses from text prompts. Besides, a novel LAMA loss is introduced, which together contribute to a significant improvement in FID by 62.4%, along with respective enhancements in R-precision for top1, top2, and top3 by 41.8%, 26.3%, and 18.3%, thereby advancing both the Text-to-Pose control accuracy and FID metrics. Our experiments across various Pose-to-Video baselines demonstrate that the poses generated by our method can produce diverse and high-quality human-motion videos. Furthermore, our model can facilitate other downstream tasks, such as pose sequence prediction and 2D-3D motion lifting.
<div id='section'>Paperid: <span id='pid'>420, <a href='https://arxiv.org/pdf/2501.16551.pdf' target='_blank'>https://arxiv.org/pdf/2501.16551.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhongyu Jiang, Wenhao Chai, Zhuoran Zhou, Cheng-Yen Yang, Hsiang-Wei Huang, Jenq-Neng Hwang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.16551">PackDiT: Joint Human Motion and Text Generation via Mutual Prompting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion generation has advanced markedly with the advent of diffusion models. Most recent studies have concentrated on generating motion sequences based on text prompts, commonly referred to as text-to-motion generation. However, the bidirectional generation of motion and text, enabling tasks such as motion-to-text alongside text-to-motion, has been largely unexplored. This capability is essential for aligning diverse modalities and supports unconditional generation. In this paper, we introduce PackDiT, the first diffusion-based generative model capable of performing various tasks simultaneously, including motion generation, motion prediction, text generation, text-to-motion, motion-to-text, and joint motion-text generation. Our core innovation leverages mutual blocks to integrate multiple diffusion transformers (DiTs) across different modalities seamlessly. We train PackDiT on the HumanML3D dataset, achieving state-of-the-art text-to-motion performance with an FID score of 0.106, along with superior results in motion prediction and in-between tasks. Our experiments further demonstrate that diffusion models are effective for motion-to-text generation, achieving performance comparable to that of autoregressive models.
<div id='section'>Paperid: <span id='pid'>421, <a href='https://arxiv.org/pdf/2412.06146.pdf' target='_blank'>https://arxiv.org/pdf/2412.06146.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinpeng Liu, Junxuan Liang, Chenshuo Zhang, Zixuan Cai, Cewu Lu, Yong-Lu Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.06146">Homogeneous Dynamics Space for Heterogeneous Humans</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Analyses of human motion kinematics have achieved tremendous advances. However, the production mechanism, known as human dynamics, is still undercovered. In this paper, we aim to push data-driven human dynamics understanding forward. We identify a major obstacle to this as the heterogeneity of existing human motion understanding efforts. Specifically, heterogeneity exists in not only the diverse kinematics representations and hierarchical dynamics representations but also in the data from different domains, namely biomechanics and reinforcement learning. With an in-depth analysis of the existing heterogeneity, we propose to emphasize the beneath homogeneity: all of them represent the homogeneous fact of human motion, though from different perspectives. Given this, we propose Homogeneous Dynamics Space (HDyS) as a fundamental space for human dynamics by aggregating heterogeneous data and training a homogeneous latent space with inspiration from the inverse-forward dynamics procedure. Leveraging the heterogeneous representations and datasets, HDyS achieves decent mapping between human kinematics and dynamics. We demonstrate the feasibility of HDyS with extensive experiments and applications. The project page is https://foruck.github.io/HDyS.
<div id='section'>Paperid: <span id='pid'>422, <a href='https://arxiv.org/pdf/2411.16657.pdf' target='_blank'>https://arxiv.org/pdf/2411.16657.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zun Wang, Jialu Li, Han Lin, Jaehong Yoon, Mohit Bansal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.16657">DreamRunner: Fine-Grained Compositional Story-to-Video Generation with Retrieval-Augmented Motion Adaptation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Storytelling video generation (SVG) aims to produce coherent and visually rich multi-scene videos that follow a structured narrative. Existing methods primarily employ LLM for high-level planning to decompose a story into scene-level descriptions, which are then independently generated and stitched together. However, these approaches struggle with generating high-quality videos aligned with the complex single-scene description, as visualizing such complex description involves coherent composition of multiple characters and events, complex motion synthesis and muti-character customization. To address these challenges, we propose DreamRunner, a novel story-to-video generation method: First, we structure the input script using a large language model (LLM) to facilitate both coarse-grained scene planning as well as fine-grained object-level layout and motion planning. Next, DreamRunner presents retrieval-augmented test-time adaptation to capture target motion priors for objects in each scene, supporting diverse motion customization based on retrieved videos, thus facilitating the generation of new videos with complex, scripted motions. Lastly, we propose a novel spatial-temporal region-based 3D attention and prior injection module SR3AI for fine-grained object-motion binding and frame-by-frame semantic control. We compare DreamRunner with various SVG baselines, demonstrating state-of-the-art performance in character consistency, text alignment, and smooth transitions. Additionally, DreamRunner exhibits strong fine-grained condition-following ability in compositional text-to-video generation, significantly outperforming baselines on T2V-ComBench. Finally, we validate DreamRunner's robust ability to generate multi-object interactions with qualitative examples.
<div id='section'>Paperid: <span id='pid'>423, <a href='https://arxiv.org/pdf/2410.17610.pdf' target='_blank'>https://arxiv.org/pdf/2410.17610.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinpeng Liu, Junxuan Liang, Zili Lin, Haowen Hou, Yong-Lu Li, Cewu Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.17610">ImDy: Human Inverse Dynamics from Imitated Observations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Inverse dynamics (ID), which aims at reproducing the driven torques from human kinematic observations, has been a critical tool for gait analysis. However, it is hindered from wider application to general motion due to its limited scalability. Conventional optimization-based ID requires expensive laboratory setups, restricting its availability. To alleviate this problem, we propose to exploit the recently progressive human motion imitation algorithms to learn human inverse dynamics in a data-driven manner. The key insight is that the human ID knowledge is implicitly possessed by motion imitators, though not directly applicable. In light of this, we devise an efficient data collection pipeline with state-of-the-art motion imitation algorithms and physics simulators, resulting in a large-scale human inverse dynamics benchmark as Imitated Dynamics (ImDy). ImDy contains over 150 hours of motion with joint torque and full-body ground reaction force data. With ImDy, we train a data-driven human inverse dynamics solver ImDyS(olver) in a fully supervised manner, which conducts ID and ground reaction force estimation simultaneously. Experiments on ImDy and real-world data demonstrate the impressive competency of ImDyS in human inverse dynamics and ground reaction force estimation. Moreover, the potential of ImDy(-S) as a fundamental motion analysis tool is exhibited with downstream applications. The project page is https://foruck.github.io/ImDy/.
<div id='section'>Paperid: <span id='pid'>424, <a href='https://arxiv.org/pdf/2403.10826.pdf' target='_blank'>https://arxiv.org/pdf/2403.10826.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hsiang-Wei Huang, Cheng-Yen Yang, Wenhao Chai, Zhongyu Jiang, Jenq-Neng Hwang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.10826">MambaMOT: State-Space Model as Motion Predictor for Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the field of multi-object tracking (MOT), traditional methods often rely on the Kalman filter for motion prediction, leveraging its strengths in linear motion scenarios. However, the inherent limitations of these methods become evident when confronted with complex, nonlinear motions and occlusions prevalent in dynamic environments like sports and dance. This paper explores the possibilities of replacing the Kalman filter with a learning-based motion model that effectively enhances tracking accuracy and adaptability beyond the constraints of Kalman filter-based tracker. In this paper, our proposed method MambaMOT and MambaMOT+, demonstrate advanced performance on challenging MOT datasets such as DanceTrack and SportsMOT, showcasing their ability to handle intricate, non-linear motion patterns and frequent occlusions more effectively than traditional methods.
<div id='section'>Paperid: <span id='pid'>425, <a href='https://arxiv.org/pdf/2310.04189.pdf' target='_blank'>https://arxiv.org/pdf/2310.04189.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinpeng Liu, Yong-Lu Li, Ailing Zeng, Zizheng Zhou, Yang You, Cewu Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.04189">Bridging the Gap between Human Motion and Action Semantics via Kinematic Phrases</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion understanding aims to establish a reliable mapping between motion and action semantics, while it is a challenging many-to-many problem. An abstract action semantic (i.e., walk forwards) could be conveyed by perceptually diverse motions (walking with arms up or swinging). In contrast, a motion could carry different semantics w.r.t. its context and intention. This makes an elegant mapping between them difficult. Previous attempts adopted direct-mapping paradigms with limited reliability. Also, current automatic metrics fail to provide reliable assessments of the consistency between motions and action semantics. We identify the source of these problems as the significant gap between the two modalities. To alleviate this gap, we propose Kinematic Phrases (KP) that take the objective kinematic facts of human motion with proper abstraction, interpretability, and generality. Based on KP, we can unify a motion knowledge base and build a motion understanding system. Meanwhile, KP can be automatically converted from motions to text descriptions with no subjective bias, inspiring Kinematic Prompt Generation (KPG) as a novel white-box motion generation benchmark. In extensive experiments, our approach shows superiority over other methods. Our project is available at https://foruck.github.io/KP/.
<div id='section'>Paperid: <span id='pid'>426, <a href='https://arxiv.org/pdf/2309.15405.pdf' target='_blank'>https://arxiv.org/pdf/2309.15405.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Payam Nourizadeh, Michael Milford, Tobias Fischer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.15405">Teach and Repeat Navigation: A Robust Control Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robot navigation requires an autonomy pipeline that is robust to environmental changes and effective in varying conditions. Teach and Repeat (T&R) navigation has shown high performance in autonomous repeated tasks under challenging circumstances, but research within T&R has predominantly focused on motion planning as opposed to motion control. In this paper, we propose a novel T&R system based on a robust motion control technique for a skid-steering mobile robot using sliding-mode control that effectively handles uncertainties that are particularly pronounced in the T&R task, where sensor noises, parametric uncertainties, and wheel-terrain interaction are common challenges. We first theoretically demonstrate that the proposed T&R system is globally stable and robust while considering the uncertainties of the closed-loop system. When deployed on a Clearpath Jackal robot, we then show the global stability of the proposed system in both indoor and outdoor environments covering different terrains, outperforming previous state-of-the-art methods in terms of mean average trajectory error and stability in these challenging environments. This paper makes an important step towards long-term autonomous T&R navigation with ensured safety guarantees.
<div id='section'>Paperid: <span id='pid'>427, <a href='https://arxiv.org/pdf/2210.11817.pdf' target='_blank'>https://arxiv.org/pdf/2210.11817.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingqi Li, Jiaqi Gao, Yuzhen Zhang, Hongming Shan, Junping Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.11817">Motion Matters: A Novel Motion Modeling For Cross-View Gait Feature Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As a unique biometric that can be perceived at a distance, gait has broad applications in person authentication, social security, and so on. Existing gait recognition methods suffer from changes in viewpoint and clothing and barely consider extracting diverse motion features, a fundamental characteristic in gaits, from gait sequences. This paper proposes a novel motion modeling method to extract the discriminative and robust representation. Specifically, we first extract the motion features from the encoded motion sequences in the shallow layer. Then we continuously enhance the motion feature in deep layers. This motion modeling approach is independent of mainstream work in building network architectures. As a result, one can apply this motion modeling method to any backbone to improve gait recognition performance. In this paper, we combine motion modeling with one commonly used backbone~(GaitGL) as GaitGL-M to illustrate motion modeling. Extensive experimental results on two commonly-used cross-view gait datasets demonstrate the superior performance of GaitGL-M over existing state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>428, <a href='https://arxiv.org/pdf/2509.15536.pdf' target='_blank'>https://arxiv.org/pdf/2509.15536.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sen Wang, Jingyi Tian, Le Wang, Zhimin Liao, Jiayi Li, Huaiyi Dong, Kun Xia, Sanping Zhou, Wei Tang, Hua Gang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15536">SAMPO:Scale-wise Autoregression with Motion PrOmpt for generative world models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>World models allow agents to simulate the consequences of actions in imagined environments for planning, control, and long-horizon decision-making. However, existing autoregressive world models struggle with visually coherent predictions due to disrupted spatial structure, inefficient decoding, and inadequate motion modeling. In response, we propose \textbf{S}cale-wise \textbf{A}utoregression with \textbf{M}otion \textbf{P}r\textbf{O}mpt (\textbf{SAMPO}), a hybrid framework that combines visual autoregressive modeling for intra-frame generation with causal modeling for next-frame generation. Specifically, SAMPO integrates temporal causal decoding with bidirectional spatial attention, which preserves spatial locality and supports parallel decoding within each scale. This design significantly enhances both temporal consistency and rollout efficiency. To further improve dynamic scene understanding, we devise an asymmetric multi-scale tokenizer that preserves spatial details in observed frames and extracts compact dynamic representations for future frames, optimizing both memory usage and model performance. Additionally, we introduce a trajectory-aware motion prompt module that injects spatiotemporal cues about object and robot trajectories, focusing attention on dynamic regions and improving temporal consistency and physical realism. Extensive experiments show that SAMPO achieves competitive performance in action-conditioned video prediction and model-based control, improving generation quality with 4.4$\times$ faster inference. We also evaluate SAMPO's zero-shot generalization and scaling behavior, demonstrating its ability to generalize to unseen tasks and benefit from larger model sizes.
<div id='section'>Paperid: <span id='pid'>429, <a href='https://arxiv.org/pdf/2508.20604.pdf' target='_blank'>https://arxiv.org/pdf/2508.20604.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zheng Qin, Yabing Wang, Minghui Yang, Sanping Zhou, Ming Yang, Le Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.20604">Embracing Aleatoric Uncertainty: Generating Diverse 3D Human Motion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating 3D human motions from text is a challenging yet valuable task. The key aspects of this task are ensuring text-motion consistency and achieving generation diversity. Although recent advancements have enabled the generation of precise and high-quality human motions from text, achieving diversity in the generated motions remains a significant challenge. In this paper, we aim to overcome the above challenge by designing a simple yet effective text-to-motion generation method, \textit{i.e.}, Diverse-T2M. Our method introduces uncertainty into the generation process, enabling the generation of highly diverse motions while preserving the semantic consistency of the text. Specifically, we propose a novel perspective that utilizes noise signals as carriers of diversity information in transformer-based methods, facilitating a explicit modeling of uncertainty. Moreover, we construct a latent space where text is projected into a continuous representation, instead of a rigid one-to-one mapping, and integrate a latent space sampler to introduce stochastic sampling into the generation process, thereby enhancing the diversity and uncertainty of the outputs. Our results on text-to-motion generation benchmark datasets~(HumanML3D and KIT-ML) demonstrate that our method significantly enhances diversity while maintaining state-of-the-art performance in text consistency.
<div id='section'>Paperid: <span id='pid'>430, <a href='https://arxiv.org/pdf/2508.10881.pdf' target='_blank'>https://arxiv.org/pdf/2508.10881.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lingen Li, Guangzhi Wang, Zhaoyang Zhang, Yaowei Li, Xiaoyu Li, Qi Dou, Jinwei Gu, Tianfan Xue, Ying Shan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10881">ToonComposer: Streamlining Cartoon Production with Generative Post-Keyframing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Traditional cartoon and anime production involves keyframing, inbetweening, and colorization stages, which require intensive manual effort. Despite recent advances in AI, existing methods often handle these stages separately, leading to error accumulation and artifacts. For instance, inbetweening approaches struggle with large motions, while colorization methods require dense per-frame sketches. To address this, we introduce ToonComposer, a generative model that unifies inbetweening and colorization into a single post-keyframing stage. ToonComposer employs a sparse sketch injection mechanism to provide precise control using keyframe sketches. Additionally, it uses a cartoon adaptation method with the spatial low-rank adapter to tailor a modern video foundation model to the cartoon domain while keeping its temporal prior intact. Requiring as few as a single sketch and a colored reference frame, ToonComposer excels with sparse inputs, while also supporting multiple sketches at any temporal location for more precise motion control. This dual capability reduces manual workload and improves flexibility, empowering artists in real-world scenarios. To evaluate our model, we further created PKBench, a benchmark featuring human-drawn sketches that simulate real-world use cases. Our evaluation demonstrates that ToonComposer outperforms existing methods in visual quality, motion consistency, and production efficiency, offering a superior and more flexible solution for AI-assisted cartoon production.
<div id='section'>Paperid: <span id='pid'>431, <a href='https://arxiv.org/pdf/2508.10297.pdf' target='_blank'>https://arxiv.org/pdf/2508.10297.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiyi Ma, Yuanzhi Liang, Xiu Li, Chi Zhang, Xuelong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10297">InterSyn: Interleaved Learning for Dynamic Motion Synthesis in the Wild</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present Interleaved Learning for Motion Synthesis (InterSyn), a novel framework that targets the generation of realistic interaction motions by learning from integrated motions that consider both solo and multi-person dynamics. Unlike previous methods that treat these components separately, InterSyn employs an interleaved learning strategy to capture the natural, dynamic interactions and nuanced coordination inherent in real-world scenarios. Our framework comprises two key modules: the Interleaved Interaction Synthesis (INS) module, which jointly models solo and interactive behaviors in a unified paradigm from a first-person perspective to support multiple character interactions, and the Relative Coordination Refinement (REC) module, which refines mutual dynamics and ensures synchronized motions among characters. Experimental results show that the motion sequences generated by InterSyn exhibit higher text-to-motion alignment and improved diversity compared with recent methods, setting a new benchmark for robust and natural motion synthesis. Additionally, our code will be open-sourced in the future to promote further research and development in this area.
<div id='section'>Paperid: <span id='pid'>432, <a href='https://arxiv.org/pdf/2508.06205.pdf' target='_blank'>https://arxiv.org/pdf/2508.06205.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruiyan Wang, Lin Zuo, Zonghao Lin, Qiang Wang, Zhengxue Cheng, Rong Xie, Jun Ling, Li Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.06205">PA-HOI: A Physics-Aware Human and Object Interaction Dataset</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Human-Object Interaction (HOI) task explores the dynamic interactions between humans and objects in physical environments, providing essential biomechanical and cognitive-behavioral foundations for fields such as robotics, virtual reality, and human-computer interaction. However, existing HOI data sets focus on details of affordance, often neglecting the influence of physical properties of objects on human long-term motion. To bridge this gap, we introduce the PA-HOI Motion Capture dataset, which highlights the impact of objects' physical attributes on human motion dynamics, including human posture, moving velocity, and other motion characteristics. The dataset comprises 562 motion sequences of human-object interactions, with each sequence performed by subjects of different genders interacting with 35 3D objects that vary in size, shape, and weight. This dataset stands out by significantly extending the scope of existing ones for understanding how the physical attributes of different objects influence human posture, speed, motion scale, and interacting strategies. We further demonstrate the applicability of the PA-HOI dataset by integrating it with existing motion generation methods, validating its capacity to transfer realistic physical awareness.
<div id='section'>Paperid: <span id='pid'>433, <a href='https://arxiv.org/pdf/2506.12723.pdf' target='_blank'>https://arxiv.org/pdf/2506.12723.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ye Li, Yuan Meng, Zewen Sun, Kangye Ji, Chen Tang, Jiajun Fan, Xinzhu Ma, Shutao Xia, Zhi Wang, Wenwu Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.12723">SP-VLA: A Joint Model Scheduling and Token Pruning Approach for VLA Model Acceleration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models have attracted increasing attention for their strong control capabilities. However, their high computational cost and low execution frequency hinder their suitability for real-time tasks such as robotic manipulation and autonomous navigation. Existing VLA acceleration methods primarily focus on structural optimization, overlooking the fact that these models operate in sequential decision-making environments. As a result, temporal redundancy in sequential action generation and spatial redundancy in visual input remain unaddressed. To this end, we propose SP-VLA, a unified framework that accelerates VLA models by jointly scheduling models and pruning tokens. Specifically, we design an action-aware model scheduling mechanism that reduces temporal redundancy by dynamically switching between VLA model and a lightweight generator. Inspired by the human motion pattern of focusing on key decision points while relying on intuition for other actions, we categorize VLA actions into deliberative and intuitive, assigning the former to the VLA model and the latter to the lightweight generator, enabling frequency-adaptive execution through collaborative model scheduling. To address spatial redundancy, we further develop a spatio-semantic dual-aware token pruning method. Tokens are classified into spatial and semantic types and pruned based on their dual-aware importance to accelerate VLA inference. These two mechanisms work jointly to guide the VLA in focusing on critical actions and salient visual information, achieving effective acceleration while maintaining high accuracy. Experimental results demonstrate that our method achieves up to 1.5$\times$ acceleration with less than 3% drop in accuracy, outperforming existing approaches in multiple tasks.
<div id='section'>Paperid: <span id='pid'>434, <a href='https://arxiv.org/pdf/2506.12723.pdf' target='_blank'>https://arxiv.org/pdf/2506.12723.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ye Li, Yuan Meng, Zewen Sun, Kangye Ji, Chen Tang, Jiajun Fan, Xinzhu Ma, Shutao Xia, Zhi Wang, Wenwu Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.12723">SP-VLA: A Joint Model Scheduling and Token Pruning Approach for VLA Model Acceleration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models have attracted increasing attention for their strong control capabilities. However, their high computational cost and low execution frequency hinder their suitability for real-time tasks such as robotic manipulation and autonomous navigation. Existing VLA acceleration methods primarily focus on structural optimization, overlooking the fact that these models operate in sequential decision-making environments. As a result, temporal redundancy in sequential action generation and spatial redundancy in visual input remain unaddressed. To this end, we propose SP-VLA, a unified framework that accelerates VLA models by jointly scheduling models and pruning tokens. Specifically, we design an action-aware model scheduling mechanism that reduces temporal redundancy by dynamically switching between VLA model and a lightweight generator. Inspired by the human motion pattern of focusing on key decision points while relying on intuition for other actions, we categorize VLA actions into deliberative and intuitive, assigning the former to the VLA model and the latter to the lightweight generator, enabling frequency-adaptive execution through collaborative model scheduling. To address spatial redundancy, we further develop a spatio-semantic dual-aware token pruning method. Tokens are classified into spatial and semantic types and pruned based on their dual-aware importance to accelerate VLA inference. These two mechanisms work jointly to guide the VLA in focusing on critical actions and salient visual information, achieving effective acceleration while maintaining high accuracy. Extensive experiments show that our method achieves 1.5$\times$ lossless acceleration in LIBERO and 2.4$\times$ in SimplerEnv, with up to 6% average performance gain. Inference frequency and latency improve by 2.2$\times$ in SimplerEnv and 1.4$\times$ in LIBERO.
<div id='section'>Paperid: <span id='pid'>435, <a href='https://arxiv.org/pdf/2502.04847.pdf' target='_blank'>https://arxiv.org/pdf/2502.04847.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qijun Gan, Yi Ren, Chen Zhang, Zhenhui Ye, Pan Xie, Xiang Yin, Zehuan Yuan, Bingyue Peng, Jianke Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.04847">HumanDiT: Pose-Guided Diffusion Transformer for Long-form Human Motion Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion video generation has advanced significantly, while existing methods still struggle with accurately rendering detailed body parts like hands and faces, especially in long sequences and intricate motions. Current approaches also rely on fixed resolution and struggle to maintain visual consistency. To address these limitations, we propose HumanDiT, a pose-guided Diffusion Transformer (DiT)-based framework trained on a large and wild dataset containing 14,000 hours of high-quality video to produce high-fidelity videos with fine-grained body rendering. Specifically, (i) HumanDiT, built on DiT, supports numerous video resolutions and variable sequence lengths, facilitating learning for long-sequence video generation; (ii) we introduce a prefix-latent reference strategy to maintain personalized characteristics across extended sequences. Furthermore, during inference, HumanDiT leverages Keypoint-DiT to generate subsequent pose sequences, facilitating video continuation from static images or existing videos. It also utilizes a Pose Adapter to enable pose transfer with given sequences. Extensive experiments demonstrate its superior performance in generating long-form, pose-accurate videos across diverse scenarios.
<div id='section'>Paperid: <span id='pid'>436, <a href='https://arxiv.org/pdf/2502.03449.pdf' target='_blank'>https://arxiv.org/pdf/2502.03449.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuan Li, Chang Yu, Wenxin Du, Ying Jiang, Tianyi Xie, Yunuo Chen, Yin Yang, Chenfanfu Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.03449">Dress-1-to-3: Single Image to Simulation-Ready 3D Outfit with Diffusion Prior and Differentiable Physics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in large models have significantly advanced image-to-3D reconstruction. However, the generated models are often fused into a single piece, limiting their applicability in downstream tasks. This paper focuses on 3D garment generation, a key area for applications like virtual try-on with dynamic garment animations, which require garments to be separable and simulation-ready. We introduce Dress-1-to-3, a novel pipeline that reconstructs physics-plausible, simulation-ready separated garments with sewing patterns and humans from an in-the-wild image. Starting with the image, our approach combines a pre-trained image-to-sewing pattern generation model for creating coarse sewing patterns with a pre-trained multi-view diffusion model to produce multi-view images. The sewing pattern is further refined using a differentiable garment simulator based on the generated multi-view images. Versatile experiments demonstrate that our optimization approach substantially enhances the geometric alignment of the reconstructed 3D garments and humans with the input image. Furthermore, by integrating a texture generation module and a human motion generation module, we produce customized physics-plausible and realistic dynamic garment demonstrations. Project page: https://dress-1-to-3.github.io/
<div id='section'>Paperid: <span id='pid'>437, <a href='https://arxiv.org/pdf/2501.05098.pdf' target='_blank'>https://arxiv.org/pdf/2501.05098.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuhong Zhang, Jing Lin, Ailing Zeng, Guanlin Wu, Shunlin Lu, Yurong Fu, Yuanhao Cai, Ruimao Zhang, Haoqian Wang, Lei Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.05098">Motion-X++: A Large-Scale Multimodal 3D Whole-body Human Motion Dataset</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we introduce Motion-X++, a large-scale multimodal 3D expressive whole-body human motion dataset. Existing motion datasets predominantly capture body-only poses, lacking facial expressions, hand gestures, and fine-grained pose descriptions, and are typically limited to lab settings with manually labeled text descriptions, thereby restricting their scalability. To address this issue, we develop a scalable annotation pipeline that can automatically capture 3D whole-body human motion and comprehensive textural labels from RGB videos and build the Motion-X dataset comprising 81.1K text-motion pairs. Furthermore, we extend Motion-X into Motion-X++ by improving the annotation pipeline, introducing more data modalities, and scaling up the data quantities. Motion-X++ provides 19.5M 3D whole-body pose annotations covering 120.5K motion sequences from massive scenes, 80.8K RGB videos, 45.3K audios, 19.5M frame-level whole-body pose descriptions, and 120.5K sequence-level semantic labels. Comprehensive experiments validate the accuracy of our annotation pipeline and highlight Motion-X++'s significant benefits for generating expressive, precise, and natural motion with paired multimodal labels supporting several downstream tasks, including text-driven whole-body motion generation,audio-driven motion generation, 3D whole-body human mesh recovery, and 2D whole-body keypoints estimation, etc.
<div id='section'>Paperid: <span id='pid'>438, <a href='https://arxiv.org/pdf/2405.20340.pdf' target='_blank'>https://arxiv.org/pdf/2405.20340.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ling-Hao Chen, Shunlin Lu, Ailing Zeng, Hao Zhang, Benyou Wang, Ruimao Zhang, Lei Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.20340">MotionLLM: Understanding Human Behaviors from Human Motions and Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study delves into the realm of multi-modality (i.e., video and motion modalities) human behavior understanding by leveraging the powerful capabilities of Large Language Models (LLMs). Diverging from recent LLMs designed for video-only or motion-only understanding, we argue that understanding human behavior necessitates joint modeling from both videos and motion sequences (e.g., SMPL sequences) to capture nuanced body part dynamics and semantics effectively. In light of this, we present MotionLLM, a straightforward yet effective framework for human motion understanding, captioning, and reasoning. Specifically, MotionLLM adopts a unified video-motion training strategy that leverages the complementary advantages of existing coarse video-text data and fine-grained motion-text data to glean rich spatial-temporal insights. Furthermore, we collect a substantial dataset, MoVid, comprising diverse videos, motions, captions, and instructions. Additionally, we propose the MoVid-Bench, with carefully manual annotations, for better evaluation of human behavior understanding on video and motion. Extensive experiments show the superiority of MotionLLM in the caption, spatial-temporal comprehension, and reasoning ability.
<div id='section'>Paperid: <span id='pid'>439, <a href='https://arxiv.org/pdf/2405.15758.pdf' target='_blank'>https://arxiv.org/pdf/2405.15758.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuchi Wang, Junliang Guo, Jianhong Bai, Runyi Yu, Tianyu He, Xu Tan, Xu Sun, Jiang Bian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.15758">InstructAvatar: Text-Guided Emotion and Motion Control for Avatar Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent talking avatar generation models have made strides in achieving realistic and accurate lip synchronization with the audio, but often fall short in controlling and conveying detailed expressions and emotions of the avatar, making the generated video less vivid and controllable. In this paper, we propose a novel text-guided approach for generating emotionally expressive 2D avatars, offering fine-grained control, improved interactivity, and generalizability to the resulting video. Our framework, named InstructAvatar, leverages a natural language interface to control the emotion as well as the facial motion of avatars. Technically, we design an automatic annotation pipeline to construct an instruction-video paired training dataset, equipped with a novel two-branch diffusion-based generator to predict avatars with audio and text instructions at the same time. Experimental results demonstrate that InstructAvatar produces results that align well with both conditions, and outperforms existing methods in fine-grained emotion control, lip-sync quality, and naturalness. Our project page is https://wangyuchi369.github.io/InstructAvatar/.
<div id='section'>Paperid: <span id='pid'>440, <a href='https://arxiv.org/pdf/2403.11589.pdf' target='_blank'>https://arxiv.org/pdf/2403.11589.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yujiao Jiang, Qingmin Liao, Xiaoyu Li, Li Ma, Qi Zhang, Chaopeng Zhang, Zongqing Lu, Ying Shan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.11589">UV Gaussians: Joint Learning of Mesh Deformation and Gaussian Textures for Human Avatar Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reconstructing photo-realistic drivable human avatars from multi-view image sequences has been a popular and challenging topic in the field of computer vision and graphics. While existing NeRF-based methods can achieve high-quality novel view rendering of human models, both training and inference processes are time-consuming. Recent approaches have utilized 3D Gaussians to represent the human body, enabling faster training and rendering. However, they undermine the importance of the mesh guidance and directly predict Gaussians in 3D space with coarse mesh guidance. This hinders the learning procedure of the Gaussians and tends to produce blurry textures. Therefore, we propose UV Gaussians, which models the 3D human body by jointly learning mesh deformations and 2D UV-space Gaussian textures. We utilize the embedding of UV map to learn Gaussian textures in 2D space, leveraging the capabilities of powerful 2D networks to extract features. Additionally, through an independent Mesh network, we optimize pose-dependent geometric deformations, thereby guiding Gaussian rendering and significantly enhancing rendering quality. We collect and process a new dataset of human motion, which includes multi-view images, scanned models, parametric model registration, and corresponding texture maps. Experimental results demonstrate that our method achieves state-of-the-art synthesis of novel view and novel pose. The code and data will be made available on the homepage https://alex-jyj.github.io/UV-Gaussians/ once the paper is accepted.
<div id='section'>Paperid: <span id='pid'>441, <a href='https://arxiv.org/pdf/2401.15318.pdf' target='_blank'>https://arxiv.org/pdf/2401.15318.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yutao Feng, Xiang Feng, Yintong Shang, Ying Jiang, Chang Yu, Zeshun Zong, Tianjia Shao, Hongzhi Wu, Kun Zhou, Chenfanfu Jiang, Yin Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.15318">Gaussian Splashing: Unified Particles for Versatile Motion Synthesis and Rendering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We demonstrate the feasibility of integrating physics-based animations of solids and fluids with 3D Gaussian Splatting (3DGS) to create novel effects in virtual scenes reconstructed using 3DGS. Leveraging the coherence of the Gaussian Splatting and Position-Based Dynamics (PBD) in the underlying representation, we manage rendering, view synthesis, and the dynamics of solids and fluids in a cohesive manner. Similar to GaussianShader, we enhance each Gaussian kernel with an added normal, aligning the kernel's orientation with the surface normal to refine the PBD simulation. This approach effectively eliminates spiky noises that arise from rotational deformation in solids. It also allows us to integrate physically based rendering to augment the dynamic surface reflections on fluids. Consequently, our framework is capable of realistically reproducing surface highlights on dynamic fluids and facilitating interactions between scene objects and fluids from new views. For more information, please visit our project page at \url{https://gaussiansplashing.github.io/}.
<div id='section'>Paperid: <span id='pid'>442, <a href='https://arxiv.org/pdf/2312.12763.pdf' target='_blank'>https://arxiv.org/pdf/2312.12763.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Beibei Jing, Youjia Zhang, Zikai Song, Junqing Yu, Wei Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.12763">AMD:Anatomical Motion Diffusion with Interpretable Motion Decomposition and Fusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating realistic human motion sequences from text descriptions is a challenging task that requires capturing the rich expressiveness of both natural language and human motion.Recent advances in diffusion models have enabled significant progress in human motion synthesis.However, existing methods struggle to handle text inputs that describe complex or long motions.In this paper, we propose the Adaptable Motion Diffusion (AMD) model, which leverages a Large Language Model (LLM) to parse the input text into a sequence of concise and interpretable anatomical scripts that correspond to the target motion.This process exploits the LLM's ability to provide anatomical guidance for complex motion synthesis.We then devise a two-branch fusion scheme that balances the influence of the input text and the anatomical scripts on the inverse diffusion process, which adaptively ensures the semantic fidelity and diversity of the synthesized motion.Our method can effectively handle texts with complex or long motion descriptions, where existing methods often fail. Experiments on datasets with relatively more complex motions, such as CLCD1 and CLCD2, demonstrate that our AMD significantly outperforms existing state-of-the-art models.
<div id='section'>Paperid: <span id='pid'>443, <a href='https://arxiv.org/pdf/2311.12198.pdf' target='_blank'>https://arxiv.org/pdf/2311.12198.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianyi Xie, Zeshun Zong, Yuxing Qiu, Xuan Li, Yutao Feng, Yin Yang, Chenfanfu Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.12198">PhysGaussian: Physics-Integrated 3D Gaussians for Generative Dynamics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce PhysGaussian, a new method that seamlessly integrates physically grounded Newtonian dynamics within 3D Gaussians to achieve high-quality novel motion synthesis. Employing a custom Material Point Method (MPM), our approach enriches 3D Gaussian kernels with physically meaningful kinematic deformation and mechanical stress attributes, all evolved in line with continuum mechanics principles. A defining characteristic of our method is the seamless integration between physical simulation and visual rendering: both components utilize the same 3D Gaussian kernels as their discrete representations. This negates the necessity for triangle/tetrahedron meshing, marching cubes, "cage meshes," or any other geometry embedding, highlighting the principle of "what you see is what you simulate (WS$^2$)." Our method demonstrates exceptional versatility across a wide variety of materials--including elastic entities, metals, non-Newtonian fluids, and granular materials--showcasing its strong capabilities in creating diverse visual content with novel viewpoints and movements. Our project page is at: https://xpandora.github.io/PhysGaussian/
<div id='section'>Paperid: <span id='pid'>444, <a href='https://arxiv.org/pdf/2303.10404.pdf' target='_blank'>https://arxiv.org/pdf/2303.10404.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zheng Qin, Sanping Zhou, Le Wang, Jinghai Duan, Gang Hua, Wei Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.10404">MotionTrack: Learning Robust Short-term and Long-term Motions for Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The main challenge of Multi-Object Tracking~(MOT) lies in maintaining a continuous trajectory for each target. Existing methods often learn reliable motion patterns to match the same target between adjacent frames and discriminative appearance features to re-identify the lost targets after a long period. However, the reliability of motion prediction and the discriminability of appearances can be easily hurt by dense crowds and extreme occlusions in the tracking process. In this paper, we propose a simple yet effective multi-object tracker, i.e., MotionTrack, which learns robust short-term and long-term motions in a unified framework to associate trajectories from a short to long range. For dense crowds, we design a novel Interaction Module to learn interaction-aware motions from short-term trajectories, which can estimate the complex movement of each target. For extreme occlusions, we build a novel Refind Module to learn reliable long-term motions from the target's history trajectory, which can link the interrupted trajectory with its corresponding detection. Our Interaction Module and Refind Module are embedded in the well-known tracking-by-detection paradigm, which can work in tandem to maintain superior performance. Extensive experimental results on MOT17 and MOT20 datasets demonstrate the superiority of our approach in challenging scenarios, and it achieves state-of-the-art performances at various MOT metrics.
<div id='section'>Paperid: <span id='pid'>445, <a href='https://arxiv.org/pdf/2509.25304.pdf' target='_blank'>https://arxiv.org/pdf/2509.25304.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haozhe Jia, Wenshuo Chen, Yuqi Lin, Yang Yang, Lei Wang, Mang Ning, Bowen Tian, Songning Lai, Nanqian Jia, Yifan Chen, Yutao Yue
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25304">LUMA: Low-Dimension Unified Motion Alignment with Dual-Path Anchoring for Text-to-Motion Diffusion Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While current diffusion-based models, typically built on U-Net architectures, have shown promising results on the text-to-motion generation task, they still suffer from semantic misalignment and kinematic artifacts. Through analysis, we identify severe gradient attenuation in the deep layers of the network as a key bottleneck, leading to insufficient learning of high-level features. To address this issue, we propose \textbf{LUMA} (\textit{\textbf{L}ow-dimension \textbf{U}nified \textbf{M}otion \textbf{A}lignment}), a text-to-motion diffusion model that incorporates dual-path anchoring to enhance semantic alignment. The first path incorporates a lightweight MoCLIP model trained via contrastive learning without relying on external data, offering semantic supervision in the temporal domain. The second path introduces complementary alignment signals in the frequency domain, extracted from low-frequency DCT components known for their rich semantic content. These two anchors are adaptively fused through a temporal modulation mechanism, allowing the model to progressively transition from coarse alignment to fine-grained semantic refinement throughout the denoising process. Experimental results on HumanML3D and KIT-ML demonstrate that LUMA achieves state-of-the-art performance, with FID scores of 0.035 and 0.123, respectively. Furthermore, LUMA accelerates convergence by 1.4$\times$ compared to the baseline, making it an efficient and scalable solution for high-fidelity text-to-motion generation.
<div id='section'>Paperid: <span id='pid'>446, <a href='https://arxiv.org/pdf/2508.17404.pdf' target='_blank'>https://arxiv.org/pdf/2508.17404.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoyu Wang, Hao Tang, Donglin Di, Zhilu Zhang, Wangmeng Zuo, Feng Gao, Siwei Ma, Shiliang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17404">MoCo: Motion-Consistent Human Video Generation via Structure-Appearance Decoupling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating human videos with consistent motion from text prompts remains a significant challenge, particularly for whole-body or long-range motion. Existing video generation models prioritize appearance fidelity, resulting in unrealistic or physically implausible human movements with poor structural coherence. Additionally, most existing human video datasets primarily focus on facial or upper-body motions, or consist of vertically oriented dance videos, limiting the scope of corresponding generation methods to simple movements. To overcome these challenges, we propose MoCo, which decouples the process of human video generation into two components: structure generation and appearance generation. Specifically, our method first employs an efficient 3D structure generator to produce a human motion sequence from a text prompt. The remaining video appearance is then synthesized under the guidance of the generated structural sequence. To improve fine-grained control over sparse human structures, we introduce Human-Aware Dynamic Control modules and integrate dense tracking constraints during training. Furthermore, recognizing the limitations of existing datasets, we construct a large-scale whole-body human video dataset featuring complex and diverse motions. Extensive experiments demonstrate that MoCo outperforms existing approaches in generating realistic and structurally coherent human videos.
<div id='section'>Paperid: <span id='pid'>447, <a href='https://arxiv.org/pdf/2508.17404.pdf' target='_blank'>https://arxiv.org/pdf/2508.17404.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoyu Wang, Hao Tang, Donglin Di, Zhilu Zhang, Wangmeng Zuo, Feng Gao, Siwei Ma, Shiliang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17404">MoSA: Motion-Coherent Human Video Generation via Structure-Appearance Decoupling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing video generation models predominantly emphasize appearance fidelity while exhibiting limited ability to synthesize complex human motions, such as whole-body movements, long-range dynamics, and fine-grained human-environment interactions. This often leads to unrealistic or physically implausible movements with inadequate structural coherence. To conquer these challenges, we propose MoSA, which decouples the process of human video generation into two components, i.e., structure generation and appearance generation. MoSA first employs a 3D structure transformer to generate a human motion sequence from the text prompt. The remaining video appearance is then synthesized under the guidance of this structural sequence. We achieve fine-grained control over the sparse human structures by introducing Human-Aware Dynamic Control modules with a dense tracking constraint during training. The modeling of human-environment interactions is improved through the proposed contact constraint. Those two components work comprehensively to ensure the structural and appearance fidelity across the generated videos. This paper also contributes a large-scale human video dataset, which features more complex and diverse motions than existing human video datasets. We conduct comprehensive comparisons between MoSA and a variety of approaches, including general video generation models, human video generation models, and human animation models. Experiments demonstrate that MoSA substantially outperforms existing approaches across the majority of evaluation metrics.
<div id='section'>Paperid: <span id='pid'>448, <a href='https://arxiv.org/pdf/2508.10566.pdf' target='_blank'>https://arxiv.org/pdf/2508.10566.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shiyu Liu, Kui Jiang, Xianming Liu, Hongxun Yao, Xiaocheng Feng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10566">HM-Talker: Hybrid Motion Modeling for High-Fidelity Talking Head Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Audio-driven talking head video generation enhances user engagement in human-computer interaction. However, current methods frequently produce videos with motion blur and lip jitter, primarily due to their reliance on implicit modeling of audio-facial motion correlations--an approach lacking explicit articulatory priors (i.e., anatomical guidance for speech-related facial movements). To overcome this limitation, we propose HM-Talker, a novel framework for generating high-fidelity, temporally coherent talking heads. HM-Talker leverages a hybrid motion representation combining both implicit and explicit motion cues. Explicit cues use Action Units (AUs), anatomically defined facial muscle movements, alongside implicit features to minimize phoneme-viseme misalignment. Specifically, our Cross-Modal Disentanglement Module (CMDM) extracts complementary implicit/explicit motion features while predicting AUs directly from audio input aligned to visual cues. To mitigate identity-dependent biases in explicit features and enhance cross-subject generalization, we introduce the Hybrid Motion Modeling Module (HMMM). This module dynamically merges randomly paired implicit/explicit features, enforcing identity-agnostic learning. Together, these components enable robust lip synchronization across diverse identities, advancing personalized talking head synthesis. Extensive experiments demonstrate HM-Talker's superiority over state-of-the-art methods in visual quality and lip-sync accuracy.
<div id='section'>Paperid: <span id='pid'>449, <a href='https://arxiv.org/pdf/2507.20170.pdf' target='_blank'>https://arxiv.org/pdf/2507.20170.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Clinton Ansun Mo, Kun Hu, Chengjiang Long, Dong Yuan, Wan-Chi Siu, Zhiyong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.20170">PUMPS: Skeleton-Agnostic Point-based Universal Motion Pre-Training for Synthesis in Human Motion Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion skeletons drive 3D character animation by transforming bone hierarchies, but differences in proportions or structure make motion data hard to transfer across skeletons, posing challenges for data-driven motion synthesis. Temporal Point Clouds (TPCs) offer an unstructured, cross-compatible motion representation. Though reversible with skeletons, TPCs mainly serve for compatibility, not for direct motion task learning. Doing so would require data synthesis capabilities for the TPC format, which presents unexplored challenges regarding its unique temporal consistency and point identifiability. Therefore, we propose PUMPS, the primordial autoencoder architecture for TPC data. PUMPS independently reduces frame-wise point clouds into sampleable feature vectors, from which a decoder extracts distinct temporal points using latent Gaussian noise vectors as sampling identifiers. We introduce linear assignment-based point pairing to optimise the TPC reconstruction process, and negate the use of expensive point-wise attention mechanisms in the architecture. Using these latent features, we pre-train a motion synthesis model capable of performing motion prediction, transition generation, and keyframe interpolation. For these pre-training tasks, PUMPS performs remarkably well even without native dataset supervision, matching state-of-the-art performance. When fine-tuned for motion denoising or estimation, PUMPS outperforms many respective methods without deviating from its generalist architecture.
<div id='section'>Paperid: <span id='pid'>450, <a href='https://arxiv.org/pdf/2506.02452.pdf' target='_blank'>https://arxiv.org/pdf/2506.02452.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenshuo Chen, Kuimou Yu, Haozhe Jia, Kaishen Yuan, Zexu Huang, Bowen Tian, Songning Lai, Hongru Xiao, Erhang Zhang, Lei Wang, Yutao Yue
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.02452">ANT: Adaptive Neural Temporal-Aware Text-to-Motion Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While diffusion models advance text-to-motion generation, their static semantic conditioning ignores temporal-frequency demands: early denoising requires structural semantics for motion foundations while later stages need localized details for text alignment. This mismatch mirrors biological morphogenesis where developmental phases demand distinct genetic programs. Inspired by epigenetic regulation governing morphological specialization, we propose **(ANT)**, an **A**daptive **N**eural **T**emporal-Aware architecture. ANT orchestrates semantic granularity through: **(i) Semantic Temporally Adaptive (STA) Module:** Automatically partitions denoising into low-frequency structural planning and high-frequency refinement via spectral analysis. **(ii) Dynamic Classifier-Free Guidance scheduling (DCFG):** Adaptively adjusts conditional to unconditional ratio enhancing efficiency while maintaining fidelity. Extensive experiments show that ANT can be applied to various baselines, significantly improving model performance, and achieving state-of-the-art semantic alignment on StableMoFusion.
<div id='section'>Paperid: <span id='pid'>451, <a href='https://arxiv.org/pdf/2501.18232.pdf' target='_blank'>https://arxiv.org/pdf/2501.18232.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenshuo Chen, Haozhe Jia, Songning Lai, Keming Wu, Hongru Xiao, Lijie Hu, Yutao Yue
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.18232">Free-T2M: Frequency Enhanced Text-to-Motion Diffusion Model With Consistency Loss</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Rapid progress in text-to-motion generation has been largely driven by diffusion models. However, existing methods focus solely on temporal modeling, thereby overlooking frequency-domain analysis. We identify two key phases in motion denoising: the **semantic planning stage** and the **fine-grained improving stage**. To address these phases effectively, we propose **Fre**quency **e**nhanced **t**ext-**to**-**m**otion diffusion model (**Free-T2M**), incorporating stage-specific consistency losses that enhance the robustness of static features and improve fine-grained accuracy. Extensive experiments demonstrate the effectiveness of our method. Specifically, on StableMoFusion, our method reduces the FID from **0.189** to **0.051**, establishing a new SOTA performance within the diffusion architecture. These findings highlight the importance of incorporating frequency-domain insights into text-to-motion generation for more precise and robust results.
<div id='section'>Paperid: <span id='pid'>452, <a href='https://arxiv.org/pdf/2408.00352.pdf' target='_blank'>https://arxiv.org/pdf/2408.00352.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Honglei Miao, Fan Ma, Ruijie Quan, Kun Zhan, Yi Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.00352">Autonomous LLM-Enhanced Adversarial Attack for Text-to-Motion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion generation driven by deep generative models has enabled compelling applications, but the ability of text-to-motion (T2M) models to produce realistic motions from text prompts raises security concerns if exploited maliciously. Despite growing interest in T2M, few methods focus on safeguarding these models against adversarial attacks, with existing work on text-to-image models proving insufficient for the unique motion domain. In the paper, we propose ALERT-Motion, an autonomous framework leveraging large language models (LLMs) to craft targeted adversarial attacks against black-box T2M models. Unlike prior methods modifying prompts through predefined rules, ALERT-Motion uses LLMs' knowledge of human motion to autonomously generate subtle yet powerful adversarial text descriptions. It comprises two key modules: an adaptive dispatching module that constructs an LLM-based agent to iteratively refine and search for adversarial prompts; and a multimodal information contrastive module that extracts semantically relevant motion information to guide the agent's search. Through this LLM-driven approach, ALERT-Motion crafts adversarial prompts querying victim models to produce outputs closely matching targeted motions, while avoiding obvious perturbations. Evaluations across popular T2M models demonstrate ALERT-Motion's superiority over previous methods, achieving higher attack success rates with stealthier adversarial prompts. This pioneering work on T2M adversarial attacks highlights the urgency of developing defensive measures as motion generation technology advances, urging further research into safe and responsible deployment.
<div id='section'>Paperid: <span id='pid'>453, <a href='https://arxiv.org/pdf/2407.10528.pdf' target='_blank'>https://arxiv.org/pdf/2407.10528.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Peng Jin, Hao Li, Zesen Cheng, Kehan Li, Runyi Yu, Chang Liu, Xiangyang Ji, Li Yuan, Jie Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.10528">Local Action-Guided Motion Diffusion Model for Text-to-Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-to-motion generation requires not only grounding local actions in language but also seamlessly blending these individual actions to synthesize diverse and realistic global motions. However, existing motion generation methods primarily focus on the direct synthesis of global motions while neglecting the importance of generating and controlling local actions. In this paper, we propose the local action-guided motion diffusion model, which facilitates global motion generation by utilizing local actions as fine-grained control signals. Specifically, we provide an automated method for reference local action sampling and leverage graph attention networks to assess the guiding weight of each local action in the overall motion synthesis. During the diffusion process for synthesizing global motion, we calculate the local-action gradient to provide conditional guidance. This local-to-global paradigm reduces the complexity associated with direct global motion generation and promotes motion diversity via sampling diverse actions as conditions. Extensive experiments on two human motion datasets, i.e., HumanML3D and KIT, demonstrate the effectiveness of our method. Furthermore, our method provides flexibility in seamlessly combining various local actions and continuous guiding weight adjustment, accommodating diverse user preferences, which may hold potential significance for the community. The project page is available at https://jpthu17.github.io/GuidedMotion-project/.
<div id='section'>Paperid: <span id='pid'>454, <a href='https://arxiv.org/pdf/2407.09475.pdf' target='_blank'>https://arxiv.org/pdf/2407.09475.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinning Li, Jiachen Li, Sangjae Bae, David Isele
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.09475">Adaptive Prediction Ensemble: Improving Out-of-Distribution Generalization of Motion Forecasting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning-based trajectory prediction models for autonomous driving often struggle with generalization to out-of-distribution (OOD) scenarios, sometimes performing worse than simple rule-based models. To address this limitation, we propose a novel framework, Adaptive Prediction Ensemble (APE), which integrates deep learning and rule-based prediction experts. A learned routing function, trained concurrently with the deep learning model, dynamically selects the most reliable prediction based on the input scenario. Our experiments on large-scale datasets, including Waymo Open Motion Dataset (WOMD) and Argoverse, demonstrate improvement in zero-shot generalization across datasets. We show that our method outperforms individual prediction models and other variants, particularly in long-horizon prediction and scenarios with a high proportion of OOD data. This work highlights the potential of hybrid approaches for robust and generalizable motion prediction in autonomous driving. More details can be found on the project page: https://sites.google.com/view/ape-generalization.
<div id='section'>Paperid: <span id='pid'>455, <a href='https://arxiv.org/pdf/2405.05691.pdf' target='_blank'>https://arxiv.org/pdf/2405.05691.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiheng Huang, Hui Yang, Chuanchen Luo, Yuxi Wang, Shibiao Xu, Zhaoxiang Zhang, Man Zhang, Junran Peng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.05691">StableMoFusion: Towards Robust and Efficient Diffusion-based Motion Generation Framework</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Thanks to the powerful generative capacity of diffusion models, recent years have witnessed rapid progress in human motion generation. Existing diffusion-based methods employ disparate network architectures and training strategies. The effect of the design of each component is still unclear. In addition, the iterative denoising process consumes considerable computational overhead, which is prohibitive for real-time scenarios such as virtual characters and humanoid robots. For this reason, we first conduct a comprehensive investigation into network architectures, training strategies, and inference processs. Based on the profound analysis, we tailor each component for efficient high-quality human motion generation. Despite the promising performance, the tailored model still suffers from foot skating which is an ubiquitous issue in diffusion-based solutions. To eliminate footskate, we identify foot-ground contact and correct foot motions along the denoising process. By organically combining these well-designed components together, we present StableMoFusion, a robust and efficient framework for human motion generation. Extensive experimental results show that our StableMoFusion performs favorably against current state-of-the-art methods. Project page: https://h-y1heng.github.io/StableMoFusion-page/
<div id='section'>Paperid: <span id='pid'>456, <a href='https://arxiv.org/pdf/2312.00651.pdf' target='_blank'>https://arxiv.org/pdf/2312.00651.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pengxiang Li, Kai Chen, Zhili Liu, Ruiyuan Gao, Lanqing Hong, Guo Zhou, Hua Yao, Dit-Yan Yeung, Huchuan Lu, Xu Jia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.00651">TrackDiffusion: Tracklet-Conditioned Video Generation via Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite remarkable achievements in video synthesis, achieving granular control over complex dynamics, such as nuanced movement among multiple interacting objects, still presents a significant hurdle for dynamic world modeling, compounded by the necessity to manage appearance and disappearance, drastic scale changes, and ensure consistency for instances across frames. These challenges hinder the development of video generation that can faithfully mimic real-world complexity, limiting utility for applications requiring high-level realism and controllability, including advanced scene simulation and training of perception systems. To address that, we propose TrackDiffusion, a novel video generation framework affording fine-grained trajectory-conditioned motion control via diffusion models, which facilitates the precise manipulation of the object trajectories and interactions, overcoming the prevalent limitation of scale and continuity disruptions. A pivotal component of TrackDiffusion is the instance enhancer, which explicitly ensures inter-frame consistency of multiple objects, a critical factor overlooked in the current literature. Moreover, we demonstrate that generated video sequences by our TrackDiffusion can be used as training data for visual perception models. To the best of our knowledge, this is the first work to apply video diffusion models with tracklet conditions and demonstrate that generated frames can be beneficial for improving the performance of object trackers.
<div id='section'>Paperid: <span id='pid'>457, <a href='https://arxiv.org/pdf/2303.14926.pdf' target='_blank'>https://arxiv.org/pdf/2303.14926.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Clinton Ansun Mo, Kun Hu, Chengjiang Long, Zhiyong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.14926">Continuous Intermediate Token Learning with Implicit Motion Manifold for Keyframe Based Motion Interpolation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deriving sophisticated 3D motions from sparse keyframes is a particularly challenging problem, due to continuity and exceptionally skeletal precision. The action features are often derivable accurately from the full series of keyframes, and thus, leveraging the global context with transformers has been a promising data-driven embedding approach. However, existing methods are often with inputs of interpolated intermediate frame for continuity using basic interpolation methods with keyframes, which result in a trivial local minimum during training. In this paper, we propose a novel framework to formulate latent motion manifolds with keyframe-based constraints, from which the continuous nature of intermediate token representations is considered. Particularly, our proposed framework consists of two stages for identifying a latent motion subspace, i.e., a keyframe encoding stage and an intermediate token generation stage, and a subsequent motion synthesis stage to extrapolate and compose motion data from manifolds. Through our extensive experiments conducted on both the LaFAN1 and CMU Mocap datasets, our proposed method demonstrates both superior interpolation accuracy and high visual similarity to ground truth motions.
<div id='section'>Paperid: <span id='pid'>458, <a href='https://arxiv.org/pdf/2506.17912.pdf' target='_blank'>https://arxiv.org/pdf/2506.17912.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chuhao Jin, Haosen Li, Bingzi Zhang, Che Liu, Xiting Wang, Ruihua Song, Wenbing Huang, Ying Qin, Fuzheng Zhang, Di Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.17912">PlanMoGPT: Flow-Enhanced Progressive Planning for Text to Motion Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in large language models (LLMs) have enabled breakthroughs in many multimodal generation tasks, but a significant performance gap still exists in text-to-motion generation, where LLM-based methods lag far behind non-LLM methods. We identify the granularity of motion tokenization as a critical bottleneck: fine-grained tokenization induces local dependency issues, where LLMs overemphasize short-term coherence at the expense of global semantic alignment, while coarse-grained tokenization sacrifices motion details. To resolve this issue, we propose PlanMoGPT, an LLM-based framework integrating progressive planning and flow-enhanced fine-grained motion tokenization. First, our progressive planning mechanism leverages LLMs' autoregressive capabilities to hierarchically generate motion tokens by starting from sparse global plans and iteratively refining them into full sequences. Second, our flow-enhanced tokenizer doubles the downsampling resolution and expands the codebook size by eight times, minimizing detail loss during discretization, while a flow-enhanced decoder recovers motion nuances. Extensive experiments on text-to-motion benchmarks demonstrate that it achieves state-of-the-art performance, improving FID scores by 63.8% (from 0.380 to 0.141) on long-sequence generation while enhancing motion diversity by 49.9% compared to existing methods. The proposed framework successfully resolves the diversity-quality trade-off that plagues current non-LLM approaches, establishing new standards for text-to-motion generation.
<div id='section'>Paperid: <span id='pid'>459, <a href='https://arxiv.org/pdf/2506.05117.pdf' target='_blank'>https://arxiv.org/pdf/2506.05117.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zihan Xu, Mengxian Hu, Kaiyan Xiao, Qin Fang, Chengju Liu, Qijun Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.05117">Realizing Text-Driven Motion Generation on NAO Robot: A Reinforcement Learning-Optimized Control Pipeline</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion retargeting for humanoid robots, transferring human motion data to robots for imitation, presents significant challenges but offers considerable potential for real-world applications. Traditionally, this process relies on human demonstrations captured through pose estimation or motion capture systems. In this paper, we explore a text-driven approach to mapping human motion to humanoids. To address the inherent discrepancies between the generated motion representations and the kinematic constraints of humanoid robots, we propose an angle signal network based on norm-position and rotation loss (NPR Loss). It generates joint angles, which serve as inputs to a reinforcement learning-based whole-body joint motion control policy. The policy ensures tracking of the generated motions while maintaining the robot's stability during execution. Our experimental results demonstrate the efficacy of this approach, successfully transferring text-driven human motion to a real humanoid robot NAO.
<div id='section'>Paperid: <span id='pid'>460, <a href='https://arxiv.org/pdf/2504.08449.pdf' target='_blank'>https://arxiv.org/pdf/2504.08449.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jian Wang, Rishabh Dabral, Diogo Luvizon, Zhe Cao, Lingjie Liu, Thabo Beeler, Christian Theobalt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.08449">Ego4o: Egocentric Human Motion Capture and Understanding from Multi-Modal Input</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work focuses on tracking and understanding human motion using consumer wearable devices, such as VR/AR headsets, smart glasses, cellphones, and smartwatches. These devices provide diverse, multi-modal sensor inputs, including egocentric images, and 1-3 sparse IMU sensors in varied combinations. Motion descriptions can also accompany these signals. The diverse input modalities and their intermittent availability pose challenges for consistent motion capture and understanding. In this work, we present Ego4o (o for omni), a new framework for simultaneous human motion capture and understanding from multi-modal egocentric inputs. This method maintains performance with partial inputs while achieving better results when multiple modalities are combined. First, the IMU sensor inputs, the optional egocentric image, and text description of human motion are encoded into the latent space of a motion VQ-VAE. Next, the latent vectors are sent to the VQ-VAE decoder and optimized to track human motion. When motion descriptions are unavailable, the latent vectors can be input into a multi-modal LLM to generate human motion descriptions, which can further enhance motion capture accuracy. Quantitative and qualitative evaluations demonstrate the effectiveness of our method in predicting accurate human motion and high-quality motion descriptions.
<div id='section'>Paperid: <span id='pid'>461, <a href='https://arxiv.org/pdf/2503.21268.pdf' target='_blank'>https://arxiv.org/pdf/2503.21268.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ming Yan, Xincheng Lin, Yuhua Luo, Shuqi Fan, Yudi Dai, Qixin Zhong, Lincai Zhong, Yuexin Ma, Lan Xu, Chenglu Wen, Siqi Shen, Cheng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.21268">ClimbingCap: Multi-Modal Dataset and Method for Rock Climbing in World Coordinate</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human Motion Recovery (HMR) research mainly focuses on ground-based motions such as running. The study on capturing climbing motion, an off-ground motion, is sparse. This is partly due to the limited availability of climbing motion datasets, especially large-scale and challenging 3D labeled datasets. To address the insufficiency of climbing motion datasets, we collect AscendMotion, a large-scale well-annotated, and challenging climbing motion dataset. It consists of 412k RGB, LiDAR frames, and IMU measurements, including the challenging climbing motions of 22 skilled climbing coaches across 12 different rock walls. Capturing the climbing motions is challenging as it requires precise recovery of not only the complex pose but also the global position of climbers. Although multiple global HMR methods have been proposed, they cannot faithfully capture climbing motions. To address the limitations of HMR methods for climbing, we propose ClimbingCap, a motion recovery method that reconstructs continuous 3D human climbing motion in a global coordinate system. One key insight is to use the RGB and LiDAR modalities to separately reconstruct motions in camera coordinates and global coordinates and to optimize them jointly. We demonstrate the quality of the AscendMotion dataset and present promising results from ClimbingCap. The AscendMotion dataset and source code release publicly at \href{this link}{http://www.lidarhumanmotion.net/climbingcap/}
<div id='section'>Paperid: <span id='pid'>462, <a href='https://arxiv.org/pdf/2501.07039.pdf' target='_blank'>https://arxiv.org/pdf/2501.07039.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Subrata Kumer Paul, Abu Saleh Musa Miah, Rakhi Rani Paul, Md. Ekramul Hamid, Jungpil Shin, Md Abdur Rahim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.07039">IoT-Based Real-Time Medical-Related Human Activity Recognition Using Skeletons and Multi-Stage Deep Learning for Healthcare</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Internet of Things (IoT) and mobile technology have significantly transformed healthcare by enabling real-time monitoring and diagnosis of patients. Recognizing medical-related human activities (MRHA) is pivotal for healthcare systems, particularly for identifying actions that are critical to patient well-being. However, challenges such as high computational demands, low accuracy, and limited adaptability persist in Human Motion Recognition (HMR). While some studies have integrated HMR with IoT for real-time healthcare applications, limited research has focused on recognizing MRHA as essential for effective patient monitoring. This study proposes a novel HMR method for MRHA detection, leveraging multi-stage deep learning techniques integrated with IoT. The approach employs EfficientNet to extract optimized spatial features from skeleton frame sequences using seven Mobile Inverted Bottleneck Convolutions (MBConv) blocks, followed by ConvLSTM to capture spatio-temporal patterns. A classification module with global average pooling, a fully connected layer, and a dropout layer generates the final predictions. The model is evaluated on the NTU RGB+D 120 and HMDB51 datasets, focusing on MRHA, such as sneezing, falling, walking, sitting, etc. It achieves 94.85% accuracy for cross-subject evaluations and 96.45% for cross-view evaluations on NTU RGB+D 120, along with 89.00% accuracy on HMDB51. Additionally, the system integrates IoT capabilities using a Raspberry Pi and GSM module, delivering real-time alerts via Twilios SMS service to caregivers and patients. This scalable and efficient solution bridges the gap between HMR and IoT, advancing patient monitoring, improving healthcare outcomes, and reducing costs.
<div id='section'>Paperid: <span id='pid'>463, <a href='https://arxiv.org/pdf/2412.14706.pdf' target='_blank'>https://arxiv.org/pdf/2412.14706.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianrong Zhang, Hehe Fan, Yi Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.14706">EnergyMoGen: Compositional Human Motion Generation with Energy-Based Diffusion Model in Latent Space</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion models, particularly latent diffusion models, have demonstrated remarkable success in text-driven human motion generation. However, it remains challenging for latent diffusion models to effectively compose multiple semantic concepts into a single, coherent motion sequence. To address this issue, we propose EnergyMoGen, which includes two spectrums of Energy-Based Models: (1) We interpret the diffusion model as a latent-aware energy-based model that generates motions by composing a set of diffusion models in latent space; (2) We introduce a semantic-aware energy model based on cross-attention, which enables semantic composition and adaptive gradient descent for text embeddings. To overcome the challenges of semantic inconsistency and motion distortion across these two spectrums, we introduce Synergistic Energy Fusion. This design allows the motion latent diffusion model to synthesize high-quality, complex motions by combining multiple energy terms corresponding to textual descriptions. Experiments show that our approach outperforms existing state-of-the-art models on various motion generation tasks, including text-to-motion generation, compositional motion generation, and multi-concept motion generation. Additionally, we demonstrate that our method can be used to extend motion datasets and improve the text-to-motion task.
<div id='section'>Paperid: <span id='pid'>464, <a href='https://arxiv.org/pdf/2410.21229.pdf' target='_blank'>https://arxiv.org/pdf/2410.21229.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tairan He, Wenli Xiao, Toru Lin, Zhengyi Luo, Zhenjia Xu, Zhenyu Jiang, Jan Kautz, Changliu Liu, Guanya Shi, Xiaolong Wang, Linxi Fan, Yuke Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.21229">HOVER: Versatile Neural Whole-Body Controller for Humanoid Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humanoid whole-body control requires adapting to diverse tasks such as navigation, loco-manipulation, and tabletop manipulation, each demanding a different mode of control. For example, navigation relies on root velocity tracking, while tabletop manipulation prioritizes upper-body joint angle tracking. Existing approaches typically train individual policies tailored to a specific command space, limiting their transferability across modes. We present the key insight that full-body kinematic motion imitation can serve as a common abstraction for all these tasks and provide general-purpose motor skills for learning multiple modes of whole-body control. Building on this, we propose HOVER (Humanoid Versatile Controller), a multi-mode policy distillation framework that consolidates diverse control modes into a unified policy. HOVER enables seamless transitions between control modes while preserving the distinct advantages of each, offering a robust and scalable solution for humanoid control across a wide range of modes. By eliminating the need for policy retraining for each control mode, our approach improves efficiency and flexibility for future humanoid applications.
<div id='section'>Paperid: <span id='pid'>465, <a href='https://arxiv.org/pdf/2409.00736.pdf' target='_blank'>https://arxiv.org/pdf/2409.00736.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziqiang Dang, Tianxing Fan, Boming Zhao, Xujie Shen, Lei Wang, Guofeng Zhang, Zhaopeng Cui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.00736">MoManifold: Learning to Measure 3D Human Motion via Decoupled Joint Acceleration Manifolds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Incorporating temporal information effectively is important for accurate 3D human motion estimation and generation which have wide applications from human-computer interaction to AR/VR. In this paper, we present MoManifold, a novel human motion prior, which models plausible human motion in continuous high-dimensional motion space. Different from existing mathematical or VAE-based methods, our representation is designed based on the neural distance field, which makes human dynamics explicitly quantified to a score and thus can measure human motion plausibility. Specifically, we propose novel decoupled joint acceleration manifolds to model human dynamics from existing limited motion data. Moreover, we introduce a novel optimization method using the manifold distance as guidance, which facilitates a variety of motion-related tasks. Extensive experiments demonstrate that MoManifold outperforms existing SOTAs as a prior in several downstream tasks such as denoising real-world human mocap data, recovering human motion from partial 3D observations, mitigating jitters for SMPL-based pose estimators, and refining the results of motion in-betweening.
<div id='section'>Paperid: <span id='pid'>466, <a href='https://arxiv.org/pdf/2405.02791.pdf' target='_blank'>https://arxiv.org/pdf/2405.02791.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mengxian Hu, Minghao Zhu, Xun Zhou, Qingqing Yan, Shu Li, Chengju Liu, Qijun Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.02791">Efficient Text-driven Motion Generation via Latent Consistency Training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-driven human motion generation based on diffusion strategies establishes a reliable foundation for multimodal applications in human-computer interactions. However, existing advances face significant efficiency challenges due to the substantial computational overhead of iteratively solving for nonlinear reverse diffusion trajectories during the inference phase. To this end, we propose the motion latent consistency training framework (MLCT), which precomputes reverse diffusion trajectories from raw data in the training phase and enables few-step or single-step inference via self-consistency constraints in the inference phase. Specifically, a motion autoencoder with quantization constraints is first proposed for constructing concise and bounded solution distributions for motion diffusion processes. Subsequently, a classifier-free guidance format is constructed via an additional unconditional loss function to accomplish the precomputation of conditional diffusion trajectories in the training phase. Finally, a clustering guidance module based on the K-nearest-neighbor algorithm is developed for the chain-conduction optimization mechanism of self-consistency constraints, which provides additional references of solution distributions at a small query cost. By combining these enhancements, we achieve stable and consistency training in non-pixel modality and latent representation spaces. Benchmark experiments demonstrate that our method significantly outperforms traditional consistency distillation methods with reduced training cost and enhances the consistency model to perform comparably to state-of-the-art models with lower inference costs.
<div id='section'>Paperid: <span id='pid'>467, <a href='https://arxiv.org/pdf/2403.19501.pdf' target='_blank'>https://arxiv.org/pdf/2403.19501.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ming Yan, Yan Zhang, Shuqiang Cai, Shuqi Fan, Xincheng Lin, Yudi Dai, Siqi Shen, Chenglu Wen, Lan Xu, Yuexin Ma, Cheng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.19501">RELI11D: A Comprehensive Multimodal Human Motion Dataset and Method</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Comprehensive capturing of human motions requires both accurate captures of complex poses and precise localization of the human within scenes. Most of the HPE datasets and methods primarily rely on RGB, LiDAR, or IMU data. However, solely using these modalities or a combination of them may not be adequate for HPE, particularly for complex and fast movements. For holistic human motion understanding, we present RELI11D, a high-quality multimodal human motion dataset involves LiDAR, IMU system, RGB camera, and Event camera. It records the motions of 10 actors performing 5 sports in 7 scenes, including 3.32 hours of synchronized LiDAR point clouds, IMU measurement data, RGB videos and Event steams. Through extensive experiments, we demonstrate that the RELI11D presents considerable challenges and opportunities as it contains many rapid and complex motions that require precise location. To address the challenge of integrating different modalities, we propose LEIR, a multimodal baseline that effectively utilizes LiDAR Point Cloud, Event stream, and RGB through our cross-attention fusion strategy. We show that LEIR exhibits promising results for rapid motions and daily motions and that utilizing the characteristics of multiple modalities can indeed improve HPE performance. Both the dataset and source code will be released publicly to the research community, fostering collaboration and enabling further exploration in this field.
<div id='section'>Paperid: <span id='pid'>468, <a href='https://arxiv.org/pdf/2402.13185.pdf' target='_blank'>https://arxiv.org/pdf/2402.13185.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianhong Bai, Tianyu He, Yuchi Wang, Junliang Guo, Haoji Hu, Zuozhu Liu, Jiang Bian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.13185">UniEdit: A Unified Tuning-Free Framework for Video Motion and Appearance Editing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in text-guided video editing have showcased promising results in appearance editing (e.g., stylization). However, video motion editing in the temporal dimension (e.g., from eating to waving), which distinguishes video editing from image editing, is underexplored. In this work, we present UniEdit, a tuning-free framework that supports both video motion and appearance editing by harnessing the power of a pre-trained text-to-video generator within an inversion-then-generation framework. To realize motion editing while preserving source video content, based on the insights that temporal and spatial self-attention layers encode inter-frame and intra-frame dependency respectively, we introduce auxiliary motion-reference and reconstruction branches to produce text-guided motion and source features respectively. The obtained features are then injected into the main editing path via temporal and spatial self-attention layers. Extensive experiments demonstrate that UniEdit covers video motion editing and various appearance editing scenarios, and surpasses the state-of-the-art methods. Our code will be publicly available.
<div id='section'>Paperid: <span id='pid'>469, <a href='https://arxiv.org/pdf/2401.15977.pdf' target='_blank'>https://arxiv.org/pdf/2401.15977.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoyu Shi, Zhaoyang Huang, Fu-Yun Wang, Weikang Bian, Dasong Li, Yi Zhang, Manyuan Zhang, Ka Chun Cheung, Simon See, Hongwei Qin, Jifeng Dai, Hongsheng Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.15977">Motion-I2V: Consistent and Controllable Image-to-Video Generation with Explicit Motion Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce Motion-I2V, a novel framework for consistent and controllable image-to-video generation (I2V). In contrast to previous methods that directly learn the complicated image-to-video mapping, Motion-I2V factorizes I2V into two stages with explicit motion modeling. For the first stage, we propose a diffusion-based motion field predictor, which focuses on deducing the trajectories of the reference image's pixels. For the second stage, we propose motion-augmented temporal attention to enhance the limited 1-D temporal attention in video latent diffusion models. This module can effectively propagate reference image's feature to synthesized frames with the guidance of predicted trajectories from the first stage. Compared with existing methods, Motion-I2V can generate more consistent videos even at the presence of large motion and viewpoint variation. By training a sparse trajectory ControlNet for the first stage, Motion-I2V can support users to precisely control motion trajectories and motion regions with sparse trajectory and region annotations. This offers more controllability of the I2V process than solely relying on textual instructions. Additionally, Motion-I2V's second stage naturally supports zero-shot video-to-video translation. Both qualitative and quantitative comparisons demonstrate the advantages of Motion-I2V over prior approaches in consistent and controllable image-to-video generation. Please see our project page at https://xiaoyushi97.github.io/Motion-I2V/.
<div id='section'>Paperid: <span id='pid'>470, <a href='https://arxiv.org/pdf/2312.10422.pdf' target='_blank'>https://arxiv.org/pdf/2312.10422.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Songlin Yang, Wei Wang, Yushi Lan, Xiangyu Fan, Bo Peng, Lei Yang, Jing Dong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.10422">Learning Dense Correspondence for NeRF-Based Face Reenactment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Face reenactment is challenging due to the need to establish dense correspondence between various face representations for motion transfer. Recent studies have utilized Neural Radiance Field (NeRF) as fundamental representation, which further enhanced the performance of multi-view face reenactment in photo-realism and 3D consistency. However, establishing dense correspondence between different face NeRFs is non-trivial, because implicit representations lack ground-truth correspondence annotations like mesh-based 3D parametric models (e.g., 3DMM) with index-aligned vertexes. Although aligning 3DMM space with NeRF-based face representations can realize motion control, it is sub-optimal for their limited face-only modeling and low identity fidelity. Therefore, we are inspired to ask: Can we learn the dense correspondence between different NeRF-based face representations without a 3D parametric model prior? To address this challenge, we propose a novel framework, which adopts tri-planes as fundamental NeRF representation and decomposes face tri-planes into three components: canonical tri-planes, identity deformations, and motion. In terms of motion control, our key contribution is proposing a Plane Dictionary (PlaneDict) module, which efficiently maps the motion conditions to a linear weighted addition of learnable orthogonal plane bases. To the best of our knowledge, our framework is the first method that achieves one-shot multi-view face reenactment without a 3D parametric model prior. Extensive experiments demonstrate that we produce better results in fine-grained motion control and identity preservation than previous methods.
<div id='section'>Paperid: <span id='pid'>471, <a href='https://arxiv.org/pdf/2308.14748.pdf' target='_blank'>https://arxiv.org/pdf/2308.14748.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianfeng Zhang, Hanshu Yan, Zhongcong Xu, Jiashi Feng, Jun Hao Liew
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.14748">MagicAvatar: Multimodal Avatar Generation and Animation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This report presents MagicAvatar, a framework for multimodal video generation and animation of human avatars. Unlike most existing methods that generate avatar-centric videos directly from multimodal inputs (e.g., text prompts), MagicAvatar explicitly disentangles avatar video generation into two stages: (1) multimodal-to-motion and (2) motion-to-video generation. The first stage translates the multimodal inputs into motion/ control signals (e.g., human pose, depth, DensePose); while the second stage generates avatar-centric video guided by these motion signals. Additionally, MagicAvatar supports avatar animation by simply providing a few images of the target person. This capability enables the animation of the provided human identity according to the specific motion derived from the first stage. We demonstrate the flexibility of MagicAvatar through various applications, including text-guided and video-guided avatar generation, as well as multimodal avatar animation.
<div id='section'>Paperid: <span id='pid'>472, <a href='https://arxiv.org/pdf/2308.02915.pdf' target='_blank'>https://arxiv.org/pdf/2308.02915.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiaosong Qi, Le Zhuo, Aixi Zhang, Yue Liao, Fei Fang, Si Liu, Shuicheng Yan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.02915">DiffDance: Cascaded Human Motion Diffusion Model for Dance Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>When hearing music, it is natural for people to dance to its rhythm. Automatic dance generation, however, is a challenging task due to the physical constraints of human motion and rhythmic alignment with target music. Conventional autoregressive methods introduce compounding errors during sampling and struggle to capture the long-term structure of dance sequences. To address these limitations, we present a novel cascaded motion diffusion model, DiffDance, designed for high-resolution, long-form dance generation. This model comprises a music-to-dance diffusion model and a sequence super-resolution diffusion model. To bridge the gap between music and motion for conditional generation, DiffDance employs a pretrained audio representation learning model to extract music embeddings and further align its embedding space to motion via contrastive loss. During training our cascaded diffusion model, we also incorporate multiple geometric losses to constrain the model outputs to be physically plausible and add a dynamic loss weight that adaptively changes over diffusion timesteps to facilitate sample diversity. Through comprehensive experiments performed on the benchmark dataset AIST++, we demonstrate that DiffDance is capable of generating realistic dance sequences that align effectively with the input music. These results are comparable to those achieved by state-of-the-art autoregressive methods.
<div id='section'>Paperid: <span id='pid'>473, <a href='https://arxiv.org/pdf/2308.00462.pdf' target='_blank'>https://arxiv.org/pdf/2308.00462.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Songlin Yang, Wei Wang, Jun Ling, Bo Peng, Xu Tan, Jing Dong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.00462">Context-Aware Talking-Head Video Editing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Talking-head video editing aims to efficiently insert, delete, and substitute the word of a pre-recorded video through a text transcript editor. The key challenge for this task is obtaining an editing model that generates new talking-head video clips which simultaneously have accurate lip synchronization and motion smoothness. Previous approaches, including 3DMM-based (3D Morphable Model) methods and NeRF-based (Neural Radiance Field) methods, are sub-optimal in that they either require minutes of source videos and days of training time or lack the disentangled control of verbal (e.g., lip motion) and non-verbal (e.g., head pose and expression) representations for video clip insertion. In this work, we fully utilize the video context to design a novel framework for talking-head video editing, which achieves efficiency, disentangled motion control, and sequential smoothness. Specifically, we decompose this framework to motion prediction and motion-conditioned rendering: (1) We first design an animation prediction module that efficiently obtains smooth and lip-sync motion sequences conditioned on the driven speech. This module adopts a non-autoregressive network to obtain context prior and improve the prediction efficiency, and it learns a speech-animation mapping prior with better generalization to novel speech from a multi-identity video dataset. (2) We then introduce a neural rendering module to synthesize the photo-realistic and full-head video frames given the predicted motion sequence. This module adopts a pre-trained head topology and uses only few frames for efficient fine-tuning to obtain a person-specific rendering model. Extensive experiments demonstrate that our method efficiently achieves smoother editing results with higher image quality and lip accuracy using less data than previous methods.
<div id='section'>Paperid: <span id='pid'>474, <a href='https://arxiv.org/pdf/2302.03665.pdf' target='_blank'>https://arxiv.org/pdf/2302.03665.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ling-Hao Chen, Jiawei Zhang, Yewen Li, Yiren Pang, Xiaobo Xia, Tongliang Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.03665">HumanMAC: Masked Motion Completion for Human Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion prediction is a classical problem in computer vision and computer graphics, which has a wide range of practical applications. Previous effects achieve great empirical performance based on an encoding-decoding style. The methods of this style work by first encoding previous motions to latent representations and then decoding the latent representations into predicted motions. However, in practice, they are still unsatisfactory due to several issues, including complicated loss constraints, cumbersome training processes, and scarce switch of different categories of motions in prediction. In this paper, to address the above issues, we jump out of the foregoing style and propose a novel framework from a new perspective. Specifically, our framework works in a masked completion fashion. In the training stage, we learn a motion diffusion model that generates motions from random noise. In the inference stage, with a denoising procedure, we make motion prediction conditioning on observed motions to output more continuous and controllable predictions. The proposed framework enjoys promising algorithmic properties, which only needs one loss in optimization and is trained in an end-to-end manner. Additionally, it accomplishes the switch of different categories of motions effectively, which is significant in realistic tasks, e.g., the animation task. Comprehensive experiments on benchmarks confirm the superiority of the proposed framework. The project page is available at https://lhchen.top/Human-MAC.
<div id='section'>Paperid: <span id='pid'>475, <a href='https://arxiv.org/pdf/2212.06384.pdf' target='_blank'>https://arxiv.org/pdf/2212.06384.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhongcong Xu, Jianfeng Zhang, Jun Hao Liew, Wenqing Zhang, Song Bai, Jiashi Feng, Mike Zheng Shou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.06384">PV3D: A 3D Generative Model for Portrait Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in generative adversarial networks (GANs) have demonstrated the capabilities of generating stunning photo-realistic portrait images. While some prior works have applied such image GANs to unconditional 2D portrait video generation and static 3D portrait synthesis, there are few works successfully extending GANs for generating 3D-aware portrait videos. In this work, we propose PV3D, the first generative framework that can synthesize multi-view consistent portrait videos. Specifically, our method extends the recent static 3D-aware image GAN to the video domain by generalizing the 3D implicit neural representation to model the spatio-temporal space. To introduce motion dynamics to the generation process, we develop a motion generator by stacking multiple motion layers to generate motion features via modulated convolution. To alleviate motion ambiguities caused by camera/human motions, we propose a simple yet effective camera condition strategy for PV3D, enabling both temporal and multi-view consistent video generation. Moreover, PV3D introduces two discriminators for regularizing the spatial and temporal domains to ensure the plausibility of the generated portrait videos. These elaborated designs enable PV3D to generate 3D-aware motion-plausible portrait videos with high-quality appearance and geometry, significantly outperforming prior works. As a result, PV3D is able to support many downstream applications such as animating static portraits and view-consistent video motion editing. Code and models are released at https://showlab.github.io/pv3d.
<div id='section'>Paperid: <span id='pid'>476, <a href='https://arxiv.org/pdf/2505.01182.pdf' target='_blank'>https://arxiv.org/pdf/2505.01182.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyan Guo, Haoxuan Qu, Hossein Rahmani, Dewen Soh, Ping Hu, Qiuhong Ke, Jun Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.01182">TSTMotion: Training-free Scene-aware Text-to-motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-to-motion generation has recently garnered significant research interest, primarily focusing on generating human motion sequences in blank backgrounds. However, human motions commonly occur within diverse 3D scenes, which has prompted exploration into scene-aware text-to-motion generation methods. Yet, existing scene-aware methods often rely on large-scale ground-truth motion sequences in diverse 3D scenes, which poses practical challenges due to the expensive cost. To mitigate this challenge, we are the first to propose a \textbf{T}raining-free \textbf{S}cene-aware \textbf{T}ext-to-\textbf{Motion} framework, dubbed as \textbf{TSTMotion}, that efficiently empowers pre-trained blank-background motion generators with the scene-aware capability. Specifically, conditioned on the given 3D scene and text description, we adopt foundation models together to reason, predict and validate a scene-aware motion guidance. Then, the motion guidance is incorporated into the blank-background motion generators with two modifications, resulting in scene-aware text-driven motion sequences. Extensive experiments demonstrate the efficacy and generalizability of our proposed framework. We release our code in \href{https://tstmotion.github.io/}{Project Page}.
<div id='section'>Paperid: <span id='pid'>477, <a href='https://arxiv.org/pdf/2502.03207.pdf' target='_blank'>https://arxiv.org/pdf/2502.03207.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinyao Liao, Xianfang Zeng, Liao Wang, Gang Yu, Guosheng Lin, Chi Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.03207">MotionAgent: Fine-grained Controllable Video Generation via Motion Field Agent</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose MotionAgent, enabling fine-grained motion control for text-guided image-to-video generation. The key technique is the motion field agent that converts motion information in text prompts into explicit motion fields, providing flexible and precise motion guidance. Specifically, the agent extracts the object movement and camera motion described in the text and converts them into object trajectories and camera extrinsics, respectively. An analytical optical flow composition module integrates these motion representations in 3D space and projects them into a unified optical flow. An optical flow adapter takes the flow to control the base image-to-video diffusion model for generating fine-grained controlled videos. The significant improvement in the Video-Text Camera Motion metrics on VBench indicates that our method achieves precise control over camera motion. We construct a subset of VBench to evaluate the alignment of motion information in the text and the generated video, outperforming other advanced models on motion generation accuracy.
<div id='section'>Paperid: <span id='pid'>478, <a href='https://arxiv.org/pdf/2501.04541.pdf' target='_blank'>https://arxiv.org/pdf/2501.04541.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ching-Chun Chang, Yijie Lin, Isao Echizen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.04541">Cyber-Physical Steganography in Robotic Motion Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Steganography, the art of information hiding, has continually evolved across visual, auditory and linguistic domains, adapting to the ceaseless interplay between steganographic concealment and steganalytic revelation. This study seeks to extend the horizons of what constitutes a viable steganographic medium by introducing a steganographic paradigm in robotic motion control. Based on the observation of the robot's inherent sensitivity to changes in its environment, we propose a methodology to encode messages as environmental stimuli influencing the motions of the robotic agent and to decode messages from the resulting motion trajectory. The constraints of maximal robot integrity and minimal motion deviation are established as fundamental principles underlying secrecy. As a proof of concept, we conduct experiments in simulated environments across various manipulation tasks, incorporating robotic embodiments equipped with generalist multimodal policies.
<div id='section'>Paperid: <span id='pid'>479, <a href='https://arxiv.org/pdf/2411.08656.pdf' target='_blank'>https://arxiv.org/pdf/2411.08656.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaxu Zhang, Xianfang Zeng, Xin Chen, Wei Zuo, Gang Yu, Zhigang Tu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.08656">MikuDance: Animating Character Art with Mixed Motion Dynamics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose MikuDance, a diffusion-based pipeline incorporating mixed motion dynamics to animate stylized character art. MikuDance consists of two key techniques: Mixed Motion Modeling and Mixed-Control Diffusion, to address the challenges of high-dynamic motion and reference-guidance misalignment in character art animation. Specifically, a Scene Motion Tracking strategy is presented to explicitly model the dynamic camera in pixel-wise space, enabling unified character-scene motion modeling. Building on this, the Mixed-Control Diffusion implicitly aligns the scale and body shape of diverse characters with motion guidance, allowing flexible control of local character motion. Subsequently, a Motion-Adaptive Normalization module is incorporated to effectively inject global scene motion, paving the way for comprehensive character art animation. Through extensive experiments, we demonstrate the effectiveness and generalizability of MikuDance across various character art and motion guidance, consistently producing high-quality animations with remarkable motion dynamics.
<div id='section'>Paperid: <span id='pid'>480, <a href='https://arxiv.org/pdf/2410.00464.pdf' target='_blank'>https://arxiv.org/pdf/2410.00464.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bohong Chen, Yumeng Li, Yao-Xiang Ding, Tianjia Shao, Kun Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.00464">Enabling Synergistic Full-Body Control in Prompt-Based Co-Speech Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current co-speech motion generation approaches usually focus on upper body gestures following speech contents only, while lacking supporting the elaborate control of synergistic full-body motion based on text prompts, such as talking while walking. The major challenges lie in 1) the existing speech-to-motion datasets only involve highly limited full-body motions, making a wide range of common human activities out of training distribution; 2) these datasets also lack annotated user prompts. To address these challenges, we propose SynTalker, which utilizes the off-the-shelf text-to-motion dataset as an auxiliary for supplementing the missing full-body motion and prompts. The core technical contributions are two-fold. One is the multi-stage training process which obtains an aligned embedding space of motion, speech, and prompts despite the significant distributional mismatch in motion between speech-to-motion and text-to-motion datasets. Another is the diffusion-based conditional inference process, which utilizes the separate-then-combine strategy to realize fine-grained control of local body parts. Extensive experiments are conducted to verify that our approach supports precise and flexible control of synergistic full-body motion generation based on both speeches and user prompts, which is beyond the ability of existing approaches.
<div id='section'>Paperid: <span id='pid'>481, <a href='https://arxiv.org/pdf/2406.15735.pdf' target='_blank'>https://arxiv.org/pdf/2406.15735.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Min Zhao, Hongzhou Zhu, Chendong Xiang, Kaiwen Zheng, Chongxuan Li, Jun Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.15735">Identifying and Solving Conditional Image Leakage in Image-to-Video Diffusion Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion models have obtained substantial progress in image-to-video generation. However, in this paper, we find that these models tend to generate videos with less motion than expected. We attribute this to the issue called conditional image leakage, where the image-to-video diffusion models (I2V-DMs) tend to over-rely on the conditional image at large time steps. We further address this challenge from both inference and training aspects. First, we propose to start the generation process from an earlier time step to avoid the unreliable large-time steps of I2V-DMs, as well as an initial noise distribution with optimal analytic expressions (Analytic-Init) by minimizing the KL divergence between it and the actual marginal distribution to bridge the training-inference gap. Second, we design a time-dependent noise distribution (TimeNoise) for the conditional image during training, applying higher noise levels at larger time steps to disrupt it and reduce the model's dependency on it. We validate these general strategies on various I2V-DMs on our collected open-domain image benchmark and the UCF101 dataset. Extensive results show that our methods outperform baselines by producing higher motion scores with lower errors while maintaining image alignment and temporal consistency, thereby yielding superior overall performance and enabling more accurate motion control. The project page: \url{https://cond-image-leak.github.io/}.
<div id='section'>Paperid: <span id='pid'>482, <a href='https://arxiv.org/pdf/2306.11990.pdf' target='_blank'>https://arxiv.org/pdf/2306.11990.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chengxu Duan, Zhicheng Zhang, Xiaoli Liu, Yonghao Dang, Jianqin Yin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.11990">Physics-constrained Attack against Convolution-based Human Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion prediction has achieved a brilliant performance with the help of convolution-based neural networks. However, currently, there is no work evaluating the potential risk in human motion prediction when facing adversarial attacks. The adversarial attack will encounter problems against human motion prediction in naturalness and data scale. To solve the problems above, we propose a new adversarial attack method that generates the worst-case perturbation by maximizing the human motion predictor's prediction error with physical constraints. Specifically, we introduce a novel adaptable scheme that facilitates the attack to suit the scale of the target pose and two physical constraints to enhance the naturalness of the adversarial example. The evaluating experiments on three datasets show that the prediction errors of all target models are enlarged significantly, which means current convolution-based human motion prediction models are vulnerable to the proposed attack. Based on the experimental results, we provide insights on how to enhance the adversarial robustness of the human motion predictor and how to improve the adversarial attack against human motion prediction.
<div id='section'>Paperid: <span id='pid'>483, <a href='https://arxiv.org/pdf/2508.07863.pdf' target='_blank'>https://arxiv.org/pdf/2508.07863.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bin Cao, Sipeng Zheng, Ye Wang, Lujie Xia, Qianshan Wei, Qin Jin, Jing Liu, Zongqing Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.07863">Being-M0.5: A Real-Time Controllable Vision-Language-Motion Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion generation has emerged as a critical technology with transformative potential for real-world applications. However, existing vision-language-motion models (VLMMs) face significant limitations that hinder their practical deployment. We identify controllability as a main bottleneck, manifesting in five key aspects: inadequate response to diverse human commands, limited pose initialization capabilities, poor performance on long-term sequences, insufficient handling of unseen scenarios, and lack of fine-grained control over individual body parts. To overcome these limitations, we present Being-M0.5, the first real-time, controllable VLMM that achieves state-of-the-art performance across multiple motion generation tasks. Our approach is built upon HuMo100M, the largest and most comprehensive human motion dataset to date, comprising over 5 million self-collected motion sequences, 100 million multi-task instructional instances, and detailed part-level annotations that address a critical gap in existing datasets. We introduce a novel part-aware residual quantization technique for motion tokenization that enables precise, granular control over individual body parts during generation. Extensive experimental validation demonstrates Being-M0.5's superior performance across diverse motion benchmarks, while comprehensive efficiency analysis confirms its real-time capabilities. Our contributions include design insights and detailed computational analysis to guide future development of practical motion generators. We believe that HuMo100M and Being-M0.5 represent significant advances that will accelerate the adoption of motion generation technologies in real-world applications. The project page is available at https://beingbeyond.github.io/Being-M0.5.
<div id='section'>Paperid: <span id='pid'>484, <a href='https://arxiv.org/pdf/2506.05207.pdf' target='_blank'>https://arxiv.org/pdf/2506.05207.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yue Ma, Yulong Liu, Qiyuan Zhu, Ayden Yang, Kunyu Feng, Xinhua Zhang, Zhifeng Li, Sirui Han, Chenyang Qi, Qifeng Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.05207">Follow-Your-Motion: Video Motion Transfer via Efficient Spatial-Temporal Decoupled Finetuning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, breakthroughs in the video diffusion transformer have shown remarkable capabilities in diverse motion generations. As for the motion-transfer task, current methods mainly use two-stage Low-Rank Adaptations (LoRAs) finetuning to obtain better performance. However, existing adaptation-based motion transfer still suffers from motion inconsistency and tuning inefficiency when applied to large video diffusion transformers. Naive two-stage LoRA tuning struggles to maintain motion consistency between generated and input videos due to the inherent spatial-temporal coupling in the 3D attention operator. Additionally, they require time-consuming fine-tuning processes in both stages. To tackle these issues, we propose Follow-Your-Motion, an efficient two-stage video motion transfer framework that finetunes a powerful video diffusion transformer to synthesize complex motion. Specifically, we propose a spatial-temporal decoupled LoRA to decouple the attention architecture for spatial appearance and temporal motion processing. During the second training stage, we design the sparse motion sampling and adaptive RoPE to accelerate the tuning speed. To address the lack of a benchmark for this field, we introduce MotionBench, a comprehensive benchmark comprising diverse motion, including creative camera motion, single object motion, multiple object motion, and complex human motion. We show extensive evaluations on MotionBench to verify the superiority of Follow-Your-Motion.
<div id='section'>Paperid: <span id='pid'>485, <a href='https://arxiv.org/pdf/2504.02004.pdf' target='_blank'>https://arxiv.org/pdf/2504.02004.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingshuai Yao, Mengting Chen, Qinye Zhou, Yabo Zhang, Ming Liu, Xiaoming Li, Shaohui Liu, Chen Ju, Shuai Xiao, Qingwen Liu, Jinsong Lan, Wangmeng Zuo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.02004">Beyond Static Scenes: Camera-controllable Background Generation for Human Motion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we investigate the generation of new video backgrounds given a human foreground video, a camera pose, and a reference scene image. This task presents three key challenges. First, the generated background should precisely follow the camera movements corresponding to the human foreground. Second, as the camera shifts in different directions, newly revealed content should appear seamless and natural. Third, objects within the video frame should maintain consistent textures as the camera moves to ensure visual coherence. To address these challenges, we propose DynaScene, a new framework that uses camera poses extracted from the original video as an explicit control to drive background motion. Specifically, we design a multi-task learning paradigm that incorporates auxiliary tasks, namely background outpainting and scene variation, to enhance the realism of the generated backgrounds. Given the scarcity of suitable data, we constructed a large-scale, high-quality dataset tailored for this task, comprising video foregrounds, reference scene images, and corresponding camera poses. This dataset contains 200K video clips, ten times larger than existing real-world human video datasets, providing a significantly richer and more diverse training resource. Project page: https://yaomingshuai.github.io/Beyond-Static-Scenes.github.io/
<div id='section'>Paperid: <span id='pid'>486, <a href='https://arxiv.org/pdf/2503.15082.pdf' target='_blank'>https://arxiv.org/pdf/2503.15082.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Le Ma, Ziyu Meng, Tengyu Liu, Yuhan Li, Ran Song, Wei Zhang, Siyuan Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.15082">StyleLoco: Generative Adversarial Distillation for Natural Humanoid Robot Locomotion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humanoid robots are anticipated to acquire a wide range of locomotion capabilities while ensuring natural movement across varying speeds and terrains. Existing methods encounter a fundamental dilemma in learning humanoid locomotion: reinforcement learning with handcrafted rewards can achieve agile locomotion but produces unnatural gaits, while Generative Adversarial Imitation Learning (GAIL) with motion capture data yields natural movements but suffers from unstable training processes and restricted agility. Integrating these approaches proves challenging due to the inherent heterogeneity between expert policies and human motion datasets. To address this, we introduce StyleLoco, a novel two-stage framework that bridges this gap through a Generative Adversarial Distillation (GAD) process. Our framework begins by training a teacher policy using reinforcement learning to achieve agile and dynamic locomotion. It then employs a multi-discriminator architecture, where distinct discriminators concurrently extract skills from both the teacher policy and motion capture data. This approach effectively combines the agility of reinforcement learning with the natural fluidity of human-like movements while mitigating the instability issues commonly associated with adversarial training. Through extensive simulation and real-world experiments, we demonstrate that StyleLoco enables humanoid robots to perform diverse locomotion tasks with the precision of expertly trained policies and the natural aesthetics of human motion, successfully transferring styles across different movement types while maintaining stable locomotion across a broad spectrum of command inputs.
<div id='section'>Paperid: <span id='pid'>487, <a href='https://arxiv.org/pdf/2502.20390.pdf' target='_blank'>https://arxiv.org/pdf/2502.20390.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sirui Xu, Hung Yu Ling, Yu-Xiong Wang, Liang-Yan Gui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.20390">InterMimic: Towards Universal Whole-Body Control for Physics-Based Human-Object Interactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Achieving realistic simulations of humans interacting with a wide range of objects has long been a fundamental goal. Extending physics-based motion imitation to complex human-object interactions (HOIs) is challenging due to intricate human-object coupling, variability in object geometries, and artifacts in motion capture data, such as inaccurate contacts and limited hand detail. We introduce InterMimic, a framework that enables a single policy to robustly learn from hours of imperfect MoCap data covering diverse full-body interactions with dynamic and varied objects. Our key insight is to employ a curriculum strategy -- perfect first, then scale up. We first train subject-specific teacher policies to mimic, retarget, and refine motion capture data. Next, we distill these teachers into a student policy, with the teachers acting as online experts providing direct supervision, as well as high-quality references. Notably, we incorporate RL fine-tuning on the student policy to surpass mere demonstration replication and achieve higher-quality solutions. Our experiments demonstrate that InterMimic produces realistic and diverse interactions across multiple HOI datasets. The learned policy generalizes in a zero-shot manner and seamlessly integrates with kinematic generators, elevating the framework from mere imitation to generative modeling of complex human-object interactions.
<div id='section'>Paperid: <span id='pid'>488, <a href='https://arxiv.org/pdf/2502.17322.pdf' target='_blank'>https://arxiv.org/pdf/2502.17322.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zifeng Zhuang, Diyuan Shi, Runze Suo, Xiao He, Hongyin Zhang, Ting Wang, Shangke Lyu, Donglin Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.17322">TDMPBC: Self-Imitative Reinforcement Learning for Humanoid Robot Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Complex high-dimensional spaces with high Degree-of-Freedom and complicated action spaces, such as humanoid robots equipped with dexterous hands, pose significant challenges for reinforcement learning (RL) algorithms, which need to wisely balance exploration and exploitation under limited sample budgets. In general, feasible regions for accomplishing tasks within complex high-dimensional spaces are exceedingly narrow. For instance, in the context of humanoid robot motion control, the vast majority of space corresponds to falling, while only a minuscule fraction corresponds to standing upright, which is conducive to the completion of downstream tasks. Once the robot explores into a potentially task-relevant region, it should place greater emphasis on the data within that region. Building on this insight, we propose the $\textbf{S}$elf-$\textbf{I}$mitative $\textbf{R}$einforcement $\textbf{L}$earning ($\textbf{SIRL}$) framework, where the RL algorithm also imitates potentially task-relevant trajectories. Specifically, trajectory return is utilized to determine its relevance to the task and an additional behavior cloning is adopted whose weight is dynamically adjusted based on the trajectory return. As a result, our proposed algorithm achieves 120% performance improvement on the challenging HumanoidBench with 5% extra computation overhead. With further visualization, we find the significant performance gain does lead to meaningful behavior improvement that several tasks are solved successfully.
<div id='section'>Paperid: <span id='pid'>489, <a href='https://arxiv.org/pdf/2501.01770.pdf' target='_blank'>https://arxiv.org/pdf/2501.01770.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiajie Liu, Mengyuan Liu, Hong Liu, Wenhao Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.01770">TCPFormer: Learning Temporal Correlation with Implicit Pose Proxy for 3D Human Pose Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent multi-frame lifting methods have dominated the 3D human pose estimation. However, previous methods ignore the intricate dependence within the 2D pose sequence and learn single temporal correlation. To alleviate this limitation, we propose TCPFormer, which leverages an implicit pose proxy as an intermediate representation. Each proxy within the implicit pose proxy can build one temporal correlation therefore helping us learn more comprehensive temporal correlation of human motion. Specifically, our method consists of three key components: Proxy Update Module (PUM), Proxy Invocation Module (PIM), and Proxy Attention Module (PAM). PUM first uses pose features to update the implicit pose proxy, enabling it to store representative information from the pose sequence. PIM then invocates and integrates the pose proxy with the pose sequence to enhance the motion semantics of each pose. Finally, PAM leverages the above mapping between the pose sequence and pose proxy to enhance the temporal correlation of the whole pose sequence. Experiments on the Human3.6M and MPI-INF-3DHP datasets demonstrate that our proposed TCPFormer outperforms the previous state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>490, <a href='https://arxiv.org/pdf/2411.11913.pdf' target='_blank'>https://arxiv.org/pdf/2411.11913.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Can Cui, Zichong Yang, Yupeng Zhou, Juntong Peng, Sung-Yeon Park, Cong Zhang, Yunsheng Ma, Xu Cao, Wenqian Ye, Yiheng Feng, Jitesh Panchal, Lingxi Li, Yaobin Chen, Ziran Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.11913">On-Board Vision-Language Models for Personalized Autonomous Vehicle Motion Control: System Design and Real-World Validation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Personalized driving refers to an autonomous vehicle's ability to adapt its driving behavior or control strategies to match individual users' preferences and driving styles while maintaining safety and comfort standards. However, existing works either fail to capture every individual preference precisely or become computationally inefficient as the user base expands. Vision-Language Models (VLMs) offer promising solutions to this front through their natural language understanding and scene reasoning capabilities. In this work, we propose a lightweight yet effective on-board VLM framework that provides low-latency personalized driving performance while maintaining strong reasoning capabilities. Our solution incorporates a Retrieval-Augmented Generation (RAG)-based memory module that enables continuous learning of individual driving preferences through human feedback. Through comprehensive real-world vehicle deployment and experiments, our system has demonstrated the ability to provide safe, comfortable, and personalized driving experiences across various scenarios and significantly reduce takeover rates by up to 76.9%. To the best of our knowledge, this work represents the first end-to-end VLM-based motion control system in real-world autonomous vehicles.
<div id='section'>Paperid: <span id='pid'>491, <a href='https://arxiv.org/pdf/2403.19652.pdf' target='_blank'>https://arxiv.org/pdf/2403.19652.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sirui Xu, Ziyin Wang, Yu-Xiong Wang, Liang-Yan Gui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.19652">InterDreamer: Zero-Shot Text to 3D Dynamic Human-Object Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-conditioned human motion generation has experienced significant advancements with diffusion models trained on extensive motion capture data and corresponding textual annotations. However, extending such success to 3D dynamic human-object interaction (HOI) generation faces notable challenges, primarily due to the lack of large-scale interaction data and comprehensive descriptions that align with these interactions. This paper takes the initiative and showcases the potential of generating human-object interactions without direct training on text-interaction pair data. Our key insight in achieving this is that interaction semantics and dynamics can be decoupled. Being unable to learn interaction semantics through supervised training, we instead leverage pre-trained large models, synergizing knowledge from a large language model and a text-to-motion model. While such knowledge offers high-level control over interaction semantics, it cannot grasp the intricacies of low-level interaction dynamics. To overcome this issue, we further introduce a world model designed to comprehend simple physics, modeling how human actions influence object motion. By integrating these components, our novel framework, InterDreamer, is able to generate text-aligned 3D HOI sequences in a zero-shot manner. We apply InterDreamer to the BEHAVE and CHAIRS datasets, and our comprehensive experimental analysis demonstrates its capability to generate realistic and coherent interaction sequences that seamlessly align with the text directives.
<div id='section'>Paperid: <span id='pid'>492, <a href='https://arxiv.org/pdf/2403.11469.pdf' target='_blank'>https://arxiv.org/pdf/2403.11469.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaxu Zhang, Xin Chen, Gang Yu, Zhigang Tu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.11469">Generative Motion Stylization of Cross-structure Characters within Canonical Motion Space</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Stylized motion breathes life into characters. However, the fixed skeleton structure and style representation hinder existing data-driven motion synthesis methods from generating stylized motion for various characters. In this work, we propose a generative motion stylization pipeline, named MotionS, for synthesizing diverse and stylized motion on cross-structure characters using cross-modality style prompts. Our key insight is to embed motion style into a cross-modality latent space and perceive the cross-structure skeleton topologies, allowing for motion stylization within a canonical motion space. Specifically, the large-scale Contrastive-Language-Image-Pre-training (CLIP) model is leveraged to construct the cross-modality latent space, enabling flexible style representation within it. Additionally, two topology-encoded tokens are learned to capture the canonical and specific skeleton topologies, facilitating cross-structure topology shifting. Subsequently, the topology-shifted stylization diffusion is designed to generate motion content for the particular skeleton and stylize it in the shifted canonical motion space using multi-modality style descriptions. Through an extensive set of examples, we demonstrate the flexibility and generalizability of our pipeline across various characters and style descriptions. Qualitative and quantitative comparisons show the superiority of our pipeline over state-of-the-arts, consistently delivering high-quality stylized motion across a broad spectrum of skeletal structures.
<div id='section'>Paperid: <span id='pid'>493, <a href='https://arxiv.org/pdf/2306.12712.pdf' target='_blank'>https://arxiv.org/pdf/2306.12712.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>I Made Aswin Nahrendra, Minho Oh, Byeongho Yu, Hyungtae Lim, Hyun Myung
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.12712">Robust Recovery Motion Control for Quadrupedal Robots via Learned Terrain Imagination</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Quadrupedal robots have emerged as a cutting-edge platform for assisting humans, finding applications in tasks related to inspection and exploration in remote areas. Nevertheless, their floating base structure renders them susceptible to fall in cluttered environments, where manual recovery by a human operator may not always be feasible. Several recent studies have presented recovery controllers employing deep reinforcement learning algorithms. However, these controllers are not specifically designed to operate effectively in cluttered environments, such as stairs and slopes, which restricts their applicability. In this study, we propose a robust all-terrain recovery policy to facilitate rapid and secure recovery in cluttered environments. We substantiate the superiority of our proposed approach through simulations and real-world tests encompassing various terrain types.
<div id='section'>Paperid: <span id='pid'>494, <a href='https://arxiv.org/pdf/2306.05421.pdf' target='_blank'>https://arxiv.org/pdf/2306.05421.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sirui Xu, Yu-Xiong Wang, Liang-Yan Gui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.05421">Stochastic Multi-Person 3D Motion Forecasting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper aims to deal with the ignored real-world complexities in prior work on human motion forecasting, emphasizing the social properties of multi-person motion, the diversity of motion and social interactions, and the complexity of articulated motion. To this end, we introduce a novel task of stochastic multi-person 3D motion forecasting. We propose a dual-level generative modeling framework that separately models independent individual motion at the local level and social interactions at the global level. Notably, this dual-level modeling mechanism can be achieved within a shared generative model, through introducing learnable latent codes that represent intents of future motion and switching the codes' modes of operation at different levels. Our framework is general; we instantiate it with different generative models, including generative adversarial networks and diffusion models, and various multi-person forecasting models. Extensive experiments on CMU-Mocap, MuPoTS-3D, and SoMoF benchmarks show that our approach produces diverse and accurate multi-person predictions, significantly outperforming the state of the art.
<div id='section'>Paperid: <span id='pid'>495, <a href='https://arxiv.org/pdf/2212.04048.pdf' target='_blank'>https://arxiv.org/pdf/2212.04048.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xin Chen, Biao Jiang, Wen Liu, Zilong Huang, Bin Fu, Tao Chen, Jingyi Yu, Gang Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.04048">Executing your Commands via Motion Diffusion in Latent Space</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We study a challenging task, conditional human motion generation, which produces plausible human motion sequences according to various conditional inputs, such as action classes or textual descriptors. Since human motions are highly diverse and have a property of quite different distribution from conditional modalities, such as textual descriptors in natural languages, it is hard to learn a probabilistic mapping from the desired conditional modality to the human motion sequences. Besides, the raw motion data from the motion capture system might be redundant in sequences and contain noises; directly modeling the joint distribution over the raw motion sequences and conditional modalities would need a heavy computational overhead and might result in artifacts introduced by the captured noises. To learn a better representation of the various human motion sequences, we first design a powerful Variational AutoEncoder (VAE) and arrive at a representative and low-dimensional latent code for a human motion sequence. Then, instead of using a diffusion model to establish the connections between the raw motion sequences and the conditional inputs, we perform a diffusion process on the motion latent space. Our proposed Motion Latent-based Diffusion model (MLD) could produce vivid motion sequences conforming to the given conditional inputs and substantially reduce the computational overhead in both the training and inference stages. Extensive experiments on various human motion generation tasks demonstrate that our MLD achieves significant improvements over the state-of-the-art methods among extensive human motion generation tasks, with two orders of magnitude faster than previous diffusion models on raw motion sequences.
<div id='section'>Paperid: <span id='pid'>496, <a href='https://arxiv.org/pdf/2509.23852.pdf' target='_blank'>https://arxiv.org/pdf/2509.23852.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiheng Huang, Junran Peng, Silei Shen, Jingwei Yang, ZeJi Wei, ChenCheng Bai, Yonghao He, Wei Sui, Muyi Sun, Yan Liu, Xu-Cheng Yin, Man Zhang, Zhaoxiang Zhang, Chuanchen Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23852">SIG-Chat: Spatial Intent-Guided Conversational Gesture Generation Involving How, When and Where</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The accompanying actions and gestures in dialogue are often closely linked to interactions with the environment, such as looking toward the interlocutor or using gestures to point to the described target at appropriate moments. Speech and semantics guide the production of gestures by determining their timing (WHEN) and style (HOW), while the spatial locations of interactive objects dictate their directional execution (WHERE). Existing approaches either rely solely on descriptive language to generate motions or utilize audio to produce non-interactive gestures, thereby lacking the characterization of interactive timing and spatial intent. This significantly limits the applicability of conversational gesture generation, whether in robotics or in the fields of game and animation production. To address this gap, we present a full-stack solution. We first established a unique data collection method to simultaneously capture high-precision human motion and spatial intent. We then developed a generation model driven by audio, language, and spatial data, alongside dedicated metrics for evaluating interaction timing and spatial accuracy. Finally, we deployed the solution on a humanoid robot, enabling rich, context-aware physical interactions.
<div id='section'>Paperid: <span id='pid'>497, <a href='https://arxiv.org/pdf/2509.17450.pdf' target='_blank'>https://arxiv.org/pdf/2509.17450.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ying Feng, Hongjie Fang, Yinong He, Jingjing Chen, Chenxi Wang, Zihao He, Ruonan Liu, Cewu Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17450">Learning Dexterous Manipulation with Quantized Hand State</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dexterous robotic hands enable robots to perform complex manipulations that require fine-grained control and adaptability. Achieving such manipulation is challenging because the high degrees of freedom tightly couple hand and arm motions, making learning and control difficult. Successful dexterous manipulation relies not only on precise hand motions, but also on accurate spatial positioning of the arm and coordinated arm-hand dynamics. However, most existing visuomotor policies represent arm and hand actions in a single combined space, which often causes high-dimensional hand actions to dominate the coupled action space and compromise arm control. To address this, we propose DQ-RISE, which quantizes hand states to simplify hand motion prediction while preserving essential patterns, and applies a continuous relaxation that allows arm actions to diffuse jointly with these compact hand states. This design enables the policy to learn arm-hand coordination from data while preventing hand actions from overwhelming the action space. Experiments show that DQ-RISE achieves more balanced and efficient learning, paving the way toward structured and generalizable dexterous manipulation. Project website: http://rise-policy.github.io/DQ-RISE/
<div id='section'>Paperid: <span id='pid'>498, <a href='https://arxiv.org/pdf/2508.08588.pdf' target='_blank'>https://arxiv.org/pdf/2508.08588.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingyun Liang, Jingkai Zhou, Shikai Li, Chenjie Cao, Lei Sun, Yichen Qian, Weihua Chen, Fan Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08588">RealisMotion: Decomposed Human Motion Control and Video Generation in the World Space</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating human videos with realistic and controllable motions is a challenging task. While existing methods can generate visually compelling videos, they lack separate control over four key video elements: foreground subject, background video, human trajectory and action patterns. In this paper, we propose a decomposed human motion control and video generation framework that explicitly decouples motion from appearance, subject from background, and action from trajectory, enabling flexible mix-and-match composition of these elements. Concretely, we first build a ground-aware 3D world coordinate system and perform motion editing directly in the 3D space. Trajectory control is implemented by unprojecting edited 2D trajectories into 3D with focal-length calibration and coordinate transformation, followed by speed alignment and orientation adjustment; actions are supplied by a motion bank or generated via text-to-motion methods. Then, based on modern text-to-video diffusion transformer models, we inject the subject as tokens for full attention, concatenate the background along the channel dimension, and add motion (trajectory and action) control signals by addition. Such a design opens up the possibility for us to generate realistic videos of anyone doing anything anywhere. Extensive experiments on benchmark datasets and real-world cases demonstrate that our method achieves state-of-the-art performance on both element-wise controllability and overall video quality.
<div id='section'>Paperid: <span id='pid'>499, <a href='https://arxiv.org/pdf/2508.07162.pdf' target='_blank'>https://arxiv.org/pdf/2508.07162.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaotong Lin, Tianming Liang, Jian-Fang Hu, Kun-Yu Lin, Yulei Kang, Chunwei Tian, Jianhuang Lai, Wei-Shi Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.07162">CoopDiff: Anticipating 3D Human-object Interactions via Contact-consistent Decoupled Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D human-object interaction (HOI) anticipation aims to predict the future motion of humans and their manipulated objects, conditioned on the historical context. Generally, the articulated humans and rigid objects exhibit different motion patterns, due to their distinct intrinsic physical properties. However, this distinction is ignored by most of the existing works, which intend to capture the dynamics of both humans and objects within a single prediction model. In this work, we propose a novel contact-consistent decoupled diffusion framework CoopDiff, which employs two distinct branches to decouple human and object motion modeling, with the human-object contact points as shared anchors to bridge the motion generation across branches. The human dynamics branch is aimed to predict highly structured human motion, while the object dynamics branch focuses on the object motion with rigid translations and rotations. These two branches are bridged by a series of shared contact points with consistency constraint for coherent human-object motion prediction. To further enhance human-object consistency and prediction reliability, we propose a human-driven interaction module to guide object motion modeling. Extensive experiments on the BEHAVE and Human-object Interaction datasets demonstrate that our CoopDiff outperforms state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>500, <a href='https://arxiv.org/pdf/2506.12103.pdf' target='_blank'>https://arxiv.org/pdf/2506.12103.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Amazon AGI, Aaron Langford, Aayush Shah, Abhanshu Gupta, Abhimanyu Bhatter, Abhinav Goyal, Abhinav Mathur, Abhinav Mohanty, Abhishek Kumar, Abhishek Sethi, Abi Komma, Abner Pena, Achin Jain, Adam Kunysz, Adam Opyrchal, Adarsh Singh, Aditya Rawal, Adok Achar Budihal Prasad, AdriÃ  de Gispert, Agnika Kumar, Aishwarya Aryamane, Ajay Nair, Akilan M, Akshaya Iyengar, Akshaya Vishnu Kudlu Shanbhogue, Alan He, Alessandra Cervone, Alex Loeb, Alex Zhang, Alexander Fu, Alexander Lisnichenko, Alexander Zhipa, Alexandros Potamianos, Ali Kebarighotbi, Aliakbar Daronkolaei, Alok Parmesh, Amanjot Kaur Samra, Ameen Khan, Amer Rez, Amir Saffari, Amit Agarwalla, Amit Jhindal, Amith Mamidala, Ammar Asmro, Amulya Ballakur, Anand Mishra, Anand Sridharan, Anastasiia Dubinina, Andre Lenz, Andreas Doerr, Andrew Keating, Andrew Leaver, Andrew Smith, Andrew Wirth, Andy Davey, Andy Rosenbaum, Andy Sohn, Angela Chan, Aniket Chakrabarti, Anil Ramakrishna, Anirban Roy, Anita Iyer, Anjali Narayan-Chen, Ankith Yennu, Anna Dabrowska, Anna Gawlowska, Anna Rumshisky, Anna Turek, Anoop Deoras, Anton Bezruchkin, Anup Prasad, Anupam Dewan, Anwith Kiran, Apoorv Gupta, Aram Galstyan, Aravind Manoharan, Arijit Biswas, Arindam Mandal, Arpit Gupta, Arsamkhan Pathan, Arun Nagarajan, Arushan Rajasekaram, Arvind Sundararajan, Ashwin Ganesan, Ashwin Swaminathan, Athanasios Mouchtaris, Audrey Champeau, Avik Ray, Ayush Jaiswal, Ayush Sharma, Bailey Keefer, Balamurugan Muthiah, Beatriz Leon-Millan, Ben Koopman, Ben Li, Benjamin Biggs, Benjamin Ott, Bhanu Vinzamuri, Bharath Venkatesh, Bhavana Ganesh, Bhoomit Vasani, Bill Byrne, Bill Hsu, Bincheng Wang, Blake King, Blazej Gorny, Bo Feng, Bo Zheng, Bodhisattwa Paul, Bofan Sun, Bofeng Luo, Bowen Chen, Bowen Xie, Boya Yu, Brendan Jugan, Brett Panosh, Brian Collins, Brian Thompson, Can Karakus, Can Liu, Carl Lambrecht, Carly Lin, Carolyn Wang, Carrie Yuan, Casey Loyda, Cezary Walczak, Chalapathi Choppa, Chandana Satya Prakash, Chankrisna Richy Meas, Charith Peris, Charles Recaido, Charlie Xu, Charul Sharma, Chase Kernan, Chayut Thanapirom, Chengwei Su, Chenhao Xu, Chenhao Yin, Chentao Ye, Chenyang Tao, Chethan Parameshwara, Ching-Yun Chang, Chong Li, Chris Hench, Chris Tran, Christophe Dupuy, Christopher Davis, Christopher DiPersio, Christos Christodoulopoulos, Christy Li, Chun Chen, Claudio Delli Bovi, Clement Chung, Cole Hawkins, Connor Harris, Corey Ropell, Cynthia He, DK Joo, Dae Yon Hwang, Dan Rosen, Daniel Elkind, Daniel Pressel, Daniel Zhang, Danielle Kimball, Daniil Sorokin, Dave Goodell, Davide Modolo, Dawei Zhu, Deepikaa Suresh, Deepti Ragha, Denis Filimonov, Denis Foo Kune, Denis Romasanta Rodriguez, Devamanyu Hazarika, Dhananjay Ram, Dhawal Parkar, Dhawal Patel, Dhwanil Desai, Dinesh Singh Rajput, Disha Sule, Diwakar Singh, Dmitriy Genzel, Dolly Goldenberg, Dongyi He, Dumitru Hanciu, Dushan Tharmal, Dzmitry Siankovich, Edi Cikovic, Edwin Abraham, Ekraam Sabir, Elliott Olson, Emmett Steven, Emre Barut, Eric Jackson, Ethan Wu, Evelyn Chen, Ezhilan Mahalingam, Fabian Triefenbach, Fan Yang, Fangyu Liu, Fanzi Wu, Faraz Tavakoli, Farhad Khozeimeh, Feiyang Niu, Felix Hieber, Feng Li, Firat Elbey, Florian Krebs, Florian Saupe, Florian SprÃ¼nken, Frank Fan, Furqan Khan, Gabriela De Vincenzo, Gagandeep Kang, George Ding, George He, George Yeung, Ghada Qaddoumi, Giannis Karamanolakis, Goeric Huybrechts, Gokul Maddali, Gonzalo Iglesias, Gordon McShane, Gozde Sahin, Guangtai Huang, Gukyeong Kwon, Gunnar A. Sigurdsson, Gurpreet Chadha, Gururaj Kosuru, Hagen Fuerstenau, Hah Hah, Haja Maideen, Hajime Hosokawa, Han Liu, Han-Kai Hsu, Hann Wang, Hao Li, Hao Yang, Haofeng Zhu, Haozheng Fan, Harman Singh, Harshavardhan Kaluvala, Hashim Saeed, He Xie, Helian Feng, Hendrix Luo, Hengzhi Pei, Henrik Nielsen, Hesam Ilati, Himanshu Patel, Hongshan Li, Hongzhou Lin, Hussain Raza, Ian Cullinan, Imre Kiss, Inbarasan Thangamani, Indrayani Fadnavis, Ionut Teodor Sorodoc, Irem Ertuerk, Iryna Yemialyanava, Ishan Soni, Ismail Jelal, Ivan Tse, Jack FitzGerald, Jack Zhao, Jackson Rothgeb, Jacky Lee, Jake Jung, Jakub Debski, Jakub Tomczak, James Jeun, James Sanders, Jason Crowley, Jay Lee, Jayakrishna Anvesh Paidy, Jayant Tiwari, Jean Farmer, Jeff Solinsky, Jenna Lau, Jeremy Savareese, Jerzy Zagorski, Ji Dai, Jiacheng, Gu, Jiahui Li, Jian, Zheng, Jianhua Lu, Jianhua Wang, Jiawei Dai, Jiawei Mo, Jiaxi Xu, Jie Liang, Jie Yang, Jim Logan, Jimit Majmudar, Jing Liu, Jinghong Miao, Jingru Yi, Jingyang Jin, Jiun-Yu Kao, Jixuan Wang, Jiyang Wang, Joe Pemberton, Joel Carlson, Joey Blundell, John Chin-Jew, John He, Jonathan Ho, Jonathan Hueser, Jonathan Lunt, Jooyoung Lee, Joshua Tan, Joyjit Chatterjee, Judith Gaspers, Jue Wang, Jun Fang, Jun Tang, Jun Wan, Jun Wu, Junlei Wang, Junyi Shi, Justin Chiu, Justin Satriano, Justin Yee, Jwala Dhamala, Jyoti Bansal, Kai Zhen, Kai-Wei Chang, Kaixiang Lin, Kalyan Raman, Kanthashree Mysore Sathyendra, Karabo Moroe, Karan Bhandarkar, Karan Kothari, Karolina Owczarzak, Karthick Gopalswamy, Karthick Ravi, Karthik Ramakrishnan, Karthika Arumugam, Kartik Mehta, Katarzyna Konczalska, Kavya Ravikumar, Ke Tran, Kechen Qin, Kelin Li, Kelvin Li, Ketan Kulkarni, Kevin Angelo Rodrigues, Keyur Patel, Khadige Abboud, Kiana Hajebi, Klaus Reiter, Kris Schultz, Krishna Anisetty, Krishna Kotnana, Kristen Li, Kruthi Channamallikarjuna, Krzysztof Jakubczyk, Kuba Pierewoj, Kunal Pal, Kunwar Srivastav, Kyle Bannerman, Lahari Poddar, Lakshmi Prasad, Larry Tseng, Laxmikant Naik, Leena Chennuru Vankadara, Lenon Minorics, Leo Liu, Leonard Lausen, Leonardo F. R. Ribeiro, Li Zhang, Lili Gehorsam, Ling Qi, Lisa Bauer, Lori Knapp, Lu Zeng, Lucas Tong, Lulu Wong, Luoxin Chen, Maciej Rudnicki, Mahdi Namazifar, Mahesh Jaliminche, Maira Ladeira Tanke, Manasi Gupta, Mandeep Ahlawat, Mani Khanuja, Mani Sundaram, Marcin Leyk, Mariusz Momotko, Markus Boese, Markus Dreyer, Markus Mueller, Mason Fu, Mateusz GÃ³rski, Mateusz Mastalerczyk, Matias Mora, Matt Johnson, Matt Scott, Matthew Wen, Max Barysau, Maya Boumerdassi, Maya Krishnan, Mayank Gupta, Mayank Hirani, Mayank Kulkarni, Meganathan Narayanasamy, Melanie Bradford, Melanie Gens, Melissa Burke, Meng Jin, Miao Chen, Michael Denkowski, Michael Heymel, Michael Krestyaninov, Michal Obirek, Michalina Wichorowska, MichaÅ Miotk, Milosz Watroba, Mingyi Hong, Mingzhi Yu, Miranda Liu, Mohamed Gouda, Mohammad El-Shabani, Mohammad Ghavamzadeh, Mohit Bansal, Morteza Ziyadi, Nan Xia, Nathan Susanj, Nav Bhasin, Neha Goswami, Nehal Belgamwar, Nicolas Anastassacos, Nicolas Bergeron, Nidhi Jain, Nihal Jain, Niharika Chopparapu, Nik Xu, Nikko Strom, Nikolaos Malandrakis, Nimisha Mishra, Ninad Parkhi, Ninareh Mehrabi, Nishita Sant, Nishtha Gupta, Nitesh Sekhar, Nithin Rajeev, Nithish Raja Chidambaram, Nitish Dhar, Noor Bhagwagar, Noy Konforty, Omar Babu, Omid Razavi, Orchid Majumder, Osama Dar, Oscar Hsu, Pablo Kvitca, Pallavi Pandey, Parker Seegmiller, Patrick Lange, Paul Ferraro, Payal Motwani, Pegah Kharazmi, Pei Wang, Pengfei Liu, Peter Bradtke, Peter GÃ¶tz, Peter Zhou, Pichao Wang, Piotr Poskart, Pooja Sonawane, Pradeep Natarajan, Pradyun Ramadorai, Pralam Shah, Prasad Nirantar, Prasanthi Chavali, Prashan Wanigasekara, Prashant Saraf, Prashun Dey, Pratyush Pant, Prerak Pradhan, Preyaa Patel, Priyanka Dadlani, Prudhvee Narasimha Sadha, Qi Dong, Qian Hu, Qiaozi, Gao, Qing Liu, Quinn Lam, Quynh Do, R. Manmatha, Rachel Willis, Rafael Liu, Rafal Ellert, Rafal Kalinski, Rafi Al Attrach, Ragha Prasad, Ragini Prasad, Raguvir Kunani, Rahul Gupta, Rahul Sharma, Rahul Tewari, Rajaganesh Baskaran, Rajan Singh, Rajiv Gupta, Rajiv Reddy, Rajshekhar Das, Rakesh Chada, Rakesh Vaideeswaran Mahesh, Ram Chandrasekaran, Ramesh Nallapati, Ran Xue, Rashmi Gangadharaiah, Ravi Rachakonda, Renxian Zhang, Rexhina Blloshmi, Rishabh Agrawal, Robert Enyedi, Robert Lowe, Robik Shrestha, Robinson Piramuthu, Rohail Asad, Rohan Khanna, Rohan Mukherjee, Rohit Mittal, Rohit Prasad, Rohith Mysore Vijaya Kumar, Ron Diamant, Ruchita Gupta, Ruiwen Li, Ruoying Li, Rushabh Fegade, Ruxu Zhang, Ryan Arbow, Ryan Chen, Ryan Gabbard, Ryan Hoium, Ryan King, Sabarishkumar Iyer, Sachal Malick, Sahar Movaghati, Sai Balakavi, Sai Jakka, Sai Kashyap Paruvelli, Sai Muralidhar Jayanthi, Saicharan Shriram Mujumdar, Sainyam Kapoor, Sajjad Beygi, Saket Dingliwal, Saleh Soltan, Sam Ricklin, Sam Tucker, Sameer Sinha, Samridhi Choudhary, Samson Tan, Samuel Broscheit, Samuel Schulter, Sanchit Agarwal, Sandeep Atluri, Sander Valstar, Sanjana Shankar, Sanyukta Sanyukta, Sarthak Khanna, Sarvpriye Khetrapal, Satish Janakiraman, Saumil Shah, Saurabh Akolkar, Saurabh Giri, Saurabh Khandelwal, Saurabh Pawar, Saurabh Sahu, Sean Huang, Sejun Ra, Senthilkumar Gopal, Sergei Dobroshinsky, Shadi Saba, Shamik Roy, Shamit Lal, Shankar Ananthakrishnan, Sharon Li, Shashwat Srijan, Shekhar Bhide, Sheng Long Tang, Sheng Zha, Shereen Oraby, Sherif Mostafa, Shiqi Li, Shishir Bharathi, Shivam Prakash, Shiyuan Huang, Shreya Yembarwar, Shreyas Pansare, Shreyas Subramanian, Shrijeet Joshi, Shuai Liu, Shuai Tang, Shubham Chandak, Shubham Garg, Shubham Katiyar, Shubham Mehta, Shubham Srivastav, Shuo Yang, Siddalingesha D S, Siddharth Choudhary, Siddharth Singh Senger, Simon Babb, Sina Moeini, Siqi Deng, Siva Loganathan, Slawomir Domagala, Sneha Narkar, Sneha Wadhwa, Songyang Zhang, Songyao Jiang, Sony Trenous, Soumajyoti Sarkar, Soumya Saha, Sourabh Reddy, Sourav Dokania, Spurthideepika Sandiri, Spyros Matsoukas, Sravan Bodapati, Sri Harsha Reddy Wdaru, Sridevi Yagati Venkateshdatta, Srikanth Ronanki, Srinivasan R Veeravanallur, Sriram Venkatapathy, Sriramprabhu Sankaraguru, Sruthi Gorantla, Sruthi Karuturi, Stefan Schroedl, Subendhu Rongali, Subhasis Kundu, Suhaila Shakiah, Sukriti Tiwari, Sumit Bharti, Sumita Sami, Sumith Mathew, Sunny Yu, Sunwoo Kim, Suraj Bajirao Malode, Susana Cumplido Riel, Swapnil Palod, Swastik Roy, Syed Furqhan, Tagyoung Chung, Takuma Yoshitani, Taojiannan Yang, Tejaswi Chillakura, Tejwant Bajwa, Temi Lajumoke, Thanh Tran, Thomas Gueudre, Thomas Jung, Tianhui Li, Tim Seemman, Timothy Leffel, Tingting Xiang, Tirth Patel, Tobias Domhan, Tobias Falke, Toby Guo, Tom Li, Tomasz Horszczaruk, Tomasz Jedynak, Tushar Kulkarni, Tyst Marin, Tytus Metrycki, Tzu-Yen Wang, Umang Jain, Upendra Singh, Utkarsh Chirimar, Vaibhav Gupta, Vanshil Shah, Varad Deshpande, Varad Gunjal, Varsha Srikeshava, Varsha Vivek, Varun Bharadwaj, Varun Gangal, Varun Kumar, Venkatesh Elango, Vicente Ordonez, Victor Soto, Vignesh Radhakrishnan, Vihang Patel, Vikram Singh, Vinay Varma Kolanuvada, Vinayshekhar Bannihatti Kumar, Vincent Auvray, Vincent Cartillier, Vincent Ponzo, Violet Peng, Vishal Khandelwal, Vishal Naik, Vishvesh Sahasrabudhe, Vitaliy Korolev, Vivek Gokuladas, Vivek Madan, Vivek Subramanian, Volkan Cevher, Vrinda Gupta, Wael Hamza, Wei Zhang, Weitong Ruan, Weiwei Cheng, Wen Zhang, Wenbo Zhao, Wenyan Yao, Wenzhuo Ouyang, Wesley Dashner, William Campbell, William Lin, Willian Martin, Wyatt Pearson, Xiang Jiang, Xiangxing Lu, Xiangyang Shi, Xianwen Peng, Xiaofeng Gao, Xiaoge Jiang, Xiaohan Fei, Xiaohui Wang, Xiaozhou Joey Zhou, Xin Feng, Xinyan Zhao, Xinyao Wang, Xinyu Li, Xu Zhang, Xuan Wang, Xuandi Fu, Xueling Yuan, Xuning Wang, Yadunandana Rao, Yair Tavizon, Yan Rossiytsev, Yanbei Chen, Yang Liu, Yang Zou, Yangsook Park, Yannick Versley, Yanyan Zhang, Yash Patel, Yen-Cheng Lu, Yi Pan, Yi-Hsiang, Lai, Yichen Hu, Yida Wang, Yiheng Zhou, Yilin Xiang, Ying Shi, Ying Wang, Yishai Galatzer, Yongxin Wang, Yorick Shen, Yuchen Sun, Yudi Purwatama, Yue, Wu, Yue Gu, Yuechun Wang, Yujun Zeng, Yuncong Chen, Yunke Zhou, Yusheng Xie, Yvon Guy, Zbigniew Ambrozinski, Zhaowei Cai, Zhen Zhang, Zheng Wang, Zhenghui Jin, Zhewei Zhao, Zhiheng Li, Zhiheng Luo, Zhikang Zhang, Zhilin Fang, Zhiqi Bu, Zhiyuan Wang, Zhizhong Li, Zijian Wang, Zimeng, Qiu, Zishi Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.12103">The Amazon Nova Family of Models: Technical Report and Model Card</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present Amazon Nova, a new generation of state-of-the-art foundation models that deliver frontier intelligence and industry-leading price performance. Amazon Nova Pro is a highly-capable multimodal model with the best combination of accuracy, speed, and cost for a wide range of tasks. Amazon Nova Lite is a low-cost multimodal model that is lightning fast for processing images, video, documents and text. Amazon Nova Micro is a text-only model that delivers our lowest-latency responses at very low cost. Amazon Nova Canvas is an image generation model that creates professional grade images with rich customization controls. Amazon Nova Reel is a video generation model offering high-quality outputs, customization, and motion control. Our models were built responsibly and with a commitment to customer trust, security, and reliability. We report benchmarking results for core capabilities, agentic performance, long context, functional adaptation, runtime performance, and human evaluation.
<div id='section'>Paperid: <span id='pid'>501, <a href='https://arxiv.org/pdf/2506.02618.pdf' target='_blank'>https://arxiv.org/pdf/2506.02618.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jialiang Zhang, Haoran Geng, Yang You, Congyue Deng, Pieter Abbeel, Jitendra Malik, Leonidas Guibas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.02618">Rodrigues Network for Learning Robot Actions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding and predicting articulated actions is important in robot learning. However, common architectures such as MLPs and Transformers lack inductive biases that reflect the underlying kinematic structure of articulated systems. To this end, we propose the Neural Rodrigues Operator, a learnable generalization of the classical forward kinematics operation, designed to inject kinematics-aware inductive bias into neural computation. Building on this operator, we design the Rodrigues Network (RodriNet), a novel neural architecture specialized for processing actions. We evaluate the expressivity of our network on two synthetic tasks on kinematic and motion prediction, showing significant improvements compared to standard backbones. We further demonstrate its effectiveness in two realistic applications: (i) imitation learning on robotic benchmarks with the Diffusion Policy, and (ii) single-image 3D hand reconstruction. Our results suggest that integrating structured kinematic priors into the network architecture improves action learning in various domains.
<div id='section'>Paperid: <span id='pid'>502, <a href='https://arxiv.org/pdf/2501.18543.pdf' target='_blank'>https://arxiv.org/pdf/2501.18543.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Placido Falqueto, Alberto Sanfeliu, Luigi Palopoli, Daniele Fontanelli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.18543">Learning Priors of Human Motion With Vision Transformers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A clear understanding of where humans move in a scenario, their usual paths and speeds, and where they stop, is very important for different applications, such as mobility studies in urban areas or robot navigation tasks within human-populated environments. We propose in this article, a neural architecture based on Vision Transformers (ViTs) to provide this information. This solution can arguably capture spatial correlations more effectively than Convolutional Neural Networks (CNNs). In the paper, we describe the methodology and proposed neural architecture and show the experiments' results with a standard dataset. We show that the proposed ViT architecture improves the metrics compared to a method based on a CNN.
<div id='section'>Paperid: <span id='pid'>503, <a href='https://arxiv.org/pdf/2501.02690.pdf' target='_blank'>https://arxiv.org/pdf/2501.02690.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weikang Bian, Zhaoyang Huang, Xiaoyu Shi, Yijin Li, Fu-Yun Wang, Hongsheng Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.02690">GS-DiT: Advancing Video Generation with Pseudo 4D Gaussian Fields through Efficient Dense 3D Point Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>4D video control is essential in video generation as it enables the use of sophisticated lens techniques, such as multi-camera shooting and dolly zoom, which are currently unsupported by existing methods. Training a video Diffusion Transformer (DiT) directly to control 4D content requires expensive multi-view videos. Inspired by Monocular Dynamic novel View Synthesis (MDVS) that optimizes a 4D representation and renders videos according to different 4D elements, such as camera pose and object motion editing, we bring pseudo 4D Gaussian fields to video generation. Specifically, we propose a novel framework that constructs a pseudo 4D Gaussian field with dense 3D point tracking and renders the Gaussian field for all video frames. Then we finetune a pretrained DiT to generate videos following the guidance of the rendered video, dubbed as GS-DiT. To boost the training of the GS-DiT, we also propose an efficient Dense 3D Point Tracking (D3D-PT) method for the pseudo 4D Gaussian field construction. Our D3D-PT outperforms SpatialTracker, the state-of-the-art sparse 3D point tracking method, in accuracy and accelerates the inference speed by two orders of magnitude. During the inference stage, GS-DiT can generate videos with the same dynamic content while adhering to different camera parameters, addressing a significant limitation of current video generation models. GS-DiT demonstrates strong generalization capabilities and extends the 4D controllability of Gaussian splatting to video generation beyond just camera poses. It supports advanced cinematic effects through the manipulation of the Gaussian field and camera intrinsics, making it a powerful tool for creative video production. Demos are available at https://wkbian.github.io/Projects/GS-DiT/.
<div id='section'>Paperid: <span id='pid'>504, <a href='https://arxiv.org/pdf/2412.14484.pdf' target='_blank'>https://arxiv.org/pdf/2412.14484.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kunpeng Song, Tingbo Hou, Zecheng He, Haoyu Ma, Jialiang Wang, Animesh Sinha, Sam Tsai, Yaqiao Luo, Xiaoliang Dai, Li Chen, Xide Xia, Peizhao Zhang, Peter Vajda, Ahmed Elgammal, Felix Juefei-Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.14484">Llama Learns to Direct: DirectorLLM for Human-Centric Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we introduce DirectorLLM, a novel video generation model that employs a large language model (LLM) to orchestrate human poses within videos. As foundational text-to-video models rapidly evolve, the demand for high-quality human motion and interaction grows. To address this need and enhance the authenticity of human motions, we extend the LLM from a text generator to a video director and human motion simulator. Utilizing open-source resources from Llama 3, we train the DirectorLLM to generate detailed instructional signals, such as human poses, to guide video generation. This approach offloads the simulation of human motion from the video generator to the LLM, effectively creating informative outlines for human-centric scenes. These signals are used as conditions by the video renderer, facilitating more realistic and prompt-following video generation. As an independent LLM module, it can be applied to different video renderers, including UNet and DiT, with minimal effort. Experiments on automatic evaluation benchmarks and human evaluations show that our model outperforms existing ones in generating videos with higher human motion fidelity, improved prompt faithfulness, and enhanced rendered subject naturalness.
<div id='section'>Paperid: <span id='pid'>505, <a href='https://arxiv.org/pdf/2412.09623.pdf' target='_blank'>https://arxiv.org/pdf/2412.09623.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weiqi Li, Shijie Zhao, Chong Mou, Xuhan Sheng, Zhenyu Zhang, Qian Wang, Junlin Li, Li Zhang, Jian Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.09623">OmniDrag: Enabling Motion Control for Omnidirectional Image-to-Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As virtual reality gains popularity, the demand for controllable creation of immersive and dynamic omnidirectional videos (ODVs) is increasing. While previous text-to-ODV generation methods achieve impressive results, they struggle with content inaccuracies and inconsistencies due to reliance solely on textual inputs. Although recent motion control techniques provide fine-grained control for video generation, directly applying these methods to ODVs often results in spatial distortion and unsatisfactory performance, especially with complex spherical motions. To tackle these challenges, we propose OmniDrag, the first approach enabling both scene- and object-level motion control for accurate, high-quality omnidirectional image-to-video generation. Building on pretrained video diffusion models, we introduce an omnidirectional control module, which is jointly fine-tuned with temporal attention layers to effectively handle complex spherical motion. In addition, we develop a novel spherical motion estimator that accurately extracts motion-control signals and allows users to perform drag-style ODV generation by simply drawing handle and target points. We also present a new dataset, named Move360, addressing the scarcity of ODV data with large scene and object motions. Experiments demonstrate the significant superiority of OmniDrag in achieving holistic scene-level and fine-grained object-level control for ODV generation. The project page is available at https://lwq20020127.github.io/OmniDrag.
<div id='section'>Paperid: <span id='pid'>506, <a href='https://arxiv.org/pdf/2412.02700.pdf' target='_blank'>https://arxiv.org/pdf/2412.02700.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Daniel Geng, Charles Herrmann, Junhwa Hur, Forrester Cole, Serena Zhang, Tobias Pfaff, Tatiana Lopez-Guevara, Carl Doersch, Yusuf Aytar, Michael Rubinstein, Chen Sun, Oliver Wang, Andrew Owens, Deqing Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.02700">Motion Prompting: Controlling Video Generation with Motion Trajectories</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion control is crucial for generating expressive and compelling video content; however, most existing video generation models rely mainly on text prompts for control, which struggle to capture the nuances of dynamic actions and temporal compositions. To this end, we train a video generation model conditioned on spatio-temporally sparse or dense motion trajectories. In contrast to prior motion conditioning work, this flexible representation can encode any number of trajectories, object-specific or global scene motion, and temporally sparse motion; due to its flexibility we refer to this conditioning as motion prompts. While users may directly specify sparse trajectories, we also show how to translate high-level user requests into detailed, semi-dense motion prompts, a process we term motion prompt expansion. We demonstrate the versatility of our approach through various applications, including camera and object motion control, "interacting" with an image, motion transfer, and image editing. Our results showcase emergent behaviors, such as realistic physics, suggesting the potential of motion prompts for probing video models and interacting with future generative world models. Finally, we evaluate quantitatively, conduct a human study, and demonstrate strong performance. Video results are available on our webpage: https://motion-prompting.github.io/
<div id='section'>Paperid: <span id='pid'>507, <a href='https://arxiv.org/pdf/2410.10790.pdf' target='_blank'>https://arxiv.org/pdf/2410.10790.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianqi Chen, Panwen Hu, Xiaojun Chang, Zhenwei Shi, Michael Kampffmeyer, Xiaodan Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.10790">Sitcom-Crafter: A Plot-Driven Human Motion Generation System in 3D Scenes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in human motion synthesis have focused on specific types of motions, such as human-scene interaction, locomotion or human-human interaction, however, there is a lack of a unified system capable of generating a diverse combination of motion types. In response, we introduce Sitcom-Crafter, a comprehensive and extendable system for human motion generation in 3D space, which can be guided by extensive plot contexts to enhance workflow efficiency for anime and game designers. The system is comprised of eight modules, three of which are dedicated to motion generation, while the remaining five are augmentation modules that ensure consistent fusion of motion sequences and system functionality. Central to the generation modules is our novel 3D scene-aware human-human interaction module, which addresses collision issues by synthesizing implicit 3D Signed Distance Function (SDF) points around motion spaces, thereby minimizing human-scene collisions without additional data collection costs. Complementing this, our locomotion and human-scene interaction modules leverage existing methods to enrich the system's motion generation capabilities. Augmentation modules encompass plot comprehension for command generation, motion synchronization for seamless integration of different motion types, hand pose retrieval to enhance motion realism, motion collision revision to prevent human collisions, and 3D retargeting to ensure visual fidelity. Experimental evaluations validate the system's ability to generate high-quality, diverse, and physically realistic motions, underscoring its potential for advancing creative workflows. Project page: https://windvchen.github.io/Sitcom-Crafter.
<div id='section'>Paperid: <span id='pid'>508, <a href='https://arxiv.org/pdf/2406.16601.pdf' target='_blank'>https://arxiv.org/pdf/2406.16601.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sifan Wu, Zhenguang Liu, Beibei Zhang, Roger Zimmermann, Zhongjie Ba, Xiaosong Zhang, Kui Ren
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.16601">Do As I Do: Pose Guided Human Motion Copy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion copy is an intriguing yet challenging task in artificial intelligence and computer vision, which strives to generate a fake video of a target person performing the motion of a source person. The problem is inherently challenging due to the subtle human-body texture details to be generated and the temporal consistency to be considered. Existing approaches typically adopt a conventional GAN with an L1 or L2 loss to produce the target fake video, which intrinsically necessitates a large number of training samples that are challenging to acquire. Meanwhile, current methods still have difficulties in attaining realistic image details and temporal consistency, which unfortunately can be easily perceived by human observers. Motivated by this, we try to tackle the issues from three aspects: (1) We constrain pose-to-appearance generation with a perceptual loss and a theoretically motivated Gromov-Wasserstein loss to bridge the gap between pose and appearance. (2) We present an episodic memory module in the pose-to-appearance generation to propel continuous learning that helps the model learn from its past poor generations. We also utilize geometrical cues of the face to optimize facial details and refine each key body part with a dedicated local GAN. (3) We advocate generating the foreground in a sequence-to-sequence manner rather than a single-frame manner, explicitly enforcing temporal inconsistency. Empirical results on five datasets, iPER, ComplexMotion, SoloDance, Fish, and Mouse datasets, demonstrate that our method is capable of generating realistic target videos while precisely copying motion from a source video. Our method significantly outperforms state-of-the-art approaches and gains 7.2% and 12.4% improvements in PSNR and FID respectively.
<div id='section'>Paperid: <span id='pid'>509, <a href='https://arxiv.org/pdf/2405.17405.pdf' target='_blank'>https://arxiv.org/pdf/2405.17405.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruizhi Shao, Youxin Pang, Zerong Zheng, Jingxiang Sun, Yebin Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.17405">Human4DiT: 360-degree Human Video Generation with 4D Diffusion Transformer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a novel approach for generating 360-degree high-quality, spatio-temporally coherent human videos from a single image. Our framework combines the strengths of diffusion transformers for capturing global correlations across viewpoints and time, and CNNs for accurate condition injection. The core is a hierarchical 4D transformer architecture that factorizes self-attention across views, time steps, and spatial dimensions, enabling efficient modeling of the 4D space. Precise conditioning is achieved by injecting human identity, camera parameters, and temporal signals into the respective transformers. To train this model, we collect a multi-dimensional dataset spanning images, videos, multi-view data, and limited 4D footage, along with a tailored multi-dimensional training strategy. Our approach overcomes the limitations of previous methods based on generative adversarial networks or vanilla diffusion models, which struggle with complex motions, viewpoint changes, and generalization. Through extensive experiments, we demonstrate our method's ability to synthesize 360-degree realistic, coherent human motion videos, paving the way for advanced multimedia applications in areas such as virtual reality and animation.
<div id='section'>Paperid: <span id='pid'>510, <a href='https://arxiv.org/pdf/2403.13331.pdf' target='_blank'>https://arxiv.org/pdf/2403.13331.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaosong Jia, Shaoshuai Shi, Zijun Chen, Li Jiang, Wenlong Liao, Tao He, Junchi Yan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.13331">AMP: Autoregressive Motion Prediction Revisited with Next Token Prediction for Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As an essential task in autonomous driving (AD), motion prediction aims to predict the future states of surround objects for navigation. One natural solution is to estimate the position of other agents in a step-by-step manner where each predicted time-step is conditioned on both observed time-steps and previously predicted time-steps, i.e., autoregressive prediction. Pioneering works like SocialLSTM and MFP design their decoders based on this intuition. However, almost all state-of-the-art works assume that all predicted time-steps are independent conditioned on observed time-steps, where they use a single linear layer to generate positions of all time-steps simultaneously. They dominate most motion prediction leaderboards due to the simplicity of training MLPs compared to autoregressive networks.
  In this paper, we introduce the GPT style next token prediction into motion forecasting. In this way, the input and output could be represented in a unified space and thus the autoregressive prediction becomes more feasible. However, different from language data which is composed of homogeneous units -words, the elements in the driving scene could have complex spatial-temporal and semantic relations. To this end, we propose to adopt three factorized attention modules with different neighbors for information aggregation and different position encoding styles to capture their relations, e.g., encoding the transformation between coordinate systems for spatial relativity while adopting RoPE for temporal relativity. Empirically, by equipping with the aforementioned tailored designs, the proposed method achieves state-of-the-art performance in the Waymo Open Motion and Waymo Interaction datasets. Notably, AMP outperforms other recent autoregressive motion prediction methods: MotionLM and StateTransformer, which demonstrates the effectiveness of the proposed designs.
<div id='section'>Paperid: <span id='pid'>511, <a href='https://arxiv.org/pdf/2403.07788.pdf' target='_blank'>https://arxiv.org/pdf/2403.07788.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chen Wang, Haochen Shi, Weizhuo Wang, Ruohan Zhang, Li Fei-Fei, C. Karen Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.07788">DexCap: Scalable and Portable Mocap Data Collection System for Dexterous Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Imitation learning from human hand motion data presents a promising avenue for imbuing robots with human-like dexterity in real-world manipulation tasks. Despite this potential, substantial challenges persist, particularly with the portability of existing hand motion capture (mocap) systems and the complexity of translating mocap data into effective robotic policies. To tackle these issues, we introduce DexCap, a portable hand motion capture system, alongside DexIL, a novel imitation algorithm for training dexterous robot skills directly from human hand mocap data. DexCap offers precise, occlusion-resistant tracking of wrist and finger motions based on SLAM and electromagnetic field together with 3D observations of the environment. Utilizing this rich dataset, DexIL employs inverse kinematics and point cloud-based imitation learning to seamlessly replicate human actions with robot hands. Beyond direct learning from human motion, DexCap also offers an optional human-in-the-loop correction mechanism during policy rollouts to refine and further improve task performance. Through extensive evaluation across six challenging dexterous manipulation tasks, our approach not only demonstrates superior performance but also showcases the system's capability to effectively learn from in-the-wild mocap data, paving the way for future data collection methods in the pursuit of human-level robot dexterity. More details can be found at https://dex-cap.github.io
<div id='section'>Paperid: <span id='pid'>512, <a href='https://arxiv.org/pdf/2309.13813.pdf' target='_blank'>https://arxiv.org/pdf/2309.13813.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Peiyu Luo, Shilong Yao, Yiyao Yue, Jiankun Wang, Hong Yan, Max Q. -H. Meng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.13813">Efficient RRT*-based Safety-Constrained Motion Planning for Continuum Robots in Dynamic Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Continuum robots, characterized by their high flexibility and infinite degrees of freedom (DoFs), have gained prominence in applications such as minimally invasive surgery and hazardous environment exploration. However, the intrinsic complexity of continuum robots requires a significant amount of time for their motion planning, posing a hurdle to their practical implementation. To tackle these challenges, efficient motion planning methods such as Rapidly Exploring Random Trees (RRT) and its variant, RRT*, have been employed. This paper introduces a unique RRT*-based motion control method tailored for continuum robots. Our approach embeds safety constraints derived from the robots' posture states, facilitating autonomous navigation and obstacle avoidance in rapidly changing environments. Simulation results show efficient trajectory planning amidst multiple dynamic obstacles and provide a robust performance evaluation based on the generated postures. Finally, preliminary tests were conducted on a two-segment cable-driven continuum robot prototype, confirming the effectiveness of the proposed planning approach. This method is versatile and can be adapted and deployed for various types of continuum robots through parameter adjustments.
<div id='section'>Paperid: <span id='pid'>513, <a href='https://arxiv.org/pdf/2303.14346.pdf' target='_blank'>https://arxiv.org/pdf/2303.14346.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sanbao Su, Songyang Han, Yiming Li, Zhili Zhang, Chen Feng, Caiwen Ding, Fei Miao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.14346">Collaborative Multi-Object Tracking with Conformal Uncertainty Propagation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Object detection and multiple object tracking (MOT) are essential components of self-driving systems. Accurate detection and uncertainty quantification are both critical for onboard modules, such as perception, prediction, and planning, to improve the safety and robustness of autonomous vehicles. Collaborative object detection (COD) has been proposed to improve detection accuracy and reduce uncertainty by leveraging the viewpoints of multiple agents. However, little attention has been paid to how to leverage the uncertainty quantification from COD to enhance MOT performance. In this paper, as the first attempt to address this challenge, we design an uncertainty propagation framework called MOT-CUP. Our framework first quantifies the uncertainty of COD through direct modeling and conformal prediction, and propagates this uncertainty information into the motion prediction and association steps. MOT-CUP is designed to work with different collaborative object detectors and baseline MOT algorithms. We evaluate MOT-CUP on V2X-Sim, a comprehensive collaborative perception dataset, and demonstrate a 2% improvement in accuracy and a 2.67X reduction in uncertainty compared to the baselines, e.g. SORT and ByteTrack. In scenarios characterized by high occlusion levels, our MOT-CUP demonstrates a noteworthy $4.01\%$ improvement in accuracy. MOT-CUP demonstrates the importance of uncertainty quantification in both COD and MOT, and provides the first attempt to improve the accuracy and reduce the uncertainty in MOT based on COD through uncertainty propagation. Our code is public on https://coperception.github.io/MOT-CUP/.
<div id='section'>Paperid: <span id='pid'>514, <a href='https://arxiv.org/pdf/2303.10945.pdf' target='_blank'>https://arxiv.org/pdf/2303.10945.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junyang Chen, Xiaoyu Xian, Zhijing Yang, Tianshui Chen, Yongyi Lu, Yukai Shi, Jinshan Pan, Liang Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.10945">Open-World Pose Transfer via Sequential Test-Time Adaption</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pose transfer aims to transfer a given person into a specified posture, has recently attracted considerable attention. A typical pose transfer framework usually employs representative datasets to train a discriminative model, which is often violated by out-of-distribution (OOD) instances. Recently, test-time adaption (TTA) offers a feasible solution for OOD data by using a pre-trained model that learns essential features with self-supervision. However, those methods implicitly make an assumption that all test distributions have a unified signal that can be learned directly. In open-world conditions, the pose transfer task raises various independent signals: OOD appearance and skeleton, which need to be extracted and distributed in speciality. To address this point, we develop a SEquential Test-time Adaption (SETA). In the test-time phrase, SETA extracts and distributes external appearance texture by augmenting OOD data for self-supervised training. To make non-Euclidean similarity among different postures explicit, SETA uses the image representations derived from a person re-identification (Re-ID) model for similarity computation. By addressing implicit posture representation in the test-time sequentially, SETA greatly improves the generalization performance of current pose transfer models. In our experiment, we first show that pose transfer can be applied to open-world applications, including Tiktok reenactment and celebrity motion synthesis.
<div id='section'>Paperid: <span id='pid'>515, <a href='https://arxiv.org/pdf/2303.09192.pdf' target='_blank'>https://arxiv.org/pdf/2303.09192.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuhang He, Irving Fang, Yiming Li, Rushi Bhavesh Shah, Chen Feng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.09192">Metric-Free Exploration for Topological Mapping by Task and Motion Imitation in Feature Space</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose DeepExplorer, a simple and lightweight metric-free exploration method for topological mapping of unknown environments. It performs task and motion planning (TAMP) entirely in image feature space. The task planner is a recurrent network using the latest image observation sequence to hallucinate a feature as the next-best exploration goal. The motion planner then utilizes the current and the hallucinated features to generate an action taking the agent towards that goal. The two planners are jointly trained via deeply-supervised imitation learning from expert demonstrations. During exploration, we iteratively call the two planners to predict the next action, and the topological map is built by constantly appending the latest image observation and action to the map and using visual place recognition (VPR) for loop closing. The resulting topological map efficiently represents an environment's connectivity and traversability, so it can be used for tasks such as visual navigation. We show DeepExplorer's exploration efficiency and strong sim2sim generalization capability on large-scale simulation datasets like Gibson and MP3D. Its effectiveness is further validated via the image-goal navigation performance on the resulting topological map. We further show its strong zero-shot sim2real generalization capability in real-world experiments. The source code is available at \url{https://ai4ce.github.io/DeepExplorer/}.
<div id='section'>Paperid: <span id='pid'>516, <a href='https://arxiv.org/pdf/2103.13240.pdf' target='_blank'>https://arxiv.org/pdf/2103.13240.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chinmay Vilas Samak, Tanmay Vilas Samak, Sivanathan Kandhasamy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2103.13240">Proximally Optimal Predictive Control Algorithm for Path Tracking of Self-Driving Cars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work presents proximally optimal predictive control algorithm, which is essentially a model-based lateral controller for steered autonomous vehicles that selects an optimal steering command within the neighborhood of previous steering angle based on the predicted vehicle location. The proposed algorithm was formulated with an aim of overcoming the limitations associated with the existing control laws for autonomous steering - namely PID, Pure-Pursuit and Stanley controllers. Particularly, our approach was aimed at bridging the gap between tracking efficiency and computational cost, thereby ensuring effective path tracking in real-time. The effectiveness of our approach was investigated through a series of dynamic simulation experiments pertaining to autonomous path tracking, employing an adaptive control law for longitudinal motion control of the vehicle. We measured the latency of the proposed algorithm in order to comment on its real-time factor and validated our approach by comparing it against the established control laws in terms of both crosstrack and heading errors recorded throughout the respective path tracking simulations.
<div id='section'>Paperid: <span id='pid'>517, <a href='https://arxiv.org/pdf/2103.10030.pdf' target='_blank'>https://arxiv.org/pdf/2103.10030.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tanmay Vilas Samak, Chinmay Vilas Samak, Ming Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2103.10030">AutoDRIVE Simulator: A Simulator for Scaled Autonomous Vehicle Research and Education</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>AutoDRIVE is envisioned to be an integrated research and education platform for scaled autonomous vehicles and related applications. This work is a stepping-stone towards achieving the greater goal of realizing such a platform. Particularly, this work introduces the AutoDRIVE Simulator, a high-fidelity simulator for scaled autonomous vehicles. The proposed simulation ecosystem is developed atop the Unity game engine, and exploits its features in order to simulate realistic system dynamics and render photorealistic graphics. It comprises of a scaled vehicle model equipped with a comprehensive sensor suite for redundant perception, a set of actuators for constrained motion control and a fully functional lighting system for illumination and signaling. It also provides a modular environment development kit, which comprises of various environment modules that aid in reconfigurable construction of the scene. Additionally, the simulator features a communication bridge in order to extend an interface to the autonomous driving software stack developed independently by the users. This work describes some of the prominent components of this simulation system along with some key features that it has to offer in order to accelerate education and research aimed at autonomous driving.
<div id='section'>Paperid: <span id='pid'>518, <a href='https://arxiv.org/pdf/2510.03776.pdf' target='_blank'>https://arxiv.org/pdf/2510.03776.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tiago Rodrigues de Almeida, Yufei Zhu, Andrey Rudenko, Tomasz P. Kucner, Johannes A. Stork, Martin Magnusson, Achim J. Lilienthal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03776">Trajectory prediction for heterogeneous agents: A performance analysis on small and imbalanced datasets</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robots and other intelligent systems navigating in complex dynamic environments should predict future actions and intentions of surrounding agents to reach their goals efficiently and avoid collisions. The dynamics of those agents strongly depends on their tasks, roles, or observable labels. Class-conditioned motion prediction is thus an appealing way to reduce forecast uncertainty and get more accurate predictions for heterogeneous agents. However, this is hardly explored in the prior art, especially for mobile robots and in limited data applications. In this paper, we analyse different class-conditioned trajectory prediction methods on two datasets. We propose a set of conditional pattern-based and efficient deep learning-based baselines, and evaluate their performance on robotics and outdoors datasets (THÖR-MAGNI and Stanford Drone Dataset). Our experiments show that all methods improve accuracy in most of the settings when considering class labels. More importantly, we observe that there are significant differences when learning from imbalanced datasets, or in new environments where sufficient data is not available. In particular, we find that deep learning methods perform better on balanced datasets, but in applications with limited data, e.g., cold start of a robot in a new environment, or imbalanced classes, pattern-based methods may be preferable.
<div id='section'>Paperid: <span id='pid'>519, <a href='https://arxiv.org/pdf/2510.02469.pdf' target='_blank'>https://arxiv.org/pdf/2510.02469.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sung-Yeon Park, Adam Lee, Juanwu Lu, Can Cui, Luyang Jiang, Rohit Gupta, Kyungtae Han, Ahmadreza Moradipari, Ziran Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.02469">SIMSplat: Predictive Driving Scene Editing with Language-aligned 4D Gaussian Splatting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Driving scene manipulation with sensor data is emerging as a promising alternative to traditional virtual driving simulators. However, existing frameworks struggle to generate realistic scenarios efficiently due to limited editing capabilities. To address these challenges, we present SIMSplat, a predictive driving scene editor with language-aligned Gaussian splatting. As a language-controlled editor, SIMSplat enables intuitive manipulation using natural language prompts. By aligning language with Gaussian-reconstructed scenes, it further supports direct querying of road objects, allowing precise and flexible editing. Our method provides detailed object-level editing, including adding new objects and modifying the trajectories of both vehicles and pedestrians, while also incorporating predictive path refinement through multi-agent motion prediction to generate realistic interactions among all agents in the scene. Experiments on the Waymo dataset demonstrate SIMSplat's extensive editing capabilities and adaptability across a wide range of scenarios. Project page: https://sungyeonparkk.github.io/simsplat/
<div id='section'>Paperid: <span id='pid'>520, <a href='https://arxiv.org/pdf/2509.23635.pdf' target='_blank'>https://arxiv.org/pdf/2509.23635.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruibing Hou, Mingshuang Luo, Hongyu Pan, Hong Chang, Shiguang Shan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23635">MotionVerse: A Unified Multimodal Framework for Motion Comprehension, Generation and Editing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper proposes MotionVerse, a unified framework that harnesses the capabilities of Large Language Models (LLMs) to comprehend, generate, and edit human motion in both single-person and multi-person scenarios. To efficiently represent motion data, we employ a motion tokenizer with residual quantization, which converts continuous motion sequences into multi-stream discrete tokens. Furthermore, we introduce a \textit{Delay Parallel} Modeling strategy, which temporally staggers the encoding of residual token streams. This design enables LLMs to effectively capture inter-stream dependencies while maintaining computational efficiency comparable to single-stream modeling. Moreover, to alleviate modality interference between motion and language, we design a \textit{dual-tower architecture} with modality-specific parameters, ensuring stable integration of motion information for both comprehension and generation tasks. Comprehensive ablation studies demonstrate the effectiveness of each component in MotionVerse, and extensive experiments showcase its superior performance across a wide range of motion-relevant tasks.
<div id='section'>Paperid: <span id='pid'>521, <a href='https://arxiv.org/pdf/2508.20085.pdf' target='_blank'>https://arxiv.org/pdf/2508.20085.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhecheng Yuan, Tianming Wei, Langzhe Gu, Pu Hua, Tianhai Liang, Yuanpei Chen, Huazhe Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.20085">HERMES: Human-to-Robot Embodied Learning from Multi-Source Motion Data for Mobile Dexterous Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Leveraging human motion data to impart robots with versatile manipulation skills has emerged as a promising paradigm in robotic manipulation. Nevertheless, translating multi-source human hand motions into feasible robot behaviors remains challenging, particularly for robots equipped with multi-fingered dexterous hands characterized by complex, high-dimensional action spaces. Moreover, existing approaches often struggle to produce policies capable of adapting to diverse environmental conditions. In this paper, we introduce HERMES, a human-to-robot learning framework for mobile bimanual dexterous manipulation. First, HERMES formulates a unified reinforcement learning approach capable of seamlessly transforming heterogeneous human hand motions from multiple sources into physically plausible robotic behaviors. Subsequently, to mitigate the sim2real gap, we devise an end-to-end, depth image-based sim2real transfer method for improved generalization to real-world scenarios. Furthermore, to enable autonomous operation in varied and unstructured environments, we augment the navigation foundation model with a closed-loop Perspective-n-Point (PnP) localization mechanism, ensuring precise alignment of visual goals and effectively bridging autonomous navigation and dexterous manipulation. Extensive experimental results demonstrate that HERMES consistently exhibits generalizable behaviors across diverse, in-the-wild scenarios, successfully performing numerous complex mobile bimanual dexterous manipulation tasks. Project Page:https://gemcollector.github.io/HERMES/.
<div id='section'>Paperid: <span id='pid'>522, <a href='https://arxiv.org/pdf/2508.18691.pdf' target='_blank'>https://arxiv.org/pdf/2508.18691.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Himanshu Gaurav Singh, Pieter Abbeel, Jitendra Malik, Antonio Loquercio
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.18691">Deep Sensorimotor Control by Imitating Predictive Models of Human Motion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As the embodiment gap between a robot and a human narrows, new opportunities arise to leverage datasets of humans interacting with their surroundings for robot learning. We propose a novel technique for training sensorimotor policies with reinforcement learning by imitating predictive models of human motions. Our key insight is that the motion of keypoints on human-inspired robot end-effectors closely mirrors the motion of corresponding human body keypoints. This enables us to use a model trained to predict future motion on human data \emph{zero-shot} on robot data. We train sensorimotor policies to track the predictions of such a model, conditioned on a history of past robot states, while optimizing a relatively sparse task reward. This approach entirely bypasses gradient-based kinematic retargeting and adversarial losses, which limit existing methods from fully leveraging the scale and diversity of modern human-scene interaction datasets. Empirically, we find that our approach can work across robots and tasks, outperforming existing baselines by a large margin. In addition, we find that tracking a human motion model can substitute for carefully designed dense rewards and curricula in manipulation tasks. Code, data and qualitative results available at https://jirl-upenn.github.io/track_reward/.
<div id='section'>Paperid: <span id='pid'>523, <a href='https://arxiv.org/pdf/2508.12081.pdf' target='_blank'>https://arxiv.org/pdf/2508.12081.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haidong Xu, Guangwei Xu, Zhedong Zheng, Xiatian Zhu, Wei Ji, Xiangtai Li, Ruijie Guo, Meishan Zhang, Min zhang, Hao Fei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.12081">VimoRAG: Video-based Retrieval-augmented 3D Motion Generation for Motion Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces VimoRAG, a novel video-based retrieval-augmented motion generation framework for motion large language models (LLMs). As motion LLMs face severe out-of-domain/out-of-vocabulary issues due to limited annotated data, VimoRAG leverages large-scale in-the-wild video databases to enhance 3D motion generation by retrieving relevant 2D human motion signals. While video-based motion RAG is nontrivial, we address two key bottlenecks: (1) developing an effective motion-centered video retrieval model that distinguishes human poses and actions, and (2) mitigating the issue of error propagation caused by suboptimal retrieval results. We design the Gemini Motion Video Retriever mechanism and the Motion-centric Dual-alignment DPO Trainer, enabling effective retrieval and generation processes. Experimental results show that VimoRAG significantly boosts the performance of motion LLMs constrained to text-only input.
<div id='section'>Paperid: <span id='pid'>524, <a href='https://arxiv.org/pdf/2506.23690.pdf' target='_blank'>https://arxiv.org/pdf/2506.23690.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuai Tan, Biao Gong, Yujie Wei, Shiwei Zhang, Zhuoxin Liu, Dandan Zheng, Jingdong Chen, Yan Wang, Hao Ouyang, Kecheng Zheng, Yujun Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.23690">SynMotion: Semantic-Visual Adaptation for Motion Customized Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion-based video motion customization facilitates the acquisition of human motion representations from a few video samples, while achieving arbitrary subjects transfer through precise textual conditioning. Existing approaches often rely on semantic-level alignment, expecting the model to learn new motion concepts and combine them with other entities (e.g., ''cats'' or ''dogs'') to produce visually appealing results. However, video data involve complex spatio-temporal patterns, and focusing solely on semantics cause the model to overlook the visual complexity of motion. Conversely, tuning only the visual representation leads to semantic confusion in representing the intended action. To address these limitations, we propose SynMotion, a new motion-customized video generation model that jointly leverages semantic guidance and visual adaptation. At the semantic level, we introduce the dual-embedding semantic comprehension mechanism which disentangles subject and motion representations, allowing the model to learn customized motion features while preserving its generative capabilities for diverse subjects. At the visual level, we integrate parameter-efficient motion adapters into a pre-trained video generation model to enhance motion fidelity and temporal coherence. Furthermore, we introduce a new embedding-specific training strategy which \textbf{alternately optimizes} subject and motion embeddings, supported by the manually constructed Subject Prior Video (SPV) training dataset. This strategy promotes motion specificity while preserving generalization across diverse subjects. Lastly, we introduce MotionBench, a newly curated benchmark with diverse motion patterns. Experimental results across both T2V and I2V settings demonstrate that \method outperforms existing baselines. Project page: https://lucaria-academy.github.io/SynMotion/
<div id='section'>Paperid: <span id='pid'>525, <a href='https://arxiv.org/pdf/2505.20857.pdf' target='_blank'>https://arxiv.org/pdf/2505.20857.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhefeng Cao, Ben Liu, Sen Li, Wei Zhang, Hua Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.20857">G-DReaM: Graph-conditioned Diffusion Retargeting across Multiple Embodiments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion retargeting for specific robot from existing motion datasets is one critical step in transferring motion patterns from human behaviors to and across various robots. However, inconsistencies in topological structure, geometrical parameters as well as joint correspondence make it difficult to handle diverse embodiments with a unified retargeting architecture. In this work, we propose a novel unified graph-conditioned diffusion-based motion generation framework for retargeting reference motions across diverse embodiments. The intrinsic characteristics of heterogeneous embodiments are represented with graph structure that effectively captures topological and geometrical features of different robots. Such a graph-based encoding further allows for knowledge exploitation at the joint level with a customized attention mechanisms developed in this work. For lacking ground truth motions of the desired embodiment, we utilize an energy-based guidance formulated as retargeting losses to train the diffusion model. As one of the first cross-embodiment motion retargeting methods in robotics, our experiments validate that the proposed model can retarget motions across heterogeneous embodiments in a unified manner. Moreover, it demonstrates a certain degree of generalization to both diverse skeletal structures and similar motion patterns.
<div id='section'>Paperid: <span id='pid'>526, <a href='https://arxiv.org/pdf/2505.03729.pdf' target='_blank'>https://arxiv.org/pdf/2505.03729.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Arthur Allshire, Hongsuk Choi, Junyi Zhang, David McAllister, Anthony Zhang, Chung Min Kim, Trevor Darrell, Pieter Abbeel, Jitendra Malik, Angjoo Kanazawa
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.03729">Visual Imitation Enables Contextual Humanoid Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>How can we teach humanoids to climb staircases and sit on chairs using the surrounding environment context? Arguably, the simplest way is to just show them-casually capture a human motion video and feed it to humanoids. We introduce VIDEOMIMIC, a real-to-sim-to-real pipeline that mines everyday videos, jointly reconstructs the humans and the environment, and produces whole-body control policies for humanoid robots that perform the corresponding skills. We demonstrate the results of our pipeline on real humanoid robots, showing robust, repeatable contextual control such as staircase ascents and descents, sitting and standing from chairs and benches, as well as other dynamic whole-body skills-all from a single policy, conditioned on the environment and global root commands. VIDEOMIMIC offers a scalable path towards teaching humanoids to operate in diverse real-world environments.
<div id='section'>Paperid: <span id='pid'>527, <a href='https://arxiv.org/pdf/2505.03729.pdf' target='_blank'>https://arxiv.org/pdf/2505.03729.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Arthur Allshire, Hongsuk Choi, Junyi Zhang, David McAllister, Anthony Zhang, Chung Min Kim, Trevor Darrell, Pieter Abbeel, Jitendra Malik, Angjoo Kanazawa
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.03729">Visual Imitation Enables Contextual Humanoid Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>How can we teach humanoids to climb staircases and sit on chairs using the surrounding environment context? Arguably, the simplest way is to just show them-casually capture a human motion video and feed it to humanoids. We introduce VIDEOMIMIC, a real-to-sim-to-real pipeline that mines everyday videos, jointly reconstructs the humans and the environment, and produces whole-body control policies for humanoid robots that perform the corresponding skills. We demonstrate the results of our pipeline on real humanoid robots, showing robust, repeatable contextual control such as staircase ascents and descents, sitting and standing from chairs and benches, as well as other dynamic whole-body skills-all from a single policy, conditioned on the environment and global root commands. VIDEOMIMIC offers a scalable path towards teaching humanoids to operate in diverse real-world environments.
<div id='section'>Paperid: <span id='pid'>528, <a href='https://arxiv.org/pdf/2503.01616.pdf' target='_blank'>https://arxiv.org/pdf/2503.01616.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haichao Liu, Sikai Guo, Pengfei Mai, Jiahang Cao, Haoang Li, Jun Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.01616">RoboDexVLM: Visual Language Model-Enabled Task Planning and Motion Control for Dexterous Robot Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces RoboDexVLM, an innovative framework for robot task planning and grasp detection tailored for a collaborative manipulator equipped with a dexterous hand. Previous methods focus on simplified and limited manipulation tasks, which often neglect the complexities associated with grasping a diverse array of objects in a long-horizon manner. In contrast, our proposed framework utilizes a dexterous hand capable of grasping objects of varying shapes and sizes while executing tasks based on natural language commands. The proposed approach has the following core components: First, a robust task planner with a task-level recovery mechanism that leverages vision-language models (VLMs) is designed, which enables the system to interpret and execute open-vocabulary commands for long sequence tasks. Second, a language-guided dexterous grasp perception algorithm is presented based on robot kinematics and formal methods, tailored for zero-shot dexterous manipulation with diverse objects and commands. Comprehensive experimental results validate the effectiveness, adaptability, and robustness of RoboDexVLM in handling long-horizon scenarios and performing dexterous grasping. These results highlight the framework's ability to operate in complex environments, showcasing its potential for open-vocabulary dexterous manipulation. Our open-source project page can be found at https://henryhcliu.github.io/robodexvlm.
<div id='section'>Paperid: <span id='pid'>529, <a href='https://arxiv.org/pdf/2502.18180.pdf' target='_blank'>https://arxiv.org/pdf/2502.18180.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lei Li, Sen Jia, Jianhao Wang, Zhaochong An, Jiaang Li, Jenq-Neng Hwang, Serge Belongie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.18180">ChatMotion: A Multimodal Multi-Agent for Human Motion Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Advancements in Multimodal Large Language Models (MLLMs) have improved human motion understanding. However, these models remain constrained by their "instruct-only" nature, lacking interactivity and adaptability for diverse analytical perspectives. To address these challenges, we introduce ChatMotion, a multimodal multi-agent framework for human motion analysis. ChatMotion dynamically interprets user intent, decomposes complex tasks into meta-tasks, and activates specialized function modules for motion comprehension. It integrates multiple specialized modules, such as the MotionCore, to analyze human motion from various perspectives. Extensive experiments demonstrate ChatMotion's precision, adaptability, and user engagement for human motion understanding.
<div id='section'>Paperid: <span id='pid'>530, <a href='https://arxiv.org/pdf/2502.11149.pdf' target='_blank'>https://arxiv.org/pdf/2502.11149.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zongzhao Li, Jiacheng Cen, Bing Su, Wenbing Huang, Tingyang Xu, Yu Rong, Deli Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.11149">Large Language-Geometry Model: When LLM meets Equivariance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurately predicting 3D structures and dynamics of physical systems is crucial in scientific applications. Existing approaches that rely on geometric Graph Neural Networks (GNNs) effectively enforce $\mathrm{E}(3)$-equivariance, but they often fall in leveraging extensive broader information. While direct application of Large Language Models (LLMs) can incorporate external knowledge, they lack the capability for spatial reasoning with guaranteed equivariance. In this paper, we propose EquiLLM, a novel framework for representing 3D physical systems that seamlessly integrates E(3)-equivariance with LLM capabilities. Specifically, EquiLLM comprises four key components: geometry-aware prompting, an equivariant encoder, an LLM, and an equivariant adaptor. Essentially, the LLM guided by the instructive prompt serves as a sophisticated invariant feature processor, while 3D directional information is exclusively handled by the equivariant encoder and adaptor modules. Experimental results demonstrate that EquiLLM delivers significant improvements over previous methods across molecular dynamics simulation, human motion simulation, and antibody design, highlighting its promising generalizability.
<div id='section'>Paperid: <span id='pid'>531, <a href='https://arxiv.org/pdf/2501.04595.pdf' target='_blank'>https://arxiv.org/pdf/2501.04595.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zifan Wang, Ziqing Chen, Junyu Chen, Jilong Wang, Yuxin Yang, Yunze Liu, Xueyi Liu, He Wang, Li Yi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.04595">MobileH2R: Learning Generalizable Human to Mobile Robot Handover Exclusively from Scalable and Diverse Synthetic Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces MobileH2R, a framework for learning generalizable vision-based human-to-mobile-robot (H2MR) handover skills. Unlike traditional fixed-base handovers, this task requires a mobile robot to reliably receive objects in a large workspace enabled by its mobility. Our key insight is that generalizable handover skills can be developed in simulators using high-quality synthetic data, without the need for real-world demonstrations. To achieve this, we propose a scalable pipeline for generating diverse synthetic full-body human motion data, an automated method for creating safe and imitation-friendly demonstrations, and an efficient 4D imitation learning method for distilling large-scale demonstrations into closed-loop policies with base-arm coordination. Experimental evaluations in both simulators and the real world show significant improvements (at least +15% success rate) over baseline methods in all cases. Experiments also validate that large-scale and diverse synthetic data greatly enhances robot learning, highlighting our scalable framework.
<div id='section'>Paperid: <span id='pid'>532, <a href='https://arxiv.org/pdf/2412.17730.pdf' target='_blank'>https://arxiv.org/pdf/2412.17730.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yun Liu, Bowen Yang, Licheng Zhong, He Wang, Li Yi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.17730">Mimicking-Bench: A Benchmark for Generalizable Humanoid-Scene Interaction Learning via Human Mimicking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning generic skills for humanoid robots interacting with 3D scenes by mimicking human data is a key research challenge with significant implications for robotics and real-world applications. However, existing methodologies and benchmarks are constrained by the use of small-scale, manually collected demonstrations, lacking the general dataset and benchmark support necessary to explore scene geometry generalization effectively. To address this gap, we introduce Mimicking-Bench, the first comprehensive benchmark designed for generalizable humanoid-scene interaction learning through mimicking large-scale human animation references. Mimicking-Bench includes six household full-body humanoid-scene interaction tasks, covering 11K diverse object shapes, along with 20K synthetic and 3K real-world human interaction skill references. We construct a complete humanoid skill learning pipeline and benchmark approaches for motion retargeting, motion tracking, imitation learning, and their various combinations. Extensive experiments highlight the value of human mimicking for skill learning, revealing key challenges and research directions.
<div id='section'>Paperid: <span id='pid'>533, <a href='https://arxiv.org/pdf/2412.08948.pdf' target='_blank'>https://arxiv.org/pdf/2412.08948.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuehai He, Shuohang Wang, Jianwei Yang, Xiaoxia Wu, Yiping Wang, Kuan Wang, Zheng Zhan, Olatunji Ruwase, Yelong Shen, Xin Eric Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.08948">Mojito: Motion Trajectory and Intensity Control for Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in diffusion models have shown great promise in producing high-quality video content. However, efficiently training video diffusion models capable of integrating directional guidance and controllable motion intensity remains a challenging and under-explored area. To tackle these challenges, this paper introduces Mojito, a diffusion model that incorporates both motion trajectory and intensity control for text-to-video generation. Specifically, Mojito features a Directional Motion Control (DMC) module that leverages cross-attention to efficiently direct the generated object's motion without training, alongside a Motion Intensity Modulator (MIM) that uses optical flow maps generated from videos to guide varying levels of motion intensity. Extensive experiments demonstrate Mojito's effectiveness in achieving precise trajectory and intensity control with high computational efficiency, generating motion patterns that closely match specified directions and intensities, providing realistic dynamics that align well with natural motion in real-world scenarios.
<div id='section'>Paperid: <span id='pid'>534, <a href='https://arxiv.org/pdf/2411.19258.pdf' target='_blank'>https://arxiv.org/pdf/2411.19258.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Amon Lahr, Joshua NÃ¤f, Kim P. Wabersich, Jonathan Frey, Pascal Siehl, Andrea Carron, Moritz Diehl, Melanie N. Zeilinger
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.19258">L4acados: Learning-based models for acados, applied to Gaussian process-based predictive control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Incorporating learning-based models, such as artificial neural networks or Gaussian processes, into model predictive control (MPC) strategies can significantly improve control performance and online adaptation capabilities for real-world applications. Still, enabling state-of-the-art implementations of learning-based models for MPC is complicated by the challenge of interfacing machine learning frameworks with real-time optimal control software. This work aims at filling this gap by incorporating external sensitivities in sequential quadratic programming solvers for nonlinear optimal control. To this end, we provide L4acados, a general framework for incorporating Python-based residual models in the real-time optimal control software acados. By computing external sensitivities via a user-defined Python module, L4acados enables the implementation of MPC controllers with learning-based residual models in acados, while supporting parallelization of sensitivity computations when preparing the quadratic subproblems. We demonstrate significant speed-ups and superior scaling properties of L4acados compared to available software using a neural-network-based control example. Last, we provide an efficient and modular real-time implementation of Gaussian process-based MPC using L4acados, which is applied to two hardware examples: autonomous miniature racing, as well as motion control of a full-scale autonomous vehicle for an ISO lane change maneuver.
<div id='section'>Paperid: <span id='pid'>535, <a href='https://arxiv.org/pdf/2410.18977.pdf' target='_blank'>https://arxiv.org/pdf/2410.18977.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ling-Hao Chen, Shunlin Lu, Wenxun Dai, Zhiyang Dou, Xuan Ju, Jingbo Wang, Taku Komura, Lei Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.18977">Pay Attention and Move Better: Harnessing Attention for Interactive Motion Generation and Training-free Editing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This research delves into the problem of interactive editing of human motion generation. Previous motion diffusion models lack explicit modeling of the word-level text-motion correspondence and good explainability, hence restricting their fine-grained editing ability. To address this issue, we propose an attention-based motion diffusion model, namely MotionCLR, with CLeaR modeling of attention mechanisms. Technically, MotionCLR models the in-modality and cross-modality interactions with self-attention and cross-attention, respectively. More specifically, the self-attention mechanism aims to measure the sequential similarity between frames and impacts the order of motion features. By contrast, the cross-attention mechanism works to find the fine-grained word-sequence correspondence and activate the corresponding timesteps in the motion sequence. Based on these key properties, we develop a versatile set of simple yet effective motion editing methods via manipulating attention maps, such as motion (de-)emphasizing, in-place motion replacement, and example-based motion generation, etc. For further verification of the explainability of the attention mechanism, we additionally explore the potential of action-counting and grounded motion generation ability via attention maps. Our experimental results show that our method enjoys good generation and editing ability with good explainability.
<div id='section'>Paperid: <span id='pid'>536, <a href='https://arxiv.org/pdf/2410.12237.pdf' target='_blank'>https://arxiv.org/pdf/2410.12237.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yufei Zhu, Andrey Rudenko, Luigi Palmieri, Lukas Heuer, Achim J. Lilienthal, Martin Magnusson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.12237">Fast Online Learning of CLiFF-maps in Changing Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Maps of dynamics are effective representations of motion patterns learned from prior observations, with recent research demonstrating their ability to enhance various downstream tasks such as human-aware robot navigation, long-term human motion prediction, and robot localization. Current advancements have primarily concentrated on methods for learning maps of human flow in environments where the flow is static, i.e., not assumed to change over time. In this paper we propose an online update method of the CLiFF-map (an advanced map of dynamics type that models motion patterns as velocity and orientation mixtures) to actively detect and adapt to human flow changes. As new observations are collected, our goal is to update a CLiFF-map to effectively and accurately integrate them, while retaining relevant historic motion patterns. The proposed online update method maintains a probabilistic representation in each observed location, updating parameters by continuously tracking sufficient statistics. In experiments using both synthetic and real-world datasets, we show that our method is able to maintain accurate representations of human motion dynamics, contributing to high performance flow-compliant planning downstream tasks, while being orders of magnitude faster than the comparable baselines.
<div id='section'>Paperid: <span id='pid'>537, <a href='https://arxiv.org/pdf/2410.09681.pdf' target='_blank'>https://arxiv.org/pdf/2410.09681.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Christopher Diehl, Peter Karkus, Sushant Veer, Marco Pavone, Torsten Bertram
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.09681">LoRD: Adapting Differentiable Driving Policies to Distribution Shifts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Distribution shifts between operational domains can severely affect the performance of learned models in self-driving vehicles (SDVs). While this is a well-established problem, prior work has mostly explored naive solutions such as fine-tuning, focusing on the motion prediction task. In this work, we explore novel adaptation strategies for differentiable autonomy stacks consisting of prediction, planning, and control, perform evaluation in closed-loop, and investigate the often-overlooked issue of catastrophic forgetting. Specifically, we introduce two simple yet effective techniques: a low-rank residual decoder (LoRD) and multi-task fine-tuning. Through experiments across three models conducted on two real-world autonomous driving datasets (nuPlan, exiD), we demonstrate the effectiveness of our methods and highlight a significant performance gap between open-loop and closed-loop evaluation in prior approaches. Our approach improves forgetting by up to 23.33% and the closed-loop OOD driving score by 9.93% in comparison to standard fine-tuning.
<div id='section'>Paperid: <span id='pid'>538, <a href='https://arxiv.org/pdf/2409.16154.pdf' target='_blank'>https://arxiv.org/pdf/2409.16154.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alexander Prutsch, Horst Bischof, Horst Possegger
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.16154">Efficient Motion Prediction: A Lightweight & Accurate Trajectory Prediction Model With Fast Training and Inference Speed</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>For efficient and safe autonomous driving, it is essential that autonomous vehicles can predict the motion of other traffic agents. While highly accurate, current motion prediction models often impose significant challenges in terms of training resource requirements and deployment on embedded hardware. We propose a new efficient motion prediction model, which achieves highly competitive benchmark results while training only a few hours on a single GPU. Due to our lightweight architectural choices and the focus on reducing the required training resources, our model can easily be applied to custom datasets. Furthermore, its low inference latency makes it particularly suitable for deployment in autonomous applications with limited computing resources.
<div id='section'>Paperid: <span id='pid'>539, <a href='https://arxiv.org/pdf/2406.06300.pdf' target='_blank'>https://arxiv.org/pdf/2406.06300.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tim Schreiter, Andrey Rudenko, Martin Magnusson, Achim J. Lilienthal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.06300">Human Gaze and Head Rotation during Navigation, Exploration and Object Manipulation in Shared Environments with Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The human gaze is an important cue to signal intention, attention, distraction, and the regions of interest in the immediate surroundings. Gaze tracking can transform how robots perceive, understand, and react to people, enabling new modes of robot control, interaction, and collaboration. In this paper, we use gaze tracking data from a rich dataset of human motion (THÃR-MAGNI) to investigate the coordination between gaze direction and head rotation of humans engaged in various indoor activities involving navigation, interaction with objects, and collaboration with a mobile robot. In particular, we study the spread and central bias of fixations in diverse activities and examine the correlation between gaze direction and head rotation. We introduce various human motion metrics to enhance the understanding of gaze behavior in dynamic interactions. Finally, we apply semantic object labeling to decompose the gaze distribution into activity-relevant regions.
<div id='section'>Paperid: <span id='pid'>540, <a href='https://arxiv.org/pdf/2404.19722.pdf' target='_blank'>https://arxiv.org/pdf/2404.19722.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingbo Wang, Zhengyi Luo, Ye Yuan, Yixuan Li, Bo Dai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.19722">PACER+: On-Demand Pedestrian Animation Controller in Driving Scenarios</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We address the challenge of content diversity and controllability in pedestrian simulation for driving scenarios. Recent pedestrian animation frameworks have a significant limitation wherein they primarily focus on either following trajectory [46] or the content of the reference video [57], consequently overlooking the potential diversity of human motion within such scenarios. This limitation restricts the ability to generate pedestrian behaviors that exhibit a wider range of variations and realistic motions and therefore restricts its usage to provide rich motion content for other components in the driving simulation system, e.g., suddenly changed motion to which the autonomous vehicle should respond. In our approach, we strive to surpass the limitation by showcasing diverse human motions obtained from various sources, such as generated human motions, in addition to following the given trajectory. The fundamental contribution of our framework lies in combining the motion tracking task with trajectory following, which enables the tracking of specific motion parts (e.g., upper body) while simultaneously following the given trajectory by a single policy. This way, we significantly enhance both the diversity of simulated human motion within the given scenario and the controllability of the content, including language-based control. Our framework facilitates the generation of a wide range of human motions, contributing to greater realism and adaptability in pedestrian simulations for driving scenarios. More information is on our project page https://wangjingbo1219.github.io/papers/CVPR2024_PACER_PLUS/PACERPLUSPage.html .
<div id='section'>Paperid: <span id='pid'>541, <a href='https://arxiv.org/pdf/2404.15366.pdf' target='_blank'>https://arxiv.org/pdf/2404.15366.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiao-Yin Liu, Guotao Li, Xiao-Hu Zhou, Xu Liang, Zeng-Guang Hou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.15366">A Weight-aware-based Multi-source Unsupervised Domain Adaptation Method for Human Motion Intention Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate recognition of human motion intention (HMI) is beneficial for exoskeleton robots to improve the wearing comfort level and achieve natural human-robot interaction. A classifier trained on labeled source subjects (domains) performs poorly on unlabeled target subject since the difference in individual motor characteristics. The unsupervised domain adaptation (UDA) method has become an effective way to this problem. However, the labeled data are collected from multiple source subjects that might be different not only from the target subject but also from each other. The current UDA methods for HMI recognition ignore the difference between each source subject, which reduces the classification accuracy. Therefore, this paper considers the differences between source subjects and develops a novel theory and algorithm for UDA to recognize HMI, where the margin disparity discrepancy (MDD) is extended to multi-source UDA theory and a novel weight-aware-based multi-source UDA algorithm (WMDD) is proposed. The source domain weight, which can be adjusted adaptively by the MDD between each source subject and target subject, is incorporated into UDA to measure the differences between source subjects. The developed multi-source UDA theory is theoretical and the generalization error on target subject is guaranteed. The theory can be transformed into an optimization problem for UDA, successfully bridging the gap between theory and algorithm. Moreover, a lightweight network is employed to guarantee the real-time of classification and the adversarial learning between feature generator and ensemble classifiers is utilized to further improve the generalization ability. The extensive experiments verify theoretical analysis and show that WMDD outperforms previous UDA methods on HMI recognition tasks.
<div id='section'>Paperid: <span id='pid'>542, <a href='https://arxiv.org/pdf/2403.13640.pdf' target='_blank'>https://arxiv.org/pdf/2403.13640.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yufei Zhu, Han Fan, Andrey Rudenko, Martin Magnusson, Erik Schaffernicht, Achim J. Lilienthal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.13640">LaCE-LHMP: Airflow Modelling-Inspired Long-Term Human Motion Prediction By Enhancing Laminar Characteristics in Human Flow</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Long-term human motion prediction (LHMP) is essential for safely operating autonomous robots and vehicles in populated environments. It is fundamental for various applications, including motion planning, tracking, human-robot interaction and safety monitoring. However, accurate prediction of human trajectories is challenging due to complex factors, including, for example, social norms and environmental conditions. The influence of such factors can be captured through Maps of Dynamics (MoDs), which encode spatial motion patterns learned from (possibly scattered and partial) past observations of motion in the environment and which can be used for data-efficient, interpretable motion prediction (MoD-LHMP). To address the limitations of prior work, especially regarding accuracy and sensitivity to anomalies in long-term prediction, we propose the Laminar Component Enhanced LHMP approach (LaCE-LHMP). Our approach is inspired by data-driven airflow modelling, which estimates laminar and turbulent flow components and uses predominantly the laminar components to make flow predictions. Based on the hypothesis that human trajectory patterns also manifest laminar flow (that represents predictable motion) and turbulent flow components (that reflect more unpredictable and arbitrary motion), LaCE-LHMP extracts the laminar patterns in human dynamics and uses them for human motion prediction. We demonstrate the superior prediction performance of LaCE-LHMP through benchmark comparisons with state-of-the-art LHMP methods, offering an unconventional perspective and a more intuitive understanding of human movement patterns.
<div id='section'>Paperid: <span id='pid'>543, <a href='https://arxiv.org/pdf/2403.09285.pdf' target='_blank'>https://arxiv.org/pdf/2403.09285.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tim Schreiter, Tiago Rodrigues de Almeida, Yufei Zhu, Eduardo Gutierrez Maestro, Lucas Morillo-Mendez, Andrey Rudenko, Luigi Palmieri, Tomasz P. Kucner, Martin Magnusson, Achim J. Lilienthal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.09285">THÃR-MAGNI: A Large-scale Indoor Motion Capture Recording of Human Movement and Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a new large dataset of indoor human and robot navigation and interaction, called THÃR-MAGNI, that is designed to facilitate research on social navigation: e.g., modelling and predicting human motion, analyzing goal-oriented interactions between humans and robots, and investigating visual attention in a social interaction context. THÃR-MAGNI was created to fill a gap in available datasets for human motion analysis and HRI. This gap is characterized by a lack of comprehensive inclusion of exogenous factors and essential target agent cues, which hinders the development of robust models capable of capturing the relationship between contextual cues and human behavior in different scenarios. Unlike existing datasets, THÃR-MAGNI includes a broader set of contextual features and offers multiple scenario variations to facilitate factor isolation. The dataset includes many social human-human and human-robot interaction scenarios, rich context annotations, and multi-modal data, such as walking trajectories, gaze tracking data, and lidar and camera streams recorded from a mobile robot. We also provide a set of tools for visualization and processing of the recorded data. THÃR-MAGNI is, to the best of our knowledge, unique in the amount and diversity of sensor data collected in a contextualized and socially dynamic environment, capturing natural human-robot interactions.
<div id='section'>Paperid: <span id='pid'>544, <a href='https://arxiv.org/pdf/2312.13604.pdf' target='_blank'>https://arxiv.org/pdf/2312.13604.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Keqiang Sun, Dor Litvak, Yunzhi Zhang, Hongsheng Li, Jiajun Wu, Shangzhe Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.13604">Ponymation: Learning Articulated 3D Animal Motions from Unlabeled Online Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a new method for learning a generative model of articulated 3D animal motions from raw, unlabeled online videos. Unlike existing approaches for 3D motion synthesis, our model requires no pose annotations or parametric shape models for training; it learns purely from a collection of unlabeled web video clips, leveraging semantic correspondences distilled from self-supervised image features. At the core of our method is a video Photo-Geometric Auto-Encoding framework that decomposes each training video clip into a set of explicit geometric and photometric representations, including a rest-pose 3D shape, an articulated pose sequence, and texture, with the objective of re-rendering the input video via a differentiable renderer. This decomposition allows us to learn a generative model over the underlying articulated pose sequences akin to a Variational Auto-Encoding (VAE) formulation, but without requiring any external pose annotations. At inference time, we can generate new motion sequences by sampling from the learned motion VAE, and create plausible 4D animations of an animal automatically within seconds given a single input image.
<div id='section'>Paperid: <span id='pid'>545, <a href='https://arxiv.org/pdf/2312.08459.pdf' target='_blank'>https://arxiv.org/pdf/2312.08459.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shivangi Aneja, Justus Thies, Angela Dai, Matthias NieÃner
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.08459">FaceTalk: Audio-Driven Motion Diffusion for Neural Parametric Head Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce FaceTalk, a novel generative approach designed for synthesizing high-fidelity 3D motion sequences of talking human heads from input audio signal. To capture the expressive, detailed nature of human heads, including hair, ears, and finer-scale eye movements, we propose to couple speech signal with the latent space of neural parametric head models to create high-fidelity, temporally coherent motion sequences. We propose a new latent diffusion model for this task, operating in the expression space of neural parametric head models, to synthesize audio-driven realistic head sequences. In the absence of a dataset with corresponding NPHM expressions to audio, we optimize for these correspondences to produce a dataset of temporally-optimized NPHM expressions fit to audio-video recordings of people talking. To the best of our knowledge, this is the first work to propose a generative approach for realistic and high-quality motion synthesis of volumetric human heads, representing a significant advancement in the field of audio-driven 3D animation. Notably, our approach stands out in its ability to generate plausible motion sequences that can produce high-fidelity head animation coupled with the NPHM shape space. Our experimental results substantiate the effectiveness of FaceTalk, consistently achieving superior and visually natural motion, encompassing diverse facial expressions and styles, outperforming existing methods by 75% in perceptual user study evaluation.
<div id='section'>Paperid: <span id='pid'>546, <a href='https://arxiv.org/pdf/2309.07066.pdf' target='_blank'>https://arxiv.org/pdf/2309.07066.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yufei Zhu, Andrey Rudenko, Tomasz P. Kucner, Luigi Palmieri, Kai O. Arras, Achim J. Lilienthal, Martin Magnusson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.07066">CLiFF-LHMP: Using Spatial Dynamics Patterns for Long-Term Human Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion prediction is important for mobile service robots and intelligent vehicles to operate safely and smoothly around people. The more accurate predictions are, particularly over extended periods of time, the better a system can, e.g., assess collision risks and plan ahead. In this paper, we propose to exploit maps of dynamics (MoDs, a class of general representations of place-dependent spatial motion patterns, learned from prior observations) for long-term human motion prediction (LHMP). We present a new MoD-informed human motion prediction approach, named CLiFF-LHMP, which is data efficient, explainable, and insensitive to errors from an upstream tracking system. Our approach uses CLiFF-map, a specific MoD trained with human motion data recorded in the same environment. We bias a constant velocity prediction with samples from the CLiFF-map to generate multi-modal trajectory predictions. In two public datasets we show that this algorithm outperforms the state of the art for predictions over very extended periods of time, achieving 45% more accurate prediction performance at 50s compared to the baseline.
<div id='section'>Paperid: <span id='pid'>547, <a href='https://arxiv.org/pdf/2306.03617.pdf' target='_blank'>https://arxiv.org/pdf/2306.03617.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yufei Zhu, Andrey Rudenko, Tomasz P. Kucner, Achim J. Lilienthal, Martin Magnusson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.03617">A Data-Efficient Approach for Long-Term Human Motion Prediction Using Maps of Dynamics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion prediction is essential for the safe and smooth operation of mobile service robots and intelligent vehicles around people. Commonly used neural network-based approaches often require large amounts of complete trajectories to represent motion dynamics in complex semantically-rich spaces. This requirement may complicate deployment of physical systems in new environments, especially when the data is being collected online from onboard sensors. In this paper we explore a data-efficient alternative using maps of dynamics (MoD) to represent place-dependent multi-modal spatial motion patterns, learned from prior observations. Our approach can perform efficient human motion prediction in the long-term perspective of up to 60 seconds. We quantitatively evaluate its accuracy with limited amount of training data in comparison to an LSTM-based baseline, and qualitatively show that the predicted trajectories reflect the natural semantic properties of the environment, e.g. the locations of short- and long-term goals, navigation in narrow passages, around obstacles, etc.
<div id='section'>Paperid: <span id='pid'>548, <a href='https://arxiv.org/pdf/2306.00416.pdf' target='_blank'>https://arxiv.org/pdf/2306.00416.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi Shi, Jingbo Wang, Xuekun Jiang, Bingkun Lin, Bo Dai, Xue Bin Peng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.00416">Interactive Character Control with Auto-Regressive Motion Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Real-time character control is an essential component for interactive experiences, with a broad range of applications, including physics simulations, video games, and virtual reality. The success of diffusion models for image synthesis has led to the use of these models for motion synthesis. However, the majority of these motion diffusion models are primarily designed for offline applications, where space-time models are used to synthesize an entire sequence of frames simultaneously with a pre-specified length. To enable real-time motion synthesis with diffusion model that allows time-varying controls, we propose A-MDM (Auto-regressive Motion Diffusion Model). Our conditional diffusion model takes an initial pose as input, and auto-regressively generates successive motion frames conditioned on the previous frame. Despite its streamlined network architecture, which uses simple MLPs, our framework is capable of generating diverse, long-horizon, and high-fidelity motion sequences. Furthermore, we introduce a suite of techniques for incorporating interactive controls into A-MDM, such as task-oriented sampling, in-painting, and hierarchical reinforcement learning. These techniques enable a pre-trained A-MDM to be efficiently adapted for a variety of new downstream tasks. We conduct a comprehensive suite of experiments to demonstrate the effectiveness of A-MDM, and compare its performance against state-of-the-art auto-regressive methods.
<div id='section'>Paperid: <span id='pid'>549, <a href='https://arxiv.org/pdf/2304.01041.pdf' target='_blank'>https://arxiv.org/pdf/2304.01041.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haichao Liu, Kai Chen, Yulin Li, Zhenmin Huang, Jianghua Duan, Jun Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.01041">Integrated Behavior Planning and Motion Control for Autonomous Vehicles with Traffic Rules Compliance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this article, we propose an optimization-based integrated behavior planning and motion control scheme, which is an interpretable and adaptable urban autonomous driving solution that complies with complex traffic rules while ensuring driving safety. Inherently, to ensure compliance with traffic rules, an innovative design of potential functions (PFs) is presented to characterize various traffic rules related to traffic lights, traversable and non-traversable traffic line markings, etc. These PFs are further incorporated as part of the model predictive control (MPC) formulation. In this sense, high-level behavior planning is attained implicitly along with motion control as an integrated architecture, facilitating flexible maneuvers with safety guarantees. Due to the well-designed objective function of the MPC scheme, our integrated behavior planning and motion control scheme is competent for various urban driving scenarios and able to generate versatile behaviors, such as overtaking with adaptive cruise control, turning in the intersection, and merging in and out of the roundabout. As demonstrated from a series of simulations with challenging scenarios in CARLA, it is noteworthy that the proposed framework admits real-time performance and high generalizability.
<div id='section'>Paperid: <span id='pid'>550, <a href='https://arxiv.org/pdf/2303.13129.pdf' target='_blank'>https://arxiv.org/pdf/2303.13129.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Quanzhou Li, Jingbo Wang, Chen Change Loy, Bo Dai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.13129">Task-Oriented Human-Object Interactions Generation with Implicit Neural Representations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Digital human motion synthesis is a vibrant research field with applications in movies, AR/VR, and video games. Whereas methods were proposed to generate natural and realistic human motions, most only focus on modeling humans and largely ignore object movements. Generating task-oriented human-object interaction motions in simulation is challenging. For different intents of using the objects, humans conduct various motions, which requires the human first to approach the objects and then make them move consistently with the human instead of staying still. Also, to deploy in downstream applications, the synthesized motions are desired to be flexible in length, providing options to personalize the predicted motions for various purposes. To this end, we propose TOHO: Task-Oriented Human-Object Interactions Generation with Implicit Neural Representations, which generates full human-object interaction motions to conduct specific tasks, given only the task type, the object, and a starting human status. TOHO generates human-object motions in three steps: 1) it first estimates the keyframe poses of conducting a task given the task type and object information; 2) then, it infills the keyframes and generates continuous motions; 3) finally, it applies a compact closed-form object motion estimation to generate the object motion. Our method generates continuous motions that are parameterized only by the temporal coordinate, which allows for upsampling or downsampling of the sequence to arbitrary frames and adjusting the motion speeds by designing the temporal coordinate vector. We demonstrate the effectiveness of our method, both qualitatively and quantitatively. This work takes a step further toward general human-scene interaction simulation.
<div id='section'>Paperid: <span id='pid'>551, <a href='https://arxiv.org/pdf/2208.14925.pdf' target='_blank'>https://arxiv.org/pdf/2208.14925.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tim Schreiter, Tiago Rodrigues de Almeida, Yufei Zhu, Eduardo Gutierrez Maestro, Lucas Morillo-Mendez, Andrey Rudenko, Tomasz P. Kucner, Oscar Martinez Mozos, Martin Magnusson, Luigi Palmieri, Kai O. Arras, Achim J. Lilienthal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2208.14925">The Magni Human Motion Dataset: Accurate, Complex, Multi-Modal, Natural, Semantically-Rich and Contextualized</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Rapid development of social robots stimulates active research in human motion modeling, interpretation and prediction, proactive collision avoidance, human-robot interaction and co-habitation in shared spaces. Modern approaches to this end require high quality datasets for training and evaluation. However, the majority of available datasets suffers from either inaccurate tracking data or unnatural, scripted behavior of the tracked people. This paper attempts to fill this gap by providing high quality tracking information from motion capture, eye-gaze trackers and on-board robot sensors in a semantically-rich environment. To induce natural behavior of the recorded participants, we utilise loosely scripted task assignment, which induces the participants navigate through the dynamic laboratory environment in a natural and purposeful way. The motion dataset, presented in this paper, sets a high quality standard, as the realistic and accurate data is enhanced with semantic information, enabling development of new algorithms which rely not only on the tracking information but also on contextual cues of the moving agents, static and dynamic environment.
<div id='section'>Paperid: <span id='pid'>552, <a href='https://arxiv.org/pdf/2510.03022.pdf' target='_blank'>https://arxiv.org/pdf/2510.03022.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rui Zhong, Yizhe Sun, Junjie Wen, Jinming Li, Chuang Cheng, Wei Dai, Zhiwen Zeng, Huimin Lu, Yichen Zhu, Yi Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03022">HumanoidExo: Scalable Whole-Body Humanoid Manipulation via Wearable Exoskeleton</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A significant bottleneck in humanoid policy learning is the acquisition of large-scale, diverse datasets, as collecting reliable real-world data remains both difficult and cost-prohibitive. To address this limitation, we introduce HumanoidExo, a novel system that transfers human motion to whole-body humanoid data. HumanoidExo offers a high-efficiency solution that minimizes the embodiment gap between the human demonstrator and the robot, thereby tackling the scarcity of whole-body humanoid data. By facilitating the collection of more voluminous and diverse datasets, our approach significantly enhances the performance of humanoid robots in dynamic, real-world scenarios. We evaluated our method across three challenging real-world tasks: table-top manipulation, manipulation integrated with stand-squat motions, and whole-body manipulation. Our results empirically demonstrate that HumanoidExo is a crucial addition to real-robot data, as it enables the humanoid policy to generalize to novel environments, learn complex whole-body control from only five real-robot demonstrations, and even acquire new skills (i.e., walking) solely from HumanoidExo data.
<div id='section'>Paperid: <span id='pid'>553, <a href='https://arxiv.org/pdf/2509.23169.pdf' target='_blank'>https://arxiv.org/pdf/2509.23169.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bolin Chen, Ru-Ling Liao, Yan Ye, Jie Chen, Shanzhi Yin, Xinrui Ju, Shiqi Wang, Yibo Fan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23169">Sparse2Dense: A Keypoint-driven Generative Framework for Human Video Compression and Vertex Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>For bandwidth-constrained multimedia applications, simultaneously achieving ultra-low bitrate human video compression and accurate vertex prediction remains a critical challenge, as it demands the harmonization of dynamic motion modeling, detailed appearance synthesis, and geometric consistency. To address this challenge, we propose Sparse2Dense, a keypoint-driven generative framework that leverages extremely sparse 3D keypoints as compact transmitted symbols to enable ultra-low bitrate human video compression and precise human vertex prediction. The key innovation is the multi-task learning-based and keypoint-aware deep generative model, which could encode complex human motion via compact 3D keypoints and leverage these sparse keypoints to estimate dense motion for video synthesis with temporal coherence and realistic textures. Additionally, a vertex predictor is integrated to learn human vertex geometry through joint optimization with video generation, ensuring alignment between visual content and geometric structure. Extensive experiments demonstrate that the proposed Sparse2Dense framework achieves competitive compression performance for human video over traditional/generative video codecs, whilst enabling precise human vertex prediction for downstream geometry applications. As such, Sparse2Dense is expected to facilitate bandwidth-efficient human-centric media transmission, such as real-time motion analysis, virtual human animation, and immersive entertainment.
<div id='section'>Paperid: <span id='pid'>554, <a href='https://arxiv.org/pdf/2509.13534.pdf' target='_blank'>https://arxiv.org/pdf/2509.13534.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chunxin Zheng, Kai Chen, Zhihai Bi, Yulin Li, Liang Pan, Jinni Zhou, Haoang Li, Jun Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.13534">Embracing Bulky Objects with Humanoid Robots: Whole-Body Manipulation with Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Whole-body manipulation (WBM) for humanoid robots presents a promising approach for executing embracing tasks involving bulky objects, where traditional grasping relying on end-effectors only remains limited in such scenarios due to inherent stability and payload constraints. This paper introduces a reinforcement learning framework that integrates a pre-trained human motion prior with a neural signed distance field (NSDF) representation to achieve robust whole-body embracing. Our method leverages a teacher-student architecture to distill large-scale human motion data, generating kinematically natural and physically feasible whole-body motion patterns. This facilitates coordinated control across the arms and torso, enabling stable multi-contact interactions that enhance the robustness in manipulation and also the load capacity. The embedded NSDF further provides accurate and continuous geometric perception, improving contact awareness throughout long-horizon tasks. We thoroughly evaluate the approach through comprehensive simulations and real-world experiments. The results demonstrate improved adaptability to diverse shapes and sizes of objects and also successful sim-to-real transfer. These indicate that the proposed framework offers an effective and practical solution for multi-contact and long-horizon WBM tasks of humanoid robots.
<div id='section'>Paperid: <span id='pid'>555, <a href='https://arxiv.org/pdf/2509.10426.pdf' target='_blank'>https://arxiv.org/pdf/2509.10426.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianxin Shi, Zengqi Peng, Xiaolong Chen, Tianyu Wo, Jun Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.10426">DECAMP: Towards Scene-Consistent Multi-Agent Motion Prediction with Disentangled Context-Aware Pre-Training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Trajectory prediction is a critical component of autonomous driving, essential for ensuring both safety and efficiency on the road. However, traditional approaches often struggle with the scarcity of labeled data and exhibit suboptimal performance in multi-agent prediction scenarios. To address these challenges, we introduce a disentangled context-aware pre-training framework for multi-agent motion prediction, named DECAMP. Unlike existing methods that entangle representation learning with pretext tasks, our framework decouples behavior pattern learning from latent feature reconstruction, prioritizing interpretable dynamics and thereby enhancing scene representation for downstream prediction. Additionally, our framework incorporates context-aware representation learning alongside collaborative spatial-motion pretext tasks, which enables joint optimization of structural and intentional reasoning while capturing the underlying dynamic intentions. Our experiments on the Argoverse 2 benchmark showcase the superior performance of our method, and the results attained underscore its effectiveness in multi-agent motion forecasting. To the best of our knowledge, this is the first context autoencoder framework for multi-agent motion forecasting in autonomous driving. The code and models will be made publicly available.
<div id='section'>Paperid: <span id='pid'>556, <a href='https://arxiv.org/pdf/2509.09283.pdf' target='_blank'>https://arxiv.org/pdf/2509.09283.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yueqi Zhang, Quancheng Qian, Taixian Hou, Peng Zhai, Xiaoyi Wei, Kangmai Hu, Jiafu Yi, Lihua Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09283">RENet: Fault-Tolerant Motion Control for Quadruped Robots via Redundant Estimator Networks under Visual Collapse</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-based locomotion in outdoor environments presents significant challenges for quadruped robots. Accurate environmental prediction and effective handling of depth sensor noise during real-world deployment remain difficult, severely restricting the outdoor applications of such algorithms. To address these deployment challenges in vision-based motion control, this letter proposes the Redundant Estimator Network (RENet) framework. The framework employs a dual-estimator architecture that ensures robust motion performance while maintaining deployment stability during onboard vision failures. Through an online estimator adaptation, our method enables seamless transitions between estimation modules when handling visual perception uncertainties. Experimental validation on a real-world robot demonstrates the framework's effectiveness in complex outdoor environments, showing particular advantages in scenarios with degraded visual perception. This framework demonstrates its potential as a practical solution for reliable robotic deployment in challenging field conditions. Project website: https://RENet-Loco.github.io/
<div id='section'>Paperid: <span id='pid'>557, <a href='https://arxiv.org/pdf/2507.07633.pdf' target='_blank'>https://arxiv.org/pdf/2507.07633.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhitao Wang, Hengyu Man, Wenrui Li, Xingtao Wang, Xiaopeng Fan, Debin Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07633">T-GVC: Trajectory-Guided Generative Video Coding at Ultra-Low Bitrates</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in video generation techniques have given rise to an emerging paradigm of generative video coding for Ultra-Low Bitrate (ULB) scenarios by leveraging powerful generative priors. However, most existing methods are limited by domain specificity (e.g., facial or human videos) or excessive dependence on high-level text guidance, which tend to inadequately capture fine-grained motion details, leading to unrealistic or incoherent reconstructions. To address these challenges, we propose Trajectory-Guided Generative Video Coding (dubbed T-GVC), a novel framework that bridges low-level motion tracking with high-level semantic understanding. T-GVC features a semantic-aware sparse motion sampling pipeline that extracts pixel-wise motion as sparse trajectory points based on their semantic importance, significantly reducing the bitrate while preserving critical temporal semantic information. In addition, by integrating trajectory-aligned loss constraints into diffusion processes, we introduce a training-free guidance mechanism in latent space to ensure physically plausible motion patterns without sacrificing the inherent capabilities of generative models. Experimental results demonstrate that T-GVC outperforms both traditional and neural video codecs under ULB conditions. Furthermore, additional experiments confirm that our framework achieves more precise motion control than existing text-guided methods, paving the way for a novel direction of generative video coding guided by geometric motion modeling.
<div id='section'>Paperid: <span id='pid'>558, <a href='https://arxiv.org/pdf/2505.05851.pdf' target='_blank'>https://arxiv.org/pdf/2505.05851.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Janik Kaden, Maximilian Hilger, Tim Schreiter, Marius Schaab, Thomas Graichen, Andrey Rudenko, Ulrich Heinkel, Achim J. Lilienthal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.05851">Collecting Human Motion Data in Large and Occlusion-Prone Environments using Ultra-Wideband Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With robots increasingly integrating into human environments, understanding and predicting human motion is essential for safe and efficient interactions. Modern human motion and activity prediction approaches require high quality and quantity of data for training and evaluation, usually collected from motion capture systems, onboard or stationary sensors. Setting up these systems is challenging due to the intricate setup of hardware components, extensive calibration procedures, occlusions, and substantial costs. These constraints make deploying such systems in new and large environments difficult and limit their usability for in-the-wild measurements. In this paper we investigate the possibility to apply the novel Ultra-Wideband (UWB) localization technology as a scalable alternative for human motion capture in crowded and occlusion-prone environments. We include additional sensing modalities such as eye-tracking, onboard robot LiDAR and radar sensors, and record motion capture data as ground truth for evaluation and comparison. The environment imitates a museum setup, with up to four active participants navigating toward random goals in a natural way, and offers more than 130 minutes of multi-modal data. Our investigation provides a step toward scalable and accurate motion data collection beyond vision-based systems, laying a foundation for evaluating sensing modalities like UWB in larger and complex environments like warehouses, airports, or convention centers.
<div id='section'>Paperid: <span id='pid'>559, <a href='https://arxiv.org/pdf/2504.12667.pdf' target='_blank'>https://arxiv.org/pdf/2504.12667.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lin Liu, Caiyan Jia, Ziying Song, Hongyu Pan, Bencheng Liao, Wenchao Sun, Yongchang Zhang, Lei Yang, Yandan Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.12667">Fully Unified Motion Planning for End-to-End Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current end-to-end autonomous driving methods typically learn only from expert planning data collected from a single ego vehicle, severely limiting the diversity of learnable driving policies and scenarios. However, a critical yet overlooked fact is that in any driving scenario, multiple high-quality trajectories from other vehicles coexist with a specific ego vehicle's trajectory. Existing methods fail to fully exploit this valuable resource, missing important opportunities to improve the models' performance (including long-tail scenarios) through learning from other experts. Intuitively, Jointly learning from both ego and other vehicles' expert data is beneficial for planning tasks. However, this joint learning faces two critical challenges. (1) Different scene observation perspectives across vehicles hinder inter-vehicle alignment of scene feature representations; (2) The absence of partial modality in other vehicles' data (e.g., vehicle states) compared to ego-vehicle data introduces learning bias. To address these challenges, we propose FUMP (Fully Unified Motion Planning), a novel two-stage trajectory generation framework. Building upon probabilistic decomposition, we model the planning task as a specialized subtask of motion prediction. Specifically, our approach decouples trajectory planning into two stages. In Stage 1, a shared decoder jointly generates initial trajectories for both tasks. In Stage 2, the model performs planning-specific refinement conditioned on an ego-vehicle's state. The transition between the two stages is bridged by a state predictor trained exclusively on ego-vehicle data. To address the cross-vehicle discrepancy in observational perspectives, we propose an Equivariant Context-Sharing Adapter (ECSA) before Stage 1 for improving cross-vehicle generalization of scene representations.
<div id='section'>Paperid: <span id='pid'>560, <a href='https://arxiv.org/pdf/2504.03939.pdf' target='_blank'>https://arxiv.org/pdf/2504.03939.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianle Wu, Mojtaba Esfandiari, Peiyao Zhang, Russell H. Taylor, Peter Gehlbach, Iulian Iordachita
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.03939">Deep Learning-Enhanced Robotic Subretinal Injection with Real-Time Retinal Motion Compensation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Subretinal injection is a critical procedure for delivering therapeutic agents to treat retinal diseases such as age-related macular degeneration (AMD). However, retinal motion caused by physiological factors such as respiration and heartbeat significantly impacts precise needle positioning, increasing the risk of retinal pigment epithelium (RPE) damage. This paper presents a fully autonomous robotic subretinal injection system that integrates intraoperative optical coherence tomography (iOCT) imaging and deep learning-based motion prediction to synchronize needle motion with retinal displacement. A Long Short-Term Memory (LSTM) neural network is used to predict internal limiting membrane (ILM) motion, outperforming a Fast Fourier Transform (FFT)-based baseline model. Additionally, a real-time registration framework aligns the needle tip position with the robot's coordinate frame. Then, a dynamic proportional speed control strategy ensures smooth and adaptive needle insertion. Experimental validation in both simulation and ex vivo open-sky porcine eyes demonstrates precise motion synchronization and successful subretinal injections. The experiment achieves a mean tracking error below 16.4 Î¼m in pre-insertion phases. These results show the potential of AI-driven robotic assistance to improve the safety and accuracy of retinal microsurgery.
<div id='section'>Paperid: <span id='pid'>561, <a href='https://arxiv.org/pdf/2503.22249.pdf' target='_blank'>https://arxiv.org/pdf/2503.22249.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xianqi Zhang, Hongliang Wei, Wenrui Wang, Xingtao Wang, Xiaopeng Fan, Debin Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.22249">FLAM: Foundation Model-Based Body Stabilization for Humanoid Locomotion and Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humanoid robots have attracted significant attention in recent years. Reinforcement Learning (RL) is one of the main ways to control the whole body of humanoid robots. RL enables agents to complete tasks by learning from environment interactions, guided by task rewards. However, existing RL methods rarely explicitly consider the impact of body stability on humanoid locomotion and manipulation. Achieving high performance in whole-body control remains a challenge for RL methods that rely solely on task rewards. In this paper, we propose a Foundation model-based method for humanoid Locomotion And Manipulation (FLAM for short). FLAM integrates a stabilizing reward function with a basic policy. The stabilizing reward function is designed to encourage the robot to learn stable postures, thereby accelerating the learning process and facilitating task completion. Specifically, the robot pose is first mapped to the 3D virtual human model. Then, the human pose is stabilized and reconstructed through a human motion reconstruction model. Finally, the pose before and after reconstruction is used to compute the stabilizing reward. By combining this stabilizing reward with the task reward, FLAM effectively guides policy learning. Experimental results on a humanoid robot benchmark demonstrate that FLAM outperforms state-of-the-art RL methods, highlighting its effectiveness in improving stability and overall performance.
<div id='section'>Paperid: <span id='pid'>562, <a href='https://arxiv.org/pdf/2503.21779.pdf' target='_blank'>https://arxiv.org/pdf/2503.21779.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weihao Yu, Yuanhao Cai, Ruyi Zha, Zhiwen Fan, Chenxin Li, Yixuan Yuan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.21779">X$^{2}$-Gaussian: 4D Radiative Gaussian Splatting for Continuous-time Tomographic Reconstruction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Four-dimensional computed tomography (4D CT) reconstruction is crucial for capturing dynamic anatomical changes but faces inherent limitations from conventional phase-binning workflows. Current methods discretize temporal resolution into fixed phases with respiratory gating devices, introducing motion misalignment and restricting clinical practicality. In this paper, We propose X$^2$-Gaussian, a novel framework that enables continuous-time 4D-CT reconstruction by integrating dynamic radiative Gaussian splatting with self-supervised respiratory motion learning. Our approach models anatomical dynamics through a spatiotemporal encoder-decoder architecture that predicts time-varying Gaussian deformations, eliminating phase discretization. To remove dependency on external gating devices, we introduce a physiology-driven periodic consistency loss that learns patient-specific breathing cycles directly from projections via differentiable optimization. Extensive experiments demonstrate state-of-the-art performance, achieving a 9.93 dB PSNR gain over traditional methods and 2.25 dB improvement against prior Gaussian splatting techniques. By unifying continuous motion modeling with hardware-free period learning, X$^2$-Gaussian advances high-fidelity 4D CT reconstruction for dynamic clinical imaging. Project website at: https://x2-gaussian.github.io/.
<div id='section'>Paperid: <span id='pid'>563, <a href='https://arxiv.org/pdf/2503.16421.pdf' target='_blank'>https://arxiv.org/pdf/2503.16421.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Quanhao Li, Zhen Xing, Rui Wang, Hui Zhang, Qi Dai, Zuxuan Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.16421">MagicMotion: Controllable Video Generation with Dense-to-Sparse Trajectory Guidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in video generation have led to remarkable improvements in visual quality and temporal coherence. Upon this, trajectory-controllable video generation has emerged to enable precise object motion control through explicitly defined spatial paths. However, existing methods struggle with complex object movements and multi-object motion control, resulting in imprecise trajectory adherence, poor object consistency, and compromised visual quality. Furthermore, these methods only support trajectory control in a single format, limiting their applicability in diverse scenarios. Additionally, there is no publicly available dataset or benchmark specifically tailored for trajectory-controllable video generation, hindering robust training and systematic evaluation. To address these challenges, we introduce MagicMotion, a novel image-to-video generation framework that enables trajectory control through three levels of conditions from dense to sparse: masks, bounding boxes, and sparse boxes. Given an input image and trajectories, MagicMotion seamlessly animates objects along defined trajectories while maintaining object consistency and visual quality. Furthermore, we present MagicData, a large-scale trajectory-controlled video dataset, along with an automated pipeline for annotation and filtering. We also introduce MagicBench, a comprehensive benchmark that assesses both video quality and trajectory control accuracy across different numbers of objects. Extensive experiments demonstrate that MagicMotion outperforms previous methods across various metrics. Our project page are publicly available at https://quanhaol.github.io/magicmotion-site.
<div id='section'>Paperid: <span id='pid'>564, <a href='https://arxiv.org/pdf/2411.18521.pdf' target='_blank'>https://arxiv.org/pdf/2411.18521.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Demir Arikan, Peiyao Zhang, Michael Sommersperger, Shervin Dehghani, Mojtaba Esfandiari, Russel H. Taylor, M. Ali Nasseri, Peter Gehlbach, Nassir Navab, Iulian Iordachita
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.18521">Towards Motion Compensation in Autonomous Robotic Subretinal Injections</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Exudative (wet) age-related macular degeneration (AMD) is a leading cause of vision loss in older adults, typically treated with intravitreal injections. Emerging therapies, such as subretinal injections of stem cells, gene therapy, small molecules and RPE cells require precise delivery to avoid damaging delicate retinal structures. Robotic systems can potentially offer the necessary precision for these procedures. This paper presents a novel approach for motion compensation in robotic subretinal injections, utilizing real time Optical Coherence Tomography (OCT). The proposed method leverages B$^5$-scans, a rapid acquisition of small-volume OCT data, for dynamic tracking of retinal motion along the Z-axis, compensating for physiological movements such as breathing and heartbeat. Validation experiments on ex vivo porcine eyes revealed challenges in maintaining a consistent tool-to-retina distance, with deviations of up to 200 $Î¼m$ for 100 $Î¼m$ amplitude motions and over 80 $Î¼m$ for 25 $Î¼m$ amplitude motions over one minute. Subretinal injections faced additional difficulties, with phase shifts causing the needle to move off-target and inject into the vitreous. These results highlight the need for improved motion prediction and horizontal stability to enhance the accuracy and safety of robotic subretinal procedures.
<div id='section'>Paperid: <span id='pid'>565, <a href='https://arxiv.org/pdf/2411.01442.pdf' target='_blank'>https://arxiv.org/pdf/2411.01442.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Beomseok Kang, Priyabrata Saha, Sudarshan Sharma, Biswadeep Chakraborty, Saibal Mukhopadhyay
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.01442">Online Relational Inference for Evolving Multi-agent Interacting Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a novel framework, Online Relational Inference (ORI), designed to efficiently identify hidden interaction graphs in evolving multi-agent interacting systems using streaming data. Unlike traditional offline methods that rely on a fixed training set, ORI employs online backpropagation, updating the model with each new data point, thereby allowing it to adapt to changing environments in real-time. A key innovation is the use of an adjacency matrix as a trainable parameter, optimized through a new adaptive learning rate technique called AdaRelation, which adjusts based on the historical sensitivity of the decoder to changes in the interaction graph. Additionally, a data augmentation method named Trajectory Mirror (TM) is introduced to improve generalization by exposing the model to varied trajectory patterns. Experimental results on both synthetic datasets and real-world data (CMU MoCap for human motion) demonstrate that ORI significantly improves the accuracy and adaptability of relational inference in dynamic settings compared to existing methods. This approach is model-agnostic, enabling seamless integration with various neural relational inference (NRI) architectures, and offers a robust solution for real-time applications in complex, evolving systems.
<div id='section'>Paperid: <span id='pid'>566, <a href='https://arxiv.org/pdf/2408.09397.pdf' target='_blank'>https://arxiv.org/pdf/2408.09397.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chao Xu, Mingze Sun, Zhi-Qi Cheng, Fei Wang, Yang Liu, Baigui Sun, Ruqi Huang, Alexander Hauptmann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.09397">Combo: Co-speech holistic 3D human motion generation and efficient customizable adaptation in harmony</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose a novel framework, Combo, for harmonious co-speech holistic 3D human motion generation and efficient customizable adaption. In particular, we identify that one fundamental challenge as the multiple-input-multiple-output (MIMO) nature of the generative model of interest. More concretely, on the input end, the model typically consumes both speech signals and character guidance (e.g., identity and emotion), which not only poses challenge on learning capacity but also hinders further adaptation to varying guidance; on the output end, holistic human motions mainly consist of facial expressions and body movements, which are inherently correlated but non-trivial to coordinate in current data-driven generation process. In response to the above challenge, we propose tailored designs to both ends. For the former, we propose to pre-train on data regarding a fixed identity with neutral emotion, and defer the incorporation of customizable conditions (identity and emotion) to fine-tuning stage, which is boosted by our novel X-Adapter for parameter-efficient fine-tuning. For the latter, we propose a simple yet effective transformer design, DU-Trans, which first divides into two branches to learn individual features of face expression and body movements, and then unites those to learn a joint bi-directional distribution and directly predicts combined coefficients. Evaluated on BEAT2 and SHOW datasets, Combo is highly effective in generating high-quality motions but also efficient in transferring identity and emotion. Project website: \href{https://xc-csc101.github.io/combo/}{Combo}.
<div id='section'>Paperid: <span id='pid'>567, <a href='https://arxiv.org/pdf/2408.02110.pdf' target='_blank'>https://arxiv.org/pdf/2408.02110.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Feichi Lu, Zijian Dong, Jie Song, Otmar Hilliges
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.02110">AvatarPose: Avatar-guided 3D Pose Estimation of Close Human Interaction from Sparse Multi-view Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite progress in human motion capture, existing multi-view methods often face challenges in estimating the 3D pose and shape of multiple closely interacting people. This difficulty arises from reliance on accurate 2D joint estimations, which are hard to obtain due to occlusions and body contact when people are in close interaction. To address this, we propose a novel method leveraging the personalized implicit neural avatar of each individual as a prior, which significantly improves the robustness and precision of this challenging pose estimation task. Concretely, the avatars are efficiently reconstructed via layered volume rendering from sparse multi-view videos. The reconstructed avatar prior allows for the direct optimization of 3D poses based on color and silhouette rendering loss, bypassing the issues associated with noisy 2D detections. To handle interpenetration, we propose a collision loss on the overlapping shape regions of avatars to add penetration constraints. Moreover, both 3D poses and avatars are optimized in an alternating manner. Our experimental results demonstrate state-of-the-art performance on several public datasets.
<div id='section'>Paperid: <span id='pid'>568, <a href='https://arxiv.org/pdf/2407.10485.pdf' target='_blank'>https://arxiv.org/pdf/2407.10485.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mufeng Yao, Jinlong Peng, Qingdong He, Bo Peng, Hao Chen, Mingmin Chi, Chao Liu, Jon Atli Benediktsson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.10485">MM-Tracker: Motion Mamba with Margin Loss for UAV-platform Multiple Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multiple object tracking (MOT) from unmanned aerial vehicle (UAV) platforms requires efficient motion modeling. This is because UAV-MOT faces both local object motion and global camera motion. Motion blur also increases the difficulty of detecting large moving objects. Previous UAV motion modeling approaches either focus only on local motion or ignore motion blurring effects, thus limiting their tracking performance and speed. To address these issues, we propose the Motion Mamba Module, which explores both local and global motion features through cross-correlation and bi-directional Mamba Modules for better motion modeling. To address the detection difficulties caused by motion blur, we also design motion margin loss to effectively improve the detection accuracy of motion blurred objects. Based on the Motion Mamba module and motion margin loss, our proposed MM-Tracker surpasses the state-of-the-art in two widely open-source UAV-MOT datasets. Code will be available.
<div id='section'>Paperid: <span id='pid'>569, <a href='https://arxiv.org/pdf/2407.05890.pdf' target='_blank'>https://arxiv.org/pdf/2407.05890.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaqi Chen, Bingqian Lin, Xinmin Liu, Lin Ma, Xiaodan Liang, Kwan-Yee K. Wong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.05890">Affordances-Oriented Planning using Foundation Models for Continuous Vision-Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>LLM-based agents have demonstrated impressive zero-shot performance in vision-language navigation (VLN) task. However, existing LLM-based methods often focus only on solving high-level task planning by selecting nodes in predefined navigation graphs for movements, overlooking low-level control in navigation scenarios. To bridge this gap, we propose AO-Planner, a novel Affordances-Oriented Planner for continuous VLN task. Our AO-Planner integrates various foundation models to achieve affordances-oriented low-level motion planning and high-level decision-making, both performed in a zero-shot setting. Specifically, we employ a Visual Affordances Prompting (VAP) approach, where the visible ground is segmented by SAM to provide navigational affordances, based on which the LLM selects potential candidate waypoints and plans low-level paths towards selected waypoints. We further propose a high-level PathAgent which marks planned paths into the image input and reasons the most probable path by comprehending all environmental information. Finally, we convert the selected path into 3D coordinates using camera intrinsic parameters and depth information, avoiding challenging 3D predictions for LLMs. Experiments on the challenging R2R-CE and RxR-CE datasets show that AO-Planner achieves state-of-the-art zero-shot performance (8.8% improvement on SPL). Our method can also serve as a data annotator to obtain pseudo-labels, distilling its waypoint prediction ability into a learning-based predictor. This new predictor does not require any waypoint data from the simulator and achieves 47% SR competing with supervised methods. We establish an effective connection between LLM and 3D world, presenting novel prospects for employing foundation models in low-level motion control.
<div id='section'>Paperid: <span id='pid'>570, <a href='https://arxiv.org/pdf/2404.18630.pdf' target='_blank'>https://arxiv.org/pdf/2404.18630.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenbo Wang, Hsuan-I Ho, Chen Guo, Boxiang Rong, Artur Grigorev, Jie Song, Juan Jose Zarate, Otmar Hilliges
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.18630">4D-DRESS: A 4D Dataset of Real-world Human Clothing with Semantic Annotations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The studies of human clothing for digital avatars have predominantly relied on synthetic datasets. While easy to collect, synthetic data often fall short in realism and fail to capture authentic clothing dynamics. Addressing this gap, we introduce 4D-DRESS, the first real-world 4D dataset advancing human clothing research with its high-quality 4D textured scans and garment meshes. 4D-DRESS captures 64 outfits in 520 human motion sequences, amounting to 78k textured scans. Creating a real-world clothing dataset is challenging, particularly in annotating and segmenting the extensive and complex 4D human scans. To address this, we develop a semi-automatic 4D human parsing pipeline. We efficiently combine a human-in-the-loop process with automation to accurately label 4D scans in diverse garments and body movements. Leveraging precise annotations and high-quality garment meshes, we establish several benchmarks for clothing simulation and reconstruction. 4D-DRESS offers realistic and challenging data that complements synthetic sources, paving the way for advancements in research of lifelike human clothing. Website: https://ait.ethz.ch/4d-dress.
<div id='section'>Paperid: <span id='pid'>571, <a href='https://arxiv.org/pdf/2404.06860.pdf' target='_blank'>https://arxiv.org/pdf/2404.06860.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fulong Ma, Weiqing Qi, Guoyang Zhao, Linwei Zheng, Sheng Wang, Yuxuan Liu, Ming Liu, Jun Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.06860">Monocular 3D lane detection for Autonomous Driving: Recent Achievements, Challenges, and Outlooks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D lane detection is essential in autonomous driving as it extracts structural and traffic information from the road in three-dimensional space, aiding self-driving cars in logical, safe, and comfortable path planning and motion control. Given the cost of sensors and the advantages of visual data in color information, 3D lane detection based on monocular vision is an important research direction in the realm of autonomous driving, increasingly gaining attention in both industry and academia. Regrettably, recent advancements in visual perception seem inadequate for the development of fully reliable 3D lane detection algorithms, which also hampers the progress of vision-based fully autonomous vehicles. We believe that there is still considerable room for improvement in 3D lane detection algorithms for autonomous vehicles using visual sensors, and significant enhancements are needed. This review looks back and analyzes the current state of achievements in the field of 3D lane detection research. It covers all current monocular-based 3D lane detection processes, discusses the performance of these cutting-edge algorithms, analyzes the time complexity of various algorithms, and highlights the main achievements and limitations of ongoing research efforts. The survey also includes a comprehensive discussion of available 3D lane detection datasets and the challenges that researchers face but have not yet resolved. Finally, our work outlines future research directions and invites researchers and practitioners to join this exciting field.
<div id='section'>Paperid: <span id='pid'>572, <a href='https://arxiv.org/pdf/2311.14922.pdf' target='_blank'>https://arxiv.org/pdf/2311.14922.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ge Sun, Sheng Wang, Lei Zhu, Ming Liu, Jun Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.14922">GDTS: Goal-Guided Diffusion Model with Tree Sampling for Multi-Modal Pedestrian Trajectory Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate prediction of pedestrian trajectories is crucial for improving the safety of autonomous driving. However, this task is generally nontrivial due to the inherent stochasticity of human motion, which naturally requires the predictor to generate multi-modal prediction. Previous works leverage various generative methods, such as GAN and VAE, for pedestrian trajectory prediction. Nevertheless, these methods may suffer from mode collapse and relatively low-quality results. The denoising diffusion probabilistic model (DDPM) has recently been applied to trajectory prediction due to its simple training process and powerful reconstruction ability. However, current diffusion-based methods do not fully utilize input information and usually require many denoising iterations that lead to a long inference time or an additional network for initialization. To address these challenges and facilitate the use of diffusion models in multi-modal trajectory prediction, we propose GDTS, a novel Goal-Guided Diffusion Model with Tree Sampling for multi-modal trajectory prediction. Considering the "goal-driven" characteristics of human motion, GDTS leverages goal estimation to guide the generation of the diffusion network. A two-stage tree sampling algorithm is presented, which leverages common features to reduce the inference time and improve accuracy for multi-modal prediction. Experimental results demonstrate that our proposed framework achieves comparable state-of-the-art performance with real-time inference speed in public datasets.
<div id='section'>Paperid: <span id='pid'>573, <a href='https://arxiv.org/pdf/2311.05599.pdf' target='_blank'>https://arxiv.org/pdf/2311.05599.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sammy Christen, Lan Feng, Wei Yang, Yu-Wei Chao, Otmar Hilliges, Jie Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.05599">SynH2R: Synthesizing Hand-Object Motions for Learning Human-to-Robot Handovers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-based human-to-robot handover is an important and challenging task in human-robot interaction. Recent work has attempted to train robot policies by interacting with dynamic virtual humans in simulated environments, where the policies can later be transferred to the real world. However, a major bottleneck is the reliance on human motion capture data, which is expensive to acquire and difficult to scale to arbitrary objects and human grasping motions. In this paper, we introduce a framework that can generate plausible human grasping motions suitable for training the robot. To achieve this, we propose a hand-object synthesis method that is designed to generate handover-friendly motions similar to humans. This allows us to generate synthetic training and testing data with 100x more objects than previous work. In our experiments, we show that our method trained purely with synthetic data is competitive with state-of-the-art methods that rely on real human motion data both in simulation and on a real system. In addition, we can perform evaluations on a larger scale compared to prior work. With our newly introduced test set, we show that our model can better scale to a large variety of unseen objects and human motions compared to the baselines. Project page: https://eth-ait.github.io/synthetic-handovers/
<div id='section'>Paperid: <span id='pid'>574, <a href='https://arxiv.org/pdf/2308.16154.pdf' target='_blank'>https://arxiv.org/pdf/2308.16154.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiqi Zhong, Luming Liang, Ilya Zharkov, Ulrich Neumann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.16154">MMVP: Motion-Matrix-based Video Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A central challenge of video prediction lies where the system has to reason the objects' future motions from image frames while simultaneously maintaining the consistency of their appearances across frames. This work introduces an end-to-end trainable two-stream video prediction framework, Motion-Matrix-based Video Prediction (MMVP), to tackle this challenge. Unlike previous methods that usually handle motion prediction and appearance maintenance within the same set of modules, MMVP decouples motion and appearance information by constructing appearance-agnostic motion matrices. The motion matrices represent the temporal similarity of each and every pair of feature patches in the input frames, and are the sole input of the motion prediction module in MMVP. This design improves video prediction in both accuracy and efficiency, and reduces the model size. Results of extensive experiments demonstrate that MMVP outperforms state-of-the-art systems on public data sets by non-negligible large margins (about 1 db in PSNR, UCF Sports) in significantly smaller model sizes (84% the size or smaller).
<div id='section'>Paperid: <span id='pid'>575, <a href='https://arxiv.org/pdf/2303.17209.pdf' target='_blank'>https://arxiv.org/pdf/2303.17209.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiming Zhao, Denys Rozumnyi, Jie Song, Otmar Hilliges, Marc Pollefeys, Martin R. Oswald
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.17209">Human from Blur: Human Pose Tracking from Blurry Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a method to estimate 3D human poses from substantially blurred images. The key idea is to tackle the inverse problem of image deblurring by modeling the forward problem with a 3D human model, a texture map, and a sequence of poses to describe human motion. The blurring process is then modeled by a temporal image aggregation step. Using a differentiable renderer, we can solve the inverse problem by backpropagating the pixel-wise reprojection error to recover the best human motion representation that explains a single or multiple input images. Since the image reconstruction loss alone is insufficient, we present additional regularization terms. To the best of our knowledge, we present the first method to tackle this problem. Our method consistently outperforms other methods on significantly blurry inputs since they lack one or multiple key functionalities that our method unifies, i.e. image deblurring with sub-frame accuracy and explicit 3D modeling of non-rigid human motion.
<div id='section'>Paperid: <span id='pid'>576, <a href='https://arxiv.org/pdf/2303.13397.pdf' target='_blank'>https://arxiv.org/pdf/2303.13397.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ce Zheng, Xianpeng Liu, Qucheng Peng, Tianfu Wu, Pu Wang, Chen Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.13397">DiffMesh: A Motion-aware Diffusion Framework for Human Mesh Recovery from Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human mesh recovery (HMR) provides rich human body information for various real-world applications. While image-based HMR methods have achieved impressive results, they often struggle to recover humans in dynamic scenarios, leading to temporal inconsistencies and non-smooth 3D motion predictions due to the absence of human motion. In contrast, video-based approaches leverage temporal information to mitigate this issue. In this paper, we present DiffMesh, an innovative motion-aware Diffusion-like framework for video-based HMR. DiffMesh establishes a bridge between diffusion models and human motion, efficiently generating accurate and smooth output mesh sequences by incorporating human motion within the forward process and reverse process in the diffusion model. Extensive experiments are conducted on the widely used datasets (Human3.6M \cite{h36m_pami} and 3DPW \cite{pw3d2018}), which demonstrate the effectiveness and efficiency of our DiffMesh. Visual comparisons in real-world scenarios further highlight DiffMesh's suitability for practical applications.
<div id='section'>Paperid: <span id='pid'>577, <a href='https://arxiv.org/pdf/2510.08131.pdf' target='_blank'>https://arxiv.org/pdf/2510.08131.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kesen Zhao, Jiaxin Shi, Beier Zhu, Junbao Zhou, Xiaolong Shen, Yuan Zhou, Qianru Sun, Hanwang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.08131">Real-Time Motion-Controllable Autoregressive Video Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Real-time motion-controllable video generation remains challenging due to the inherent latency of bidirectional diffusion models and the lack of effective autoregressive (AR) approaches. Existing AR video diffusion models are limited to simple control signals or text-to-video generation, and often suffer from quality degradation and motion artifacts in few-step generation. To address these challenges, we propose AR-Drag, the first RL-enhanced few-step AR video diffusion model for real-time image-to-video generation with diverse motion control. We first fine-tune a base I2V model to support basic motion control, then further improve it via reinforcement learning with a trajectory-based reward model. Our design preserves the Markov property through a Self-Rollout mechanism and accelerates training by selectively introducing stochasticity in denoising steps. Extensive experiments demonstrate that AR-Drag achieves high visual fidelity and precise motion alignment, significantly reducing latency compared with state-of-the-art motion-controllable VDMs, while using only 1.3B parameters. Additional visualizations can be found on our project page: https://kesenzhao.github.io/AR-Drag.github.io/.
<div id='section'>Paperid: <span id='pid'>578, <a href='https://arxiv.org/pdf/2508.15185.pdf' target='_blank'>https://arxiv.org/pdf/2508.15185.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dingzhu Wen, Sijing Xie, Xiaowen Cao, Yuanhao Cui, Jie Xu, Yuanming Shi, Shuguang Cui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15185">Integrated Sensing, Communication, and Computation for Over-the-Air Federated Edge Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper studies an over-the-air federated edge learning (Air-FEEL) system with integrated sensing, communication, and computation (ISCC), in which one edge server coordinates multiple edge devices to wirelessly sense the objects and use the sensing data to collaboratively train a machine learning model for recognition tasks. In this system, over-the-air computation (AirComp) is employed to enable one-shot model aggregation from edge devices. Under this setup, we analyze the convergence behavior of the ISCC-enabled Air-FEEL in terms of the loss function degradation, by particularly taking into account the wireless sensing noise during the training data acquisition and the AirComp distortions during the over-the-air model aggregation. The result theoretically shows that sensing, communication, and computation compete for network resources to jointly decide the convergence rate. Based on the analysis, we design the ISCC parameters under the target of maximizing the loss function degradation while ensuring the latency and energy budgets in each round. The challenge lies on the tightly coupled processes of sensing, communication, and computation among different devices. To tackle the challenge, we derive a low-complexity ISCC algorithm by alternately optimizing the batch size control and the network resource allocation. It is found that for each device, less sensing power should be consumed if a larger batch of data samples is obtained and vice versa. Besides, with a given batch size, the optimal computation speed of one device is the minimum one that satisfies the latency constraint. Numerical results based on a human motion recognition task verify the theoretical convergence analysis and show that the proposed ISCC algorithm well coordinates the batch size control and resource allocation among sensing, communication, and computation to enhance the learning performance.
<div id='section'>Paperid: <span id='pid'>579, <a href='https://arxiv.org/pdf/2508.09383.pdf' target='_blank'>https://arxiv.org/pdf/2508.09383.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guoxian Song, Hongyi Xu, Xiaochen Zhao, You Xie, Tianpei Gu, Zenan Li, Chenxu Zhang, Linjie Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09383">X-UniMotion: Animating Human Images with Expressive, Unified and Identity-Agnostic Motion Latents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present X-UniMotion, a unified and expressive implicit latent representation for whole-body human motion, encompassing facial expressions, body poses, and hand gestures. Unlike prior motion transfer methods that rely on explicit skeletal poses and heuristic cross-identity adjustments, our approach encodes multi-granular motion directly from a single image into a compact set of four disentangled latent tokens -- one for facial expression, one for body pose, and one for each hand. These motion latents are both highly expressive and identity-agnostic, enabling high-fidelity, detailed cross-identity motion transfer across subjects with diverse identities, poses, and spatial configurations. To achieve this, we introduce a self-supervised, end-to-end framework that jointly learns the motion encoder and latent representation alongside a DiT-based video generative model, trained on large-scale, diverse human motion datasets. Motion-identity disentanglement is enforced via 2D spatial and color augmentations, as well as synthetic 3D renderings of cross-identity subject pairs under shared poses. Furthermore, we guide motion token learning with auxiliary decoders that promote fine-grained, semantically aligned, and depth-aware motion embeddings. Extensive experiments show that X-UniMotion outperforms state-of-the-art methods, producing highly expressive animations with superior motion fidelity and identity preservation.
<div id='section'>Paperid: <span id='pid'>580, <a href='https://arxiv.org/pdf/2508.04224.pdf' target='_blank'>https://arxiv.org/pdf/2508.04224.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiahui Li, Shengeng Tang, Jingxuan He, Gang Huang, Zhangye Wang, Yantao Pan, Lechao Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.04224">SplitGaussian: Reconstructing Dynamic Scenes via Visual Geometry Decomposition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reconstructing dynamic 3D scenes from monocular video remains fundamentally challenging due to the need to jointly infer motion, structure, and appearance from limited observations. Existing dynamic scene reconstruction methods based on Gaussian Splatting often entangle static and dynamic elements in a shared representation, leading to motion leakage, geometric distortions, and temporal flickering. We identify that the root cause lies in the coupled modeling of geometry and appearance across time, which hampers both stability and interpretability. To address this, we propose \textbf{SplitGaussian}, a novel framework that explicitly decomposes scene representations into static and dynamic components. By decoupling motion modeling from background geometry and allowing only the dynamic branch to deform over time, our method prevents motion artifacts in static regions while supporting view- and time-dependent appearance refinement. This disentangled design not only enhances temporal consistency and reconstruction fidelity but also accelerates convergence. Extensive experiments demonstrate that SplitGaussian outperforms prior state-of-the-art methods in rendering quality, geometric stability, and motion separation.
<div id='section'>Paperid: <span id='pid'>581, <a href='https://arxiv.org/pdf/2508.04049.pdf' target='_blank'>https://arxiv.org/pdf/2508.04049.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiayi He, Xu Wang, Shengeng Tang, Yaxiong Wang, Lechao Cheng, Dan Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.04049">Motion is the Choreographer: Learning Latent Pose Dynamics for Seamless Sign Language Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Sign language video generation requires producing natural signing motions with realistic appearances under precise semantic control, yet faces two critical challenges: excessive signer-specific data requirements and poor generalization. We propose a new paradigm for sign language video generation that decouples motion semantics from signer identity through a two-phase synthesis framework. First, we construct a signer-independent multimodal motion lexicon, where each gloss is stored as identity-agnostic pose, gesture, and 3D mesh sequences, requiring only one recording per sign. This compact representation enables our second key innovation: a discrete-to-continuous motion synthesis stage that transforms retrieved gloss sequences into temporally coherent motion trajectories, followed by identity-aware neural rendering to produce photorealistic videos of arbitrary signers. Unlike prior work constrained by signer-specific datasets, our method treats motion as a first-class citizen: the learned latent pose dynamics serve as a portable "choreography layer" that can be visually realized through different human appearances. Extensive experiments demonstrate that disentangling motion from identity is not just viable but advantageous - enabling both high-quality synthesis and unprecedented flexibility in signer personalization.
<div id='section'>Paperid: <span id='pid'>582, <a href='https://arxiv.org/pdf/2508.02944.pdf' target='_blank'>https://arxiv.org/pdf/2508.02944.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenxu Zhang, Zenan Li, Hongyi Xu, You Xie, Xiaochen Zhao, Tianpei Gu, Guoxian Song, Xin Chen, Chao Liang, Jianwen Jiang, Linjie Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02944">X-Actor: Emotional and Expressive Long-Range Portrait Acting from Audio</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present X-Actor, a novel audio-driven portrait animation framework that generates lifelike, emotionally expressive talking head videos from a single reference image and an input audio clip. Unlike prior methods that emphasize lip synchronization and short-range visual fidelity in constrained speaking scenarios, X-Actor enables actor-quality, long-form portrait performance capturing nuanced, dynamically evolving emotions that flow coherently with the rhythm and content of speech. Central to our approach is a two-stage decoupled generation pipeline: an audio-conditioned autoregressive diffusion model that predicts expressive yet identity-agnostic facial motion latent tokens within a long temporal context window, followed by a diffusion-based video synthesis module that translates these motions into high-fidelity video animations. By operating in a compact facial motion latent space decoupled from visual and identity cues, our autoregressive diffusion model effectively captures long-range correlations between audio and facial dynamics through a diffusion-forcing training paradigm, enabling infinite-length emotionally-rich motion prediction without error accumulation. Extensive experiments demonstrate that X-Actor produces compelling, cinematic-style performances that go beyond standard talking head animations and achieves state-of-the-art results in long-range, audio-driven emotional portrait acting.
<div id='section'>Paperid: <span id='pid'>583, <a href='https://arxiv.org/pdf/2508.02362.pdf' target='_blank'>https://arxiv.org/pdf/2508.02362.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xu Wang, Shengeng Tang, Fei Wang, Lechao Cheng, Dan Guo, Feng Xue, Richang Hong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02362">Text2Lip: Progressive Lip-Synced Talking Face Generation from Text via Viseme-Guided Rendering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating semantically coherent and visually accurate talking faces requires bridging the gap between linguistic meaning and facial articulation. Although audio-driven methods remain prevalent, their reliance on high-quality paired audio visual data and the inherent ambiguity in mapping acoustics to lip motion pose significant challenges in terms of scalability and robustness. To address these issues, we propose Text2Lip, a viseme-centric framework that constructs an interpretable phonetic-visual bridge by embedding textual input into structured viseme sequences. These mid-level units serve as a linguistically grounded prior for lip motion prediction. Furthermore, we design a progressive viseme-audio replacement strategy based on curriculum learning, enabling the model to gradually transition from real audio to pseudo-audio reconstructed from enhanced viseme features via cross-modal attention. This allows for robust generation in both audio-present and audio-free scenarios. Finally, a landmark-guided renderer synthesizes photorealistic facial videos with accurate lip synchronization. Extensive evaluations show that Text2Lip outperforms existing approaches in semantic fidelity, visual realism, and modality robustness, establishing a new paradigm for controllable and flexible talking face generation. Our project homepage is https://plyon1.github.io/Text2Lip/.
<div id='section'>Paperid: <span id='pid'>584, <a href='https://arxiv.org/pdf/2507.14915.pdf' target='_blank'>https://arxiv.org/pdf/2507.14915.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaojie Li, Ronghui Li, Shukai Fang, Shuzhao Xie, Xiaoyang Guo, Jiaqing Zhou, Junkun Peng, Zhi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.14915">Music-Aligned Holistic 3D Dance Generation via Hierarchical Motion Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Well-coordinated, music-aligned holistic dance enhances emotional expressiveness and audience engagement. However, generating such dances remains challenging due to the scarcity of holistic 3D dance datasets, the difficulty of achieving cross-modal alignment between music and dance, and the complexity of modeling interdependent motion across the body, hands, and face. To address these challenges, we introduce SoulDance, a high-precision music-dance paired dataset captured via professional motion capture systems, featuring meticulously annotated holistic dance movements. Building on this dataset, we propose SoulNet, a framework designed to generate music-aligned, kinematically coordinated holistic dance sequences. SoulNet consists of three principal components: (1) Hierarchical Residual Vector Quantization, which models complex, fine-grained motion dependencies across the body, hands, and face; (2) Music-Aligned Generative Model, which composes these hierarchical motion units into expressive and coordinated holistic dance; (3) Music-Motion Retrieval Module, a pre-trained cross-modal model that functions as a music-dance alignment prior, ensuring temporal synchronization and semantic coherence between generated dance and input music throughout the generation process. Extensive experiments demonstrate that SoulNet significantly surpasses existing approaches in generating high-quality, music-coordinated, and well-aligned holistic 3D dance sequences.
<div id='section'>Paperid: <span id='pid'>585, <a href='https://arxiv.org/pdf/2504.09885.pdf' target='_blank'>https://arxiv.org/pdf/2504.09885.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zihao Liu, Mingwen Ou, Zunnan Xu, Jiaqi Huang, Haonan Han, Ronghui Li, Xiu Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.09885">Separate to Collaborate: Dual-Stream Diffusion Model for Coordinated Piano Hand Motion Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automating the synthesis of coordinated bimanual piano performances poses significant challenges, particularly in capturing the intricate choreography between the hands while preserving their distinct kinematic signatures. In this paper, we propose a dual-stream neural framework designed to generate synchronized hand gestures for piano playing from audio input, addressing the critical challenge of modeling both hand independence and coordination. Our framework introduces two key innovations: (i) a decoupled diffusion-based generation framework that independently models each hand's motion via dual-noise initialization, sampling distinct latent noise for each while leveraging a shared positional condition, and (ii) a Hand-Coordinated Asymmetric Attention (HCAA) mechanism suppresses symmetric (common-mode) noise to highlight asymmetric hand-specific features, while adaptively enhancing inter-hand coordination during denoising. Comprehensive evaluations demonstrate that our framework outperforms existing state-of-the-art methods across multiple metrics. Our project is available at https://monkek123king.github.io/S2C_page/.
<div id='section'>Paperid: <span id='pid'>586, <a href='https://arxiv.org/pdf/2503.02587.pdf' target='_blank'>https://arxiv.org/pdf/2503.02587.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Piotr Koczy, Michael C. Welle, Danica Kragic
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.02587">Learning Dexterous In-Hand Manipulation with Multifingered Hands via Visuomotor Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a framework for learning dexterous in-hand manipulation with multifingered hands using visuomotor diffusion policies. Our system enables complex in-hand manipulation tasks, such as unscrewing a bottle lid with one hand, by leveraging a fast and responsive teleoperation setup for the four-fingered Allegro Hand. We collect high-quality expert demonstrations using an augmented reality (AR) interface that tracks hand movements and applies inverse kinematics and motion retargeting for precise control. The AR headset provides real-time visualization, while gesture controls streamline teleoperation. To enhance policy learning, we introduce a novel demonstration outlier removal approach based on HDBSCAN clustering and the Global-Local Outlier Score from Hierarchies (GLOSH) algorithm, effectively filtering out low-quality demonstrations that could degrade performance. We evaluate our approach extensively in real-world settings and provide all experimental videos on the project website: https://dex-manip.github.io/
<div id='section'>Paperid: <span id='pid'>587, <a href='https://arxiv.org/pdf/2502.17414.pdf' target='_blank'>https://arxiv.org/pdf/2502.17414.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zeyuan Chen, Hongyi Xu, Guoxian Song, You Xie, Chenxu Zhang, Xin Chen, Chao Wang, Di Chang, Linjie Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.17414">X-Dancer: Expressive Music to Human Dance Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present X-Dancer, a novel zero-shot music-driven image animation pipeline that creates diverse and long-range lifelike human dance videos from a single static image. As its core, we introduce a unified transformer-diffusion framework, featuring an autoregressive transformer model that synthesize extended and music-synchronized token sequences for 2D body, head and hands poses, which then guide a diffusion model to produce coherent and realistic dance video frames. Unlike traditional methods that primarily generate human motion in 3D, X-Dancer addresses data limitations and enhances scalability by modeling a wide spectrum of 2D dance motions, capturing their nuanced alignment with musical beats through readily available monocular videos. To achieve this, we first build a spatially compositional token representation from 2D human pose labels associated with keypoint confidences, encoding both large articulated body movements (e.g., upper and lower body) and fine-grained motions (e.g., head and hands). We then design a music-to-motion transformer model that autoregressively generates music-aligned dance pose token sequences, incorporating global attention to both musical style and prior motion context. Finally we leverage a diffusion backbone to animate the reference image with these synthesized pose tokens through AdaIN, forming a fully differentiable end-to-end framework. Experimental results demonstrate that X-Dancer is able to produce both diverse and characterized dance videos, substantially outperforming state-of-the-art methods in term of diversity, expressiveness and realism. Code and model will be available for research purposes.
<div id='section'>Paperid: <span id='pid'>588, <a href='https://arxiv.org/pdf/2502.01143.pdf' target='_blank'>https://arxiv.org/pdf/2502.01143.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tairan He, Jiawei Gao, Wenli Xiao, Yuanhang Zhang, Zi Wang, Jiashun Wang, Zhengyi Luo, Guanqi He, Nikhil Sobanbab, Chaoyi Pan, Zeji Yi, Guannan Qu, Kris Kitani, Jessica Hodgins, Linxi "Jim" Fan, Yuke Zhu, Changliu Liu, Guanya Shi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.01143">ASAP: Aligning Simulation and Real-World Physics for Learning Agile Humanoid Whole-Body Skills</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humanoid robots hold the potential for unparalleled versatility in performing human-like, whole-body skills. However, achieving agile and coordinated whole-body motions remains a significant challenge due to the dynamics mismatch between simulation and the real world. Existing approaches, such as system identification (SysID) and domain randomization (DR) methods, often rely on labor-intensive parameter tuning or result in overly conservative policies that sacrifice agility. In this paper, we present ASAP (Aligning Simulation and Real-World Physics), a two-stage framework designed to tackle the dynamics mismatch and enable agile humanoid whole-body skills. In the first stage, we pre-train motion tracking policies in simulation using retargeted human motion data. In the second stage, we deploy the policies in the real world and collect real-world data to train a delta (residual) action model that compensates for the dynamics mismatch. Then, ASAP fine-tunes pre-trained policies with the delta action model integrated into the simulator to align effectively with real-world dynamics. We evaluate ASAP across three transfer scenarios: IsaacGym to IsaacSim, IsaacGym to Genesis, and IsaacGym to the real-world Unitree G1 humanoid robot. Our approach significantly improves agility and whole-body coordination across various dynamic motions, reducing tracking error compared to SysID, DR, and delta dynamics learning baselines. ASAP enables highly agile motions that were previously difficult to achieve, demonstrating the potential of delta action learning in bridging simulation and real-world dynamics. These results suggest a promising sim-to-real direction for developing more expressive and agile humanoids.
<div id='section'>Paperid: <span id='pid'>589, <a href='https://arxiv.org/pdf/2412.17377.pdf' target='_blank'>https://arxiv.org/pdf/2412.17377.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Youliang Zhang, Ronghui Li, Yachao Zhang, Liang Pan, Jingbo Wang, Yebin Liu, Xiu Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.17377">A Plug-and-Play Physical Motion Restoration Approach for In-the-Wild High-Difficulty Motions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Extracting physically plausible 3D human motion from videos is a critical task. Although existing simulation-based motion imitation methods can enhance the physical quality of daily motions estimated from monocular video capture, extending this capability to high-difficulty motions remains an open challenge. This can be attributed to some flawed motion clips in video-based motion capture results and the inherent complexity in modeling high-difficulty motions. Therefore, sensing the advantage of segmentation in localizing human body, we introduce a mask-based motion correction module (MCM) that leverages motion context and video mask to repair flawed motions, producing imitation-friendly motions; and propose a physics-based motion transfer module (PTM), which employs a pretrain and adapt approach for motion imitation, improving physical plausibility with the ability to handle in-the-wild and challenging motions. Our approach is designed as a plug-and-play module to physically refine the video motion capture results, including high-difficulty in-the-wild motions. Finally, to validate our approach, we collected a challenging in-the-wild test set to establish a benchmark, and our method has demonstrated effectiveness on both the new benchmark and existing public datasets.https://physicalmotionrestoration.github.io
<div id='section'>Paperid: <span id='pid'>590, <a href='https://arxiv.org/pdf/2412.16982.pdf' target='_blank'>https://arxiv.org/pdf/2412.16982.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ronghui Li, Youliang Zhang, Yachao Zhang, Yuxiang Zhang, Mingyang Su, Jie Guo, Ziwei Liu, Yebin Liu, Xiu Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.16982">InterDance:Reactive 3D Dance Generation with Realistic Duet Interactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humans perform a variety of interactive motions, among which duet dance is one of the most challenging interactions. However, in terms of human motion generative models, existing works are still unable to generate high-quality interactive motions, especially in the field of duet dance. On the one hand, it is due to the lack of large-scale high-quality datasets. On the other hand, it arises from the incomplete representation of interactive motion and the lack of fine-grained optimization of interactions. To address these challenges, we propose, InterDance, a large-scale duet dance dataset that significantly enhances motion quality, data scale, and the variety of dance genres. Built upon this dataset, we propose a new motion representation that can accurately and comprehensively describe interactive motion. We further introduce a diffusion-based framework with an interaction refinement guidance strategy to optimize the realism of interactions progressively. Extensive experiments demonstrate the effectiveness of our dataset and algorithm.
<div id='section'>Paperid: <span id='pid'>591, <a href='https://arxiv.org/pdf/2411.18654.pdf' target='_blank'>https://arxiv.org/pdf/2411.18654.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haonan Han, Xiangzuo Wu, Huan Liao, Zunnan Xu, Zhongyuan Hu, Ronghui Li, Yachao Zhang, Xiu Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.18654">AToM: Aligning Text-to-Motion Model at Event-Level with GPT-4Vision Reward</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, text-to-motion models have opened new possibilities for creating realistic human motion with greater efficiency and flexibility. However, aligning motion generation with event-level textual descriptions presents unique challenges due to the complex relationship between textual prompts and desired motion outcomes. To address this, we introduce AToM, a framework that enhances the alignment between generated motion and text prompts by leveraging reward from GPT-4Vision. AToM comprises three main stages: Firstly, we construct a dataset MotionPrefer that pairs three types of event-level textual prompts with generated motions, which cover the integrity, temporal relationship and frequency of motion. Secondly, we design a paradigm that utilizes GPT-4Vision for detailed motion annotation, including visual data formatting, task-specific instructions and scoring rules for each sub-task. Finally, we fine-tune an existing text-to-motion model using reinforcement learning guided by this paradigm. Experimental results demonstrate that AToM significantly improves the event-level alignment quality of text-to-motion generation.
<div id='section'>Paperid: <span id='pid'>592, <a href='https://arxiv.org/pdf/2411.14951.pdf' target='_blank'>https://arxiv.org/pdf/2411.14951.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhuo Li, Mingshuang Luo, Ruibing Hou, Xin Zhao, Hao Liu, Hong Chang, Zimo Liu, Chen Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.14951">Morph: A Motion-free Physics Optimization Framework for Human Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion generation has been widely studied due to its crucial role in areas such as digital humans and humanoid robot control. However, many current motion generation approaches disregard physics constraints, frequently resulting in physically implausible motions with pronounced artifacts such as floating and foot sliding. Meanwhile, training an effective motion physics optimizer with noisy motion data remains largely unexplored. In this paper, we propose \textbf{Morph}, a \textbf{Mo}tion-F\textbf{r}ee \textbf{ph}ysics optimization framework, consisting of a Motion Generator and a Motion Physics Refinement module, for enhancing physical plausibility without relying on expensive real-world motion data. Specifically, the motion generator is responsible for providing large-scale synthetic, noisy motion data, while the motion physics refinement module utilizes these synthetic data to learn a motion imitator within a physics simulator, enforcing physical constraints to project the noisy motions into a physically-plausible space. Additionally, we introduce a prior reward module to enhance the stability of the physics optimization process and generate smoother and more stable motions. These physically refined motions are then used to fine-tune the motion generator, further enhancing its capability. This collaborative training paradigm enables mutual enhancement between the motion generator and the motion physics refinement module, significantly improving practicality and robustness in real-world applications. Experiments on both text-to-motion and music-to-dance generation tasks demonstrate that our framework achieves state-of-the-art motion quality while improving physical plausibility drastically.
<div id='section'>Paperid: <span id='pid'>593, <a href='https://arxiv.org/pdf/2410.24037.pdf' target='_blank'>https://arxiv.org/pdf/2410.24037.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sunjae Yoon, Gwanhyeong Koo, Younghwan Lee, Chang D. Yoo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.24037">TPC: Test-time Procrustes Calibration for Diffusion-based Human Image Animation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human image animation aims to generate a human motion video from the inputs of a reference human image and a target motion video. Current diffusion-based image animation systems exhibit high precision in transferring human identity into targeted motion, yet they still exhibit irregular quality in their outputs. Their optimal precision is achieved only when the physical compositions (i.e., scale and rotation) of the human shapes in the reference image and target pose frame are aligned. In the absence of such alignment, there is a noticeable decline in fidelity and consistency. Especially, in real-world environments, this compositional misalignment commonly occurs, posing significant challenges to the practical usage of current systems. To this end, we propose Test-time Procrustes Calibration (TPC), which enhances the robustness of diffusion-based image animation systems by maintaining optimal performance even when faced with compositional misalignment, effectively addressing real-world scenarios. The TPC provides a calibrated reference image for the diffusion model, enhancing its capability to understand the correspondence between human shapes in the reference and target images. Our method is simple and can be applied to any diffusion-based image animation system in a model-agnostic manner, improving the effectiveness at test time without additional training.
<div id='section'>Paperid: <span id='pid'>594, <a href='https://arxiv.org/pdf/2406.17333.pdf' target='_blank'>https://arxiv.org/pdf/2406.17333.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mike Allenspach, Michael Pantic, Rik Girod, Lionel Ott, Roland Siegwart
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.17333">Task Adaptation in Industrial Human-Robot Interaction: Leveraging Riemannian Motion Policies</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In real-world industrial environments, modern robots often rely on human operators for crucial decision-making and mission synthesis from individual tasks. Effective and safe collaboration between humans and robots requires systems that can adjust their motion based on human intentions, enabling dynamic task planning and adaptation. Addressing the needs of industrial applications, we propose a motion control framework that (i) removes the need for manual control of the robot's movement; (ii) facilitates the formulation and combination of complex tasks; and (iii) allows the seamless integration of human intent recognition and robot motion planning. For this purpose, we leverage a modular and purely reactive approach for task parametrization and motion generation, embodied by Riemannian Motion Policies. The effectiveness of our method is demonstrated, evaluated, and compared to \remove{state-of-the-art approaches}\add{a representative state-of-the-art approach} in experimental scenarios inspired by realistic industrial Human-Robot Interaction settings.
<div id='section'>Paperid: <span id='pid'>595, <a href='https://arxiv.org/pdf/2406.08858.pdf' target='_blank'>https://arxiv.org/pdf/2406.08858.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tairan He, Zhengyi Luo, Xialin He, Wenli Xiao, Chong Zhang, Weinan Zhang, Kris Kitani, Changliu Liu, Guanya Shi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.08858">OmniH2O: Universal and Dexterous Human-to-Humanoid Whole-Body Teleoperation and Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present OmniH2O (Omni Human-to-Humanoid), a learning-based system for whole-body humanoid teleoperation and autonomy. Using kinematic pose as a universal control interface, OmniH2O enables various ways for a human to control a full-sized humanoid with dexterous hands, including using real-time teleoperation through VR headset, verbal instruction, and RGB camera. OmniH2O also enables full autonomy by learning from teleoperated demonstrations or integrating with frontier models such as GPT-4. OmniH2O demonstrates versatility and dexterity in various real-world whole-body tasks through teleoperation or autonomy, such as playing multiple sports, moving and manipulating objects, and interacting with humans. We develop an RL-based sim-to-real pipeline, which involves large-scale retargeting and augmentation of human motion datasets, learning a real-world deployable policy with sparse sensor input by imitating a privileged teacher policy, and reward designs to enhance robustness and stability. We release the first humanoid whole-body control dataset, OmniH2O-6, containing six everyday tasks, and demonstrate humanoid whole-body skill learning from teleoperated datasets.
<div id='section'>Paperid: <span id='pid'>596, <a href='https://arxiv.org/pdf/2403.15931.pdf' target='_blank'>https://arxiv.org/pdf/2403.15931.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>You Xie, Hongyi Xu, Guoxian Song, Chao Wang, Yichun Shi, Linjie Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.15931">X-Portrait: Expressive Portrait Animation with Hierarchical Motion Attention</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose X-Portrait, an innovative conditional diffusion model tailored for generating expressive and temporally coherent portrait animation. Specifically, given a single portrait as appearance reference, we aim to animate it with motion derived from a driving video, capturing both highly dynamic and subtle facial expressions along with wide-range head movements. As its core, we leverage the generative prior of a pre-trained diffusion model as the rendering backbone, while achieve fine-grained head pose and expression control with novel controlling signals within the framework of ControlNet. In contrast to conventional coarse explicit controls such as facial landmarks, our motion control module is learned to interpret the dynamics directly from the original driving RGB inputs. The motion accuracy is further enhanced with a patch-based local control module that effectively enhance the motion attention to small-scale nuances like eyeball positions. Notably, to mitigate the identity leakage from the driving signals, we train our motion control modules with scaling-augmented cross-identity images, ensuring maximized disentanglement from the appearance reference modules. Experimental results demonstrate the universal effectiveness of X-Portrait across a diverse range of facial portraits and expressive driving sequences, and showcase its proficiency in generating captivating portrait animations with consistently maintained identity characteristics.
<div id='section'>Paperid: <span id='pid'>597, <a href='https://arxiv.org/pdf/2311.18168.pdf' target='_blank'>https://arxiv.org/pdf/2311.18168.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Karren D. Yang, Anurag Ranjan, Jen-Hao Rick Chang, Raviteja Vemulapalli, Oncel Tuzel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.18168">Probabilistic Speech-Driven 3D Facial Motion Synthesis: New Benchmarks, Methods, and Applications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We consider the task of animating 3D facial geometry from speech signal. Existing works are primarily deterministic, focusing on learning a one-to-one mapping from speech signal to 3D face meshes on small datasets with limited speakers. While these models can achieve high-quality lip articulation for speakers in the training set, they are unable to capture the full and diverse distribution of 3D facial motions that accompany speech in the real world. Importantly, the relationship between speech and facial motion is one-to-many, containing both inter-speaker and intra-speaker variations and necessitating a probabilistic approach. In this paper, we identify and address key challenges that have so far limited the development of probabilistic models: lack of datasets and metrics that are suitable for training and evaluating them, as well as the difficulty of designing a model that generates diverse results while remaining faithful to a strong conditioning signal as speech. We first propose large-scale benchmark datasets and metrics suitable for probabilistic modeling. Then, we demonstrate a probabilistic model that achieves both diversity and fidelity to speech, outperforming other methods across the proposed benchmarks. Finally, we showcase useful applications of probabilistic models trained on these large-scale datasets: we can generate diverse speech-driven 3D facial motion that matches unseen speaker styles extracted from reference clips; and our synthetic meshes can be used to improve the performance of downstream audio-visual models.
<div id='section'>Paperid: <span id='pid'>598, <a href='https://arxiv.org/pdf/2310.12432.pdf' target='_blank'>https://arxiv.org/pdf/2310.12432.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Linrui Zhang, Zhenghao Peng, Quanyi Li, Bolei Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.12432">CAT: Closed-loop Adversarial Training for Safe End-to-End Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Driving safety is a top priority for autonomous vehicles. Orthogonal to prior work handling accident-prone traffic events by algorithm designs at the policy level, we investigate a Closed-loop Adversarial Training (CAT) framework for safe end-to-end driving in this paper through the lens of environment augmentation. CAT aims to continuously improve the safety of driving agents by training the agent on safety-critical scenarios that are dynamically generated over time. A novel resampling technique is developed to turn log-replay real-world driving scenarios into safety-critical ones via probabilistic factorization, where the adversarial traffic generation is modeled as the multiplication of standard motion prediction sub-problems. Consequently, CAT can launch more efficient physical attacks compared to existing safety-critical scenario generation methods and yields a significantly less computational cost in the iterative learning pipeline. We incorporate CAT into the MetaDrive simulator and validate our approach on hundreds of driving scenarios imported from real-world driving datasets. Experimental results demonstrate that CAT can effectively generate adversarial scenarios countering the agent being trained. After training, the agent can achieve superior driving safety in both log-replay and safety-critical traffic scenarios on the held-out test set. Code and data are available at https://metadriverse.github.io/cat.
<div id='section'>Paperid: <span id='pid'>599, <a href='https://arxiv.org/pdf/2308.04828.pdf' target='_blank'>https://arxiv.org/pdf/2308.04828.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiang Wang, Junlong Du, Ke Yan, Shouhong Ding
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.04828">Seeing in Flowing: Adapting CLIP for Action Recognition with Motion Prompts Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Contrastive Language-Image Pre-training (CLIP) has recently shown remarkable generalization on "zero-shot" training and has applied to many downstream tasks. We explore the adaptation of CLIP to achieve a more efficient and generalized action recognition method. We propose that the key lies in explicitly modeling the motion cues flowing in video frames. To that end, we design a two-stream motion modeling block to capture motion and spatial information at the same time. And then, the obtained motion cues are utilized to drive a dynamic prompts learner to generate motion-aware prompts, which contain much semantic information concerning human actions. In addition, we propose a multimodal communication block to achieve a collaborative learning and further improve the performance. We conduct extensive experiments on HMDB-51, UCF-101, and Kinetics-400 datasets. Our method outperforms most existing state-of-the-art methods by a significant margin on "few-shot" and "zero-shot" training. We also achieve competitive performance on "closed-set" training with extremely few trainable parameters and additional computational costs.
<div id='section'>Paperid: <span id='pid'>600, <a href='https://arxiv.org/pdf/2303.03767.pdf' target='_blank'>https://arxiv.org/pdf/2303.03767.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hai Ci, Mickel Liu, Xuehai Pan, Fangwei Zhong, Yizhou Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.03767">Proactive Multi-Camera Collaboration For 3D Human Pose Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a multi-agent reinforcement learning (MARL) scheme for proactive Multi-Camera Collaboration in 3D Human Pose Estimation in dynamic human crowds. Traditional fixed-viewpoint multi-camera solutions for human motion capture (MoCap) are limited in capture space and susceptible to dynamic occlusions. Active camera approaches proactively control camera poses to find optimal viewpoints for 3D reconstruction. However, current methods still face challenges with credit assignment and environment dynamics. To address these issues, our proposed method introduces a novel Collaborative Triangulation Contribution Reward (CTCR) that improves convergence and alleviates multi-agent credit assignment issues resulting from using 3D reconstruction accuracy as the shared reward. Additionally, we jointly train our model with multiple world dynamics learning tasks to better capture environment dynamics and encourage anticipatory behaviors for occlusion avoidance. We evaluate our proposed method in four photo-realistic UE4 environments to ensure validity and generalizability. Empirical results show that our method outperforms fixed and active baselines in various scenarios with different numbers of cameras and humans.
<div id='section'>Paperid: <span id='pid'>601, <a href='https://arxiv.org/pdf/2302.09656.pdf' target='_blank'>https://arxiv.org/pdf/2302.09656.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Michele Caprio, Souradeep Dutta, Kuk Jin Jang, Vivian Lin, Radoslav Ivanov, Oleg Sokolsky, Insup Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.09656">Credal Bayesian Deep Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Uncertainty quantification and robustness to distribution shifts are important goals in machine learning and artificial intelligence. Although Bayesian Neural Networks (BNNs) allow for uncertainty in the predictions to be assessed, different sources of predictive uncertainty cannot be distinguished properly. We present Credal Bayesian Deep Learning (CBDL). Heuristically, CBDL allows to train an (uncountably) infinite ensemble of BNNs, using only finitely many elements. This is possible thanks to prior and likelihood finitely generated credal sets (FGCSs), a concept from the imprecise probability literature. Intuitively, convex combinations of a finite collection of prior-likelihood pairs are able to represent infinitely many such pairs. After training, CBDL outputs a set of posteriors on the parameters of the neural network. At inference time, such posterior set is used to derive a set of predictive distributions that is in turn utilized to distinguish between (predictive) aleatoric and epistemic uncertainties, and to quantify them. The predictive set also produces either (i) a collection of outputs enjoying desirable probabilistic guarantees, or (ii) the single output that is deemed the best, that is, the one having the highest predictive lower probability -- another imprecise-probabilistic concept. CBDL is more robust than single BNNs to prior and likelihood misspecification, and to distribution shift. We show that CBDL is better at quantifying and disentangling different types of (predictive) uncertainties than single BNNs and ensemble of BNNs. In addition, we apply CBDL to two case studies to demonstrate its downstream tasks capabilities: one, for motion prediction in autonomous driving scenarios, and two, to model blood glucose and insulin dynamics for artificial pancreas control. We show that CBDL performs better when compared to an ensemble of BNNs baseline.
<div id='section'>Paperid: <span id='pid'>602, <a href='https://arxiv.org/pdf/2207.05053.pdf' target='_blank'>https://arxiv.org/pdf/2207.05053.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianglong Ye, Jiashun Wang, Binghao Huang, Yuzhe Qin, Xiaolong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2207.05053">Learning Continuous Grasping Function with a Dexterous Hand from Human Demonstrations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose to learn to generate grasping motion for manipulation with a dexterous hand using implicit functions. With continuous time inputs, the model can generate a continuous and smooth grasping plan. We name the proposed model Continuous Grasping Function (CGF). CGF is learned via generative modeling with a Conditional Variational Autoencoder using 3D human demonstrations. We will first convert the large-scale human-object interaction trajectories to robot demonstrations via motion retargeting, and then use these demonstrations to train CGF. During inference, we perform sampling with CGF to generate different grasping plans in the simulator and select the successful ones to transfer to the real robot. By training on diverse human data, our CGF allows generalization to manipulate multiple objects. Compared to previous planning algorithms, CGF is more efficient and achieves significant improvement on success rate when transferred to grasping with the real Allegro Hand. Our project page is available at https://jianglongye.com/cgf .
<div id='section'>Paperid: <span id='pid'>603, <a href='https://arxiv.org/pdf/2509.25704.pdf' target='_blank'>https://arxiv.org/pdf/2509.25704.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cheng Guo, Giuseppe L'Erario, Giulio Romualdi, Mattia Leonori, Marta Lorenzini, Arash Ajoudani, Daniele Pucci
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25704">Physics-Informed Learning for Human Whole-Body Kinematics Prediction via Sparse IMUs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate and physically feasible human motion prediction is crucial for safe and seamless human-robot collaboration. While recent advancements in human motion capture enable real-time pose estimation, the practical value of many existing approaches is limited by the lack of future predictions and consideration of physical constraints. Conventional motion prediction schemes rely heavily on past poses, which are not always available in real-world scenarios. To address these limitations, we present a physics-informed learning framework that integrates domain knowledge into both training and inference to predict human motion using inertial measurements from only 5 IMUs. We propose a network that accounts for the spatial characteristics of human movements. During training, we incorporate forward and differential kinematics functions as additional loss components to regularize the learned joint predictions. At the inference stage, we refine the prediction from the previous iteration to update a joint state buffer, which is used as extra inputs to the network. Experimental results demonstrate that our approach achieves high accuracy, smooth transitions between motions, and generalizes well to unseen subjects
<div id='section'>Paperid: <span id='pid'>604, <a href='https://arxiv.org/pdf/2509.05337.pdf' target='_blank'>https://arxiv.org/pdf/2509.05337.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Younggeol Cho, Gokhan Solak, Olivia Nocentini, Marta Lorenzini, Andrea Fortuna, Arash Ajoudani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.05337">Anticipatory Fall Detection in Humans with Hybrid Directed Graph Neural Networks and Long Short-Term Memory</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Detecting and preventing falls in humans is a critical component of assistive robotic systems. While significant progress has been made in detecting falls, the prediction of falls before they happen, and analysis of the transient state between stability and an impending fall remain unexplored. In this paper, we propose a anticipatory fall detection method that utilizes a hybrid model combining Dynamic Graph Neural Networks (DGNN) with Long Short-Term Memory (LSTM) networks that decoupled the motion prediction and gait classification tasks to anticipate falls with high accuracy. Our approach employs real-time skeletal features extracted from video sequences as input for the proposed model. The DGNN acts as a classifier, distinguishing between three gait states: stable, transient, and fall. The LSTM-based network then predicts human movement in subsequent time steps, enabling early detection of falls. The proposed model was trained and validated using the OUMVLP-Pose and URFD datasets, demonstrating superior performance in terms of prediction error and recognition accuracy compared to models relying solely on DGNN and models from literature. The results indicate that decoupling prediction and classification improves performance compared to addressing the unified problem using only the DGNN. Furthermore, our method allows for the monitoring of the transient state, offering valuable insights that could enhance the functionality of advanced assistance systems.
<div id='section'>Paperid: <span id='pid'>605, <a href='https://arxiv.org/pdf/2508.21043.pdf' target='_blank'>https://arxiv.org/pdf/2508.21043.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhi Su, Bike Zhang, Nima Rahmanian, Yuman Gao, Qiayuan Liao, Caitlin Regan, Koushil Sreenath, S. Shankar Sastry
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.21043">HITTER: A HumanoId Table TEnnis Robot via Hierarchical Planning and Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humanoid robots have recently achieved impressive progress in locomotion and whole-body control, yet they remain constrained in tasks that demand rapid interaction with dynamic environments through manipulation. Table tennis exemplifies such a challenge: with ball speeds exceeding 5 m/s, players must perceive, predict, and act within sub-second reaction times, requiring both agility and precision. To address this, we present a hierarchical framework for humanoid table tennis that integrates a model-based planner for ball trajectory prediction and racket target planning with a reinforcement learning-based whole-body controller. The planner determines striking position, velocity and timing, while the controller generates coordinated arm and leg motions that mimic human strikes and maintain stability and agility across consecutive rallies. Moreover, to encourage natural movements, human motion references are incorporated during training. We validate our system on a general-purpose humanoid robot, achieving up to 106 consecutive shots with a human opponent and sustained exchanges against another humanoid. These results demonstrate real-world humanoid table tennis with sub-second reactive control, marking a step toward agile and interactive humanoid behaviors.
<div id='section'>Paperid: <span id='pid'>606, <a href='https://arxiv.org/pdf/2508.08241.pdf' target='_blank'>https://arxiv.org/pdf/2508.08241.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiayuan Liao, Takara E. Truong, Xiaoyu Huang, Guy Tevet, Koushil Sreenath, C. Karen Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08241">BeyondMimic: From Motion Tracking to Versatile Humanoid Control via Guided Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning skills from human motions offers a promising path toward generalizable policies for versatile humanoid whole-body control, yet two key cornerstones are missing: (1) a high-quality motion tracking framework that faithfully transforms large-scale kinematic references into robust and extremely dynamic motions on real hardware, and (2) a distillation approach that can effectively learn these motion primitives and compose them to solve downstream tasks. We address these gaps with BeyondMimic, a real-world framework to learn from human motions for versatile and naturalistic humanoid control via guided diffusion. Our framework provides a motion tracking pipeline capable of challenging skills such as jumping spins, sprinting, and cartwheels with state-of-the-art motion quality. Moving beyond simply mimicking existing motions, we further introduce a unified diffusion policy that enables zero-shot task-specific control at test time using simple cost functions. Deployed on hardware, BeyondMimic performs diverse tasks at test time, including waypoint navigation, joystick teleoperation, and obstacle avoidance, bridging sim-to-real motion tracking and flexible synthesis of human motion primitives for whole-body control. https://beyondmimic.github.io/.
<div id='section'>Paperid: <span id='pid'>607, <a href='https://arxiv.org/pdf/2507.16841.pdf' target='_blank'>https://arxiv.org/pdf/2507.16841.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Waseem Akram, Muhayy Ud Din, Abdelhaleem Saad, Irfan Hussain
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.16841">AquaChat: An LLM-Guided ROV Framework for Adaptive Inspection of Aquaculture Net Pens</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Inspection of aquaculture net pens is essential for maintaining the structural integrity, biosecurity, and operational efficiency of fish farming systems. Traditional inspection approaches rely on pre-programmed missions or manual control, offering limited adaptability to dynamic underwater conditions and user-specific demands. In this study, we propose AquaChat, a novel Remotely Operated Vehicle (ROV) framework that integrates Large Language Models (LLMs) for intelligent and adaptive net pen inspection. The system features a multi-layered architecture: (1) a high-level planning layer that interprets natural language user commands using an LLM to generate symbolic task plans; (2) a mid-level task manager that translates plans into ROV control sequences; and (3) a low-level motion control layer that executes navigation and inspection tasks with precision. Real-time feedback and event-triggered replanning enhance robustness in challenging aquaculture environments. The framework is validated through experiments in both simulated and controlled aquatic environments representative of aquaculture net pens. Results demonstrate improved task flexibility, inspection accuracy, and operational efficiency. AquaChat illustrates the potential of integrating language-based AI with marine robotics to enable intelligent, user-interactive inspection systems for sustainable aquaculture operations.
<div id='section'>Paperid: <span id='pid'>608, <a href='https://arxiv.org/pdf/2507.12463.pdf' target='_blank'>https://arxiv.org/pdf/2507.12463.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Renjie Li, Ruijie Ye, Mingyang Wu, Hao Frank Yang, Zhiwen Fan, Hezhen Hu, Zhengzhong Tu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.12463">MMHU: A Massive-Scale Multimodal Benchmark for Human Behavior Understanding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humans are integral components of the transportation ecosystem, and understanding their behaviors is crucial to facilitating the development of safe driving systems. Although recent progress has explored various aspects of human behavior$\unicode{x2014}$such as motion, trajectories, and intention$\unicode{x2014}$a comprehensive benchmark for evaluating human behavior understanding in autonomous driving remains unavailable. In this work, we propose $\textbf{MMHU}$, a large-scale benchmark for human behavior analysis featuring rich annotations, such as human motion and trajectories, text description for human motions, human intention, and critical behavior labels relevant to driving safety. Our dataset encompasses 57k human motion clips and 1.73M frames gathered from diverse sources, including established driving datasets such as Waymo, in-the-wild videos from YouTube, and self-collected data. A human-in-the-loop annotation pipeline is developed to generate rich behavior captions. We provide a thorough dataset analysis and benchmark multiple tasks$\unicode{x2014}$ranging from motion prediction to motion generation and human behavior question answering$\unicode{x2014}$thereby offering a broad evaluation suite. Project page : https://MMHU-Benchmark.github.io.
<div id='section'>Paperid: <span id='pid'>609, <a href='https://arxiv.org/pdf/2506.24086.pdf' target='_blank'>https://arxiv.org/pdf/2506.24086.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bingfan Zhu, Biao Jiang, Sunyi Wang, Shixiang Tang, Tao Chen, Linjie Luo, Youyi Zheng, Xin Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.24086">MotionGPT3: Human Motion as a Second Modality</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Though recent advances in multimodal models have demonstrated strong capabilities and opportunities in unified understanding and generation, the development of unified motion-language models remains underexplored. To enable such models with high-fidelity human motion, two core challenges must be addressed. The first is the reconstruction gap between the continuous motion modality and discrete representation in an autoregressive manner, and the second is the degradation of language intelligence during unified training. Inspired by the mixture of experts, we propose MotionGPT3, a bimodal motion-language model that treats human motion as a second modality, decoupling motion modeling via separate model parameters and enabling both effective cross-modal interaction and efficient multimodal scaling training. To preserve language intelligence, the text branch retains the original structure and parameters of the pretrained language model, while a new motion branch is integrated via a shared attention mechanism, enabling bidirectional information flow between two modalities. We first employ a motion Variational Autoencoder (VAE) to encode raw human motion into latent representations. Based on this continuous latent space, the motion branch predicts motion latents directly from intermediate hidden states using a diffusion head, bypassing discrete tokenization. Extensive experiments show that our approach achieves competitive performance on both motion understanding and generation tasks while preserving strong language capabilities, establishing a unified bimodal motion diffusion framework within an autoregressive manner.
<div id='section'>Paperid: <span id='pid'>610, <a href='https://arxiv.org/pdf/2506.14305.pdf' target='_blank'>https://arxiv.org/pdf/2506.14305.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhirui Sun, Xingrong Diao, Yao Wang, Bi-Ke Zhu, Jiankun Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.14305">Socially Aware Robot Crowd Navigation via Online Uncertainty-Driven Risk Adaptation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Navigation in human-robot shared crowded environments remains challenging, as robots are expected to move efficiently while respecting human motion conventions. However, many existing approaches emphasize safety or efficiency while overlooking social awareness. This article proposes Learning-Risk Model Predictive Control (LR-MPC), a data-driven navigation algorithm that balances efficiency, safety, and social awareness. LR-MPC consists of two phases: an offline risk learning phase, where a Probabilistic Ensemble Neural Network (PENN) is trained using risk data from a heuristic MPC-based baseline (HR-MPC), and an online adaptive inference phase, where local waypoints are sampled and globally guided by a Multi-RRT planner. Each candidate waypoint is evaluated for risk by PENN, and predictions are filtered using epistemic and aleatoric uncertainty to ensure robust decision-making. The safest waypoint is selected as the MPC input for real-time navigation. Extensive experiments demonstrate that LR-MPC outperforms baseline methods in success rate and social awareness, enabling robots to navigate complex crowds with high adaptability and low disruption. A website about this work is available at https://sites.google.com/view/lr-mpc.
<div id='section'>Paperid: <span id='pid'>611, <a href='https://arxiv.org/pdf/2504.09862.pdf' target='_blank'>https://arxiv.org/pdf/2504.09862.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zengyuan Lai, Jiarui Yang, Songpengcheng Xia, Lizhou Lin, Lan Sun, Renwen Wang, Jianran Liu, Qi Wu, Ling Pei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.09862">RadarLLM: Empowering Large Language Models to Understand Human Motion from Millimeter-wave Point Cloud Sequence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Millimeter-wave radar provides a privacy-preserving solution for human motion analysis, yet its sparse point clouds pose significant challenges for semantic understanding. We present Radar-LLM, the first framework that leverages large language models (LLMs) for human motion understanding using millimeter-wave radar as the sensing modality. Our approach introduces two key innovations: (1) a motion-guided radar tokenizer based on our Aggregate VQ-VAE architecture that incorporates deformable body templates and masked trajectory modeling to encode spatiotemporal point clouds into compact semantic tokens, and (2) a radar-aware language model that establishes cross-modal alignment between radar and text in a shared embedding space. To address data scarcity, we introduce a physics-aware synthesis pipeline that generates realistic radar-text pairs from motion-text datasets. Extensive experiments demonstrate that Radar-LLM achieves state-of-the-art performance across both synthetic and real-world benchmarks, enabling accurate translation of millimeter-wave signals to natural language descriptions. This breakthrough facilitates comprehensive motion understanding in privacy-sensitive applications like healthcare and smart homes. We will release the full implementation to support further research on https://inowlzy.github.io/RadarLLM/.
<div id='section'>Paperid: <span id='pid'>612, <a href='https://arxiv.org/pdf/2504.01275.pdf' target='_blank'>https://arxiv.org/pdf/2504.01275.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Subhradip Chakraborty, Shay Snyder, Md Abdullah-Al Kaiser, Maryam Parsa, Gregory Schwartz, Akhilesh R. Jaiswal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.01275">A Retina-Inspired Pathway to Real-Time Motion Prediction inside Image Sensors for Extreme-Edge Intelligence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The ability to predict motion in real time is fundamental to many maneuvering activities in animals, particularly those critical for survival, such as attack and escape responses. Given its significance, it is no surprise that motion prediction in animals begins in the retina. Similarly, autonomous systems utilizing computer vision could greatly benefit from the capability to predict motion in real time. Therefore, for computer vision applications, motion prediction should be integrated directly at the camera pixel level. Towards that end, we present a retina-inspired neuromorphic framework capable of performing real-time, energy-efficient MP directly within camera pixels. Our hardware-algorithm framework, implemented using GlobalFoundries 22nm FDSOI technology, integrates key retinal MP compute blocks, including a biphasic filter, spike adder, nonlinear circuit, and a 2D array for multi-directional motion prediction. Additionally, integrating the sensor and MP compute die using a 3D Cu-Cu hybrid bonding approach improves design compactness by minimizing area usage and simplifying routing complexity. Validated on real-world object stimuli, the model delivers efficient, low-latency MP for decision-making scenarios reliant on predictive visual computation, while consuming only 18.56 pJ/MP in our mixed-signal hardware implementation.
<div id='section'>Paperid: <span id='pid'>613, <a href='https://arxiv.org/pdf/2504.01204.pdf' target='_blank'>https://arxiv.org/pdf/2504.01204.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuan Li, Qianli Ma, Tsung-Yi Lin, Yongxin Chen, Chenfanfu Jiang, Ming-Yu Liu, Donglai Xiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.01204">Articulated Kinematics Distillation from Video Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present Articulated Kinematics Distillation (AKD), a framework for generating high-fidelity character animations by merging the strengths of skeleton-based animation and modern generative models. AKD uses a skeleton-based representation for rigged 3D assets, drastically reducing the Degrees of Freedom (DoFs) by focusing on joint-level control, which allows for efficient, consistent motion synthesis. Through Score Distillation Sampling (SDS) with pre-trained video diffusion models, AKD distills complex, articulated motions while maintaining structural integrity, overcoming challenges faced by 4D neural deformation fields in preserving shape consistency. This approach is naturally compatible with physics-based simulation, ensuring physically plausible interactions. Experiments show that AKD achieves superior 3D consistency and motion quality compared with existing works on text-to-4D generation. Project page: https://research.nvidia.com/labs/dir/akd/
<div id='section'>Paperid: <span id='pid'>614, <a href='https://arxiv.org/pdf/2503.17544.pdf' target='_blank'>https://arxiv.org/pdf/2503.17544.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yan Zhang, Yao Feng, AlpÃ¡r Cseke, Nitin Saini, Nathan Bajandas, Nicolas Heron, Michael J. Black
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.17544">PRIMAL: Physically Reactive and Interactive Motor Model for Avatar Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We formulate the motor system of an interactive avatar as a generative motion model that can drive the body to move through 3D space in a perpetual, realistic, controllable, and responsive manner. Although human motion generation has been extensively studied, many existing methods lack the responsiveness and realism of real human movements. Inspired by recent advances in foundation models, we propose PRIMAL, which is learned with a two-stage paradigm. In the pretraining stage, the model learns body movements from a large number of sub-second motion segments, providing a generative foundation from which more complex motions are built. This training is fully unsupervised without annotations. Given a single-frame initial state during inference, the pretrained model not only generates unbounded, realistic, and controllable motion, but also enables the avatar to be responsive to induced impulses in real time. In the adaptation phase, we employ a novel ControlNet-like adaptor to fine-tune the base model efficiently, adapting it to new tasks such as few-shot personalized action generation and spatial target reaching. Evaluations show that our proposed method outperforms state-of-the-art baselines. We leverage the model to create a real-time character animation system in Unreal Engine that feels highly responsive and natural. Code, models, and more results are available at: https://yz-cnsdqz.github.io/eigenmotion/PRIMAL
<div id='section'>Paperid: <span id='pid'>615, <a href='https://arxiv.org/pdf/2502.17822.pdf' target='_blank'>https://arxiv.org/pdf/2502.17822.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Peng Zhang, Xin Li, Xin Lin, Liang He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.17822">Easy-Poly: A Easy Polyhedral Framework For 3D Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in 3D multi-object tracking (3D MOT) have predominantly relied on tracking-by-detection pipelines. However, these approaches often neglect potential enhancements in 3D detection processes, leading to high false positives (FP), missed detections (FN), and identity switches (IDS), particularly in challenging scenarios such as crowded scenes, small-object configurations, and adverse weather conditions. Furthermore, limitations in data preprocessing, association mechanisms, motion modeling, and life-cycle management hinder overall tracking robustness. To address these issues, we present Easy-Poly, a real-time, filter-based 3D MOT framework for multiple object categories. Our contributions include: (1) An Augmented Proposal Generator utilizing multi-modal data augmentation and refined SpConv operations, significantly improving mAP and NDS on nuScenes; (2) A Dynamic Track-Oriented (DTO) data association algorithm that effectively manages uncertainties and occlusions through optimal assignment and multiple hypothesis handling; (3) A Dynamic Motion Modeling (DMM) incorporating a confidence-weighted Kalman filter and adaptive noise covariances, enhancing MOTA and AMOTA in challenging conditions; and (4) An extended life-cycle management system with adjustive thresholds to reduce ID switches and false terminations. Experimental results show that Easy-Poly outperforms state-of-the-art methods such as Poly-MOT and Fast-Poly, achieving notable gains in mAP (e.g., from 63.30% to 64.96% with LargeKernel3D) and AMOTA (e.g., from 73.1% to 74.5%), while also running in real-time. These findings highlight Easy-Poly's adaptability and robustness in diverse scenarios, making it a compelling choice for autonomous driving and related 3D MOT applications. The source code of this paper will be published upon acceptance.
<div id='section'>Paperid: <span id='pid'>616, <a href='https://arxiv.org/pdf/2412.10235.pdf' target='_blank'>https://arxiv.org/pdf/2412.10235.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Songpengcheng Xia, Yu Zhang, Zhuo Su, Xiaozheng Zheng, Zheng Lv, Guidong Wang, Yongjie Zhang, Qi Wu, Lei Chu, Ling Pei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.10235">EnvPoser: Environment-aware Realistic Human Motion Estimation from Sparse Observations with Uncertainty Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Estimating full-body motion using the tracking signals of head and hands from VR devices holds great potential for various applications. However, the sparsity and unique distribution of observations present a significant challenge, resulting in an ill-posed problem with multiple feasible solutions (i.e., hypotheses). This amplifies uncertainty and ambiguity in full-body motion estimation, especially for the lower-body joints. Therefore, we propose a new method, EnvPoser, that employs a two-stage framework to perform full-body motion estimation using sparse tracking signals and pre-scanned environment from VR devices. EnvPoser models the multi-hypothesis nature of human motion through an uncertainty-aware estimation module in the first stage. In the second stage, we refine these multi-hypothesis estimates by integrating semantic and geometric environmental constraints, ensuring that the final motion estimation aligns realistically with both the environmental context and physical interactions. Qualitative and quantitative experiments on two public datasets demonstrate that our method achieves state-of-the-art performance, highlighting significant improvements in human motion estimation within motion-environment interaction scenarios.
<div id='section'>Paperid: <span id='pid'>617, <a href='https://arxiv.org/pdf/2412.07493.pdf' target='_blank'>https://arxiv.org/pdf/2412.07493.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Muhayy Ud Din, Jan Rosell, Waseem Akram, Isiah Zaplana, Maximo A Roa, Irfan Hussain
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.07493">Onto-LLM-TAMP: Knowledge-oriented Task and Motion Planning using Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Performing complex manipulation tasks in dynamic environments requires efficient Task and Motion Planning (TAMP) approaches that combine high-level symbolic plans with low-level motion control. Advances in Large Language Models (LLMs), such as GPT-4, are transforming task planning by offering natural language as an intuitive and flexible way to describe tasks, generate symbolic plans, and reason. However, the effectiveness of LLM-based TAMP approaches is limited due to static and template-based prompting, which limits adaptability to dynamic environments and complex task contexts. To address these limitations, this work proposes a novel Onto-LLM-TAMP framework that employs knowledge-based reasoning to refine and expand user prompts with task-contextual reasoning and knowledge-based environment state descriptions. Integrating domain-specific knowledge into the prompt ensures semantically accurate and context-aware task plans. The proposed framework demonstrates its effectiveness by resolving semantic errors in symbolic plan generation, such as maintaining logical temporal goal ordering in scenarios involving hierarchical object placement. The proposed framework is validated through both simulation and real-world scenarios, demonstrating significant improvements over the baseline approach in terms of adaptability to dynamic environments and the generation of semantically correct task plans.
<div id='section'>Paperid: <span id='pid'>618, <a href='https://arxiv.org/pdf/2412.07493.pdf' target='_blank'>https://arxiv.org/pdf/2412.07493.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Muhayy Ud Din, Jan Rosell, Waseem Akram, Isiah Zaplana, Maximo A Roa, Irfan Hussain
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.07493">LLM-guided Task and Motion Planning using Knowledge-based Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Performing complex manipulation tasks in dynamic environments requires efficient Task and Motion Planning (TAMP) approaches that combine high-level symbolic plans with low-level motion control. Advances in Large Language Models (LLMs), such as GPT-4, are transforming task planning by offering natural language as an intuitive and flexible way to describe tasks, generate symbolic plans, and reason. However, the effectiveness of LLM-based TAMP approaches is limited due to static and template-based prompting, which limits adaptability to dynamic environments and complex task contexts. To address these limitations, this work proposes a novel Onto-LLM-TAMP framework that employs knowledge-based reasoning to refine and expand user prompts with task-contextual reasoning and knowledge-based environment state descriptions. Integrating domain-specific knowledge into the prompt ensures semantically accurate and context-aware task plans. The proposed framework demonstrates its effectiveness by resolving semantic errors in symbolic plan generation, such as maintaining logical temporal goal ordering in scenarios involving hierarchical object placement. The proposed framework is validated through both simulation and real-world scenarios, demonstrating significant improvements over the baseline approach in terms of adaptability to dynamic environments and the generation of semantically correct task plans.
<div id='section'>Paperid: <span id='pid'>619, <a href='https://arxiv.org/pdf/2412.07320.pdf' target='_blank'>https://arxiv.org/pdf/2412.07320.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shanlin Sun, Gabriel De Araujo, Jiaqi Xu, Shenghan Zhou, Hanwen Zhang, Ziheng Huang, Chenyu You, Xiaohui Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.07320">CoMA: Compositional Human Motion Generation with Multi-modal Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D human motion generation has seen substantial advancement in recent years. While state-of-the-art approaches have improved performance significantly, they still struggle with complex and detailed motions unseen in training data, largely due to the scarcity of motion datasets and the prohibitive cost of generating new training examples. To address these challenges, we introduce CoMA, an agent-based solution for complex human motion generation, editing, and comprehension. CoMA leverages multiple collaborative agents powered by large language and vision models, alongside a mask transformer-based motion generator featuring body part-specific encoders and codebooks for fine-grained control. Our framework enables generation of both short and long motion sequences with detailed instructions, text-guided motion editing, and self-correction for improved quality. Evaluations on the HumanML3D dataset demonstrate competitive performance against state-of-the-art methods. Additionally, we create a set of context-rich, compositional, and long text prompts, where user studies show our method significantly outperforms existing approaches.
<div id='section'>Paperid: <span id='pid'>620, <a href='https://arxiv.org/pdf/2411.04005.pdf' target='_blank'>https://arxiv.org/pdf/2411.04005.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuanpei Chen, Chen Wang, Yaodong Yang, C. Karen Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.04005">Object-Centric Dexterous Manipulation from Human Motion Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Manipulating objects to achieve desired goal states is a basic but important skill for dexterous manipulation. Human hand motions demonstrate proficient manipulation capability, providing valuable data for training robots with multi-finger hands. Despite this potential, substantial challenges arise due to the embodiment gap between human and robot hands. In this work, we introduce a hierarchical policy learning framework that uses human hand motion data for training object-centric dexterous robot manipulation. At the core of our method is a high-level trajectory generative model, learned with a large-scale human hand motion capture dataset, to synthesize human-like wrist motions conditioned on the desired object goal states. Guided by the generated wrist motions, deep reinforcement learning is further used to train a low-level finger controller that is grounded in the robot's embodiment to physically interact with the object to achieve the goal. Through extensive evaluation across 10 household objects, our approach not only demonstrates superior performance but also showcases generalization capability to novel object geometries and goal states. Furthermore, we transfer the learned policies from simulation to a real-world bimanual dexterous robot system, further demonstrating its applicability in real-world scenarios. Project website: https://cypypccpy.github.io/obj-dex.github.io/.
<div id='section'>Paperid: <span id='pid'>621, <a href='https://arxiv.org/pdf/2409.12456.pdf' target='_blank'>https://arxiv.org/pdf/2409.12456.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sibo Tian, Minghui Zheng, Xiao Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.12456">Bayesian-Optimized One-Step Diffusion Model with Knowledge Distillation for Real-Time 3D Human Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion prediction is a cornerstone of human-robot collaboration (HRC), as robots need to infer the future movements of human workers based on past motion cues to proactively plan their motion, ensuring safety in close collaboration scenarios. The diffusion model has demonstrated remarkable performance in predicting high-quality motion samples with reasonable diversity, but suffers from a slow generative process which necessitates multiple model evaluations, hindering real-world applications. To enable real-time prediction, in this work, we propose training a one-step multi-layer perceptron-based (MLP-based) diffusion model for motion prediction using knowledge distillation and Bayesian optimization. Our method contains two steps. First, we distill a pretrained diffusion-based motion predictor, TransFusion, directly into a one-step diffusion model with the same denoiser architecture. Then, to further reduce the inference time, we remove the computationally expensive components from the original denoiser and use knowledge distillation once again to distill the obtained one-step diffusion model into an even smaller model based solely on MLPs. Bayesian optimization is used to tune the hyperparameters for training the smaller diffusion model. Extensive experimental studies are conducted on benchmark datasets, and our model can significantly improve the inference speed, achieving real-time prediction without noticeable degradation in performance.
<div id='section'>Paperid: <span id='pid'>622, <a href='https://arxiv.org/pdf/2407.02272.pdf' target='_blank'>https://arxiv.org/pdf/2407.02272.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoru Wang, Wentao Zhu, Luyi Miao, Yishu Xu, Feng Gao, Qi Tian, Yizhou Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.02272">Aligning Human Motion Generation with Human Perceptions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion generation is a critical task with a wide range of applications. Achieving high realism in generated motions requires naturalness, smoothness, and plausibility. Despite rapid advancements in the field, current generation methods often fall short of these goals. Furthermore, existing evaluation metrics typically rely on ground-truth-based errors, simple heuristics, or distribution distances, which do not align well with human perceptions of motion quality. In this work, we propose a data-driven approach to bridge this gap by introducing a large-scale human perceptual evaluation dataset, MotionPercept, and a human motion critic model, MotionCritic, that capture human perceptual preferences. Our critic model offers a more accurate metric for assessing motion quality and could be readily integrated into the motion generation pipeline to enhance generation quality. Extensive experiments demonstrate the effectiveness of our approach in both evaluating and improving the quality of generated human motions by aligning with human perceptions. Code and data are publicly available at https://motioncritic.github.io/.
<div id='section'>Paperid: <span id='pid'>623, <a href='https://arxiv.org/pdf/2406.17256.pdf' target='_blank'>https://arxiv.org/pdf/2406.17256.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jaihyun Lew, Jooyoung Choi, Chaehun Shin, Dahuin Jung, Sungroh Yoon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.17256">Disentangled Motion Modeling for Video Frame Interpolation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video Frame Interpolation (VFI) aims to synthesize intermediate frames between existing frames to enhance visual smoothness and quality. Beyond the conventional methods based on the reconstruction loss, recent works have employed generative models for improved perceptual quality. However, they require complex training and large computational costs for pixel space modeling. In this paper, we introduce disentangled Motion Modeling (MoMo), a diffusion-based approach for VFI that enhances visual quality by focusing on intermediate motion modeling. We propose a disentangled two-stage training process. In the initial stage, frame synthesis and flow models are trained to generate accurate frames and flows optimal for synthesis. In the subsequent stage, we introduce a motion diffusion model, which incorporates our novel U-Net architecture specifically designed for optical flow, to generate bi-directional flows between frames. By learning the simpler low-frequency representation of motions, MoMo achieves superior perceptual quality with reduced computational demands compared to the generative modeling methods on the pixel space. MoMo surpasses state-of-the-art methods in perceptual metrics across various benchmarks, demonstrating its efficacy and efficiency in VFI.
<div id='section'>Paperid: <span id='pid'>624, <a href='https://arxiv.org/pdf/2406.09982.pdf' target='_blank'>https://arxiv.org/pdf/2406.09982.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jacinto Colan, Ana Davila, Yasuhisa Hasegawa
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.09982">Constrained Motion Planning for a Robotic Endoscope Holder based on Hierarchical Quadratic Programming</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Minimally Invasive Surgeries (MIS) are challenging for surgeons due to the limited field of view and constrained range of motion imposed by narrow access ports. These challenges can be addressed by robot-assisted endoscope systems which provide precise and stabilized positioning, as well as constrained and smooth motion control of the endoscope. In this work, we propose an online hierarchical optimization framework for visual servoing control of the endoscope in MIS. The framework prioritizes maintaining a remote-center-of-motion (RCM) constraint to prevent tissue damage, while a visual tracking task is defined as a secondary task to enable autonomous tracking of visual features of interest. We validated our approach using a 6-DOF Denso VS050 manipulator and achieved optimization solving times under 0.4 ms and maximum RCM deviation of approximately 0.4 mm. Our results demonstrate the effectiveness of the proposed approach in addressing the constrained motion planning challenges of MIS, enabling precise and autonomous endoscope positioning and visual tracking.
<div id='section'>Paperid: <span id='pid'>625, <a href='https://arxiv.org/pdf/2405.09779.pdf' target='_blank'>https://arxiv.org/pdf/2405.09779.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wansong Liu, Kareem Eltouny, Sibo Tian, Xiao Liang, Minghui Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.09779">Integrating Uncertainty-Aware Human Motion Prediction into Graph-Based Manipulator Motion Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>There has been a growing utilization of industrial robots as complementary collaborators for human workers in re-manufacturing sites. Such a human-robot collaboration (HRC) aims to assist human workers in improving the flexibility and efficiency of labor-intensive tasks. In this paper, we propose a human-aware motion planning framework for HRC to effectively compute collision-free motions for manipulators when conducting collaborative tasks with humans. We employ a neural human motion prediction model to enable proactive planning for manipulators. Particularly, rather than blindly trusting and utilizing predicted human trajectories in the manipulator planning, we quantify uncertainties of the neural prediction model to further ensure human safety. Moreover, we integrate the uncertainty-aware prediction into a graph that captures key workspace elements and illustrates their interconnections. Then a graph neural network is leveraged to operate on the constructed graph. Consequently, robot motion planning considers both the dependencies among all the elements in the workspace and the potential influence of future movements of human workers. We experimentally validate the proposed planning framework using a 6-degree-of-freedom manipulator in a shared workspace where a human is performing disassembling tasks. The results demonstrate the benefits of our approach in terms of improving the smoothness and safety of HRC. A brief video introduction of this work is available as the supplemental materials.
<div id='section'>Paperid: <span id='pid'>626, <a href='https://arxiv.org/pdf/2402.13045.pdf' target='_blank'>https://arxiv.org/pdf/2402.13045.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wansong Liu, Sibo Tian, Boyi Hu, Xiao Liang, Minghui Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.13045">A Recurrent Neural Network Enhanced Unscented Kalman Filter for Human Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a deep learning enhanced adaptive unscented Kalman filter (UKF) for predicting human arm motion in the context of manufacturing. Unlike previous network-based methods that solely rely on captured human motion data, which is represented as bone vectors in this paper, we incorporate a human arm dynamic model into the motion prediction algorithm and use the UKF to iteratively forecast human arm motions. Specifically, a Lagrangian-mechanics-based physical model is employed to correlate arm motions with associated muscle forces. Then a Recurrent Neural Network (RNN) is integrated into the framework to predict future muscle forces, which are transferred back to future arm motions based on the dynamic model. Given the absence of measurement data for future human motions that can be input into the UKF to update the state, we integrate another RNN to directly predict human future motions and treat the prediction as surrogate measurement data fed into the UKF. A noteworthy aspect of this study involves the quantification of uncertainties associated with both the data-driven and physical models in one unified framework. These quantified uncertainties are used to dynamically adapt the measurement and process noises of the UKF over time. This adaption, driven by the uncertainties of the RNN models, addresses inaccuracies stemming from the data-driven model and mitigates discrepancies between the assumed and true physical models, ultimately enhancing the accuracy and robustness of our predictions. Compared to the traditional RNN-based prediction, our method demonstrates improved accuracy and robustness in extensive experimental validations of various types of human motions.
<div id='section'>Paperid: <span id='pid'>627, <a href='https://arxiv.org/pdf/2401.11499.pdf' target='_blank'>https://arxiv.org/pdf/2401.11499.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shaoheng Fang, Zuhong Liu, Mingyu Wang, Chenxin Xu, Yiqi Zhong, Siheng Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.11499">Self-Supervised Bird's Eye View Motion Prediction with Cross-Modality Signals</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning the dense bird's eye view (BEV) motion flow in a self-supervised manner is an emerging research for robotics and autonomous driving. Current self-supervised methods mainly rely on point correspondences between point clouds, which may introduce the problems of fake flow and inconsistency, hindering the model's ability to learn accurate and realistic motion. In this paper, we introduce a novel cross-modality self-supervised training framework that effectively addresses these issues by leveraging multi-modality data to obtain supervision signals. We design three innovative supervision signals to preserve the inherent properties of scene motion, including the masked Chamfer distance loss, the piecewise rigidity loss, and the temporal consistency loss. Through extensive experiments, we demonstrate that our proposed self-supervised framework outperforms all previous self-supervision methods for the motion prediction task.
<div id='section'>Paperid: <span id='pid'>628, <a href='https://arxiv.org/pdf/2311.04726.pdf' target='_blank'>https://arxiv.org/pdf/2311.04726.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wentao Zhu, Jason Qin, Yuke Lou, Hang Ye, Xiaoxuan Ma, Hai Ci, Yizhou Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.04726">Social Motion Prediction with Cognitive Hierarchies</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humans exhibit a remarkable capacity for anticipating the actions of others and planning their own actions accordingly. In this study, we strive to replicate this ability by addressing the social motion prediction problem. We introduce a new benchmark, a novel formulation, and a cognition-inspired framework. We present Wusi, a 3D multi-person motion dataset under the context of team sports, which features intense and strategic human interactions and diverse pose distributions. By reformulating the problem from a multi-agent reinforcement learning perspective, we incorporate behavioral cloning and generative adversarial imitation learning to boost learning efficiency and generalization. Furthermore, we take into account the cognitive aspects of the human social action planning process and develop a cognitive hierarchy framework to predict strategic human social interactions. We conduct comprehensive experiments to validate the effectiveness of our proposed dataset and approach. Code and data are available at https://walter0807.github.io/Social-CH/.
<div id='section'>Paperid: <span id='pid'>629, <a href='https://arxiv.org/pdf/2309.13144.pdf' target='_blank'>https://arxiv.org/pdf/2309.13144.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ingrid Navarro, Jay Patrikar, Joao P. A. Dantas, Rohan Baijal, Ian Higgins, Sebastian Scherer, Jean Oh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.13144">SoRTS: Learned Tree Search for Long Horizon Social Robot Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The fast-growing demand for fully autonomous robots in shared spaces calls for the development of trustworthy agents that can safely and seamlessly navigate in crowded environments. Recent models for motion prediction show promise in characterizing social interactions in such environments. Still, adapting them for navigation is challenging as they often suffer from generalization failures. Prompted by this, we propose Social Robot Tree Search (SoRTS), an algorithm for safe robot navigation in social domains. SoRTS aims to augment existing socially aware motion prediction models for long-horizon navigation using Monte Carlo Tree Search.
  We use social navigation in general aviation as a case study to evaluate our approach and further the research in full-scale aerial autonomy. In doing so, we introduce XPlaneROS, a high-fidelity aerial simulator that enables human-robot interaction. We use XPlaneROS to conduct a first-of-its-kind user study where 26 FAA-certified pilots interact with a human pilot, our algorithm, and its ablation. Our results, supported by statistical evidence, show that SoRTS exhibits a comparable performance to competent human pilots, significantly outperforming its ablation. Finally, we complement these results with a broad set of self-play experiments to showcase our algorithm's performance in scenarios with increasing complexity.
<div id='section'>Paperid: <span id='pid'>630, <a href='https://arxiv.org/pdf/2307.16106.pdf' target='_blank'>https://arxiv.org/pdf/2307.16106.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sibo Tian, Minghui Zheng, Xiao Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.16106">TransFusion: A Practical and Effective Transformer-based Diffusion Model for 3D Human Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Predicting human motion plays a crucial role in ensuring a safe and effective human-robot close collaboration in intelligent remanufacturing systems of the future. Existing works can be categorized into two groups: those focusing on accuracy, predicting a single future motion, and those generating diverse predictions based on observations. The former group fails to address the uncertainty and multi-modal nature of human motion, while the latter group often produces motion sequences that deviate too far from the ground truth or become unrealistic within historical contexts. To tackle these issues, we propose TransFusion, an innovative and practical diffusion-based model for 3D human motion prediction which can generate samples that are more likely to happen while maintaining a certain level of diversity. Our model leverages Transformer as the backbone with long skip connections between shallow and deep layers. Additionally, we employ the discrete cosine transform to model motion sequences in the frequency space, thereby improving performance. In contrast to prior diffusion-based models that utilize extra modules like cross-attention and adaptive layer normalization to condition the prediction on past observed motion, we treat all inputs, including conditions, as tokens to create a more lightweight model compared to existing approaches. Extensive experimental studies are conducted on benchmark datasets to validate the effectiveness of our human motion prediction model.
<div id='section'>Paperid: <span id='pid'>631, <a href='https://arxiv.org/pdf/2307.10894.pdf' target='_blank'>https://arxiv.org/pdf/2307.10894.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wentao Zhu, Xiaoxuan Ma, Dongwoo Ro, Hai Ci, Jinlu Zhang, Jiaxin Shi, Feng Gao, Qi Tian, Yizhou Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.10894">Human Motion Generation: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion generation aims to generate natural human pose sequences and shows immense potential for real-world applications. Substantial progress has been made recently in motion data collection technologies and generation methods, laying the foundation for increasing interest in human motion generation. Most research within this field focuses on generating human motions based on conditional signals, such as text, audio, and scene contexts. While significant advancements have been made in recent years, the task continues to pose challenges due to the intricate nature of human motion and its implicit relationship with conditional signals. In this survey, we present a comprehensive literature review of human motion generation, which, to the best of our knowledge, is the first of its kind in this field. We begin by introducing the background of human motion and generative models, followed by an examination of representative methods for three mainstream sub-tasks: text-conditioned, audio-conditioned, and scene-conditioned human motion generation. Additionally, we provide an overview of common datasets and evaluation metrics. Lastly, we discuss open problems and outline potential future research directions. We hope that this survey could provide the community with a comprehensive glimpse of this rapidly evolving field and inspire novel ideas that address the outstanding challenges.
<div id='section'>Paperid: <span id='pid'>632, <a href='https://arxiv.org/pdf/2307.03610.pdf' target='_blank'>https://arxiv.org/pdf/2307.03610.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kareem A. Eltouny, Wansong Liu, Sibo Tian, Minghui Zheng, Xiao Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.03610">DE-TGN: Uncertainty-Aware Human Motion Forecasting using Deep Ensembles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Ensuring the safety of human workers in a collaborative environment with robots is of utmost importance. Although accurate pose prediction models can help prevent collisions between human workers and robots, they are still susceptible to critical errors. In this study, we propose a novel approach called deep ensembles of temporal graph neural networks (DE-TGN) that not only accurately forecast human motion but also provide a measure of prediction uncertainty. By leveraging deep ensembles and employing stochastic Monte-Carlo dropout sampling, we construct a volumetric field representing a range of potential future human poses based on covariance ellipsoids. To validate our framework, we conducted experiments using three motion capture datasets including Human3.6M, and two human-robot interaction scenarios, achieving state-of-the-art prediction error. Moreover, we discovered that deep ensembles not only enable us to quantify uncertainty but also improve the accuracy of our predictions.
<div id='section'>Paperid: <span id='pid'>633, <a href='https://arxiv.org/pdf/2304.11118.pdf' target='_blank'>https://arxiv.org/pdf/2304.11118.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Angela Castillo, Maria Escobar, Guillaume Jeanneret, Albert Pumarola, Pablo ArbelÃ¡ez, Ali Thabet, Artsiom Sanakoyeu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.11118">BoDiffusion: Diffusing Sparse Observations for Full-Body Human Motion Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Mixed reality applications require tracking the user's full-body motion to enable an immersive experience. However, typical head-mounted devices can only track head and hand movements, leading to a limited reconstruction of full-body motion due to variability in lower body configurations. We propose BoDiffusion -- a generative diffusion model for motion synthesis to tackle this under-constrained reconstruction problem. We present a time and space conditioning scheme that allows BoDiffusion to leverage sparse tracking inputs while generating smooth and realistic full-body motion sequences. To the best of our knowledge, this is the first approach that uses the reverse diffusion process to model full-body tracking as a conditional sequence generation task. We conduct experiments on the large-scale motion-capture dataset AMASS and show that our approach outperforms the state-of-the-art approaches by a significant margin in terms of full-body motion realism and joint reconstruction error.
<div id='section'>Paperid: <span id='pid'>634, <a href='https://arxiv.org/pdf/2304.08577.pdf' target='_blank'>https://arxiv.org/pdf/2304.08577.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuming Du, Robin Kips, Albert Pumarola, Sebastian Starke, Ali Thabet, Artsiom Sanakoyeu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.08577">Avatars Grow Legs: Generating Smooth Human Motion from Sparse Tracking Inputs with Diffusion Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the recent surge in popularity of AR/VR applications, realistic and accurate control of 3D full-body avatars has become a highly demanded feature. A particular challenge is that only a sparse tracking signal is available from standalone HMDs (Head Mounted Devices), often limited to tracking the user's head and wrists. While this signal is resourceful for reconstructing the upper body motion, the lower body is not tracked and must be synthesized from the limited information provided by the upper body joints. In this paper, we present AGRoL, a novel conditional diffusion model specifically designed to track full bodies given sparse upper-body tracking signals. Our model is based on a simple multi-layer perceptron (MLP) architecture and a novel conditioning scheme for motion data. It can predict accurate and smooth full-body motion, particularly the challenging lower body movement. Unlike common diffusion architectures, our compact architecture can run in real-time, making it suitable for online body-tracking applications. We train and evaluate our model on AMASS motion capture dataset, and demonstrate that our approach outperforms state-of-the-art methods in generated motion accuracy and smoothness. We further justify our design choices through extensive experiments and ablation studies.
<div id='section'>Paperid: <span id='pid'>635, <a href='https://arxiv.org/pdf/2304.04681.pdf' target='_blank'>https://arxiv.org/pdf/2304.04681.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenjie Yin, Ruibo Tu, Hang Yin, Danica Kragic, Hedvig KjellstrÃ¶m, MÃ¥rten BjÃ¶rkman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.04681">Controllable Motion Synthesis and Reconstruction with Autoregressive Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Data-driven and controllable human motion synthesis and prediction are active research areas with various applications in interactive media and social robotics. Challenges remain in these fields for generating diverse motions given past observations and dealing with imperfect poses. This paper introduces MoDiff, an autoregressive probabilistic diffusion model over motion sequences conditioned on control contexts of other modalities. Our model integrates a cross-modal Transformer encoder and a Transformer-based decoder, which are found effective in capturing temporal correlations in motion and control modalities. We also introduce a new data dropout method based on the diffusion forward process to provide richer data representations and robust generation. We demonstrate the superior performance of MoDiff in controllable motion synthesis for locomotion with respect to two baselines and show the benefits of diffusion data dropout for robust synthesis and reconstruction of high-fidelity motion close to recorded data.
<div id='section'>Paperid: <span id='pid'>636, <a href='https://arxiv.org/pdf/2303.08333.pdf' target='_blank'>https://arxiv.org/pdf/2303.08333.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiayu Zou, Zheng Zhu, Yun Ye, Xingang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.08333">DiffBEV: Conditional Diffusion Model for Bird's Eye View Perception</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>BEV perception is of great importance in the field of autonomous driving, serving as the cornerstone of planning, controlling, and motion prediction. The quality of the BEV feature highly affects the performance of BEV perception. However, taking the noises in camera parameters and LiDAR scans into consideration, we usually obtain BEV representation with harmful noises. Diffusion models naturally have the ability to denoise noisy samples to the ideal data, which motivates us to utilize the diffusion model to get a better BEV representation. In this work, we propose an end-to-end framework, named DiffBEV, to exploit the potential of diffusion model to generate a more comprehensive BEV representation. To the best of our knowledge, we are the first to apply diffusion model to BEV perception. In practice, we design three types of conditions to guide the training of the diffusion model which denoises the coarse samples and refines the semantic feature in a progressive way. What's more, a cross-attention module is leveraged to fuse the context of BEV feature and the semantic content of conditional diffusion model. DiffBEV achieves a 25.9% mIoU on the nuScenes dataset, which is 6.2% higher than the best-performing existing approach. Quantitative and qualitative results on multiple benchmarks demonstrate the effectiveness of DiffBEV in BEV semantic segmentation and 3D object detection tasks. The code will be available soon.
<div id='section'>Paperid: <span id='pid'>637, <a href='https://arxiv.org/pdf/2303.07932.pdf' target='_blank'>https://arxiv.org/pdf/2303.07932.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Max van Haren, Lennart Blanken, Tom Oomen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.07932">A Kernel-Based Identification Approach to LPV Feedforward: With Application to Motion Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The increasing demands for motion control result in a situation where Linear Parameter-Varying (LPV) dynamics have to be taken into account. Inverse-model feedforward control for LPV motion systems is challenging, since the inverse of an LPV system is often dynamically dependent on the scheduling sequence. The aim of this paper is to develop an identification approach that directly identifies dynamically scheduled feedforward controllers for LPV motion systems from data. In this paper, the feedforward controller is parameterized in basis functions, similar to, e.g., mass-acceleration feedforward, and is identified by a kernel-based approach such that the parameter dependency for LPV motion systems is addressed. The resulting feedforward includes dynamic dependence and is learned accurately. The developed framework is validated on an example.
<div id='section'>Paperid: <span id='pid'>638, <a href='https://arxiv.org/pdf/2303.01443.pdf' target='_blank'>https://arxiv.org/pdf/2303.01443.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jorge de Heuvel, Weixian Shi, Xiangyu Zeng, Maren Bennewitz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.01443">Subgoal-Driven Navigation in Dynamic Environments Using Attention-Based Deep Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Collision-free, goal-directed navigation in environments containing unknown static and dynamic obstacles is still a great challenge, especially when manual tuning of navigation policies or costly motion prediction needs to be avoided. In this paper, we therefore propose a subgoal-driven hierarchical navigation architecture that is trained with deep reinforcement learning and decouples obstacle avoidance and motor control. In particular, we separate the navigation task into the prediction of the next subgoal position for avoiding collisions while moving toward the final target position, and the prediction of the robot's velocity controls. By relying on 2D lidar, our method learns to avoid obstacles while still achieving goal-directed behavior as well as to generate low-level velocity control commands to reach the subgoals. In our architecture, we apply the attention mechanism on the robot's 2D lidar readings and compute the importance of lidar scan segments for avoiding collisions. As we show in simulated and real-world experiments with a Turtlebot robot, our proposed method leads to smooth and safe trajectories among humans and significantly outperforms a state-of-the-art approach in terms of success rate. A supplemental video describing our approach is available online.
<div id='section'>Paperid: <span id='pid'>639, <a href='https://arxiv.org/pdf/2302.07135.pdf' target='_blank'>https://arxiv.org/pdf/2302.07135.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bo Zhou, Yu-Jung Tsai, Jiazhen Zhang, Xueqi Guo, Huidong Xie, Xiongchao Chen, Tianshun Miao, Yihuan Lu, James S. Duncan, Chi Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.07135">Fast-MC-PET: A Novel Deep Learning-aided Motion Correction and Reconstruction Framework for Accelerated PET</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Patient motion during PET is inevitable. Its long acquisition time not only increases the motion and the associated artifacts but also the patient's discomfort, thus PET acceleration is desirable. However, accelerating PET acquisition will result in reconstructed images with low SNR, and the image quality will still be degraded by motion-induced artifacts. Most of the previous PET motion correction methods are motion type specific that require motion modeling, thus may fail when multiple types of motion present together. Also, those methods are customized for standard long acquisition and could not be directly applied to accelerated PET. To this end, modeling-free universal motion correction reconstruction for accelerated PET is still highly under-explored. In this work, we propose a novel deep learning-aided motion correction and reconstruction framework for accelerated PET, called Fast-MC-PET. Our framework consists of a universal motion correction (UMC) and a short-to-long acquisition reconstruction (SL-Reon) module. The UMC enables modeling-free motion correction by estimating quasi-continuous motion from ultra-short frame reconstructions and using this information for motion-compensated reconstruction. Then, the SL-Recon converts the accelerated UMC image with low counts to a high-quality image with high counts for our final reconstruction output. Our experimental results on human studies show that our Fast-MC-PET can enable 7-fold acceleration and use only 2 minutes acquisition to generate high-quality reconstruction images that outperform/match previous motion correction reconstruction methods using standard 15 minutes long acquisition data.
<div id='section'>Paperid: <span id='pid'>640, <a href='https://arxiv.org/pdf/2210.06551.pdf' target='_blank'>https://arxiv.org/pdf/2210.06551.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wentao Zhu, Xiaoxuan Ma, Zhaoyang Liu, Libin Liu, Wayne Wu, Yizhou Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.06551">MotionBERT: A Unified Perspective on Learning Human Motion Representations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a unified perspective on tackling various human-centric video tasks by learning human motion representations from large-scale and heterogeneous data resources. Specifically, we propose a pretraining stage in which a motion encoder is trained to recover the underlying 3D motion from noisy partial 2D observations. The motion representations acquired in this way incorporate geometric, kinematic, and physical knowledge about human motion, which can be easily transferred to multiple downstream tasks. We implement the motion encoder with a Dual-stream Spatio-temporal Transformer (DSTformer) neural network. It could capture long-range spatio-temporal relationships among the skeletal joints comprehensively and adaptively, exemplified by the lowest 3D pose estimation error so far when trained from scratch. Furthermore, our proposed framework achieves state-of-the-art performance on all three downstream tasks by simply finetuning the pretrained motion encoder with a simple regression head (1-2 layers), which demonstrates the versatility of the learned motion representations. Code and models are available at https://motionbert.github.io/
<div id='section'>Paperid: <span id='pid'>641, <a href='https://arxiv.org/pdf/2202.00257.pdf' target='_blank'>https://arxiv.org/pdf/2202.00257.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Max van Haren, Maurice Poot, Jim Portegies, Tom Oomen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2202.00257">Position-Dependent Snap Feedforward: A Gaussian Process Framework</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Mechatronic systems have increasingly high performance requirements for motion control. The low-frequency contribution of the flexible dynamics, i.e. the compliance, should be compensated for by means of snap feedforward to achieve high accuracy. Position-dependent compliance, which often occurs in motion systems, requires the snap feedforward parameter to be modeled as a function of position. Position-dependent compliance is compensated for by using a Gaussian process to model the snap feedforward parameter as a continuous function of position. A simulation of a flexible beam shows that a significant performance increase is achieved when using the Gaussian process snap feedforward parameter to compensate for position-dependent compliance.
<div id='section'>Paperid: <span id='pid'>642, <a href='https://arxiv.org/pdf/2201.07511.pdf' target='_blank'>https://arxiv.org/pdf/2201.07511.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Max van Haren, Maurice Poot, Dragan KostiÄ, Robin van Es, Jim Portegies, Tom Oomen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2201.07511">Gaussian Process Position-Dependent Feedforward: With Application to a Wire Bonder</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Mechatronic systems have increasingly stringent performance requirements for motion control, leading to a situation where many factors, such as position-dependency, cannot be neglected in feedforward control. The aim of this paper is to compensate for position-dependent effects by modeling feedforward parameters as a function of position. A framework to model and identify feedforward parameters as a continuous function of position is developed by combining Gaussian processes and feedforward parameter learning techniques. The framework results in a fully data-driven approach, which can be readily implemented for industrial control applications. The framework is experimentally validated and shows a significant performance increase on a commercial wire bonder.
<div id='section'>Paperid: <span id='pid'>643, <a href='https://arxiv.org/pdf/2111.12159.pdf' target='_blank'>https://arxiv.org/pdf/2111.12159.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andreas Aristidou, Anastasios Yiannakidis, Kfir Aberman, Daniel Cohen-Or, Ariel Shamir, Yiorgos Chrysanthou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2111.12159">Rhythm is a Dancer: Music-Driven Motion Synthesis with Global Structure</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Synthesizing human motion with a global structure, such as a choreography, is a challenging task. Existing methods tend to concentrate on local smooth pose transitions and neglect the global context or the theme of the motion. In this work, we present a music-driven motion synthesis framework that generates long-term sequences of human motions which are synchronized with the input beats, and jointly form a global structure that respects a specific dance genre. In addition, our framework enables generation of diverse motions that are controlled by the content of the music, and not only by the beat. Our music-driven dance synthesis framework is a hierarchical system that consists of three levels: pose, motif, and choreography. The pose level consists of an LSTM component that generates temporally coherent sequences of poses. The motif level guides sets of consecutive poses to form a movement that belongs to a specific distribution using a novel motion perceptual-loss. And the choreography level selects the order of the performed movements and drives the system to follow the global structure of a dance genre. Our results demonstrate the effectiveness of our music-driven framework to generate natural and consistent movements on various dance types, having control over the content of the synthesized motions, and respecting the overall structure of the dance.
<div id='section'>Paperid: <span id='pid'>644, <a href='https://arxiv.org/pdf/2006.12075.pdf' target='_blank'>https://arxiv.org/pdf/2006.12075.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingyi Shi, Kfir Aberman, Andreas Aristidou, Taku Komura, Dani Lischinski, Daniel Cohen-Or, Baoquan Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2006.12075">MotioNet: 3D Human Motion Reconstruction from Monocular Video with Skeleton Consistency</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce MotioNet, a deep neural network that directly reconstructs the motion of a 3D human skeleton from monocular video.While previous methods rely on either rigging or inverse kinematics (IK) to associate a consistent skeleton with temporally coherent joint rotations, our method is the first data-driven approach that directly outputs a kinematic skeleton, which is a complete, commonly used, motion representation. At the crux of our approach lies a deep neural network with embedded kinematic priors, which decomposes sequences of 2D joint positions into two separate attributes: a single, symmetric, skeleton, encoded by bone lengths, and a sequence of 3D joint rotations associated with global root positions and foot contact labels. These attributes are fed into an integrated forward kinematics (FK) layer that outputs 3D positions, which are compared to a ground truth. In addition, an adversarial loss is applied to the velocities of the recovered rotations, to ensure that they lie on the manifold of natural joint rotations. The key advantage of our approach is that it learns to infer natural joint rotations directly from the training data, rather than assuming an underlying model, or inferring them from joint positions using a data-agnostic IK solver. We show that enforcing a single consistent skeleton along with temporally coherent joint rotations constrains the solution space, leading to a more robust handling of self-occlusions and depth ambiguities.
<div id='section'>Paperid: <span id='pid'>645, <a href='https://arxiv.org/pdf/2508.13911.pdf' target='_blank'>https://arxiv.org/pdf/2508.13911.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chunji Lv, Zequn Chen, Donglin Di, Weinan Zhang, Hao Li, Wei Chen, Changsheng Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.13911">PhysGM: Large Physical Gaussian Model for Feed-Forward 4D Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While physics-grounded 3D motion synthesis has seen significant progress, current methods face critical limitations. They typically rely on pre-reconstructed 3D Gaussian Splatting (3DGS) representations, while physics integration depends on either inflexible, manually defined physical attributes or unstable, optimization-heavy guidance from video models. To overcome these challenges, we introduce PhysGM, a feed-forward framework that jointly predicts a 3D Gaussian representation and its physical properties from a single image, enabling immediate, physical simulation and high-fidelity 4D rendering. We first establish a base model by jointly optimizing for Gaussian reconstruction and probabilistic physics prediction. The model is then refined with physically plausible reference videos to enhance both rendering fidelity and physics prediction accuracy. We adopt the Direct Preference Optimization (DPO) to align its simulations with reference videos, circumventing Score Distillation Sampling (SDS) optimization which needs back-propagating gradients through the complex differentiable simulation and rasterization. To facilitate the training, we introduce a new dataset PhysAssets of over 24,000 3D assets, annotated with physical properties and corresponding guiding videos. Experimental results demonstrate that our method effectively generates high-fidelity 4D simulations from a single image in one minute. This represents a significant speedup over prior works while delivering realistic rendering results. Our project page is at:https://hihixiaolv.github.io/PhysGM.github.io/
<div id='section'>Paperid: <span id='pid'>646, <a href='https://arxiv.org/pdf/2508.10269.pdf' target='_blank'>https://arxiv.org/pdf/2508.10269.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kejun Li, Jeeseop Kim, Maxime Brunet, Marine PÃ©triaux, Yisong Yue, Aaron D. Ames
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10269">Hybrid Data-Driven Predictive Control for Robust and Reactive Exoskeleton Locomotion Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robust bipedal locomotion in exoskeletons requires the ability to dynamically react to changes in the environment in real time. This paper introduces the hybrid data-driven predictive control (HDDPC) framework, an extension of the data-enabled predictive control, that addresses these challenges by simultaneously planning foot contact schedules and continuous domain trajectories. The proposed framework utilizes a Hankel matrix-based representation to model system dynamics, incorporating step-to-step (S2S) transitions to enhance adaptability in dynamic environments. By integrating contact scheduling with trajectory planning, the framework offers an efficient, unified solution for locomotion motion synthesis that enables robust and reactive walking through online replanning. We validate the approach on the Atalante exoskeleton, demonstrating improved robustness and adaptability.
<div id='section'>Paperid: <span id='pid'>647, <a href='https://arxiv.org/pdf/2508.09003.pdf' target='_blank'>https://arxiv.org/pdf/2508.09003.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Filippo A. Spinelli, Yifan Zhai, Fang Nan, Pascal Egli, Julian Nubert, Thilo Bleumer, Lukas Miller, Ferdinand Hofmann, Marco Hutter
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09003">Large Scale Robotic Material Handling: Learning, Planning, and Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Bulk material handling involves the efficient and precise moving of large quantities of materials, a core operation in many industries, including cargo ship unloading, waste sorting, construction, and demolition. These repetitive, labor-intensive, and safety-critical operations are typically performed using large hydraulic material handlers equipped with underactuated grippers. In this work, we present a comprehensive framework for the autonomous execution of large-scale material handling tasks. The system integrates specialized modules for environment perception, pile attack point selection, path planning, and motion control. The main contributions of this work are two reinforcement learning-based modules: an attack point planner that selects optimal grasping locations on the material pile to maximize removal efficiency and minimize the number of scoops, and a robust trajectory following controller that addresses the precision and safety challenges associated with underactuated grippers in movement, while utilizing their free-swinging nature to release material through dynamic throwing. We validate our framework through real-world experiments on a 40 t material handler in a representative worksite, focusing on two key tasks: high-throughput bulk pile management and high-precision truck loading. Comparative evaluations against human operators demonstrate the system's effectiveness in terms of precision, repeatability, and operational safety. To the best of our knowledge, this is the first complete automation of material handling tasks on a full scale.
<div id='section'>Paperid: <span id='pid'>648, <a href='https://arxiv.org/pdf/2507.02363.pdf' target='_blank'>https://arxiv.org/pdf/2507.02363.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiahao Wu, Rui Peng, Jianbo Jiao, Jiayu Yang, Luyang Tang, Kaiqiang Xiong, Jie Liang, Jinbo Yan, Runling Liu, Ronggang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02363">LocalDyGS: Multi-view Global Dynamic Scene Modeling via Adaptive Local Implicit Feature Decoupling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Due to the complex and highly dynamic motions in the real world, synthesizing dynamic videos from multi-view inputs for arbitrary viewpoints is challenging. Previous works based on neural radiance field or 3D Gaussian splatting are limited to modeling fine-scale motion, greatly restricting their application. In this paper, we introduce LocalDyGS, which consists of two parts to adapt our method to both large-scale and fine-scale motion scenes: 1) We decompose a complex dynamic scene into streamlined local spaces defined by seeds, enabling global modeling by capturing motion within each local space. 2) We decouple static and dynamic features for local space motion modeling. A static feature shared across time steps captures static information, while a dynamic residual field provides time-specific features. These are combined and decoded to generate Temporal Gaussians, modeling motion within each local space. As a result, we propose a novel dynamic scene reconstruction framework to model highly dynamic real-world scenes more realistically. Our method not only demonstrates competitive performance on various fine-scale datasets compared to state-of-the-art (SOTA) methods, but also represents the first attempt to model larger and more complex highly dynamic scenes. Project page: https://wujh2001.github.io/LocalDyGS/.
<div id='section'>Paperid: <span id='pid'>649, <a href='https://arxiv.org/pdf/2507.02085.pdf' target='_blank'>https://arxiv.org/pdf/2507.02085.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wanjia Zhao, Jiaqi Han, Siyi Gu, Mingjian Jiang, James Zou, Stefano Ermon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02085">GeoAda: Efficiently Finetune Geometric Diffusion Models with Equivariant Adapters</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Geometric diffusion models have shown remarkable success in molecular dynamics and structure generation. However, efficiently fine-tuning them for downstream tasks with varying geometric controls remains underexplored. In this work, we propose an SE(3)-equivariant adapter framework ( GeoAda) that enables flexible and parameter-efficient fine-tuning for controlled generative tasks without modifying the original model architecture. GeoAda introduces a structured adapter design: control signals are first encoded through coupling operators, then processed by a trainable copy of selected pretrained model layers, and finally projected back via decoupling operators followed by an equivariant zero-initialized convolution. By fine-tuning only these lightweight adapter modules, GeoAda preserves the model's geometric consistency while mitigating overfitting and catastrophic forgetting. We theoretically prove that the proposed adapters maintain SE(3)-equivariance, ensuring that the geometric inductive biases of the pretrained diffusion model remain intact during adaptation. We demonstrate the wide applicability of GeoAda across diverse geometric control types, including frame control, global control, subgraph control, and a broad range of application domains such as particle dynamics, molecular dynamics, human motion prediction, and molecule generation. Empirical results show that GeoAda achieves state-of-the-art fine-tuning performance while preserving original task accuracy, whereas other baselines experience significant performance degradation due to overfitting and catastrophic forgetting.
<div id='section'>Paperid: <span id='pid'>650, <a href='https://arxiv.org/pdf/2506.07713.pdf' target='_blank'>https://arxiv.org/pdf/2506.07713.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ge Wang, Songlin Fan, Hangxu Liu, Quanjian Song, Hewei Wang, Jinfeng Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.07713">Consistent Video Editing as Flow-Driven Image-to-Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the prosper of video diffusion models, down-stream applications like video editing have been significantly promoted without consuming much computational cost. One particular challenge in this task lies at the motion transfer process from the source video to the edited one, where it requires the consideration of the shape deformation in between, meanwhile maintaining the temporal consistency in the generated video sequence. However, existing methods fail to model complicated motion patterns for video editing, and are fundamentally limited to object replacement, where tasks with non-rigid object motions like multi-object and portrait editing are largely neglected. In this paper, we observe that optical flows offer a promising alternative in complex motion modeling, and present FlowV2V to re-investigate video editing as a task of flow-driven Image-to-Video (I2V) generation. Specifically, FlowV2V decomposes the entire pipeline into first-frame editing and conditional I2V generation, and simulates pseudo flow sequence that aligns with the deformed shape, thus ensuring the consistency during editing. Experimental results on DAVIS-EDIT with improvements of 13.67% and 50.66% on DOVER and warping error illustrate the superior temporal consistency and sample quality of FlowV2V compared to existing state-of-the-art ones. Furthermore, we conduct comprehensive ablation studies to analyze the internal functionalities of the first-frame paradigm and flow alignment in the proposed method.
<div id='section'>Paperid: <span id='pid'>651, <a href='https://arxiv.org/pdf/2506.02661.pdf' target='_blank'>https://arxiv.org/pdf/2506.02661.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingyang Huang, Peng Zhang, Bang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.02661">MotionRAG-Diff: A Retrieval-Augmented Diffusion Framework for Long-Term Music-to-Dance Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating long-term, coherent, and realistic music-conditioned dance sequences remains a challenging task in human motion synthesis. Existing approaches exhibit critical limitations: motion graph methods rely on fixed template libraries, restricting creative generation; diffusion models, while capable of producing novel motions, often lack temporal coherence and musical alignment. To address these challenges, we propose $\textbf{MotionRAG-Diff}$, a hybrid framework that integrates Retrieval-Augmented Generation (RAG) with diffusion-based refinement to enable high-quality, musically coherent dance generation for arbitrary long-term music inputs. Our method introduces three core innovations: (1) A cross-modal contrastive learning architecture that aligns heterogeneous music and dance representations in a shared latent space, establishing unsupervised semantic correspondence without paired data; (2) An optimized motion graph system for efficient retrieval and seamless concatenation of motion segments, ensuring realism and temporal coherence across long sequences; (3) A multi-condition diffusion model that jointly conditions on raw music signals and contrastive features to enhance motion quality and global synchronization. Extensive experiments demonstrate that MotionRAG-Diff achieves state-of-the-art performance in motion quality, diversity, and music-motion synchronization accuracy. This work establishes a new paradigm for music-driven dance generation by synergizing retrieval-based template fidelity with diffusion-based creative enhancement.
<div id='section'>Paperid: <span id='pid'>652, <a href='https://arxiv.org/pdf/2505.16084.pdf' target='_blank'>https://arxiv.org/pdf/2505.16084.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zewei Zhang, Chenhao Li, Takahiro Miki, Marco Hutter
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.16084">Motion Priors Reimagined: Adapting Flat-Terrain Skills for Complex Quadruped Mobility</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reinforcement learning (RL)-based motion imitation methods trained on demonstration data can effectively learn natural and expressive motions with minimal reward engineering but often struggle to generalize to novel environments. We address this by proposing a hierarchical RL framework in which a low-level policy is first pre-trained to imitate animal motions on flat ground, thereby establishing motion priors. A subsequent high-level, goal-conditioned policy then builds on these priors, learning residual corrections that enable perceptive locomotion, local obstacle avoidance, and goal-directed navigation across diverse and rugged terrains. Simulation experiments illustrate the effectiveness of learned residuals in adapting to progressively challenging uneven terrains while still preserving the locomotion characteristics provided by the motion priors. Furthermore, our results demonstrate improvements in motion regularization over baseline models trained without motion priors under similar reward setups. Real-world experiments with an ANYmal-D quadruped robot confirm our policy's capability to generalize animal-like locomotion skills to complex terrains, demonstrating smooth and efficient locomotion and local navigation performance amidst challenging terrains with obstacles.
<div id='section'>Paperid: <span id='pid'>653, <a href='https://arxiv.org/pdf/2505.16084.pdf' target='_blank'>https://arxiv.org/pdf/2505.16084.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zewei Zhang, Chenhao Li, Takahiro Miki, Marco Hutter
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.16084">Motion Priors Reimagined: Adapting Flat-Terrain Skills for Complex Quadruped Mobility</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reinforcement learning (RL)-based motion imitation methods trained on demonstration data can effectively learn natural and expressive motions with minimal reward engineering but often struggle to generalize to novel environments. We address this by proposing a hierarchical RL framework in which a low-level policy is first pre-trained to imitate animal motions on flat ground, thereby establishing motion priors. A subsequent high-level, goal-conditioned policy then builds on these priors, learning residual corrections that enable perceptive locomotion, local obstacle avoidance, and goal-directed navigation across diverse and rugged terrains. Simulation experiments illustrate the effectiveness of learned residuals in adapting to progressively challenging uneven terrains while still preserving the locomotion characteristics provided by the motion priors. Furthermore, our results demonstrate improvements in motion regularization over baseline models trained without motion priors under similar reward setups. Real-world experiments with an ANYmal-D quadruped robot confirm our policy's capability to generalize animal-like locomotion skills to complex terrains, demonstrating smooth and efficient locomotion and local navigation performance amidst challenging terrains with obstacles.
<div id='section'>Paperid: <span id='pid'>654, <a href='https://arxiv.org/pdf/2505.03738.pdf' target='_blank'>https://arxiv.org/pdf/2505.03738.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jialong Li, Xuxin Cheng, Tianshu Huang, Shiqi Yang, Ri-Zhao Qiu, Xiaolong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.03738">AMO: Adaptive Motion Optimization for Hyper-Dexterous Humanoid Whole-Body Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humanoid robots derive much of their dexterity from hyper-dexterous whole-body movements, enabling tasks that require a large operational workspace: such as picking objects off the ground. However, achieving these capabilities on real humanoids remains challenging due to their high degrees of freedom (DoF) and nonlinear dynamics. We propose Adaptive Motion Optimization (AMO), a framework that integrates sim-to-real reinforcement learning (RL) with trajectory optimization for real-time, adaptive whole-body control. To mitigate distribution bias in motion imitation RL, we construct a hybrid AMO dataset and train a network capable of robust, on-demand adaptation to potentially O.O.D. commands. We validate AMO in simulation and on a 29-DoF Unitree G1 humanoid robot, demonstrating superior stability and an expanded workspace compared to strong baselines. Finally, we show that AMO's consistent performance supports autonomous task execution via imitation learning, underscoring the system's versatility and robustness.
<div id='section'>Paperid: <span id='pid'>655, <a href='https://arxiv.org/pdf/2505.02833.pdf' target='_blank'>https://arxiv.org/pdf/2505.02833.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanjie Ze, Zixuan Chen, JoÃ£o Pedro AraÃºjo, Zi-ang Cao, Xue Bin Peng, Jiajun Wu, C. Karen Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.02833">TWIST: Teleoperated Whole-Body Imitation System</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Teleoperating humanoid robots in a whole-body manner marks a fundamental step toward developing general-purpose robotic intelligence, with human motion providing an ideal interface for controlling all degrees of freedom. Yet, most current humanoid teleoperation systems fall short of enabling coordinated whole-body behavior, typically limiting themselves to isolated locomotion or manipulation tasks. We present the Teleoperated Whole-Body Imitation System (TWIST), a system for humanoid teleoperation through whole-body motion imitation. We first generate reference motion clips by retargeting human motion capture data to the humanoid robot. We then develop a robust, adaptive, and responsive whole-body controller using a combination of reinforcement learning and behavior cloning (RL+BC). Through systematic analysis, we demonstrate how incorporating privileged future motion frames and real-world motion capture (MoCap) data improves tracking accuracy. TWIST enables real-world humanoid robots to achieve unprecedented, versatile, and coordinated whole-body motor skills--spanning whole-body manipulation, legged manipulation, locomotion, and expressive movement--using a single unified neural network controller. Our project website: https://humanoid-teleop.github.io
<div id='section'>Paperid: <span id='pid'>656, <a href='https://arxiv.org/pdf/2504.02560.pdf' target='_blank'>https://arxiv.org/pdf/2504.02560.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yongqi Zhai, Luyang Tang, Wei Jiang, Jiayu Yang, Ronggang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.02560">L-LBVC: Long-Term Motion Estimation and Prediction for Learned Bi-Directional Video Compression</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, learned video compression (LVC) has shown superior performance under low-delay configuration. However, the performance of learned bi-directional video compression (LBVC) still lags behind traditional bi-directional coding. The performance gap mainly arises from inaccurate long-term motion estimation and prediction of distant frames, especially in large motion scenes. To solve these two critical problems, this paper proposes a novel LBVC framework, namely L-LBVC. Firstly, we propose an adaptive motion estimation module that can handle both short-term and long-term motions. Specifically, we directly estimate the optical flows for adjacent frames and non-adjacent frames with small motions. For non-adjacent frames with large motions, we recursively accumulate local flows between adjacent frames to estimate long-term flows. Secondly, we propose an adaptive motion prediction module that can largely reduce the bit cost for motion coding. To improve the accuracy of long-term motion prediction, we adaptively downsample reference frames during testing to match the motion ranges observed during training. Experiments show that our L-LBVC significantly outperforms previous state-of-the-art LVC methods and even surpasses VVC (VTM) on some test datasets under random access configuration.
<div id='section'>Paperid: <span id='pid'>657, <a href='https://arxiv.org/pdf/2503.23300.pdf' target='_blank'>https://arxiv.org/pdf/2503.23300.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenqi Jia, Bolin Lai, Miao Liu, Danfei Xu, James M. Rehg
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.23300">Learning Predictive Visuomotor Coordination</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding and predicting human visuomotor coordination is crucial for applications in robotics, human-computer interaction, and assistive technologies. This work introduces a forecasting-based task for visuomotor modeling, where the goal is to predict head pose, gaze, and upper-body motion from egocentric visual and kinematic observations. We propose a \textit{Visuomotor Coordination Representation} (VCR) that learns structured temporal dependencies across these multimodal signals. We extend a diffusion-based motion modeling framework that integrates egocentric vision and kinematic sequences, enabling temporally coherent and accurate visuomotor predictions. Our approach is evaluated on the large-scale EgoExo4D dataset, demonstrating strong generalization across diverse real-world activities. Our results highlight the importance of multimodal integration in understanding visuomotor coordination, contributing to research in visuomotor learning and human behavior modeling.
<div id='section'>Paperid: <span id='pid'>658, <a href='https://arxiv.org/pdf/2503.04829.pdf' target='_blank'>https://arxiv.org/pdf/2503.04829.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tao Wang, Zhihua Wu, Qiaozhi He, Jiaming Chu, Ling Qian, Yu Cheng, Junliang Xing, Jian Zhao, Lei Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.04829">StickMotion: Generating 3D Human Motions by Drawing a Stickman</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-to-motion generation, which translates textual descriptions into human motions, has been challenging in accurately capturing detailed user-imagined motions from simple text inputs. This paper introduces StickMotion, an efficient diffusion-based network designed for multi-condition scenarios, which generates desired motions based on traditional text and our proposed stickman conditions for global and local control of these motions, respectively. We address the challenges introduced by the user-friendly stickman from three perspectives: 1) Data generation. We develop an algorithm to generate hand-drawn stickmen automatically across different dataset formats. 2) Multi-condition fusion. We propose a multi-condition module that integrates into the diffusion process and obtains outputs of all possible condition combinations, reducing computational complexity and enhancing StickMotion's performance compared to conventional approaches with the self-attention module. 3) Dynamic supervision. We empower StickMotion to make minor adjustments to the stickman's position within the output sequences, generating more natural movements through our proposed dynamic supervision strategy. Through quantitative experiments and user studies, sketching stickmen saves users about 51.5% of their time generating motions consistent with their imagination. Our codes, demos, and relevant data will be released to facilitate further research and validation within the scientific community.
<div id='section'>Paperid: <span id='pid'>659, <a href='https://arxiv.org/pdf/2412.19860.pdf' target='_blank'>https://arxiv.org/pdf/2412.19860.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenzhang Sun, Xiang Li, Donglin Di, Zhuding Liang, Qiyuan Zhang, Hao Li, Wei Chen, Jianxun Cui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.19860">UniAvatar: Taming Lifelike Audio-Driven Talking Head Generation with Comprehensive Motion and Lighting Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, animating portrait images using audio input is a popular task. Creating lifelike talking head videos requires flexible and natural movements, including facial and head dynamics, camera motion, realistic light and shadow effects. Existing methods struggle to offer comprehensive, multifaceted control over these aspects. In this work, we introduce UniAvatar, a designed method that provides extensive control over a wide range of motion and illumination conditions. Specifically, we use the FLAME model to render all motion information onto a single image, maintaining the integrity of 3D motion details while enabling fine-grained, pixel-level control. Beyond motion, this approach also allows for comprehensive global illumination control. We design independent modules to manage both 3D motion and illumination, permitting separate and combined control. Extensive experiments demonstrate that our method outperforms others in both broad-range motion control and lighting control. Additionally, to enhance the diversity of motion and environmental contexts in current datasets, we collect and plan to publicly release two datasets, DH-FaceDrasMvVid-100 and DH-FaceReliVid-200, which capture significant head movements during speech and various lighting scenarios.
<div id='section'>Paperid: <span id='pid'>660, <a href='https://arxiv.org/pdf/2412.13196.pdf' target='_blank'>https://arxiv.org/pdf/2412.13196.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mazeyu Ji, Xuanbin Peng, Fangchen Liu, Jialong Li, Ge Yang, Xuxin Cheng, Xiaolong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.13196">ExBody2: Advanced Expressive Humanoid Whole-Body Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper tackles the challenge of enabling real-world humanoid robots to perform expressive and dynamic whole-body motions while maintaining overall stability and robustness. We propose Advanced Expressive Whole-Body Control (Exbody2), a method for producing whole-body tracking controllers that are trained on both human motion capture and simulated data and then transferred to the real world. We introduce a technique for decoupling the velocity tracking of the entire body from tracking body landmarks. We use a teacher policy to produce intermediate data that better conforms to the robot's kinematics and to automatically filter away infeasible whole-body motions. This two-step approach enabled us to produce a student policy that can be deployed on the robot that can walk, crouch, and dance. We also provide insight into the trade-off between versatility and the tracking performance on specific motions. We observed significant improvement of tracking performance after fine-tuning on a small amount of data, at the expense of the others.
<div id='section'>Paperid: <span id='pid'>661, <a href='https://arxiv.org/pdf/2412.07773.pdf' target='_blank'>https://arxiv.org/pdf/2412.07773.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenhao Lu, Xuxin Cheng, Jialong Li, Shiqi Yang, Mazeyu Ji, Chengjing Yuan, Ge Yang, Sha Yi, Xiaolong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.07773">Mobile-TeleVision: Predictive Motion Priors for Humanoid Whole-Body Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humanoid robots require both robust lower-body locomotion and precise upper-body manipulation. While recent Reinforcement Learning (RL) approaches provide whole-body loco-manipulation policies, they lack precise manipulation with high DoF arms. In this paper, we propose decoupling upper-body control from locomotion, using inverse kinematics (IK) and motion retargeting for precise manipulation, while RL focuses on robust lower-body locomotion. We introduce PMP (Predictive Motion Priors), trained with Conditional Variational Autoencoder (CVAE) to effectively represent upper-body motions. The locomotion policy is trained conditioned on this upper-body motion representation, ensuring that the system remains robust with both manipulation and locomotion. We show that CVAE features are crucial for stability and robustness, and significantly outperforms RL-based whole-body control in precise manipulation. With precise upper-body motion and robust lower-body locomotion control, operators can remotely control the humanoid to walk around and explore different environments, while performing diverse manipulation tasks.
<div id='section'>Paperid: <span id='pid'>662, <a href='https://arxiv.org/pdf/2412.01234.pdf' target='_blank'>https://arxiv.org/pdf/2412.01234.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenru Liu, Yongkang Song, Chengzhen Meng, Zhiyu Huang, Haochen Liu, Chen Lv, Jun Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.01234">Integrating Decision-Making Into Differentiable Optimization Guided Learning for End-to-End Planning of Autonomous Vehicles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We address the decision-making capability within an end-to-end planning framework that focuses on motion prediction, decision-making, and trajectory planning. Specifically, we formulate decision-making and trajectory planning as a differentiable nonlinear optimization problem, which ensures compatibility with learning-based modules to establish an end-to-end trainable architecture. This optimization introduces explicit objectives related to safety, traveling efficiency, and riding comfort, guiding the learning process in our proposed pipeline. Intrinsic constraints resulting from the decision-making task are integrated into the optimization formulation and preserved throughout the learning process. By integrating the differentiable optimizer with a neural network predictor, the proposed framework is end-to-end trainable, aligning various driving tasks with ultimate performance goals defined by the optimization objectives. The proposed framework is trained and validated using the Waymo Open Motion dataset. The open-loop testing reveals that while the planning outcomes using our method do not always resemble the expert trajectory, they consistently outperform baseline approaches with improved safety, traveling efficiency, and riding comfort. The closed-loop testing further demonstrates the effectiveness of optimizing decisions and improving driving performance. Ablation studies demonstrate that the initialization provided by the learning-based prediction module is essential for the convergence of the optimizer as well as the overall driving performance.
<div id='section'>Paperid: <span id='pid'>663, <a href='https://arxiv.org/pdf/2410.20907.pdf' target='_blank'>https://arxiv.org/pdf/2410.20907.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Seyed Adel Alizadeh Kolagar, Mehdi Heydari Shahna, Jouni Mattila
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.20907">Combining Deep Reinforcement Learning with a Jerk-Bounded Trajectory Generator for Kinematically Constrained Motion Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep reinforcement learning (DRL) is emerging as a promising method for adaptive robotic motion and complex task automation, effectively addressing the limitations of traditional control methods. However, ensuring safety throughout both the learning process and policy deployment remains a key challenge due to the risky exploration inherent in DRL, as well as the discrete nature of actions taken at intervals. These discontinuities, despite being part of a continuous action space, can lead to abrupt changes between successive actions, causing instability and unsafe intermediate states. To address these challenges, this paper proposes an integrated framework that combines DRL with a jerk-bounded trajectory generator (JBTG) and a robust low-level control strategy, significantly enhancing the safety, stability, and reliability of robotic manipulators. The low-level controller ensures the precise execution of DRL-generated commands, while the JBTG refines these motions to produce smooth, continuous trajectories that prevent abrupt or unsafe actions. The framework also includes pre-calculated safe velocity zones for smooth braking, preventing joint limit violations and ensuring compliance with kinematic constraints. This approach not only guarantees the robustness and safety of the robotic system but also optimizes motion control, making it suitable for practical applications. The effectiveness of the proposed framework is demonstrated through its application to a highly complex heavy-duty manipulator.
<div id='section'>Paperid: <span id='pid'>664, <a href='https://arxiv.org/pdf/2410.12773.pdf' target='_blank'>https://arxiv.org/pdf/2410.12773.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenyu Jiang, Yuqi Xie, Jinhan Li, Ye Yuan, Yifeng Zhu, Yuke Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.12773">Harmon: Whole-Body Motion Generation of Humanoid Robots from Language Descriptions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humanoid robots, with their human-like embodiment, have the potential to integrate seamlessly into human environments. Critical to their coexistence and cooperation with humans is the ability to understand natural language communications and exhibit human-like behaviors. This work focuses on generating diverse whole-body motions for humanoid robots from language descriptions. We leverage human motion priors from extensive human motion datasets to initialize humanoid motions and employ the commonsense reasoning capabilities of Vision Language Models (VLMs) to edit and refine these motions. Our approach demonstrates the capability to produce natural, expressive, and text-aligned humanoid motions, validated through both simulated and real-world experiments. More videos can be found at https://ut-austin-rpl.github.io/Harmon/.
<div id='section'>Paperid: <span id='pid'>665, <a href='https://arxiv.org/pdf/2410.07849.pdf' target='_blank'>https://arxiv.org/pdf/2410.07849.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Giulio Romualdi, Paolo Maria Viceconte, Lorenzo Moretti, Ines Sorrentino, Stefano Dafarra, Silvio Traversaro, Daniele Pucci
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.07849">Online DNN-driven Nonlinear MPC for Stylistic Humanoid Robot Walking with Step Adjustment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a three-layered architecture that enables stylistic locomotion with online contact location adjustment. Our method combines an autoregressive Deep Neural Network (DNN) acting as a trajectory generation layer with a model-based trajectory adjustment and trajectory control layers. The DNN produces centroidal and postural references serving as an initial guess and regularizer for the other layers. Being the DNN trained on human motion capture data, the resulting robot motion exhibits locomotion patterns, resembling a human walking style. The trajectory adjustment layer utilizes non-linear optimization to ensure dynamically feasible center of mass (CoM) motion while addressing step adjustments. We compare two implementations of the trajectory adjustment layer: one as a receding horizon planner (RHP) and the other as a model predictive controller (MPC). To enhance MPC performance, we introduce a Kalman filter to reduce measurement noise. The filter parameters are automatically tuned with a Genetic Algorithm. Experimental results on the ergoCub humanoid robot demonstrate the system's ability to prevent falls, replicate human walking styles, and withstand disturbances up to 68 Newton.
  Website: https://sites.google.com/view/dnn-mpc-walking
  Youtube video: https://www.youtube.com/watch?v=x3tzEfxO-xQ
<div id='section'>Paperid: <span id='pid'>666, <a href='https://arxiv.org/pdf/2410.05015.pdf' target='_blank'>https://arxiv.org/pdf/2410.05015.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Simon Bultmann, Raphael Memmesheimer, Jan Nogga, Julian Hau, Sven Behnke
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.05015">Anticipating Human Behavior for Safe Navigation and Efficient Collaborative Manipulation with Mobile Service Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The anticipation of human behavior is a crucial capability for robots to interact with humans safely and efficiently. We employ a smart edge sensor network to provide global observations, future predictions, and goal information to integrate anticipatory behavior for the control of a mobile manipulation robot. We present approaches to anticipate human behavior in the context of safe navigation and collaborative mobile manipulation. First, we anticipate human motion by employing projections of predicted human trajectories from smart edge sensor observations into the planning map of a mobile robot. Second, we anticipate human intentions in a collaborative furniture-carrying task to achieve a given room layout. Our experiments indicate that anticipating human behavior allows for safer navigation and more efficient collaboration. Finally, we showcase an integrated robotic system that anticipates human behavior while collaborating with an operator to achieve a target room layout, including the placement of tables and chairs.
<div id='section'>Paperid: <span id='pid'>667, <a href='https://arxiv.org/pdf/2409.18896.pdf' target='_blank'>https://arxiv.org/pdf/2409.18896.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Denys Iliash, Hanxiao Jiang, Yiming Zhang, Manolis Savva, Angel X. Chang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.18896">S2O: Static to Openable Enhancement for Articulated 3D Objects</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite much progress in large 3D datasets there are currently few interactive 3D object datasets, and their scale is limited due to the manual effort required in their construction. We introduce the static to openable (S2O) task which creates interactive articulated 3D objects from static counterparts through openable part detection, motion prediction, and interior geometry completion. We formulate a unified framework to tackle this task, and curate a challenging dataset of openable 3D objects that serves as a test bed for systematic evaluation. Our experiments benchmark methods from prior work, extended and improved methods, and simple yet effective heuristics for the S2O task. We find that turning static 3D objects into interactively openable counterparts is possible but that all methods struggle to generalize to realistic settings of the task, and we highlight promising future work directions. Our work enables efficient creation of interactive 3D objects for robotic manipulation and embodied AI tasks.
<div id='section'>Paperid: <span id='pid'>668, <a href='https://arxiv.org/pdf/2409.15179.pdf' target='_blank'>https://arxiv.org/pdf/2409.15179.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yue Han, Junwei Zhu, Yuxiang Feng, Xiaozhong Ji, Keke He, Xiangtai Li, zhucun xue, Yong Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.15179">MIMAFace: Face Animation via Motion-Identity Modulated Appearance Feature Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current diffusion-based face animation methods generally adopt a ReferenceNet (a copy of U-Net) and a large amount of curated self-acquired data to learn appearance features, as robust appearance features are vital for ensuring temporal stability. However, when trained on public datasets, the results often exhibit a noticeable performance gap in image quality and temporal consistency. To address this issue, we meticulously examine the essential appearance features in the facial animation tasks, which include motion-agnostic (e.g., clothing, background) and motion-related (e.g., facial details) texture components, along with high-level discriminative identity features. Drawing from this analysis, we introduce a Motion-Identity Modulated Appearance Learning Module (MIA) that modulates CLIP features at both motion and identity levels. Additionally, to tackle the semantic/ color discontinuities between clips, we design an Inter-clip Affinity Learning Module (ICA) to model temporal relationships across clips. Our method achieves precise facial motion control (i.e., expressions and gaze), faithful identity preservation, and generates animation videos that maintain both intra/inter-clip temporal consistency. Moreover, it easily adapts to various modalities of driving sources. Extensive experiments demonstrate the superiority of our method.
<div id='section'>Paperid: <span id='pid'>669, <a href='https://arxiv.org/pdf/2407.21136.pdf' target='_blank'>https://arxiv.org/pdf/2407.21136.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxuan Bian, Ailing Zeng, Xuan Ju, Xian Liu, Zhaoyang Zhang, Wei Liu, Qiang Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.21136">MotionCraft: Crafting Whole-Body Motion with Plug-and-Play Multimodal Controls</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Whole-body multimodal motion generation, controlled by text, speech, or music, has numerous applications including video generation and character animation. However, employing a unified model to achieve various generation tasks with different condition modalities presents two main challenges: motion distribution drifts across different tasks (e.g., co-speech gestures and text-driven daily actions) and the complex optimization of mixed conditions with varying granularities (e.g., text and audio). Additionally, inconsistent motion formats across different tasks and datasets hinder effective training toward multimodal motion generation. In this paper, we propose MotionCraft, a unified diffusion transformer that crafts whole-body motion with plug-and-play multimodal control. Our framework employs a coarse-to-fine training strategy, starting with the first stage of text-to-motion semantic pre-training, followed by the second stage of multimodal low-level control adaptation to handle conditions of varying granularities. To effectively learn and transfer motion knowledge across different distributions, we design MC-Attn for parallel modeling of static and dynamic human topology graphs. To overcome the motion format inconsistency of existing benchmarks, we introduce MC-Bench, the first available multimodal whole-body motion generation benchmark based on the unified SMPL-X format. Extensive experiments show that MotionCraft achieves state-of-the-art performance on various standard motion generation tasks.
<div id='section'>Paperid: <span id='pid'>670, <a href='https://arxiv.org/pdf/2407.15122.pdf' target='_blank'>https://arxiv.org/pdf/2407.15122.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Konstantinos Gounis, Nikolaos Passalis, Anastasios Tefas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.15122">UAV Active Perception and Motion Control for Improving Navigation Using Low-Cost Sensors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this study a model pipeline is proposed that combines computer vision with control-theoretic methods and utilizes low cost sensors. The proposed work enables perception-aware motion control for a quadrotor UAV to detect and navigate to objects of interest such as wind turbines and electric towers. The distance to the object of interest was estimated utilizing RGB as the primary sensory input. For the needs of the study, the Microsoft AirSim simulator was used. As a first step, a YOLOv8 model was integrated providing the basic position setpoints towards the detection. From the YOLOv8 inference, a target yaw angle was derived. The subsequent algorithms, combining performant in computational terms computer vision methods and YOLOv8, actively drove the drone to measure the height of the detection. Based on the height, an estimate of the depth was retrieved. In addition to this step, a convolutional neural network was developed, namely ActvePerceptionNet aiming at active YOLOv8 inference. The latter was validated for wind turbines where the rotational motion of the propeller was found to affect object confidence in a near periodical fashion. The results of the simulation experiments conducted in this study showed efficient object height and distance estimation and effective localization.
<div id='section'>Paperid: <span id='pid'>671, <a href='https://arxiv.org/pdf/2407.06188.pdf' target='_blank'>https://arxiv.org/pdf/2407.06188.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yukang Cao, Xinying Guo, Mingyuan Zhang, Haozhe Xie, Chenyang Gu, Ziwei Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.06188">CrowdMoGen: Zero-Shot Text-Driven Collective Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While recent advances in text-to-motion generation have shown promising results, they typically assume all individuals are grouped as a single unit. Scaling these methods to handle larger crowds and ensuring that individuals respond appropriately to specific events remains a significant challenge. This is primarily due to the complexities of scene planning, which involves organizing groups, planning their activities, and coordinating interactions, and controllable motion generation. In this paper, we present CrowdMoGen, the first zero-shot framework for collective motion generation, which effectively groups individuals and generates event-aligned motion sequences from text prompts. 1) Being limited by the available datasets for training an effective scene planning module in a supervised manner, we instead propose a crowd scene planner that leverages pre-trained large language models (LLMs) to organize individuals into distinct groups. While LLMs offer high-level guidance for group divisions, they lack the low-level understanding of human motion. To address this, we further propose integrating an SMPL-based joint prior to generate context-appropriate activities, which consists of both joint trajectories and textual descriptions. 2) Secondly, to incorporate the assigned activities into the generative network, we introduce a collective motion generator that integrates the activities into a transformer-based network in a joint-wise manner, maintaining the spatial constraints during the multi-step denoising process. Extensive experiments demonstrate that CrowdMoGen significantly outperforms previous approaches, delivering realistic, event-driven motion sequences that are spatially coherent. As the first framework of collective motion generation, CrowdMoGen has the potential to advance applications in urban simulation, crowd planning, and other large-scale interactive environments.
<div id='section'>Paperid: <span id='pid'>672, <a href='https://arxiv.org/pdf/2406.06211.pdf' target='_blank'>https://arxiv.org/pdf/2406.06211.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Abdulwahab Felemban, Eslam Mohamed Bakr, Xiaoqian Shen, Jian Ding, Abduallah Mohamed, Mohamed Elhoseiny
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.06211">iMotion-LLM: Motion Prediction Instruction Tuning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce iMotion-LLM: a Multimodal Large Language Models (LLMs) with trajectory prediction, tailored to guide interactive multi-agent scenarios. Different from conventional motion prediction approaches, iMotion-LLM capitalizes on textual instructions as key inputs for generating contextually relevant trajectories. By enriching the real-world driving scenarios in the Waymo Open Dataset with textual motion instructions, we created InstructWaymo. Leveraging this dataset, iMotion-LLM integrates a pretrained LLM, fine-tuned with LoRA, to translate scene features into the LLM input space. iMotion-LLM offers significant advantages over conventional motion prediction models. First, it can generate trajectories that align with the provided instructions if it is a feasible direction. Second, when given an infeasible direction, it can reject the instruction, thereby enhancing safety. These findings act as milestones in empowering autonomous navigation systems to interpret and predict the dynamics of multi-agent environments, laying the groundwork for future advancements in this field.
<div id='section'>Paperid: <span id='pid'>673, <a href='https://arxiv.org/pdf/2405.15267.pdf' target='_blank'>https://arxiv.org/pdf/2405.15267.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoxuan Qu, Zhaoyang He, Zeyu Hu, Yujun Cai, Jun Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.15267">Off-the-shelf ChatGPT is a Good Few-shot Human Motion Predictor</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To facilitate the application of motion prediction in practice, recently, the few-shot motion prediction task has attracted increasing research attention. Yet, in existing few-shot motion prediction works, a specific model that is dedicatedly trained over human motions is generally required. In this work, rather than tackling this task through training a specific human motion prediction model, we instead propose a novel FMP-OC framework. In FMP-OC, in a totally training-free manner, we enable Few-shot Motion Prediction, which is a non-language task, to be performed directly via utilizing the Off-the-shelf language model ChatGPT. Specifically, to lead ChatGPT as a language model to become an accurate motion predictor, in FMP-OC, we first introduce several novel designs to facilitate extracting implicit knowledge from ChatGPT. Moreover, we also incorporate our framework with a motion-in-context learning mechanism. Extensive experiments demonstrate the efficacy of our proposed framework.
<div id='section'>Paperid: <span id='pid'>674, <a href='https://arxiv.org/pdf/2405.01461.pdf' target='_blank'>https://arxiv.org/pdf/2405.01461.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenshuo Chen, Hongru Xiao, Erhang Zhang, Lijie Hu, Lei Wang, Mengyuan Liu, Chen Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.01461">SATO: Stable Text-to-Motion Framework</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Is the Text to Motion model robust? Recent advancements in Text to Motion models primarily stem from more accurate predictions of specific actions. However, the text modality typically relies solely on pre-trained Contrastive Language-Image Pretraining (CLIP) models. Our research has uncovered a significant issue with the text-to-motion model: its predictions often exhibit inconsistent outputs, resulting in vastly different or even incorrect poses when presented with semantically similar or identical text inputs. In this paper, we undertake an analysis to elucidate the underlying causes of this instability, establishing a clear link between the unpredictability of model outputs and the erratic attention patterns of the text encoder module. Consequently, we introduce a formal framework aimed at addressing this issue, which we term the Stable Text-to-Motion Framework (SATO). SATO consists of three modules, each dedicated to stable attention, stable prediction, and maintaining a balance between accuracy and robustness trade-off. We present a methodology for constructing an SATO that satisfies the stability of attention and prediction. To verify the stability of the model, we introduced a new textual synonym perturbation dataset based on HumanML3D and KIT-ML. Results show that SATO is significantly more stable against synonyms and other slight perturbations while keeping its high accuracy performance.
<div id='section'>Paperid: <span id='pid'>675, <a href='https://arxiv.org/pdf/2403.18344.pdf' target='_blank'>https://arxiv.org/pdf/2403.18344.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingxing Peng, Xusen Guo, Xianda Chen, Meixin Zhu, Kehua Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.18344">LC-LLM: Explainable Lane-Change Intention and Trajectory Predictions with Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To ensure safe driving in dynamic environments, autonomous vehicles should possess the capability to accurately predict lane change intentions of surrounding vehicles in advance and forecast their future trajectories. Existing motion prediction approaches have ample room for improvement, particularly in terms of long-term prediction accuracy and interpretability. In this paper, we address these challenges by proposing LC-LLM, an explainable lane change prediction model that leverages the strong reasoning capabilities and self-explanation abilities of Large Language Models (LLMs). Essentially, we reformulate the lane change prediction task as a language modeling problem, processing heterogeneous driving scenario information as natural language prompts for LLMs and employing supervised fine-tuning to tailor LLMs specifically for lane change prediction task. Additionally, we finetune the Chain-of-Thought (CoT) reasoning to improve prediction transparency and reliability, and include explanatory requirements in the prompts during inference stage. Therefore, our LC-LLM model not only predicts lane change intentions and trajectories but also provides CoT reasoning and explanations for its predictions, enhancing its interpretability. Extensive experiments based on the large-scale highD dataset demonstrate the superior performance and interpretability of our LC-LLM in lane change prediction task. To the best of our knowledge, this is the first attempt to utilize LLMs for predicting lane change behavior. Our study shows that LLMs can effectively encode comprehensive interaction information for driving behavior understanding.
<div id='section'>Paperid: <span id='pid'>676, <a href='https://arxiv.org/pdf/2403.15100.pdf' target='_blank'>https://arxiv.org/pdf/2403.15100.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoyu Wang, Xiaoyu Tan, Xihe Qiu, Chao Qu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.15100">Subequivariant Reinforcement Learning Framework for Coordinated Motion Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Effective coordination is crucial for motion control with reinforcement learning, especially as the complexity of agents and their motions increases. However, many existing methods struggle to account for the intricate dependencies between joints. We introduce CoordiGraph, a novel architecture that leverages subequivariant principles from physics to enhance coordination of motion control with reinforcement learning. This method embeds the principles of equivariance as inherent patterns in the learning process under gravity influence, which aids in modeling the nuanced relationships between joints vital for motion control. Through extensive experimentation with sophisticated agents in diverse environments, we highlight the merits of our approach. Compared to current leading methods, CoordiGraph notably enhances generalization and sample efficiency.
<div id='section'>Paperid: <span id='pid'>677, <a href='https://arxiv.org/pdf/2403.14947.pdf' target='_blank'>https://arxiv.org/pdf/2403.14947.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoxuan Qu, Ziyan Guo, Jun Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.14947">GPT-Connect: Interaction between Text-Driven Human Motion Generator and 3D Scenes in a Training-free Manner</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, while text-driven human motion generation has received massive research attention, most existing text-driven motion generators are generally only designed to generate motion sequences in a blank background. While this is the case, in practice, human beings naturally perform their motions in 3D scenes, rather than in a blank background. Considering this, we here aim to perform scene-aware text-drive motion generation instead. Yet, intuitively training a separate scene-aware motion generator in a supervised way can require a large amount of motion samples to be troublesomely collected and annotated in a large scale of different 3D scenes. To handle this task rather in a relatively convenient manner, in this paper, we propose a novel GPT-connect framework. In GPT-connect, we enable scene-aware motion sequences to be generated directly utilizing the existing blank-background human motion generator, via leveraging ChatGPT to connect the existing motion generator with the 3D scene in a totally training-free manner. Extensive experiments demonstrate the efficacy and generalizability of our proposed framework.
<div id='section'>Paperid: <span id='pid'>678, <a href='https://arxiv.org/pdf/2403.13261.pdf' target='_blank'>https://arxiv.org/pdf/2403.13261.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kewei Wang, Yizheng Wu, Jun Cen, Zhiyu Pan, Xingyi Li, Zhe Wang, Zhiguo Cao, Guosheng Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.13261">Self-Supervised Class-Agnostic Motion Prediction with Spatial and Temporal Consistency Regularizations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The perception of motion behavior in a dynamic environment holds significant importance for autonomous driving systems, wherein class-agnostic motion prediction methods directly predict the motion of the entire point cloud. While most existing methods rely on fully-supervised learning, the manual labeling of point cloud data is laborious and time-consuming. Therefore, several annotation-efficient methods have been proposed to address this challenge. Although effective, these methods rely on weak annotations or additional multi-modal data like images, and the potential benefits inherent in the point cloud sequence are still underexplored. To this end, we explore the feasibility of self-supervised motion prediction with only unlabeled LiDAR point clouds. Initially, we employ an optimal transport solver to establish coarse correspondences between current and future point clouds as the coarse pseudo motion labels. Training models directly using such coarse labels leads to noticeable spatial and temporal prediction inconsistencies. To mitigate these issues, we introduce three simple spatial and temporal regularization losses, which facilitate the self-supervised training process effectively. Experimental results demonstrate the significant superiority of our approach over the state-of-the-art self-supervised methods.
<div id='section'>Paperid: <span id='pid'>679, <a href='https://arxiv.org/pdf/2402.16796.pdf' target='_blank'>https://arxiv.org/pdf/2402.16796.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuxin Cheng, Yandong Ji, Junming Chen, Ruihan Yang, Ge Yang, Xiaolong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.16796">Expressive Whole-Body Control for Humanoid Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Can we enable humanoid robots to generate rich, diverse, and expressive motions in the real world? We propose to learn a whole-body control policy on a human-sized robot to mimic human motions as realistic as possible. To train such a policy, we leverage the large-scale human motion capture data from the graphics community in a Reinforcement Learning framework. However, directly performing imitation learning with the motion capture dataset would not work on the real humanoid robot, given the large gap in degrees of freedom and physical capabilities. Our method Expressive Whole-Body Control (Exbody) tackles this problem by encouraging the upper humanoid body to imitate a reference motion, while relaxing the imitation constraint on its two legs and only requiring them to follow a given velocity robustly. With training in simulation and Sim2Real transfer, our policy can control a humanoid robot to walk in different styles, shake hands with humans, and even dance with a human in the real world. We conduct extensive studies and comparisons on diverse motions in both simulation and the real world to show the effectiveness of our approach.
<div id='section'>Paperid: <span id='pid'>680, <a href='https://arxiv.org/pdf/2312.11850.pdf' target='_blank'>https://arxiv.org/pdf/2312.11850.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinshun Wang, Qiongjie Cui, Chen Chen, Mengyuan Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.11850">GCNext: Towards the Unity of Graph Convolutions for Human Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The past few years has witnessed the dominance of Graph Convolutional Networks (GCNs) over human motion prediction.Various styles of graph convolutions have been proposed, with each one meticulously designed and incorporated into a carefully-crafted network architecture. This paper breaks the limits of existing knowledge by proposing Universal Graph Convolution (UniGC), a novel graph convolution concept that re-conceptualizes different graph convolutions as its special cases. Leveraging UniGC on network-level, we propose GCNext, a novel GCN-building paradigm that dynamically determines the best-fitting graph convolutions both sample-wise and layer-wise. GCNext offers multiple use cases, including training a new GCN from scratch or refining a preexisting GCN. Experiments on Human3.6M, AMASS, and 3DPW datasets show that, by incorporating unique module-to-network designs, GCNext yields up to 9x lower computational cost than existing GCN methods, on top of achieving state-of-the-art performance.
<div id='section'>Paperid: <span id='pid'>681, <a href='https://arxiv.org/pdf/2312.03913.pdf' target='_blank'>https://arxiv.org/pdf/2312.03913.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaman Li, Alexander Clegg, Roozbeh Mottaghi, Jiajun Wu, Xavier Puig, C. Karen Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.03913">Controllable Human-Object Interaction Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Synthesizing semantic-aware, long-horizon, human-object interaction is critical to simulate realistic human behaviors. In this work, we address the challenging problem of generating synchronized object motion and human motion guided by language descriptions in 3D scenes. We propose Controllable Human-Object Interaction Synthesis (CHOIS), an approach that generates object motion and human motion simultaneously using a conditional diffusion model given a language description, initial object and human states, and sparse object waypoints. Here, language descriptions inform style and intent, and waypoints, which can be effectively extracted from high-level planning, ground the motion in the scene. Naively applying a diffusion model fails to predict object motion aligned with the input waypoints; it also cannot ensure the realism of interactions that require precise hand-object and human-floor contact. To overcome these problems, we introduce an object geometry loss as additional supervision to improve the matching between generated object motion and input object waypoints; we also design guidance terms to enforce contact constraints during the sampling process of the trained diffusion model. We demonstrate that our learned interaction module can synthesize realistic human-object interactions, adhering to provided textual descriptions and sparse waypoint conditions. Additionally, our module seamlessly integrates with a path planning module, enabling the generation of long-term interactions in 3D environments.
<div id='section'>Paperid: <span id='pid'>682, <a href='https://arxiv.org/pdf/2312.02772.pdf' target='_blank'>https://arxiv.org/pdf/2312.02772.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xu Shi, Wei Yao, Chuanchen Luo, Junran Peng, Hongwen Zhang, Yunlian Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.02772">FG-MDM: Towards Zero-Shot Human Motion Generation via ChatGPT-Refined Descriptions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, significant progress has been made in text-based motion generation, enabling the generation of diverse and high-quality human motions that conform to textual descriptions. However, generating motions beyond the distribution of original datasets remains challenging, i.e., zero-shot generation. By adopting a divide-and-conquer strategy, we propose a new framework named Fine-Grained Human Motion Diffusion Model (FG-MDM) for zero-shot human motion generation. Specifically, we first parse previous vague textual annotations into fine-grained descriptions of different body parts by leveraging a large language model. We then use these fine-grained descriptions to guide a transformer-based diffusion model, which further adopts a design of part tokens. FG-MDM can generate human motions beyond the scope of original datasets owing to descriptions that are closer to motion essence. Our experimental results demonstrate the superiority of FG-MDM over previous methods in zero-shot settings. We will release our fine-grained textual annotations for HumanML3D and KIT.
<div id='section'>Paperid: <span id='pid'>683, <a href='https://arxiv.org/pdf/2310.00430.pdf' target='_blank'>https://arxiv.org/pdf/2310.00430.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vivek Nair, Wenbo Guo, Rui Wang, James F. O'Brien, Louis Rosenberg, Dawn Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.00430">Berkeley Open Extended Reality Recordings 2023 (BOXRR-23): 4.7 Million Motion Capture Recordings from 105,852 Extended Reality Device Users</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Extended reality (XR) devices such as the Meta Quest and Apple Vision Pro have seen a recent surge in attention, with motion tracking "telemetry" data lying at the core of nearly all XR and metaverse experiences. Researchers are just beginning to understand the implications of this data for security, privacy, usability, and more, but currently lack large-scale human motion datasets to study. The BOXRR-23 dataset contains 4,717,215 motion capture recordings, voluntarily submitted by 105,852 XR device users from over 50 countries. BOXRR-23 is over 200 times larger than the largest existing motion capture research dataset and uses a new, highly efficient purpose-built XR Open Recording (XROR) file format.
<div id='section'>Paperid: <span id='pid'>684, <a href='https://arxiv.org/pdf/2309.16237.pdf' target='_blank'>https://arxiv.org/pdf/2309.16237.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaman Li, Jiajun Wu, C. Karen Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.16237">Object Motion Guided Human Motion Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modeling human behaviors in contextual environments has a wide range of applications in character animation, embodied AI, VR/AR, and robotics. In real-world scenarios, humans frequently interact with the environment and manipulate various objects to complete daily tasks. In this work, we study the problem of full-body human motion synthesis for the manipulation of large-sized objects. We propose Object MOtion guided human MOtion synthesis (OMOMO), a conditional diffusion framework that can generate full-body manipulation behaviors from only the object motion. Since naively applying diffusion models fails to precisely enforce contact constraints between the hands and the object, OMOMO learns two separate denoising processes to first predict hand positions from object motion and subsequently synthesize full-body poses based on the predicted hand positions. By employing the hand positions as an intermediate representation between the two denoising processes, we can explicitly enforce contact constraints, resulting in more physically plausible manipulation motions. With the learned model, we develop a novel system that captures full-body human manipulation motions by simply attaching a smartphone to the object being manipulated. Through extensive experiments, we demonstrate the effectiveness of our proposed pipeline and its ability to generalize to unseen objects. Additionally, as high-quality human-object interaction datasets are scarce, we collect a large-scale dataset consisting of 3D object geometry, object motion, and human motion. Our dataset contains human-object interaction motion for 15 objects, with a total duration of approximately 10 hours.
<div id='section'>Paperid: <span id='pid'>685, <a href='https://arxiv.org/pdf/2307.16463.pdf' target='_blank'>https://arxiv.org/pdf/2307.16463.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Saeid Naderiparizi, Xiaoxuan Liang, Setareh Cohan, Berend Zwartsenberg, Frank Wood
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.16463">Don't be so negative! Score-based Generative Modeling with Oracle-assisted Guidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Score-based diffusion models are a powerful class of generative models, widely utilized across diverse domains. Despite significant advancements in large-scale tasks such as text-to-image generation, their application to constrained domains has received considerably less attention. This work addresses model learning in a setting where, in addition to the training dataset, there further exists side-information in the form of an oracle that can label samples as being outside the support of the true data generating distribution. Specifically we develop a new denoising diffusion probabilistic modeling methodology, Gen-neG, that leverages this additional side-information. Gen-neG builds on classifier guidance in diffusion models to guide the generation process towards the positive support region indicated by the oracle. We empirically establish the utility of Gen-neG in applications including collision avoidance in self-driving simulators and safety-guarded human motion generation.
<div id='section'>Paperid: <span id='pid'>686, <a href='https://arxiv.org/pdf/2307.14006.pdf' target='_blank'>https://arxiv.org/pdf/2307.14006.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinshun Wang, Qiongjie Cui, Chen Chen, Shen Zhao, Mengyuan Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.14006">Learning Snippet-to-Motion Progression for Skeleton-based Human Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing Graph Convolutional Networks to achieve human motion prediction largely adopt a one-step scheme, which output the prediction straight from history input, failing to exploit human motion patterns. We observe that human motions have transitional patterns and can be split into snippets representative of each transition. Each snippet can be reconstructed from its starting and ending poses referred to as the transitional poses. We propose a snippet-to-motion multi-stage framework that breaks motion prediction into sub-tasks easier to accomplish. Each sub-task integrates three modules: transitional pose prediction, snippet reconstruction, and snippet-to-motion prediction. Specifically, we propose to first predict only the transitional poses. Then we use them to reconstruct the corresponding snippets, obtaining a close approximation to the true motion sequence. Finally we refine them to produce the final prediction output. To implement the network, we propose a novel unified graph modeling, which allows for direct and effective feature propagation compared to existing approaches which rely on separate space-time modeling. Extensive experiments on Human 3.6M, CMU Mocap and 3DPW datasets verify the effectiveness of our method which achieves state-of-the-art performance.
<div id='section'>Paperid: <span id='pid'>687, <a href='https://arxiv.org/pdf/2305.02195.pdf' target='_blank'>https://arxiv.org/pdf/2305.02195.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chen Tessler, Yoni Kasten, Yunrong Guo, Shie Mannor, Gal Chechik, Xue Bin Peng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.02195">CALM: Conditional Adversarial Latent Models for Directable Virtual Characters</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we present Conditional Adversarial Latent Models (CALM), an approach for generating diverse and directable behaviors for user-controlled interactive virtual characters. Using imitation learning, CALM learns a representation of movement that captures the complexity and diversity of human motion, and enables direct control over character movements. The approach jointly learns a control policy and a motion encoder that reconstructs key characteristics of a given motion without merely replicating it. The results show that CALM learns a semantic motion representation, enabling control over the generated motions and style-conditioning for higher-level task training. Once trained, the character can be controlled using intuitive interfaces, akin to those found in video games.
<div id='section'>Paperid: <span id='pid'>688, <a href='https://arxiv.org/pdf/2304.03532.pdf' target='_blank'>https://arxiv.org/pdf/2304.03532.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinshun Wang, Qiongjie Cui, Chen Chen, Shen Zhao, Mengyuan Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.03532">Graph-Guided MLP-Mixer for Skeleton-Based Human Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, Graph Convolutional Networks (GCNs) have been widely used in human motion prediction, but their performance remains unsatisfactory. Recently, MLP-Mixer, initially developed for vision tasks, has been leveraged into human motion prediction as a promising alternative to GCNs, which achieves both better performance and better efficiency than GCNs. Unlike GCNs, which can explicitly capture human skeleton's bone-joint structure by representing it as a graph with edges and nodes, MLP-Mixer relies on fully connected layers and thus cannot explicitly model such graph-like structure of human's. To break this limitation of MLP-Mixer's, we propose \textit{Graph-Guided Mixer}, a novel approach that equips the original MLP-Mixer architecture with the capability to model graph structure. By incorporating graph guidance, our \textit{Graph-Guided Mixer} can effectively capture and utilize the specific connectivity patterns within human skeleton's graph representation. In this paper, first we uncover a theoretical connection between MLP-Mixer and GCN that is unexplored in existing research. Building on this theoretical connection, next we present our proposed \textit{Graph-Guided Mixer}, explaining how the original MLP-Mixer architecture is reinvented to incorporate guidance from graph structure. Then we conduct an extensive evaluation on the Human3.6M, AMASS, and 3DPW datasets, which shows that our method achieves state-of-the-art performance.
<div id='section'>Paperid: <span id='pid'>689, <a href='https://arxiv.org/pdf/2303.17912.pdf' target='_blank'>https://arxiv.org/pdf/2303.17912.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Joao Pedro Araujo, Jiaman Li, Karthik Vetrivel, Rishi Agarwal, Deepak Gopinath, Jiajun Wu, Alexander Clegg, C. Karen Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.17912">CIRCLE: Capture In Rich Contextual Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Synthesizing 3D human motion in a contextual, ecological environment is important for simulating realistic activities people perform in the real world. However, conventional optics-based motion capture systems are not suited for simultaneously capturing human movements and complex scenes. The lack of rich contextual 3D human motion datasets presents a roadblock to creating high-quality generative human motion models. We propose a novel motion acquisition system in which the actor perceives and operates in a highly contextual virtual world while being motion captured in the real world. Our system enables rapid collection of high-quality human motion in highly diverse scenes, without the concern of occlusion or the need for physical scene construction in the real world. We present CIRCLE, a dataset containing 10 hours of full-body reaching motion from 5 subjects across nine scenes, paired with ego-centric information of the environment represented in various forms, such as RGBD videos. We use this dataset to train a model that generates human motion conditioned on scene information. Leveraging our dataset, the model learns to use ego-centric scene information to achieve nontrivial reaching tasks in the context of complex 3D scenes. To download the data please visit https://stanford-tml.github.io/circle_dataset/.
<div id='section'>Paperid: <span id='pid'>690, <a href='https://arxiv.org/pdf/2302.03939.pdf' target='_blank'>https://arxiv.org/pdf/2302.03939.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiyu Huang, Haochen Liu, Jingda Wu, Wenhui Huang, Chen Lv
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.03939">Learning Interaction-aware Motion Prediction Model for Decision-making in Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Predicting the behaviors of other road users is crucial to safe and intelligent decision-making for autonomous vehicles (AVs). However, most motion prediction models ignore the influence of the AV's actions and the planning module has to treat other agents as unalterable moving obstacles. To address this problem, this paper proposes an interaction-aware motion prediction model that is able to predict other agents' future trajectories according to the ego agent's future plan, i.e., their reactions to the ego's actions. Specifically, we employ Transformers to effectively encode the driving scene and incorporate the AV's plan in decoding the predicted trajectories. To train the model to accurately predict the reactions of other agents, we develop an online learning framework, where the ego agent explores the environment and collects other agents' reactions to itself. We validate the decision-making and learning framework in three highly interactive simulated driving scenarios. The results reveal that our decision-making method significantly outperforms the reinforcement learning methods in terms of data efficiency and performance. We also find that using the interaction-aware model can bring better performance than the non-interaction-aware model and the exploration process helps improve the success rate in testing.
<div id='section'>Paperid: <span id='pid'>691, <a href='https://arxiv.org/pdf/2301.01424.pdf' target='_blank'>https://arxiv.org/pdf/2301.01424.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sifan Ye, Yixing Wang, Jiaman Li, Dennis Park, C. Karen Liu, Huazhe Xu, Jiajun Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.01424">Scene Synthesis from Human Motion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large-scale capture of human motion with diverse, complex scenes, while immensely useful, is often considered prohibitively costly. Meanwhile, human motion alone contains rich information about the scene they reside in and interact with. For example, a sitting human suggests the existence of a chair, and their leg position further implies the chair's pose. In this paper, we propose to synthesize diverse, semantically reasonable, and physically plausible scenes based on human motion. Our framework, Scene Synthesis from HUMan MotiON (SUMMON), includes two steps. It first uses ContactFormer, our newly introduced contact predictor, to obtain temporally consistent contact labels from human motion. Based on these predictions, SUMMON then chooses interacting objects and optimizes physical plausibility losses; it further populates the scene with objects that do not interact with humans. Experimental results demonstrate that SUMMON synthesizes feasible, plausible, and diverse scenes and has the potential to generate extensive human-scene interaction data for the community.
<div id='section'>Paperid: <span id='pid'>692, <a href='https://arxiv.org/pdf/2212.08787.pdf' target='_blank'>https://arxiv.org/pdf/2212.08787.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiyu Huang, Haochen Liu, Jingda Wu, Chen Lv
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.08787">Conditional Predictive Behavior Planning with Inverse Reinforcement Learning for Human-like Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Making safe and human-like decisions is an essential capability of autonomous driving systems, and learning-based behavior planning presents a promising pathway toward achieving this objective. Distinguished from existing learning-based methods that directly output decisions, this work introduces a predictive behavior planning framework that learns to predict and evaluate from human driving data. This framework consists of three components: a behavior generation module that produces a diverse set of candidate behaviors in the form of trajectory proposals, a conditional motion prediction network that predicts future trajectories of other agents based on each proposal, and a scoring module that evaluates the candidate plans using maximum entropy inverse reinforcement learning (IRL). We validate the proposed framework on a large-scale real-world urban driving dataset through comprehensive experiments. The results show that the conditional prediction model can predict distinct and reasonable future trajectories given different trajectory proposals and the IRL-based scoring module can select plans that are close to human driving. The proposed framework outperforms other baseline methods in terms of similarity to human driving trajectories. Additionally, we find that the conditional prediction model improves both prediction and planning performance compared to the non-conditional model. Lastly, we note that learning the scoring module is crucial for aligning the evaluations with human drivers.
<div id='section'>Paperid: <span id='pid'>693, <a href='https://arxiv.org/pdf/2212.04636.pdf' target='_blank'>https://arxiv.org/pdf/2212.04636.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaman Li, C. Karen Liu, Jiajun Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.04636">Ego-Body Pose Estimation via Ego-Head Pose Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Estimating 3D human motion from an egocentric video sequence plays a critical role in human behavior understanding and has various applications in VR/AR. However, naively learning a mapping between egocentric videos and human motions is challenging, because the user's body is often unobserved by the front-facing camera placed on the head of the user. In addition, collecting large-scale, high-quality datasets with paired egocentric videos and 3D human motions requires accurate motion capture devices, which often limit the variety of scenes in the videos to lab-like environments. To eliminate the need for paired egocentric video and human motions, we propose a new method, Ego-Body Pose Estimation via Ego-Head Pose Estimation (EgoEgo), which decomposes the problem into two stages, connected by the head motion as an intermediate representation. EgoEgo first integrates SLAM and a learning approach to estimate accurate head motion. Subsequently, leveraging the estimated head pose as input, EgoEgo utilizes conditional diffusion to generate multiple plausible full-body motions. This disentanglement of head and body pose eliminates the need for training datasets with paired egocentric videos and 3D human motion, enabling us to leverage large-scale egocentric video datasets and motion capture datasets separately. Moreover, for systematic benchmarking, we develop a synthetic dataset, AMASS-Replica-Ego-Syn (ARES), with paired egocentric videos and human motion. On both ARES and real data, our EgoEgo model performs significantly better than the current state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>694, <a href='https://arxiv.org/pdf/2209.13204.pdf' target='_blank'>https://arxiv.org/pdf/2209.13204.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weiqiang Wang, Xuefei Zhe, Qiuhong Ke, Di Kang, Tingguang Li, Ruizhi Chen, Linchao Bao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.13204">NEURAL MARIONETTE: A Transformer-based Multi-action Human Motion Synthesis System</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a neural network-based system for long-term, multi-action human motion synthesis. The system, dubbed as NEURAL MARIONETTE, can produce high-quality and meaningful motions with smooth transitions from simple user input, including a sequence of action tags with expected action duration, and optionally a hand-drawn moving trajectory if the user specifies. The core of our system is a novel Transformer-based motion generation model, namely MARIONET, which can generate diverse motions given action tags. Different from existing motion generation models, MARIONET utilizes contextual information from the past motion clip and future action tag, dedicated to generating actions that can smoothly blend historical and future actions. Specifically, MARIONET first encodes target action tag and contextual information into an action-level latent code. The code is unfolded into frame-level control signals via a time unrolling module, which could be then combined with other frame-level control signals like the target trajectory. Motion frames are then generated in an auto-regressive way. By sequentially applying MARIONET, the system NEURAL MARIONETTE can robustly generate long-term, multi-action motions with the help of two simple schemes, namely "Shadow Start" and "Action Revision". Along with the novel system, we also present a new dataset dedicated to the multi-action motion synthesis task, which contains both action tags and their contextual information. Extensive experiments are conducted to study the action accuracy, naturalism, and transition smoothness of the motions generated by our system.
<div id='section'>Paperid: <span id='pid'>695, <a href='https://arxiv.org/pdf/2209.06314.pdf' target='_blank'>https://arxiv.org/pdf/2209.06314.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>James F. Mullen, Divya Kothandaraman, Aniket Bera, Dinesh Manocha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.06314">Placing Human Animations into 3D Scenes by Learning Interaction- and Geometry-Driven Keyframes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a novel method for placing a 3D human animation into a 3D scene while maintaining any human-scene interactions in the animation. We use the notion of computing the most important meshes in the animation for the interaction with the scene, which we call "keyframes." These keyframes allow us to better optimize the placement of the animation into the scene such that interactions in the animations (standing, laying, sitting, etc.) match the affordances of the scene (e.g., standing on the floor or laying in a bed). We compare our method, which we call PAAK, with prior approaches, including POSA, PROX ground truth, and a motion synthesis method, and highlight the benefits of our method with a perceptual study. Human raters preferred our PAAK method over the PROX ground truth data 64.6\% of the time. Additionally, in direct comparisons, the raters preferred PAAK over competing methods including 61.5\% compared to POSA.
<div id='section'>Paperid: <span id='pid'>696, <a href='https://arxiv.org/pdf/2207.10422.pdf' target='_blank'>https://arxiv.org/pdf/2207.10422.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiyu Huang, Haochen Liu, Jingda Wu, Chen Lv
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2207.10422">Differentiable Integrated Motion Prediction and Planning with Learnable Cost Function for Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Predicting the future states of surrounding traffic participants and planning a safe, smooth, and socially compliant trajectory accordingly is crucial for autonomous vehicles. There are two major issues with the current autonomous driving system: the prediction module is often separated from the planning module and the cost function for planning is hard to specify and tune. To tackle these issues, we propose a differentiable integrated prediction-planning framework (DIPP) that can also learn the cost function from data. Specifically, our framework uses a differentiable nonlinear optimizer as the motion planner, which takes as input the predicted trajectories of surrounding agents given by the neural network and optimizes the trajectory for the autonomous vehicle, enabling all operations to be differentiable, including the cost function weights. The proposed framework is trained on a large-scale real-world driving dataset to imitate human driving trajectories in the entire driving scene and validated in both open-loop and closed-loop manners. The open-loop testing results reveal that the proposed method outperforms the baseline methods across a variety of metrics and delivers planning-centric prediction results, allowing the planning module to output trajectories close to those of human drivers. In closed-loop testing, the proposed method outperforms various baseline methods, showing the ability to handle complex urban driving scenarios and robustness against the distributional shift. Importantly, we find that joint training of planning and prediction modules achieves better performance than planning with a separate trained prediction module in both open-loop and closed-loop tests. Moreover, the ablation study indicates that the learnable components in the framework are essential to ensure planning stability and performance.
<div id='section'>Paperid: <span id='pid'>697, <a href='https://arxiv.org/pdf/2507.16157.pdf' target='_blank'>https://arxiv.org/pdf/2507.16157.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weijia Peng, Mingtong Chen, Zhengbao Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.16157">Design and Optimization of Wearables for Human Motion Energy Harvesting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As wearable electronics become increasingly prevalent, there is a rise in interest and demand for sustainably designed systems that are also energy self-sufficient. The research described in this paper investigated a shoe-worn energy harvesting system designed use the mechanical energy from walking to output electrical energy. A spring is attached to electromagnetic generator embedded in the heel of the shoe to recover the vertical pressure caused by the foot strike. The simulated prototype consisted of a standard EM generator designed in MATLAB demonstrating a maximum voltage of 12V. The initial low fidelity prototype demonstrated testing the relationship between the EM generator and a simple electrical circuit, with energy output observed. Future research will explore enhancing the overall generator design, integrate a power management IC for battery protect and regulation, and combine the system into a final product, wearable footwear. This research lays a foundation for self-powered footwear and energy independent wearable electronic devices.
<div id='section'>Paperid: <span id='pid'>698, <a href='https://arxiv.org/pdf/2507.06233.pdf' target='_blank'>https://arxiv.org/pdf/2507.06233.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>InÃ¨s Hyeonsu Kim, Seokju Cho, Jahyeok Koo, Junghyun Park, Jiahui Huang, Joon-Young Lee, Seungryong Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06233">Learning to Track Any Points from Human Motion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion, with its inherent complexities, such as non-rigid deformations, articulated movements, clothing distortions, and frequent occlusions caused by limbs or other individuals, provides a rich and challenging source of supervision that is crucial for training robust and generalizable point trackers. Despite the suitability of human motion, acquiring extensive training data for point tracking remains difficult due to laborious manual annotation. Our proposed pipeline, AnthroTAP, addresses this by proposing an automated pipeline to generate pseudo-labeled training data, leveraging the Skinned Multi-Person Linear (SMPL) model. We first fit the SMPL model to detected humans in video frames, project the resulting 3D mesh vertices onto 2D image planes to generate pseudo-trajectories, handle occlusions using ray-casting, and filter out unreliable tracks based on optical flow consistency. A point tracking model trained on AnthroTAP annotated dataset achieves state-of-the-art performance on the TAP-Vid benchmark, surpassing other models trained on real videos while using 10,000 times less data and only 1 day in 4 GPUs, compared to 256 GPUs used in recent state-of-the-art.
<div id='section'>Paperid: <span id='pid'>699, <a href='https://arxiv.org/pdf/2505.19377.pdf' target='_blank'>https://arxiv.org/pdf/2505.19377.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zichong Meng, Zeyu Han, Xiaogang Peng, Yiming Xie, Huaizu Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.19377">Absolute Coordinates Make Motion Generation Easy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>State-of-the-art text-to-motion generation models rely on the kinematic-aware, local-relative motion representation popularized by HumanML3D, which encodes motion relative to the pelvis and to the previous frame with built-in redundancy. While this design simplifies training for earlier generation models, it introduces critical limitations for diffusion models and hinders applicability to downstream tasks. In this work, we revisit the motion representation and propose a radically simplified and long-abandoned alternative for text-to-motion generation: absolute joint coordinates in global space. Through systematic analysis of design choices, we show that this formulation achieves significantly higher motion fidelity, improved text alignment, and strong scalability, even with a simple Transformer backbone and no auxiliary kinematic-aware losses. Moreover, our formulation naturally supports downstream tasks such as text-driven motion control and temporal/spatial editing without additional task-specific reengineering and costly classifier guidance generation from control signals. Finally, we demonstrate promising generalization to directly generate SMPL-H mesh vertices in motion from text, laying a strong foundation for future research and motion-related applications.
<div id='section'>Paperid: <span id='pid'>700, <a href='https://arxiv.org/pdf/2505.13140.pdf' target='_blank'>https://arxiv.org/pdf/2505.13140.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Takahiro Maeda, Jinkun Cao, Norimichi Ukita, Kris Kitani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.13140">CacheFlow: Fast Human Motion Prediction by Cached Normalizing Flow</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Many density estimation techniques for 3D human motion prediction require a significant amount of inference time, often exceeding the duration of the predicted time horizon. To address the need for faster density estimation for 3D human motion prediction, we introduce a novel flow-based method for human motion prediction called CacheFlow. Unlike previous conditional generative models that suffer from time efficiency, CacheFlow takes advantage of an unconditional flow-based generative model that transforms a Gaussian mixture into the density of future motions. The results of the computation of the flow-based generative model can be precomputed and cached. Then, for conditional prediction, we seek a mapping from historical trajectories to samples in the Gaussian mixture. This mapping can be done by a much more lightweight model, thus saving significant computation overhead compared to a typical conditional flow model. In such a two-stage fashion and by caching results from the slow flow model computation, we build our CacheFlow without loss of prediction accuracy and model expressiveness. This inference process is completed in approximately one millisecond, making it 4 times faster than previous VAE methods and 30 times faster than previous diffusion-based methods on standard benchmarks such as Human3.6M and AMASS datasets. Furthermore, our method demonstrates improved density estimation accuracy and comparable prediction accuracy to a SOTA method on Human3.6M. Our code and models will be publicly available.
<div id='section'>Paperid: <span id='pid'>701, <a href='https://arxiv.org/pdf/2502.01357.pdf' target='_blank'>https://arxiv.org/pdf/2502.01357.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dong-In Kim, Dong-Hee Paek, Seung-Hyun Song, Seung-Hyun Kong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.01357">Bayesian Approximation-Based Trajectory Prediction and Tracking with 4D Radar</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate 3D multi-object tracking (MOT) is vital for autonomous vehicles, yet LiDAR and camera-based methods degrade in adverse weather. Meanwhile, Radar-based solutions remain robust but often suffer from limited vertical resolution and simplistic motion models. Existing Kalman filter-based approaches also rely on fixed noise covariance, hampering adaptability when objects make sudden maneuvers. We propose Bayes-4DRTrack, a 4D Radar-based MOT framework that adopts a transformer-based motion prediction network to capture nonlinear motion dynamics and employs Bayesian approximation in both detection and prediction steps. Moreover, our two-stage data association leverages Doppler measurements to better distinguish closely spaced targets. Evaluated on the K-Radar dataset (including adverse weather scenarios), Bayes-4DRTrack demonstrates a 5.7% gain in Average Multi-Object Tracking Accuracy (AMOTA) over methods with traditional motion models and fixed noise covariance. These results showcase enhanced robustness and accuracy in demanding, real-world conditions.
<div id='section'>Paperid: <span id='pid'>702, <a href='https://arxiv.org/pdf/2412.17487.pdf' target='_blank'>https://arxiv.org/pdf/2412.17487.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yizhe Li, Linrui Zhang, Xueqian Wang, Houde Liu, Bin Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.17487">DeepMF: Deep Motion Factorization for Closed-Loop Safety-Critical Driving Scenario Simulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Safety-critical traffic scenarios are of great practical relevance to evaluating the robustness of autonomous driving (AD) systems. Given that these long-tail events are extremely rare in real-world traffic data, there is a growing body of work dedicated to the automatic traffic scenario generation. However, nearly all existing algorithms for generating safety-critical scenarios rely on snippets of previously recorded traffic events, transforming normal traffic flow into accident-prone situations directly. In other words, safety-critical traffic scenario generation is hindsight and not applicable to newly encountered and open-ended traffic events.In this paper, we propose the Deep Motion Factorization (DeepMF) framework, which extends static safety-critical driving scenario generation to closed-loop and interactive adversarial traffic simulation. DeepMF casts safety-critical traffic simulation as a Bayesian factorization that includes the assignment of hazardous traffic participants, the motion prediction of selected opponents, the reaction estimation of autonomous vehicle (AV) and the probability estimation of the accident occur. All the aforementioned terms are calculated using decoupled deep neural networks, with inputs limited to the current observation and historical states. Consequently, DeepMF can effectively and efficiently simulate safety-critical traffic scenarios at any triggered time and for any duration by maximizing the compounded posterior probability of traffic risk. Extensive experiments demonstrate that DeepMF excels in terms of risk management, flexibility, and diversity, showcasing outstanding performance in simulating a wide range of realistic, high-risk traffic scenarios.
<div id='section'>Paperid: <span id='pid'>703, <a href='https://arxiv.org/pdf/2411.16575.pdf' target='_blank'>https://arxiv.org/pdf/2411.16575.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zichong Meng, Yiming Xie, Xiaogang Peng, Zeyu Han, Huaizu Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.16575">Rethinking Diffusion for Text-Driven Human Motion Generation: Redundant Representations, Evaluation, and Masked Autoregression</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Since 2023, Vector Quantization (VQ)-based discrete generation methods have rapidly dominated human motion generation, primarily surpassing diffusion-based continuous generation methods in standard performance metrics. However, VQ-based methods have inherent limitations. Representing continuous motion data as limited discrete tokens leads to inevitable information loss, reduces the diversity of generated motions, and restricts their ability to function effectively as motion priors or generation guidance. In contrast, the continuous space generation nature of diffusion-based methods makes them well-suited to address these limitations and with even potential for model scalability. In this work, we systematically investigate why current VQ-based methods perform well and explore the limitations of existing diffusion-based methods from the perspective of motion data representation and distribution. Drawing on these insights, we preserve the inherent strengths of a diffusion-based human motion generation model and gradually optimize it with inspiration from VQ-based approaches. Our approach introduces a human motion diffusion model enabled to perform masked autoregression, optimized with a reformed data representation and distribution. Additionally, we propose a more robust evaluation method to assess different approaches. Extensive experiments on various datasets demonstrate our method outperforms previous methods and achieves state-of-the-art performances.
<div id='section'>Paperid: <span id='pid'>704, <a href='https://arxiv.org/pdf/2410.13817.pdf' target='_blank'>https://arxiv.org/pdf/2410.13817.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jean-Pierre Sleiman, Mayank Mittal, Marco Hutter
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.13817">Guided Reinforcement Learning for Robust Multi-Contact Loco-Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reinforcement learning (RL) often necessitates a meticulous Markov Decision Process (MDP) design tailored to each task. This work aims to address this challenge by proposing a systematic approach to behavior synthesis and control for multi-contact loco-manipulation tasks, such as navigating spring-loaded doors and manipulating heavy dishwashers. We define a task-independent MDP to train RL policies using only a single demonstration per task generated from a model-based trajectory optimizer. Our approach incorporates an adaptive phase dynamics formulation to robustly track the demonstrations while accommodating dynamic uncertainties and external disturbances. We compare our method against prior motion imitation RL works and show that the learned policies achieve higher success rates across all considered tasks. These policies learn recovery maneuvers that are not present in the demonstration, such as re-grasping objects during execution or dealing with slippages. Finally, we successfully transfer the policies to a real robot, demonstrating the practical viability of our approach.
<div id='section'>Paperid: <span id='pid'>705, <a href='https://arxiv.org/pdf/2410.05186.pdf' target='_blank'>https://arxiv.org/pdf/2410.05186.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Filip NovÃ¡k, TomÃ¡Å¡ BÃ¡Äa, OndÅej ProchÃ¡zka, Martin Saska
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.05186">State estimation of marine vessels affected by waves by unmanned aerial vehicles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A novel approach for robust state estimation of marine vessels in rough water is proposed in this paper to enable tight collaboration between Unmanned Aerial Vehicles (UAVs) and a marine vessel, such as cooperative landing or object manipulation, regardless of weather conditions. Our study of marine vessel (in our case Unmanned Surface Vehicle (USV)) dynamics influenced by strong wave motion has resulted in a novel nonlinear mathematical USV model with 6 degrees of freedom (DOFs), which is required for precise USV state estimation and motion prediction. The proposed state estimation and prediction approach fuses data from multiple sensors onboard the UAV and the USV to enable redundancy and robustness under varying weather conditions of real-world applications. The proposed approach provides estimated states of the USV with 6 DOFs and predicts its future states to enable tight control of both vehicles on a receding control horizon. The proposed approach was extensively tested in the realistic Gazebo simulator and successfully experimentally validated in many real-world experiments representing different application scenarios, including agile landing on an oscillating and moving USV. A comparative study indicates that the proposed approach significantly surpassed the current state-of-the-art.
<div id='section'>Paperid: <span id='pid'>706, <a href='https://arxiv.org/pdf/2409.02634.pdf' target='_blank'>https://arxiv.org/pdf/2409.02634.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianwen Jiang, Chao Liang, Jiaqi Yang, Gaojie Lin, Tianyun Zhong, Yanbo Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.02634">Loopy: Taming Audio-Driven Portrait Avatar with Long-Term Motion Dependency</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the introduction of diffusion-based video generation techniques, audio-conditioned human video generation has recently achieved significant breakthroughs in both the naturalness of motion and the synthesis of portrait details. Due to the limited control of audio signals in driving human motion, existing methods often add auxiliary spatial signals to stabilize movements, which may compromise the naturalness and freedom of motion. In this paper, we propose an end-to-end audio-only conditioned video diffusion model named Loopy. Specifically, we designed an inter- and intra-clip temporal module and an audio-to-latents module, enabling the model to leverage long-term motion information from the data to learn natural motion patterns and improving audio-portrait movement correlation. This method removes the need for manually specified spatial motion templates used in existing methods to constrain motion during inference. Extensive experiments show that Loopy outperforms recent audio-driven portrait diffusion models, delivering more lifelike and high-quality results across various scenarios.
<div id='section'>Paperid: <span id='pid'>707, <a href='https://arxiv.org/pdf/2408.15250.pdf' target='_blank'>https://arxiv.org/pdf/2408.15250.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kleio Fragkedaki, Frank J. Jiang, Karl H. Johansson, Jonas MÃ¥rtensson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.15250">Pedestrian Motion Prediction Using Transformer-based Behavior Clustering and Data-Driven Reachability Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we present a transformer-based framework for predicting future pedestrian states based on clustered historical trajectory data. In previous studies, researchers propose enhancing pedestrian trajectory predictions by using manually crafted labels to categorize pedestrian behaviors and intentions. However, these approaches often only capture a limited range of pedestrian behaviors and introduce human bias into the predictions. To alleviate the dependency on manually crafted labels, we utilize a transformer encoder coupled with hierarchical density-based clustering to automatically identify diverse behavior patterns, and use these clusters in data-driven reachability analysis. By using a transformer-based approach, we seek to enhance the representation of pedestrian trajectories and uncover characteristics or features that are subsequently used to group trajectories into different "behavior" clusters. We show that these behavior clusters can be used with data-driven reachability analysis, yielding an end-to-end data-driven approach to predicting the future motion of pedestrians. We train and evaluate our approach on a real pedestrian dataset, showcasing its effectiveness in forecasting pedestrian movements.
<div id='section'>Paperid: <span id='pid'>708, <a href='https://arxiv.org/pdf/2408.08202.pdf' target='_blank'>https://arxiv.org/pdf/2408.08202.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiao Han, Yiming Ren, Yichen Yao, Yujing Sun, Yuexin Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.08202">Towards Practical Human Motion Prediction with LiDAR Point Clouds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion prediction is crucial for human-centric multimedia understanding and interacting. Current methods typically rely on ground truth human poses as observed input, which is not practical for real-world scenarios where only raw visual sensor data is available. To implement these methods in practice, a pre-phrase of pose estimation is essential. However, such two-stage approaches often lead to performance degradation due to the accumulation of errors. Moreover, reducing raw visual data to sparse keypoint representations significantly diminishes the density of information, resulting in the loss of fine-grained features. In this paper, we propose \textit{LiDAR-HMP}, the first single-LiDAR-based 3D human motion prediction approach, which receives the raw LiDAR point cloud as input and forecasts future 3D human poses directly. Building upon our novel structure-aware body feature descriptor, LiDAR-HMP adaptively maps the observed motion manifold to future poses and effectively models the spatial-temporal correlations of human motions for further refinement of prediction results. Extensive experiments show that our method achieves state-of-the-art performance on two public benchmarks and demonstrates remarkable robustness and efficacy in real-world deployments.
<div id='section'>Paperid: <span id='pid'>709, <a href='https://arxiv.org/pdf/2407.12783.pdf' target='_blank'>https://arxiv.org/pdf/2407.12783.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lei Zhong, Yiming Xie, Varun Jampani, Deqing Sun, Huaizu Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.12783">SMooDi: Stylized Motion Diffusion Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a novel Stylized Motion Diffusion model, dubbed SMooDi, to generate stylized motion driven by content texts and style motion sequences. Unlike existing methods that either generate motion of various content or transfer style from one sequence to another, SMooDi can rapidly generate motion across a broad range of content and diverse styles. To this end, we tailor a pre-trained text-to-motion model for stylization. Specifically, we propose style guidance to ensure that the generated motion closely matches the reference style, alongside a lightweight style adaptor that directs the motion towards the desired style while ensuring realism. Experiments across various applications demonstrate that our proposed framework outperforms existing methods in stylized motion generation.
<div id='section'>Paperid: <span id='pid'>710, <a href='https://arxiv.org/pdf/2407.09833.pdf' target='_blank'>https://arxiv.org/pdf/2407.09833.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiming Ren, Xiao Han, Yichen Yao, Xiaoxiao Long, Yujing Sun, Yuexin Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.09833">LiveHPS++: Robust and Coherent Motion Capture in Dynamic Free Environment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>LiDAR-based human motion capture has garnered significant interest in recent years for its practicability in large-scale and unconstrained environments. However, most methods rely on cleanly segmented human point clouds as input, the accuracy and smoothness of their motion results are compromised when faced with noisy data, rendering them unsuitable for practical applications. To address these limitations and enhance the robustness and precision of motion capture with noise interference, we introduce LiveHPS++, an innovative and effective solution based on a single LiDAR system. Benefiting from three meticulously designed modules, our method can learn dynamic and kinematic features from human movements, and further enable the precise capture of coherent human motions in open settings, making it highly applicable to real-world scenarios. Through extensive experiments, LiveHPS++ has proven to significantly surpass existing state-of-the-art methods across various datasets, establishing a new benchmark in the field.
<div id='section'>Paperid: <span id='pid'>711, <a href='https://arxiv.org/pdf/2407.08680.pdf' target='_blank'>https://arxiv.org/pdf/2407.08680.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zujin Guo, Wei Li, Chen Change Loy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.08680">Generalizable Implicit Motion Modeling for Video Frame Interpolation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion modeling is critical in flow-based Video Frame Interpolation (VFI). Existing paradigms either consider linear combinations of bidirectional flows or directly predict bilateral flows for given timestamps without exploring favorable motion priors, thus lacking the capability of effectively modeling spatiotemporal dynamics in real-world videos. To address this limitation, in this study, we introduce Generalizable Implicit Motion Modeling (GIMM), a novel and effective approach to motion modeling for VFI. Specifically, to enable GIMM as an effective motion modeling paradigm, we design a motion encoding pipeline to model spatiotemporal motion latent from bidirectional flows extracted from pre-trained flow estimators, effectively representing input-specific motion priors. Then, we implicitly predict arbitrary-timestep optical flows within two adjacent input frames via an adaptive coordinate-based neural network, with spatiotemporal coordinates and motion latent as inputs. Our GIMM can be easily integrated with existing flow-based VFI works by supplying accurately modeled motion. We show that GIMM performs better than the current state of the art on standard VFI benchmarks.
<div id='section'>Paperid: <span id='pid'>712, <a href='https://arxiv.org/pdf/2407.05712.pdf' target='_blank'>https://arxiv.org/pdf/2407.05712.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianwen Jiang, Gaojie Lin, Zhengkun Rong, Chao Liang, Yongming Zhu, Jiaqi Yang, Tianyun Zhong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.05712">MobilePortrait: Real-Time One-Shot Neural Head Avatars on Mobile Devices</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing neural head avatars methods have achieved significant progress in the image quality and motion range of portrait animation. However, these methods neglect the computational overhead, and to the best of our knowledge, none is designed to run on mobile devices. This paper presents MobilePortrait, a lightweight one-shot neural head avatars method that reduces learning complexity by integrating external knowledge into both the motion modeling and image synthesis, enabling real-time inference on mobile devices. Specifically, we introduce a mixed representation of explicit and implicit keypoints for precise motion modeling and precomputed visual features for enhanced foreground and background synthesis. With these two key designs and using simple U-Nets as backbones, our method achieves state-of-the-art performance with less than one-tenth the computational demand. It has been validated to reach speeds of over 100 FPS on mobile devices and support both video and audio-driven inputs.
<div id='section'>Paperid: <span id='pid'>713, <a href='https://arxiv.org/pdf/2407.02633.pdf' target='_blank'>https://arxiv.org/pdf/2407.02633.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiming Hu, Zheming Yin, Daniel Haeufle, Syn Schmitt, Andreas Bulling
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.02633">HOIMotion: Forecasting Human Motion During Human-Object Interactions Using Egocentric 3D Object Bounding Boxes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present HOIMotion - a novel approach for human motion forecasting during human-object interactions that integrates information about past body poses and egocentric 3D object bounding boxes. Human motion forecasting is important in many augmented reality applications but most existing methods have only used past body poses to predict future motion. HOIMotion first uses an encoder-residual graph convolutional network (GCN) and multi-layer perceptrons to extract features from body poses and egocentric 3D object bounding boxes, respectively. Our method then fuses pose and object features into a novel pose-object graph and uses a residual-decoder GCN to forecast future body motion. We extensively evaluate our method on the Aria digital twin (ADT) and MoGaze datasets and show that HOIMotion consistently outperforms state-of-the-art methods by a large margin of up to 8.7% on ADT and 7.2% on MoGaze in terms of mean per joint position error. Complementing these evaluations, we report a human study (N=20) that shows that the improvements achieved by our method result in forecasted poses being perceived as both more precise and more realistic than those of existing methods. Taken together, these results reveal the significant information content available in egocentric 3D object bounding boxes for human motion forecasting and the effectiveness of our method in exploiting this information.
<div id='section'>Paperid: <span id='pid'>714, <a href='https://arxiv.org/pdf/2404.15383.pdf' target='_blank'>https://arxiv.org/pdf/2404.15383.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Markos Diomataris, Nikos Athanasiou, Omid Taheri, Xi Wang, Otmar Hilliges, Michael J. Black
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.15383">WANDR: Intention-guided Human Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Synthesizing natural human motions that enable a 3D human avatar to walk and reach for arbitrary goals in 3D space remains an unsolved problem with many applications. Existing methods (data-driven or using reinforcement learning) are limited in terms of generalization and motion naturalness. A primary obstacle is the scarcity of training data that combines locomotion with goal reaching. To address this, we introduce WANDR, a data-driven model that takes an avatar's initial pose and a goal's 3D position and generates natural human motions that place the end effector (wrist) on the goal location. To solve this, we introduce novel intention features that drive rich goal-oriented movement. Intention guides the agent to the goal, and interactively adapts the generation to novel situations without needing to define sub-goals or the entire motion path. Crucially, intention allows training on datasets that have goal-oriented motions as well as those that do not. WANDR is a conditional Variational Auto-Encoder (c-VAE), which we train using the AMASS and CIRCLE datasets. We evaluate our method extensively and demonstrate its ability to generate natural and long-term motions that reach 3D goals and generalize to unseen goal locations. Our models and code are available for research purposes at wandr.is.tue.mpg.de.
<div id='section'>Paperid: <span id='pid'>715, <a href='https://arxiv.org/pdf/2404.01700.pdf' target='_blank'>https://arxiv.org/pdf/2404.01700.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Biao Jiang, Xin Chen, Chi Zhang, Fukun Yin, Zhuoyuan Li, Gang YU, Jiayuan Fan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.01700">MotionChain: Conversational Motion Controllers via Multimodal Prompts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in language models have demonstrated their adeptness in conducting multi-turn dialogues and retaining conversational context. However, this proficiency remains largely unexplored in other multimodal generative models, particularly in human motion models. By integrating multi-turn conversations in controlling continuous virtual human movements, generative human motion models can achieve an intuitive and step-by-step process of human task execution for humanoid robotics, game agents, or other embodied systems. In this work, we present MotionChain, a conversational human motion controller to generate continuous and long-term human motion through multimodal prompts. Specifically, MotionChain consists of multi-modal tokenizers that transform various data types such as text, image, and motion, into discrete tokens, coupled with a Vision-Motion-aware Language model. By leveraging large-scale language, vision-language, and vision-motion data to assist motion-related generation tasks, MotionChain thus comprehends each instruction in multi-turn conversation and generates human motions followed by these prompts. Extensive experiments validate the efficacy of MotionChain, demonstrating state-of-the-art performance in conversational motion generation, as well as more intuitive manners of controlling and interacting with virtual humans.
<div id='section'>Paperid: <span id='pid'>716, <a href='https://arxiv.org/pdf/2403.19417.pdf' target='_blank'>https://arxiv.org/pdf/2403.19417.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinyu Zhan, Lixin Yang, Yifei Zhao, Kangrui Mao, Hanlin Xu, Zenan Lin, Kailin Li, Cewu Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.19417">OAKINK2: A Dataset of Bimanual Hands-Object Manipulation in Complex Task Completion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present OAKINK2, a dataset of bimanual object manipulation tasks for complex daily activities. In pursuit of constructing the complex tasks into a structured representation, OAKINK2 introduces three level of abstraction to organize the manipulation tasks: Affordance, Primitive Task, and Complex Task. OAKINK2 features on an object-centric perspective for decoding the complex tasks, treating them as a sequence of object affordance fulfillment. The first level, Affordance, outlines the functionalities that objects in the scene can afford, the second level, Primitive Task, describes the minimal interaction units that humans interact with the object to achieve its affordance, and the third level, Complex Task, illustrates how Primitive Tasks are composed and interdependent. OAKINK2 dataset provides multi-view image streams and precise pose annotations for the human body, hands and various interacting objects. This extensive collection supports applications such as interaction reconstruction and motion synthesis. Based on the 3-level abstraction of OAKINK2, we explore a task-oriented framework for Complex Task Completion (CTC). CTC aims to generate a sequence of bimanual manipulation to achieve task objectives. Within the CTC framework, we employ Large Language Models (LLMs) to decompose the complex task objectives into sequences of Primitive Tasks and have developed a Motion Fulfillment Model that generates bimanual hand motion for each Primitive Task. OAKINK2 datasets and models are available at https://oakink.net/v2.
<div id='section'>Paperid: <span id='pid'>717, <a href='https://arxiv.org/pdf/2403.09885.pdf' target='_blank'>https://arxiv.org/pdf/2403.09885.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiming Hu, Syn Schmitt, Daniel Haeufle, Andreas Bulling
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.09885">GazeMotion: Gaze-guided Human Motion Forecasting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present GazeMotion, a novel method for human motion forecasting that combines information on past human poses with human eye gaze. Inspired by evidence from behavioural sciences showing that human eye and body movements are closely coordinated, GazeMotion first predicts future eye gaze from past gaze, then fuses predicted future gaze and past poses into a gaze-pose graph, and finally uses a residual graph convolutional network to forecast body motion. We extensively evaluate our method on the MoGaze, ADT, and GIMO benchmark datasets and show that it outperforms state-of-the-art methods by up to 7.4% improvement in mean per joint position error. Using head direction as a proxy to gaze, our method still achieves an average improvement of 5.5%. We finally report an online user study showing that our method also outperforms prior methods in terms of perceived realism. These results show the significant information content available in eye gaze for human motion forecasting as well as the effectiveness of our method in exploiting this information.
<div id='section'>Paperid: <span id='pid'>718, <a href='https://arxiv.org/pdf/2403.06828.pdf' target='_blank'>https://arxiv.org/pdf/2403.06828.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruihua Han, Shuai Wang, Shuaijun Wang, Zeqing Zhang, Jianjun Chen, Shijie Lin, Chengyang Li, Chengzhong Xu, Yonina C. Eldar, Qi Hao, Jia Pan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.06828">NeuPAN: Direct Point Robot Navigation with End-to-End Model-based Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Navigating a nonholonomic robot in a cluttered, unknown environment requires accurate perception and precise motion control for real-time collision avoidance. This paper presents NeuPAN: a real-time, highly accurate, map-free, easy-to-deploy, and environment-invariant robot motion planner. Leveraging a tightly coupled perception-to-control framework, NeuPAN has two key innovations compared to existing approaches: 1) it directly maps raw point cloud data to a latent distance feature space for collision-free motion generation, avoiding error propagation from the perception to control pipeline; 2) it is interpretable from an end-to-end model-based learning perspective. The crux of NeuPAN is solving an end-to-end mathematical model with numerous point-level constraints using a plug-and-play (PnP) proximal alternating-minimization network (PAN), incorporating neurons in the loop. This allows NeuPAN to generate real-time, physically interpretable motions. It seamlessly integrates data and knowledge engines, and its network parameters can be fine-tuned via backpropagation. We evaluate NeuPAN on a ground mobile robot, a wheel-legged robot, and an autonomous vehicle, in extensive simulated and real-world environments. Results demonstrate that NeuPAN outperforms existing baselines in terms of accuracy, efficiency, robustness, and generalization capabilities across various environments, including the cluttered sandbox, office, corridor, and parking lot. We show that NeuPAN works well in unknown and unstructured environments with arbitrarily shaped objects, transforming impassable paths into passable ones.
<div id='section'>Paperid: <span id='pid'>719, <a href='https://arxiv.org/pdf/2403.05972.pdf' target='_blank'>https://arxiv.org/pdf/2403.05972.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianwen Li, Hyunsang Park, Wenjian Hao, Lei Xin, Jalil Chavez-Galaviz, Ajinkya Chaudhary, Meredith Bloss, Kyle Pattison, Christopher Vo, Devesh Upadhyay, Shreyas Sundaram, Shaoshuai Mou, Nina Mahmoudian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.05972">C3D: Cascade Control with Change Point Detection and Deep Koopman Learning for Autonomous Surface Vehicles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we discuss the development and deployment of a robust autonomous system capable of performing various tasks in the maritime domain under unknown dynamic conditions. We investigate a data-driven approach based on modular design for ease of transfer of autonomy across different maritime surface vessel platforms. The data-driven approach alleviates issues related to a priori identification of system models that may become deficient under evolving system behaviors or shifting, unanticipated, environmental influences. Our proposed learning-based platform comprises a deep Koopman system model and a change point detector that provides guidance on domain shifts prompting relearning under severe exogenous and endogenous perturbations. Motion control of the autonomous system is achieved via an optimal controller design. The Koopman linearized model naturally lends itself to a linear-quadratic regulator (LQR) control design. We propose the C3D control architecture Cascade Control with Change Point Detection and Deep Koopman Learning. The framework is verified in station keeping task on an ASV in both simulation and real experiments. The approach achieved at least 13.9 percent improvement in mean distance error in all test cases compared to the methods that do not consider system changes.
<div id='section'>Paperid: <span id='pid'>720, <a href='https://arxiv.org/pdf/2312.12090.pdf' target='_blank'>https://arxiv.org/pdf/2312.12090.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haodong Yan, Zhiming Hu, Syn Schmitt, Andreas Bulling
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.12090">GazeMoDiff: Gaze-guided Diffusion Model for Stochastic Human Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion prediction is important for many virtual and augmented reality (VR/AR) applications such as collision avoidance and realistic avatar generation. Existing methods have synthesised body motion only from observed past motion, despite the fact that human eye gaze is known to correlate strongly with body movements and is readily available in recent VR/AR headsets. We present GazeMoDiff - a novel gaze-guided denoising diffusion model to generate stochastic human motions. Our method first uses a gaze encoder and a motion encoder to extract the gaze and motion features respectively, then employs a graph attention network to fuse these features, and finally injects the gaze-motion features into a noise prediction network via a cross-attention mechanism to progressively generate multiple reasonable human motions in the future. Extensive experiments on the MoGaze and GIMO datasets demonstrate that our method outperforms the state-of-the-art methods by a large margin in terms of multi-modal final displacement error (17.3% on MoGaze and 13.3% on GIMO). We further conducted a human study (N=21) and validated that the motions generated by our method were perceived as both more precise and more realistic than those of prior methods. Taken together, these results reveal the significant information content available in eye gaze for stochastic human motion prediction as well as the effectiveness of our method in exploiting this information.
<div id='section'>Paperid: <span id='pid'>721, <a href='https://arxiv.org/pdf/2310.11284.pdf' target='_blank'>https://arxiv.org/pdf/2310.11284.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruibo Li, Chi Zhang, Zhe Wang, Chunhua Shen, Guosheng Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.11284">Self-Supervised 3D Scene Flow Estimation and Motion Prediction using Local Rigidity Prior</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this article, we investigate self-supervised 3D scene flow estimation and class-agnostic motion prediction on point clouds. A realistic scene can be well modeled as a collection of rigidly moving parts, therefore its scene flow can be represented as a combination of the rigid motion of these individual parts. Building upon this observation, we propose to generate pseudo scene flow labels for self-supervised learning through piecewise rigid motion estimation, in which the source point cloud is decomposed into local regions and each region is treated as rigid. By rigidly aligning each region with its potential counterpart in the target point cloud, we obtain a region-specific rigid transformation to generate its pseudo flow labels. To mitigate the impact of potential outliers on label generation, when solving the rigid registration for each region, we alternately perform three steps: establishing point correspondences, measuring the confidence for the correspondences, and updating the rigid transformation based on the correspondences and their confidence. As a result, confident correspondences will dominate label generation and a validity mask will be derived for the generated pseudo labels. By using the pseudo labels together with their validity mask for supervision, models can be trained in a self-supervised manner. Extensive experiments on FlyingThings3D and KITTI datasets demonstrate that our method achieves new state-of-the-art performance in self-supervised scene flow learning, without any ground truth scene flow for supervision, even performing better than some supervised counterparts. Additionally, our method is further extended to class-agnostic motion prediction and significantly outperforms previous state-of-the-art self-supervised methods on nuScenes dataset.
<div id='section'>Paperid: <span id='pid'>722, <a href='https://arxiv.org/pdf/2310.08580.pdf' target='_blank'>https://arxiv.org/pdf/2310.08580.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiming Xie, Varun Jampani, Lei Zhong, Deqing Sun, Huaizu Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.08580">OmniControl: Control Any Joint at Any Time for Human Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a novel approach named OmniControl for incorporating flexible spatial control signals into a text-conditioned human motion generation model based on the diffusion process. Unlike previous methods that can only control the pelvis trajectory, OmniControl can incorporate flexible spatial control signals over different joints at different times with only one model. Specifically, we propose analytic spatial guidance that ensures the generated motion can tightly conform to the input control signals. At the same time, realism guidance is introduced to refine all the joints to generate more coherent motion. Both the spatial and realism guidance are essential and they are highly complementary for balancing control accuracy and motion realism. By combining them, OmniControl generates motions that are realistic, coherent, and consistent with the spatial constraints. Experiments on HumanML3D and KIT-ML datasets show that OmniControl not only achieves significant improvement over state-of-the-art methods on pelvis control but also shows promising results when incorporating the constraints over other joints.
<div id='section'>Paperid: <span id='pid'>723, <a href='https://arxiv.org/pdf/2310.04582.pdf' target='_blank'>https://arxiv.org/pdf/2310.04582.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhengyi Luo, Jinkun Cao, Josh Merel, Alexander Winkler, Jing Huang, Kris Kitani, Weipeng Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.04582">Universal Humanoid Motion Representations for Physics-Based Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a universal motion representation that encompasses a comprehensive range of motor skills for physics-based humanoid control. Due to the high dimensionality of humanoids and the inherent difficulties in reinforcement learning, prior methods have focused on learning skill embeddings for a narrow range of movement styles (e.g. locomotion, game characters) from specialized motion datasets. This limited scope hampers their applicability in complex tasks. We close this gap by significantly increasing the coverage of our motion representation space. To achieve this, we first learn a motion imitator that can imitate all of human motion from a large, unstructured motion dataset. We then create our motion representation by distilling skills directly from the imitator. This is achieved by using an encoder-decoder structure with a variational information bottleneck. Additionally, we jointly learn a prior conditioned on proprioception (humanoid's own pose and velocities) to improve model expressiveness and sampling efficiency for downstream tasks. By sampling from the prior, we can generate long, stable, and diverse human motions. Using this latent space for hierarchical RL, we show that our policies solve tasks using human-like behavior. We demonstrate the effectiveness of our motion representation by solving generative tasks (e.g. strike, terrain traversal) and motion tracking using VR controllers.
<div id='section'>Paperid: <span id='pid'>724, <a href='https://arxiv.org/pdf/2309.09222.pdf' target='_blank'>https://arxiv.org/pdf/2309.09222.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jian Xu, Shian Du, Junmei Yang, Xinghao Ding, John Paisley, Delu Zeng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.09222">Bayesian Gaussian Process ODEs via Double Normalizing Flows</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, Gaussian processes have been used to model the vector field of continuous dynamical systems, referred to as GPODEs, which are characterized by a probabilistic ODE equation. Bayesian inference for these models has been extensively studied and applied in tasks such as time series prediction. However, the use of standard GPs with basic kernels like squared exponential kernels has been common in GPODE research, limiting the model's ability to represent complex scenarios. To address this limitation, we introduce normalizing flows to reparameterize the ODE vector field, resulting in a data-driven prior distribution, thereby increasing flexibility and expressive power. We develop a data-driven variational learning algorithm that utilizes analytically tractable probability density functions of normalizing flows, enabling simultaneous learning and inference of unknown continuous dynamics. Additionally, we also apply normalizing flows to the posterior inference of GP ODEs to resolve the issue of strong mean-field assumptions in posterior inference. By applying normalizing flows in both these ways, our model improves accuracy and uncertainty estimates for Bayesian Gaussian Process ODEs. We validate the effectiveness of our approach on simulated dynamical systems and real-world human motion data, including time series prediction and missing data recovery tasks. Experimental results show that our proposed method effectively captures model uncertainty while improving accuracy.
<div id='section'>Paperid: <span id='pid'>725, <a href='https://arxiv.org/pdf/2305.06456.pdf' target='_blank'>https://arxiv.org/pdf/2305.06456.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhengyi Luo, Jinkun Cao, Alexander Winkler, Kris Kitani, Weipeng Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.06456">Perpetual Humanoid Control for Real-time Simulated Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a physics-based humanoid controller that achieves high-fidelity motion imitation and fault-tolerant behavior in the presence of noisy input (e.g. pose estimates from video or generated from language) and unexpected falls. Our controller scales up to learning ten thousand motion clips without using any external stabilizing forces and learns to naturally recover from fail-state. Given reference motion, our controller can perpetually control simulated avatars without requiring resets. At its core, we propose the progressive multiplicative control policy (PMCP), which dynamically allocates new network capacity to learn harder and harder motion sequences. PMCP allows efficient scaling for learning from large-scale motion databases and adding new tasks, such as fail-state recovery, without catastrophic forgetting. We demonstrate the effectiveness of our controller by using it to imitate noisy poses from video-based pose estimators and language-based motion generators in a live and real-time multi-person avatar use case.
<div id='section'>Paperid: <span id='pid'>726, <a href='https://arxiv.org/pdf/2303.18119.pdf' target='_blank'>https://arxiv.org/pdf/2303.18119.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Luca Fortini, Mattia Leonori, Juan M. Gandarias, Elena de Momi, Arash Ajoudani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.18119">Markerless 3D human pose tracking through multiple cameras and AI: Enabling high accuracy, robustness, and real-time performance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tracking 3D human motion in real-time is crucial for numerous applications across many fields. Traditional approaches involve attaching artificial fiducial objects or sensors to the body, limiting their usability and comfort-of-use and consequently narrowing their application fields. Recent advances in Artificial Intelligence (AI) have allowed for markerless solutions. However, most of these methods operate in 2D, while those providing 3D solutions compromise accuracy and real-time performance. To address this challenge and unlock the potential of visual pose estimation methods in real-world scenarios, we propose a markerless framework that combines multi-camera views and 2D AI-based pose estimation methods to track 3D human motion. Our approach integrates a Weighted Least Square (WLS) algorithm that computes 3D human motion from multiple 2D pose estimations provided by an AI-driven method. The method is integrated within the Open-VICO framework allowing simulation and real-world execution. Several experiments have been conducted, which have shown high accuracy and real-time performance, demonstrating the high level of readiness for real-world applications and the potential to revolutionize human motion capture.
<div id='section'>Paperid: <span id='pid'>727, <a href='https://arxiv.org/pdf/2203.03927.pdf' target='_blank'>https://arxiv.org/pdf/2203.03927.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanbo Chen, Zhengzhe Xu, Zhuozhu Jian, Gengpan Tang, Yunong Yangli, Anxing Xiao, Xueqian Wang, Bin Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2203.03927">Quadruped Guidance Robot for the Visually Impaired: A Comfort-Based Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Guidance robots that can guide people and avoid various obstacles, could potentially be owned by more visually impaired people at a fairly low cost. Most of the previous guidance robots for the visually impaired ignored the human response behavior and comfort, treating the human as an appendage dragged by the robot, which can lead to imprecise guidance of the human and sudden changes in the traction force experienced by the human. In this paper, we propose a novel quadruped guidance robot system with a comfort-based concept. We design a controllable traction device that can adjust the length and force between human and robot to ensure comfort. To allow the human to be guided safely and comfortably to the target position in complex environments, our proposed human motion planner can plan the traction force with the force-based human motion model. To track the planned force, we also propose a robot motion planner that can generate the specific robot motion command and design the force control device. Our system has been deployed on Unitree Laikago quadrupedal platform and validated in real-world scenarios.
<div id='section'>Paperid: <span id='pid'>728, <a href='https://arxiv.org/pdf/2107.09621.pdf' target='_blank'>https://arxiv.org/pdf/2107.09621.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guoliang Li, Shuai Wang, Jie Li, Rui Wang, Fan Liu, Xiaohui Peng, Tony Xiao Han, Chengzhong Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2107.09621">Integrated Sensing and Communication from Learning Perspective: An SDP3 Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Characterizing the sensing and communication performance tradeoff in integrated sensing and communication (ISAC) systems is challenging in the applications of learning-based human motion recognition. This is because of the large experimental datasets and the black-box nature of deep neural networks. This paper presents SDP3, a Simulation-Driven Performance Predictor and oPtimizer, which consists of SDP3 data simulator, SDP3 performance predictor and SDP3 performance optimizer. Specifically, the SDP3 data simulator generates vivid wireless sensing datasets in a virtual environment, the SDP3 performance predictor predicts the sensing performance based on the function regression method, and the SDP3 performance optimizer investigates the sensing and communication performance tradeoff analytically. It is shown that the simulated sensing dataset matches the experimental dataset very well in the motion recognition accuracy. By leveraging SDP3, it is found that the achievable region of recognition accuracy and communication throughput consists of a communication saturation zone, a sensing saturation zone, and a communication-sensing adversarial zone, of which the desired balanced performance for ISAC systems lies in the third one.
<div id='section'>Paperid: <span id='pid'>729, <a href='https://arxiv.org/pdf/2104.04391.pdf' target='_blank'>https://arxiv.org/pdf/2104.04391.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohsen Zand, Ali Etemad, Michael Greenspan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2104.04391">Flow-based Spatio-Temporal Structured Prediction of Motion Dynamics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Conditional Normalizing Flows (CNFs) are flexible generative models capable of representing complicated distributions with high dimensionality and large interdimensional correlations, making them appealing for structured output learning. Their effectiveness in modelling multivariates spatio-temporal structured data has yet to be completely investigated. We propose MotionFlow as a novel normalizing flows approach that autoregressively conditions the output distributions on the spatio-temporal input features. It combines deterministic and stochastic representations with CNFs to create a probabilistic neural generative approach that can model the variability seen in high dimensional structured spatio-temporal data. We specifically propose to use conditional priors to factorize the latent space for the time dependent modeling. We also exploit the use of masked convolutions as autoregressive conditionals in CNFs. As a result, our method is able to define arbitrarily expressive output probability distributions under temporal dynamics in multivariate prediction tasks. We apply our method to different tasks, including trajectory prediction, motion prediction, time series forecasting, and binary segmentation, and demonstrate that our model is able to leverage normalizing flows to learn complicated time dependent conditional distributions.
<div id='section'>Paperid: <span id='pid'>730, <a href='https://arxiv.org/pdf/2103.10534.pdf' target='_blank'>https://arxiv.org/pdf/2103.10534.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mayank Mittal, David Hoeller, Farbod Farshidian, Marco Hutter, Animesh Garg
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2103.10534">Articulated Object Interaction in Unknown Scenes with Whole-Body Mobile Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A kitchen assistant needs to operate human-scale objects, such as cabinets and ovens, in unmapped environments with dynamic obstacles. Autonomous interactions in such environments require integrating dexterous manipulation and fluid mobility. While mobile manipulators in different form factors provide an extended workspace, their real-world adoption has been limited. Executing a high-level task for general objects requires a perceptual understanding of the object as well as adaptive whole-body control among dynamic obstacles. In this paper, we propose a two-stage architecture for autonomous interaction with large articulated objects in unknown environments. The first stage, object-centric planner, only focuses on the object to provide an action-conditional sequence of states for manipulation using RGB-D data. The second stage, agent-centric planner, formulates the whole-body motion control as an optimal control problem that ensures safe tracking of the generated plan, even in scenes with moving obstacles. We show that the proposed pipeline can handle complex static and dynamic kitchen settings for both wheel-based and legged mobile manipulators. Compared to other agent-centric planners, our proposed planner achieves a higher success rate and a lower execution time. We also perform hardware tests on a legged mobile manipulator to interact with various articulated objects in a kitchen. For additional material, please check: www.pair.toronto.edu/articulated-mm/.
<div id='section'>Paperid: <span id='pid'>731, <a href='https://arxiv.org/pdf/2508.20812.pdf' target='_blank'>https://arxiv.org/pdf/2508.20812.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lorenzo Busellato, Federico Cunico, Diego Dall'Alba, Marco Emporio, Andrea Giachetti, Riccardo Muradore, Marco Cristani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.20812">Uncertainty Aware-Predictive Control Barrier Functions: Safer Human Robot Interaction through Probabilistic Motion Forecasting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To enable flexible, high-throughput automation in settings where people and robots share workspaces, collaborative robotic cells must reconcile stringent safety guarantees with the need for responsive and effective behavior. A dynamic obstacle is the stochastic, task-dependent variability of human motion: when robots fall back on purely reactive or worst-case envelopes, they brake unnecessarily, stall task progress, and tamper with the fluidity that true Human-Robot Interaction demands. In recent years, learning-based human-motion prediction has rapidly advanced, although most approaches produce worst-case scenario forecasts that often do not treat prediction uncertainty in a well-structured way, resulting in over-conservative planning algorithms, limiting their flexibility. We introduce Uncertainty-Aware Predictive Control Barrier Functions (UA-PCBFs), a unified framework that fuses probabilistic human hand motion forecasting with the formal safety guarantees of Control Barrier Functions. In contrast to other variants, our framework allows for dynamic adjustment of the safety margin thanks to the human motion uncertainty estimation provided by a forecasting module. Thanks to uncertainty estimation, UA-PCBFs empower collaborative robots with a deeper understanding of future human states, facilitating more fluid and intelligent interactions through informed motion planning. We validate UA-PCBFs through comprehensive real-world experiments with an increasing level of realism, including automated setups (to perform exactly repeatable motions) with a robotic hand and direct human-robot interactions (to validate promptness, usability, and human confidence). Relative to state-of-the-art HRI architectures, UA-PCBFs show better performance in task-critical metrics, significantly reducing the number of violations of the robot's safe space during interaction with respect to the state-of-the-art.
<div id='section'>Paperid: <span id='pid'>732, <a href='https://arxiv.org/pdf/2508.06139.pdf' target='_blank'>https://arxiv.org/pdf/2508.06139.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shaohua Pan, Xinyu Yi, Yan Zhou, Weihua Jian, Yuan Zhang, Pengfei Wan, Feng Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.06139">DiffCap: Diffusion-based Real-time Human Motion Capture using Sparse IMUs and a Monocular Camera</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Combining sparse IMUs and a monocular camera is a new promising setting to perform real-time human motion capture. This paper proposes a diffusion-based solution to learn human motion priors and fuse the two modalities of signals together seamlessly in a unified framework. By delicately considering the characteristics of the two signals, the sequential visual information is considered as a whole and transformed into a condition embedding, while the inertial measurement is concatenated with the noisy body pose frame by frame to construct a sequential input for the diffusion model. Firstly, we observe that the visual information may be unavailable in some frames due to occlusions or subjects moving out of the camera view. Thus incorporating the sequential visual features as a whole to get a single feature embedding is robust to the occasional degenerations of visual information in those frames. On the other hand, the IMU measurements are robust to occlusions and always stable when signal transmission has no problem. So incorporating them frame-wisely could better explore the temporal information for the system. Experiments have demonstrated the effectiveness of the system design and its state-of-the-art performance in pose estimation compared with the previous works. Our codes are available for research at https://shaohua-pan.github.io/diffcap-page.
<div id='section'>Paperid: <span id='pid'>733, <a href='https://arxiv.org/pdf/2507.19850.pdf' target='_blank'>https://arxiv.org/pdf/2507.19850.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bizhu Wu, Jinheng Xie, Meidan Ding, Zhe Kong, Jianfeng Ren, Ruibin Bai, Rong Qu, Linlin Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.19850">FineMotion: A Dataset and Benchmark with both Spatial and Temporal Annotation for Fine-grained Motion Generation and Editing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating realistic human motions from textual descriptions has undergone significant advancements. However, existing methods often overlook specific body part movements and their timing. In this paper, we address this issue by enriching the textual description with more details. Specifically, we propose the FineMotion dataset, which contains over 442,000 human motion snippets - short segments of human motion sequences - and their corresponding detailed descriptions of human body part movements. Additionally, the dataset includes about 95k detailed paragraphs describing the movements of human body parts of entire motion sequences. Experimental results demonstrate the significance of our dataset on the text-driven finegrained human motion generation task, especially with a remarkable +15.3% improvement in Top-3 accuracy for the MDM model. Notably, we further support a zero-shot pipeline of fine-grained motion editing, which focuses on detailed editing in both spatial and temporal dimensions via text. Dataset and code available at: CVI-SZU/FineMotion
<div id='section'>Paperid: <span id='pid'>734, <a href='https://arxiv.org/pdf/2507.01857.pdf' target='_blank'>https://arxiv.org/pdf/2507.01857.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuhao Lin, Yi-Lin Wei, Haoran Liao, Mu Lin, Chengyi Xing, Hao Li, Dandan Zhang, Mark Cutkosky, Wei-Shi Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.01857">TypeTele: Releasing Dexterity in Teleoperation by Dexterous Manipulation Types</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dexterous teleoperation plays a crucial role in robotic manipulation for real-world data collection and remote robot control. Previous dexterous teleoperation mostly relies on hand retargeting to closely mimic human hand postures. However, these approaches may fail to fully leverage the inherent dexterity of dexterous hands, which can execute unique actions through their structural advantages compared to human hands. To address this limitation, we propose TypeTele, a type-guided dexterous teleoperation system, which enables dexterous hands to perform actions that are not constrained by human motion patterns. This is achieved by introducing dexterous manipulation types into the teleoperation system, allowing operators to employ appropriate types to complete specific tasks. To support this system, we build an extensible dexterous manipulation type library to cover comprehensive dexterous postures used in manipulation tasks. During teleoperation, we employ a MLLM (Multi-modality Large Language Model)-assisted type retrieval module to identify the most suitable manipulation type based on the specific task and operator commands. Extensive experiments of real-world teleoperation and imitation learning demonstrate that the incorporation of manipulation types significantly takes full advantage of the dexterous robot's ability to perform diverse and complex tasks with higher success rates.
<div id='section'>Paperid: <span id='pid'>735, <a href='https://arxiv.org/pdf/2506.24121.pdf' target='_blank'>https://arxiv.org/pdf/2506.24121.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sisi Dai, Xinxin Su, Boyan Wan, Ruizhen Hu, Kai Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.24121">TextMesh4D: High-Quality Text-to-4D Mesh Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in diffusion generative models significantly advanced image, video, and 3D content creation from user-provided text prompts. However, the challenging problem of dynamic 3D content generation (text-to-4D) with diffusion guidance remains largely unexplored. In this paper, we introduce TextMesh4D, a novel framework for high-quality text-to-4D generation. Our approach leverages per-face Jacobians as a differentiable mesh representation and decomposes 4D generation into two stages: static object creation and dynamic motion synthesis. We further propose a flexibility-rigidity regularization term to stabilize Jacobian optimization under video diffusion priors, ensuring robust geometric performance. Experiments demonstrate that TextMesh4D achieves state-of-the-art results in terms of temporal consistency, structural fidelity, and visual realism. Moreover, TextMesh4D operates with a low GPU memory overhead-requiring only a single 24GB GPU-offering a cost-effective yet high-quality solution for text-driven 4D mesh generation. The code will be released to facilitate future research in text-to-4D generation.
<div id='section'>Paperid: <span id='pid'>736, <a href='https://arxiv.org/pdf/2506.15290.pdf' target='_blank'>https://arxiv.org/pdf/2506.15290.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andela Ilic, Jiaxi Jiang, Paul Streli, Xintong Liu, Christian Holz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.15290">Human Motion Capture from Loose and Sparse Inertial Sensors with Garment-aware Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion capture using sparse inertial sensors has shown great promise due to its portability and lack of occlusion issues compared to camera-based tracking. Existing approaches typically assume that IMU sensors are tightly attached to the human body. However, this assumption often does not hold in real-world scenarios. In this paper, we present Garment Inertial Poser (GaIP), a method for estimating full-body poses from sparse and loosely attached IMU sensors. We first simulate IMU recordings using an existing garment-aware human motion dataset. Our transformer-based diffusion models synthesize loose IMU data and estimate human poses from this challenging loose IMU data. We also demonstrate that incorporating garment-related parameters during training on loose IMU data effectively maintains expressiveness and enhances the ability to capture variations introduced by looser or tighter garments. Our experiments show that our diffusion methods trained on simulated and synthetic data outperform state-of-the-art inertial full-body pose estimators, both quantitatively and qualitatively, opening up a promising direction for future research on motion capture from such realistic sensor placements.
<div id='section'>Paperid: <span id='pid'>737, <a href='https://arxiv.org/pdf/2506.12769.pdf' target='_blank'>https://arxiv.org/pdf/2506.12769.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junpeng Yue, Zepeng Wang, Yuxuan Wang, Weishuai Zeng, Jiangxing Wang, Xinrun Xu, Yu Zhang, Sipeng Zheng, Ziluo Ding, Zongqing Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.12769">RL from Physical Feedback: Aligning Large Motion Models with Humanoid Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper focuses on a critical challenge in robotics: translating text-driven human motions into executable actions for humanoid robots, enabling efficient and cost-effective learning of new behaviors. While existing text-to-motion generation methods achieve semantic alignment between language and motion, they often produce kinematically or physically infeasible motions unsuitable for real-world deployment. To bridge this sim-to-real gap, we propose Reinforcement Learning from Physical Feedback (RLPF), a novel framework that integrates physics-aware motion evaluation with text-conditioned motion generation. RLPF employs a motion tracking policy to assess feasibility in a physics simulator, generating rewards for fine-tuning the motion generator. Furthermore, RLPF introduces an alignment verification module to preserve semantic fidelity to text instructions. This joint optimization ensures both physical plausibility and instruction alignment. Extensive experiments show that RLPF greatly outperforms baseline methods in generating physically feasible motions while maintaining semantic correspondence with text instruction, enabling successful deployment on real humanoid robots.
<div id='section'>Paperid: <span id='pid'>738, <a href='https://arxiv.org/pdf/2505.19239.pdf' target='_blank'>https://arxiv.org/pdf/2505.19239.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chen Shi, Shaoshuai Shi, Kehua Sheng, Bo Zhang, Li Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.19239">DriveX: Omni Scene Modeling for Learning Generalizable World Knowledge in Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Data-driven learning has advanced autonomous driving, yet task-specific models struggle with out-of-distribution scenarios due to their narrow optimization objectives and reliance on costly annotated data. We present DriveX, a self-supervised world model that learns generalizable scene dynamics and holistic representations (geometric, semantic, and motion) from large-scale driving videos. DriveX introduces Omni Scene Modeling (OSM), a module that unifies multimodal supervision-3D point cloud forecasting, 2D semantic representation, and image generation-to capture comprehensive scene evolution. To simplify learning complex dynamics, we propose a decoupled latent world modeling strategy that separates world representation learning from future state decoding, augmented by dynamic-aware ray sampling to enhance motion modeling. For downstream adaptation, we design Future Spatial Attention (FSA), a unified paradigm that dynamically aggregates spatiotemporal features from DriveX's predictions to enhance task-specific inference. Extensive experiments demonstrate DriveX's effectiveness: it achieves significant improvements in 3D future point cloud prediction over prior work, while attaining state-of-the-art results on diverse tasks including occupancy prediction, flow estimation, and end-to-end driving. These results validate DriveX's capability as a general-purpose world model, paving the way for robust and unified autonomous driving frameworks.
<div id='section'>Paperid: <span id='pid'>739, <a href='https://arxiv.org/pdf/2505.17333.pdf' target='_blank'>https://arxiv.org/pdf/2505.17333.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xin You, Minghui Zhang, Hanxiao Zhang, Jie Yang, Nassir Navab
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.17333">Temporal Differential Fields for 4D Motion Modeling via Image-to-Video Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Temporal modeling on regular respiration-induced motions is crucial to image-guided clinical applications. Existing methods cannot simulate temporal motions unless high-dose imaging scans including starting and ending frames exist simultaneously. However, in the preoperative data acquisition stage, the slight movement of patients may result in dynamic backgrounds between the first and last frames in a respiratory period. This additional deviation can hardly be removed by image registration, thus affecting the temporal modeling. To address that limitation, we pioneeringly simulate the regular motion process via the image-to-video (I2V) synthesis framework, which animates with the first frame to forecast future frames of a given length. Besides, to promote the temporal consistency of animated videos, we devise the Temporal Differential Diffusion Model to generate temporal differential fields, which measure the relative differential representations between adjacent frames. The prompt attention layer is devised for fine-grained differential fields, and the field augmented layer is adopted to better interact these fields with the I2V framework, promoting more accurate temporal variation of synthesized videos. Extensive results on ACDC cardiac and 4D Lung datasets reveal that our approach simulates 4D videos along the intrinsic motion trajectory, rivaling other competitive methods on perceptual similarity and temporal consistency. Codes will be available soon.
<div id='section'>Paperid: <span id='pid'>740, <a href='https://arxiv.org/pdf/2505.03728.pdf' target='_blank'>https://arxiv.org/pdf/2505.03728.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chung Min Kim, Brent Yi, Hongsuk Choi, Yi Ma, Ken Goldberg, Angjoo Kanazawa
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.03728">PyRoki: A Modular Toolkit for Robot Kinematic Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robot motion can have many goals. Depending on the task, we might optimize for pose error, speed, collision, or similarity to a human demonstration. Motivated by this, we present PyRoki: a modular, extensible, and cross-platform toolkit for solving kinematic optimization problems. PyRoki couples an interface for specifying kinematic variables and costs with an efficient nonlinear least squares optimizer. Unlike existing tools, it is also cross-platform: optimization runs natively on CPU, GPU, and TPU. In this paper, we present (i) the design and implementation of PyRoki, (ii) motion retargeting and planning case studies that highlight the advantages of PyRoki's modularity, and (iii) optimization benchmarking, where PyRoki can be 1.4-1.7x faster and converges to lower errors than cuRobo, an existing GPU-accelerated inverse kinematics library.
<div id='section'>Paperid: <span id='pid'>741, <a href='https://arxiv.org/pdf/2505.01425.pdf' target='_blank'>https://arxiv.org/pdf/2505.01425.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiefeng Li, Jinkun Cao, Haotian Zhang, Davis Rempe, Jan Kautz, Umar Iqbal, Ye Yuan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.01425">GENMO: A GENeralist Model for Human MOtion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion modeling traditionally separates motion generation and estimation into distinct tasks with specialized models. Motion generation models focus on creating diverse, realistic motions from inputs like text, audio, or keyframes, while motion estimation models aim to reconstruct accurate motion trajectories from observations like videos. Despite sharing underlying representations of temporal dynamics and kinematics, this separation limits knowledge transfer between tasks and requires maintaining separate models. We present GENMO, a unified Generalist Model for Human Motion that bridges motion estimation and generation in a single framework. Our key insight is to reformulate motion estimation as constrained motion generation, where the output motion must precisely satisfy observed conditioning signals. Leveraging the synergy between regression and diffusion, GENMO achieves accurate global motion estimation while enabling diverse motion generation. We also introduce an estimation-guided training objective that exploits in-the-wild videos with 2D annotations and text descriptions to enhance generative diversity. Furthermore, our novel architecture handles variable-length motions and mixed multimodal conditions (text, audio, video) at different time intervals, offering flexible control. This unified approach creates synergistic benefits: generative priors improve estimated motions under challenging conditions like occlusions, while diverse video data enhances generation capabilities. Extensive experiments demonstrate GENMO's effectiveness as a generalist framework that successfully handles multiple human motion tasks within a single model.
<div id='section'>Paperid: <span id='pid'>742, <a href='https://arxiv.org/pdf/2504.05046.pdf' target='_blank'>https://arxiv.org/pdf/2504.05046.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shenghao Ren, Yi Lu, Jiayi Huang, Jiayi Zhao, He Zhang, Tao Yu, Qiu Shen, Xun Cao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.05046">MotionPRO: Exploring the Role of Pressure in Human MoCap and Beyond</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing human Motion Capture (MoCap) methods mostly focus on the visual similarity while neglecting the physical plausibility. As a result, downstream tasks such as driving virtual human in 3D scene or humanoid robots in real world suffer from issues such as timing drift and jitter, spatial problems like sliding and penetration, and poor global trajectory accuracy. In this paper, we revisit human MoCap from the perspective of interaction between human body and physical world by exploring the role of pressure. Firstly, we construct a large-scale human Motion capture dataset with Pressure, RGB and Optical sensors (named MotionPRO), which comprises 70 volunteers performing 400 types of motion, encompassing a total of 12.4M pose frames. Secondly, we examine both the necessity and effectiveness of the pressure signal through two challenging tasks: (1) pose and trajectory estimation based solely on pressure: We propose a network that incorporates a small kernel decoder and a long-short-term attention module, and proof that pressure could provide accurate global trajectory and plausible lower body pose. (2) pose and trajectory estimation by fusing pressure and RGB: We impose constraints on orthographic similarity along the camera axis and whole-body contact along the vertical axis to enhance the cross-attention strategy to fuse pressure and RGB feature maps. Experiments demonstrate that fusing pressure with RGB features not only significantly improves performance in terms of objective metrics, but also plausibly drives virtual humans (SMPL) in 3D scene. Furthermore, we demonstrate that incorporating physical perception enables humanoid robots to perform more precise and stable actions, which is highly beneficial for the development of embodied artificial intelligence. Project page is available at: https://nju-cite-mocaphumanoid.github.io/MotionPRO/
<div id='section'>Paperid: <span id='pid'>743, <a href='https://arxiv.org/pdf/2503.18674.pdf' target='_blank'>https://arxiv.org/pdf/2503.18674.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Edoardo De Matteis, Matteo Migliarini, Alessio Sampieri, Indro Spinelli, Fabio Galasso
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.18674">Human Motion Unlearning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce the task of human motion unlearning to prevent the synthesis of toxic animations while preserving the general text-to-motion generative performance. Unlearning toxic motions is challenging as those can be generated from explicit text prompts and from implicit toxic combinations of safe motions (e.g., ``kicking" is ``loading and swinging a leg"). We propose the first motion unlearning benchmark by filtering toxic motions from the large and recent text-to-motion datasets of HumanML3D and Motion-X. We propose baselines, by adapting state-of-the-art image unlearning techniques to process spatio-temporal signals. Finally, we propose a novel motion unlearning model based on Latent Code Replacement, which we dub LCR. LCR is training-free and suitable to the discrete latent spaces of state-of-the-art text-to-motion diffusion models. LCR is simple and consistently outperforms baselines qualitatively and quantitatively. Project page: \href{https://www.pinlab.org/hmu}{https://www.pinlab.org/hmu}.
<div id='section'>Paperid: <span id='pid'>744, <a href='https://arxiv.org/pdf/2503.17626.pdf' target='_blank'>https://arxiv.org/pdf/2503.17626.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziang Zheng, Guojian Zhan, Bin Shuai, Shengtao Qin, Jiangtao Li, Tao Zhang, Shengbo Eben Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.17626">Transferable Latent-to-Latent Locomotion Policy for Efficient and Versatile Motion Control of Diverse Legged Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reinforcement learning (RL) has demonstrated remarkable capability in acquiring robot skills, but learning each new skill still requires substantial data collection for training. The pretrain-and-finetune paradigm offers a promising approach for efficiently adapting to new robot entities and tasks. Inspired by the idea that acquired knowledge can accelerate learning new tasks with the same robot and help a new robot master a trained task, we propose a latent training framework where a transferable latent-to-latent locomotion policy is pretrained alongside diverse task-specific observation encoders and action decoders. This policy in latent space processes encoded latent observations to generate latent actions to be decoded, with the potential to learn general abstract motion skills. To retain essential information for decision-making and control, we introduce a diffusion recovery module that minimizes information reconstruction loss during pretrain stage. During fine-tune stage, the pretrained latent-to-latent locomotion policy remains fixed, while only the lightweight task-specific encoder and decoder are optimized for efficient adaptation. Our method allows a robot to leverage its own prior experience across different tasks as well as the experience of other morphologically diverse robots to accelerate adaptation. We validate our approach through extensive simulations and real-world experiments, demonstrating that the pretrained latent-to-latent locomotion policy effectively generalizes to new robot entities and tasks with improved efficiency.
<div id='section'>Paperid: <span id='pid'>745, <a href='https://arxiv.org/pdf/2503.14229.pdf' target='_blank'>https://arxiv.org/pdf/2503.14229.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifei Dong, Fengyi Wu, Qi He, Heng Li, Minghan Li, Zebang Cheng, Yuxuan Zhou, Jingdong Sun, Qi Dai, Zhi-Qi Cheng, Alexander G Hauptmann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.14229">HA-VLN: A Benchmark for Human-Aware Navigation in Discrete-Continuous Environments with Dynamic Multi-Human Interactions, Real-World Validation, and an Open Leaderboard</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-and-Language Navigation (VLN) systems often focus on either discrete (panoramic) or continuous (free-motion) paradigms alone, overlooking the complexities of human-populated, dynamic environments. We introduce a unified Human-Aware VLN (HA-VLN) benchmark that merges these paradigms under explicit social-awareness constraints. Our contributions include: 1. A standardized task definition that balances discrete-continuous navigation with personal-space requirements; 2. An enhanced human motion dataset (HAPS 2.0) and upgraded simulators capturing realistic multi-human interactions, outdoor contexts, and refined motion-language alignment; 3. Extensive benchmarking on 16,844 human-centric instructions, revealing how multi-human dynamics and partial observability pose substantial challenges for leading VLN agents; 4. Real-world robot tests validating sim-to-real transfer in crowded indoor spaces; and 5. A public leaderboard supporting transparent comparisons across discrete and continuous tasks. Empirical results show improved navigation success and fewer collisions when social context is integrated, underscoring the need for human-centric design. By releasing all datasets, simulators, agent code, and evaluation tools, we aim to advance safer, more capable, and socially responsible VLN research.
<div id='section'>Paperid: <span id='pid'>746, <a href='https://arxiv.org/pdf/2503.13120.pdf' target='_blank'>https://arxiv.org/pdf/2503.13120.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Siyuan Fan, Wenke Huang, Xiantao Cai, Bo Du
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.13120">3D Human Interaction Generation: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D human interaction generation has emerged as a key research area, focusing on producing dynamic and contextually relevant interactions between humans and various interactive entities. Recent rapid advancements in 3D model representation methods, motion capture technologies, and generative models have laid a solid foundation for the growing interest in this domain. Existing research in this field can be broadly categorized into three areas: human-scene interaction, human-object interaction, and human-human interaction. Despite the rapid advancements in this area, challenges remain due to the need for naturalness in human motion generation and the accurate interaction between humans and interactive entities. In this survey, we present a comprehensive literature review of human interaction generation, which, to the best of our knowledge, is the first of its kind. We begin by introducing the foundational technologies, including model representations, motion capture methods, and generative models. Subsequently, we introduce the approaches proposed for the three sub-tasks, along with their corresponding datasets and evaluation metrics. Finally, we discuss potential future research directions in this area and conclude the survey. Through this survey, we aim to offer a comprehensive overview of the current advancements in the field, highlight key challenges, and inspire future research works.
<div id='section'>Paperid: <span id='pid'>747, <a href='https://arxiv.org/pdf/2502.18373.pdf' target='_blank'>https://arxiv.org/pdf/2502.18373.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dominik Hollidt, Paul Streli, Jiaxi Jiang, Yasaman Haghighi, Changlin Qian, Xintong Liu, Christian Holz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.18373">EgoSim: An Egocentric Multi-view Simulator and Real Dataset for Body-worn Cameras during Motion and Activity</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Research on egocentric tasks in computer vision has mostly focused on head-mounted cameras, such as fisheye cameras or embedded cameras inside immersive headsets. We argue that the increasing miniaturization of optical sensors will lead to the prolific integration of cameras into many more body-worn devices at various locations. This will bring fresh perspectives to established tasks in computer vision and benefit key areas such as human motion tracking, body pose estimation, or action recognition -- particularly for the lower body, which is typically occluded.
  In this paper, we introduce EgoSim, a novel simulator of body-worn cameras that generates realistic egocentric renderings from multiple perspectives across a wearer's body. A key feature of EgoSim is its use of real motion capture data to render motion artifacts, which are especially noticeable with arm- or leg-worn cameras. In addition, we introduce MultiEgoView, a dataset of egocentric footage from six body-worn cameras and ground-truth full-body 3D poses during several activities: 119 hours of data are derived from AMASS motion sequences in four high-fidelity virtual environments, which we augment with 5 hours of real-world motion data from 13 participants using six GoPro cameras and 3D body pose references from an Xsens motion capture suit.
  We demonstrate EgoSim's effectiveness by training an end-to-end video-only 3D pose estimation network. Analyzing its domain gap, we show that our dataset and simulator substantially aid training for inference on real-world data.
  EgoSim code & MultiEgoView dataset: https://siplab.org/projects/EgoSim
<div id='section'>Paperid: <span id='pid'>748, <a href='https://arxiv.org/pdf/2502.14140.pdf' target='_blank'>https://arxiv.org/pdf/2502.14140.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiming Huang, Zhiyang Dou, Lingjie Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.14140">ModSkill: Physical Character Skill Modularization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion is highly diverse and dynamic, posing challenges for imitation learning algorithms that aim to generalize motor skills for controlling simulated characters. Previous methods typically rely on a universal full-body controller for tracking reference motion (tracking-based model) or a unified full-body skill embedding space (skill embedding). However, these approaches often struggle to generalize and scale to larger motion datasets. In this work, we introduce a novel skill learning framework, ModSkill, that decouples complex full-body skills into compositional, modular skills for independent body parts. Our framework features a skill modularization attention layer that processes policy observations into modular skill embeddings that guide low-level controllers for each body part. We also propose an Active Skill Learning approach with Generative Adaptive Sampling, using large motion generation models to adaptively enhance policy learning in challenging tracking scenarios. Our results show that this modularized skill learning framework, enhanced by generative sampling, outperforms existing methods in precise full-body motion tracking and enables reusable skill embeddings for diverse goal-driven tasks.
<div id='section'>Paperid: <span id='pid'>749, <a href='https://arxiv.org/pdf/2501.13707.pdf' target='_blank'>https://arxiv.org/pdf/2501.13707.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pengteng Li, Yunfan Lu, Pinghao Song, Wuyang Li, Huizai Yao, Hui Xiong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.13707">EventVL: Understand Event Streams via Multimodal Large Language Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The event-based Vision-Language Model (VLM) recently has made good progress for practical vision tasks. However, most of these works just utilize CLIP for focusing on traditional perception tasks, which obstruct model understanding explicitly the sufficient semantics and context from event streams. To address the deficiency, we propose EventVL, the first generative event-based MLLM (Multimodal Large Language Model) framework for explicit semantic understanding. Specifically, to bridge the data gap for connecting different modalities semantics, we first annotate a large event-image/video-text dataset, containing almost 1.4 million high-quality pairs of data, which enables effective learning across various scenes, e.g., drive scene or human motion. After that, we design Event Spatiotemporal Representation to fully explore the comprehensive information by diversely aggregating and segmenting the event stream. To further promote a compact semantic space, Dynamic Semantic Alignment is introduced to improve and complete sparse semantic spaces of events. Extensive experiments show that our EventVL can significantly surpass existing MLLM baselines in event captioning and scene description generation tasks. We hope our research could contribute to the development of the event vision community.
<div id='section'>Paperid: <span id='pid'>750, <a href='https://arxiv.org/pdf/2412.14172.pdf' target='_blank'>https://arxiv.org/pdf/2412.14172.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiageng Mao, Siheng Zhao, Siqi Song, Tianheng Shi, Junjie Ye, Mingtong Zhang, Haoran Geng, Jitendra Malik, Vitor Guizilini, Yue Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.14172">Learning from Massive Human Videos for Universal Humanoid Pose Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scalable learning of humanoid robots is crucial for their deployment in real-world applications. While traditional approaches primarily rely on reinforcement learning or teleoperation to achieve whole-body control, they are often limited by the diversity of simulated environments and the high costs of demonstration collection. In contrast, human videos are ubiquitous and present an untapped source of semantic and motion information that could significantly enhance the generalization capabilities of humanoid robots. This paper introduces Humanoid-X, a large-scale dataset of over 20 million humanoid robot poses with corresponding text-based motion descriptions, designed to leverage this abundant data. Humanoid-X is curated through a comprehensive pipeline: data mining from the Internet, video caption generation, motion retargeting of humans to humanoid robots, and policy learning for real-world deployment. With Humanoid-X, we further train a large humanoid model, UH-1, which takes text instructions as input and outputs corresponding actions to control a humanoid robot. Extensive simulated and real-world experiments validate that our scalable training approach leads to superior generalization in text-based humanoid control, marking a significant step toward adaptable, real-world-ready humanoid robots.
<div id='section'>Paperid: <span id='pid'>751, <a href='https://arxiv.org/pdf/2412.10523.pdf' target='_blank'>https://arxiv.org/pdf/2412.10523.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Changan Chen, Juze Zhang, Shrinidhi K. Lakshmikanth, Yusu Fang, Ruizhi Shao, Gordon Wetzstein, Li Fei-Fei, Ehsan Adeli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.10523">The Language of Motion: Unifying Verbal and Non-verbal Language of 3D Human Motion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human communication is inherently multimodal, involving a combination of verbal and non-verbal cues such as speech, facial expressions, and body gestures. Modeling these behaviors is essential for understanding human interaction and for creating virtual characters that can communicate naturally in applications like games, films, and virtual reality. However, existing motion generation models are typically limited to specific input modalities -- either speech, text, or motion data -- and cannot fully leverage the diversity of available data. In this paper, we propose a novel framework that unifies verbal and non-verbal language using multimodal language models for human motion understanding and generation. This model is flexible in taking text, speech, and motion or any combination of them as input. Coupled with our novel pre-training strategy, our model not only achieves state-of-the-art performance on co-speech gesture generation but also requires much less data for training. Our model also unlocks an array of novel tasks such as editable gesture generation and emotion prediction from motion. We believe unifying the verbal and non-verbal language of human motion is essential for real-world applications, and language models offer a powerful approach to achieving this goal. Project page: languageofmotion.github.io.
<div id='section'>Paperid: <span id='pid'>752, <a href='https://arxiv.org/pdf/2412.06174.pdf' target='_blank'>https://arxiv.org/pdf/2412.06174.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuzhu Ji, Chuanxia Zheng, Tat-Jen Cham
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.06174">One-shot Human Motion Transfer via Occlusion-Robust Flow Prediction and Neural Texturing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion transfer aims at animating a static source image with a driving video. While recent advances in one-shot human motion transfer have led to significant improvement in results, it remains challenging for methods with 2D body landmarks, skeleton and semantic mask to accurately capture correspondences between source and driving poses due to the large variation in motion and articulation complexity. In addition, the accuracy and precision of DensePose degrade the image quality for neural-rendering-based methods. To address the limitations and by both considering the importance of appearance and geometry for motion transfer, in this work, we proposed a unified framework that combines multi-scale feature warping and neural texture mapping to recover better 2D appearance and 2.5D geometry, partly by exploiting the information from DensePose, yet adapting to its inherent limited accuracy. Our model takes advantage of multiple modalities by jointly training and fusing them, which allows it to robust neural texture features that cope with geometric errors as well as multi-scale dense motion flow that better preserves appearance. Experimental results with full and half-view body video datasets demonstrate that our model can generalize well and achieve competitive results, and that it is particularly effective in handling challenging cases such as those with substantial self-occlusions.
<div id='section'>Paperid: <span id='pid'>753, <a href='https://arxiv.org/pdf/2412.04343.pdf' target='_blank'>https://arxiv.org/pdf/2412.04343.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhouyingcheng Liao, Mingyuan Zhang, Wenjia Wang, Lei Yang, Taku Komura
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.04343">RMD: A Simple Baseline for More General Human Motion Generation via Training-free Retrieval-Augmented Motion Diffuse</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While motion generation has made substantial progress, its practical application remains constrained by dataset diversity and scale, limiting its ability to handle out-of-distribution scenarios. To address this, we propose a simple and effective baseline, RMD, which enhances the generalization of motion generation through retrieval-augmented techniques. Unlike previous retrieval-based methods, RMD requires no additional training and offers three key advantages: (1) the external retrieval database can be flexibly replaced; (2) body parts from the motion database can be reused, with an LLM facilitating splitting and recombination; and (3) a pre-trained motion diffusion model serves as a prior to improve the quality of motions obtained through retrieval and direct combination. Without any training, RMD achieves state-of-the-art performance, with notable advantages on out-of-distribution data.
<div id='section'>Paperid: <span id='pid'>754, <a href='https://arxiv.org/pdf/2411.06481.pdf' target='_blank'>https://arxiv.org/pdf/2411.06481.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zeyu Zhang, Hang Gao, Akide Liu, Qi Chen, Feng Chen, Yiran Wang, Danning Li, Rui Zhao, Zhenming Li, Zhongwen Zhou, Hao Tang, Bohan Zhuang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.06481">KMM: Key Frame Mask Mamba for Extended Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion generation is a cut-edge area of research in generative computer vision, with promising applications in video creation, game development, and robotic manipulation. The recent Mamba architecture shows promising results in efficiently modeling long and complex sequences, yet two significant challenges remain: Firstly, directly applying Mamba to extended motion generation is ineffective, as the limited capacity of the implicit memory leads to memory decay. Secondly, Mamba struggles with multimodal fusion compared to Transformers, and lack alignment with textual queries, often confusing directions (left or right) or omitting parts of longer text queries. To address these challenges, our paper presents three key contributions: Firstly, we introduce KMM, a novel architecture featuring Key frame Masking Modeling, designed to enhance Mamba's focus on key actions in motion segments. This approach addresses the memory decay problem and represents a pioneering method in customizing strategic frame-level masking in SSMs. Additionally, we designed a contrastive learning paradigm for addressing the multimodal fusion problem in Mamba and improving the motion-text alignment. Finally, we conducted extensive experiments on the go-to dataset, BABEL, achieving state-of-the-art performance with a reduction of more than 57% in FID and 70% parameters compared to previous state-of-the-art methods. See project website: https://steve-zeyu-zhang.github.io/KMM
<div id='section'>Paperid: <span id='pid'>755, <a href='https://arxiv.org/pdf/2410.20974.pdf' target='_blank'>https://arxiv.org/pdf/2410.20974.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Di Qiu, Zheng Chen, Rui Wang, Mingyuan Fan, Changqian Yu, Junshi Huang, Xiang Wen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.20974">MovieCharacter: A Tuning-Free Framework for Controllable Character Video Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in character video synthesis still depend on extensive fine-tuning or complex 3D modeling processes, which can restrict accessibility and hinder real-time applicability. To address these challenges, we propose a simple yet effective tuning-free framework for character video synthesis, named MovieCharacter, designed to streamline the synthesis process while ensuring high-quality outcomes. Our framework decomposes the synthesis task into distinct, manageable modules: character segmentation and tracking, video object removal, character motion imitation, and video composition. This modular design not only facilitates flexible customization but also ensures that each component operates collaboratively to effectively meet user needs. By leveraging existing open-source models and integrating well-established techniques, MovieCharacter achieves impressive synthesis results without necessitating substantial resources or proprietary datasets. Experimental results demonstrate that our framework enhances the efficiency, accessibility, and adaptability of character video synthesis, paving the way for broader creative and interactive applications.
<div id='section'>Paperid: <span id='pid'>756, <a href='https://arxiv.org/pdf/2410.03665.pdf' target='_blank'>https://arxiv.org/pdf/2410.03665.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Brent Yi, Vickie Ye, Maya Zheng, Yunqi Li, Lea MÃ¼ller, Georgios Pavlakos, Yi Ma, Jitendra Malik, Angjoo Kanazawa
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.03665">Estimating Body and Hand Motion in an Ego-sensed World</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present EgoAllo, a system for human motion estimation from a head-mounted device. Using only egocentric SLAM poses and images, EgoAllo guides sampling from a conditional diffusion model to estimate 3D body pose, height, and hand parameters that capture a device wearer's actions in the allocentric coordinate frame of the scene. To achieve this, our key insight is in representation: we propose spatial and temporal invariance criteria for improving model performance, from which we derive a head motion conditioning parameterization that improves estimation by up to 18%. We also show how the bodies estimated by our system can improve hand estimation: the resulting kinematic and temporal constraints can reduce world-frame errors in single-frame estimates by 40%. Project page: https://egoallo.github.io/
<div id='section'>Paperid: <span id='pid'>757, <a href='https://arxiv.org/pdf/2410.03311.pdf' target='_blank'>https://arxiv.org/pdf/2410.03311.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ye Wang, Sipeng Zheng, Bin Cao, Qianshan Wei, Weishuai Zeng, Qin Jin, Zongqing Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.03311">Scaling Large Motion Models with Million-Level Human Motions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Inspired by the recent success of LLMs, the field of human motion understanding has increasingly shifted toward developing large motion models. Despite some progress, current efforts remain far from achieving truly generalist models, primarily due to the lack of massive high-quality data. To address this gap, we present MotionLib, the first million-level dataset for motion generation, which is at least 15$\times$ larger than existing counterparts and enriched with hierarchical text descriptions. Using MotionLib, we train a large motion model named \projname, demonstrating robust performance across a wide range of human activities, including unseen ones. Through systematic investigation, for the first time, we highlight the importance of scaling both data and model size for advancing motion generation, along with key insights to achieve this goal. To better integrate the motion modality, we propose Motionbook, an innovative motion encoding approach including (1) a compact yet lossless feature to represent motions; (2) a novel 2D lookup-free motion tokenizer that preserves fine-grained motion details while expanding codebook capacity, significantly enhancing the representational power of motion tokens. We believe this work lays the groundwork for developing more versatile and powerful motion generation models in the future. For further details, visit https://beingbeyond.github.io/Being-M0/.
<div id='section'>Paperid: <span id='pid'>758, <a href='https://arxiv.org/pdf/2409.11696.pdf' target='_blank'>https://arxiv.org/pdf/2409.11696.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiawei Sun, Jiahui Li, Tingchen Liu, Chengran Yuan, Shuo Sun, Zefan Huang, Anthony Wong, Keng Peng Tee, Marcelo H. Ang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.11696">RMP-YOLO: A Robust Motion Predictor for Partially Observable Scenarios even if You Only Look Once</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce RMP-YOLO, a unified framework designed to provide robust motion predictions even with incomplete input data. Our key insight stems from the observation that complete and reliable historical trajectory data plays a pivotal role in ensuring accurate motion prediction. Therefore, we propose a new paradigm that prioritizes the reconstruction of intact historical trajectories before feeding them into the prediction modules. Our approach introduces a novel scene tokenization module to enhance the extraction and fusion of spatial and temporal features. Following this, our proposed recovery module reconstructs agents' incomplete historical trajectories by leveraging local map topology and interactions with nearby agents. The reconstructed, clean historical data is then integrated into the downstream prediction modules. Our framework is able to effectively handle missing data of varying lengths and remains robust against observation noise, while maintaining high prediction accuracy. Furthermore, our recovery module is compatible with existing prediction models, ensuring seamless integration. Extensive experiments validate the effectiveness of our approach, and deployment in real-world autonomous vehicles confirms its practical utility. In the 2024 Waymo Motion Prediction Competition, our method, RMP-YOLO, achieves state-of-the-art performance, securing third place.
<div id='section'>Paperid: <span id='pid'>759, <a href='https://arxiv.org/pdf/2409.09536.pdf' target='_blank'>https://arxiv.org/pdf/2409.09536.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Teun van de Laar, Zengjie Zhang, Shuhao Qi, Sofie Haesaert, Zhiyong Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.09536">VernaCopter: Disambiguated Natural-Language-Driven Robot via Formal Specifications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>It has been an ambition of many to control a robot for a complex task using natural language (NL). The rise of large language models (LLMs) makes it closer to coming true. However, an LLM-powered system still suffers from the ambiguity inherent in an NL and the uncertainty brought up by LLMs. This paper proposes a novel LLM-based robot motion planner, named \textit{VernaCopter}, with signal temporal logic (STL) specifications serving as a bridge between NL commands and specific task objectives. The rigorous and abstract nature of formal specifications allows the planner to generate high-quality and highly consistent paths to guide the motion control of a robot. Compared to a conventional NL-prompting-based planner, the proposed VernaCopter planner is more stable and reliable due to less ambiguous uncertainty. Its efficacy and advantage have been validated by two small but challenging experimental scenarios, implying its potential in designing NL-driven robots.
<div id='section'>Paperid: <span id='pid'>760, <a href='https://arxiv.org/pdf/2409.00904.pdf' target='_blank'>https://arxiv.org/pdf/2409.00904.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhanwen Liu, Chao Li, Yang Wang, Nan Yang, Xing Fan, Jiaqi Ma, Xiangmo Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.00904">Multi-scale Temporal Fusion Transformer for Incomplete Vehicle Trajectory Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion prediction plays an essential role in autonomous driving systems, enabling autonomous vehicles to achieve more accurate local-path planning and driving decisions based on predictions of the surrounding vehicles. However, existing methods neglect the potential missing values caused by object occlusion, perception failures, etc., which inevitably degrades the trajectory prediction performance in real traffic scenarios. To address this limitation, we propose a novel end-to-end framework for incomplete vehicle trajectory prediction, named Multi-scale Temporal Fusion Transformer (MTFT), which consists of the Multi-scale Attention Head (MAH) and the Continuity Representation-guided Multi-scale Fusion (CRMF) module. Specifically, the MAH leverages the multi-head attention mechanism to parallelly capture multi-scale motion representation of trajectory from different temporal granularities, thus mitigating the adverse effect of missing values on prediction. Furthermore, the multi-scale motion representation is input into the CRMF module for multi-scale fusion to obtain the robust temporal feature of the vehicle. During the fusion process, the continuity representation of vehicle motion is first extracted across time steps to guide the fusion, ensuring that the resulting temporal feature incorporates both detailed information and the overall trend of vehicle motion, which facilitates the accurate decoding of future trajectory that is consistent with the vehicle's motion trend. We evaluate the proposed model on four datasets derived from highway and urban traffic scenarios. The experimental results demonstrate its superior performance in the incomplete vehicle trajectory prediction task compared with state-of-the-art models, e.g., a comprehensive performance improvement of more than 39% on the HighD dataset.
<div id='section'>Paperid: <span id='pid'>761, <a href='https://arxiv.org/pdf/2408.10073.pdf' target='_blank'>https://arxiv.org/pdf/2408.10073.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Oliver Cory, Ozge Mercanoglu Sincan, Matthew Vowels, Alessia Battisti, Franz Holzknecht, Katja Tissi, Sandra Sidler-Miserez, Tobias Haug, Sarah Ebling, Richard Bowden
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.10073">Modelling the Distribution of Human Motion for Sign Language Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Sign Language Assessment (SLA) tools are useful to aid in language learning and are underdeveloped. Previous work has focused on isolated signs or comparison against a single reference video to assess Sign Languages (SL). This paper introduces a novel SLA tool designed to evaluate the comprehensibility of SL by modelling the natural distribution of human motion. We train our pipeline on data from native signers and evaluate it using SL learners. We compare our results to ratings from a human raters study and find strong correlation between human ratings and our tool. We visually demonstrate our tools ability to detect anomalous results spatio-temporally, providing actionable feedback to aid in SL learning and assessment.
<div id='section'>Paperid: <span id='pid'>762, <a href='https://arxiv.org/pdf/2407.11532.pdf' target='_blank'>https://arxiv.org/pdf/2407.11532.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alessio Sampieri, Alessio Palma, Indro Spinelli, Fabio Galasso
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.11532">Length-Aware Motion Synthesis via Latent Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The target duration of a synthesized human motion is a critical attribute that requires modeling control over the motion dynamics and style. Speeding up an action performance is not merely fast-forwarding it. However, state-of-the-art techniques for human behavior synthesis have limited control over the target sequence length.
  We introduce the problem of generating length-aware 3D human motion sequences from textual descriptors, and we propose a novel model to synthesize motions of variable target lengths, which we dub "Length-Aware Latent Diffusion" (LADiff). LADiff consists of two new modules: 1) a length-aware variational auto-encoder to learn motion representations with length-dependent latent codes; 2) a length-conforming latent diffusion model to generate motions with a richness of details that increases with the required target sequence length. LADiff significantly improves over the state-of-the-art across most of the existing motion synthesis metrics on the two established benchmarks of HumanML3D and KIT-ML.
<div id='section'>Paperid: <span id='pid'>763, <a href='https://arxiv.org/pdf/2407.10061.pdf' target='_blank'>https://arxiv.org/pdf/2407.10061.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zeyu Zhang, Akide Liu, Qi Chen, Feng Chen, Ian Reid, Richard Hartley, Bohan Zhuang, Hao Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.10061">InfiniMotion: Mamba Boosts Memory in Transformer for Arbitrary Long Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-to-motion generation holds potential for film, gaming, and robotics, yet current methods often prioritize short motion generation, making it challenging to produce long motion sequences effectively: (1) Current methods struggle to handle long motion sequences as a single input due to prohibitively high computational cost; (2) Breaking down the generation of long motion sequences into shorter segments can result in inconsistent transitions and requires interpolation or inpainting, which lacks entire sequence modeling. To solve these challenges, we propose InfiniMotion, a method that generates continuous motion sequences of arbitrary length within an autoregressive framework. We highlight its groundbreaking capability by generating a continuous 1-hour human motion with around 80,000 frames. Specifically, we introduce the Motion Memory Transformer with Bidirectional Mamba Memory, enhancing the transformer's memory to process long motion sequences effectively without overwhelming computational resources. Notably our method achieves over 30% improvement in FID and 6 times longer demonstration compared to previous state-of-the-art methods, showcasing significant advancements in long motion generation. See project webpage: https://steve-zeyu-zhang.github.io/InfiniMotion/
<div id='section'>Paperid: <span id='pid'>764, <a href='https://arxiv.org/pdf/2407.01593.pdf' target='_blank'>https://arxiv.org/pdf/2407.01593.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sariah Mghames, Luca Castri, Marc Hanheide, Nicola Bellotto
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.01593">neuROSym: Deployment and Evaluation of a ROS-based Neuro-Symbolic Model for Human Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous mobile robots can rely on several human motion detection and prediction systems for safe and efficient navigation in human environments, but the underline model architectures can have different impacts on the trustworthiness of the robot in the real world. Among existing solutions for context-aware human motion prediction, some approaches have shown the benefit of integrating symbolic knowledge with state-of-the-art neural networks. In particular, a recent neuro-symbolic architecture (NeuroSyM) has successfully embedded context with a Qualitative Trajectory Calculus (QTC) for spatial interactions representation. This work achieved better performance than neural-only baseline architectures on offline datasets. In this paper, we extend the original architecture to provide neuROSym, a ROS package for robot deployment in real-world scenarios, which can run, visualise, and evaluate previous neural-only and neuro-symbolic models for motion prediction online. We evaluated these models, NeuroSyM and a baseline SGAN, on a TIAGo robot in two scenarios with different human motion patterns. We assessed accuracy and runtime performance of the prediction models, showing a general improvement in case our neuro-symbolic architecture is used. We make the neuROSym package1 publicly available to the robotics community.
<div id='section'>Paperid: <span id='pid'>765, <a href='https://arxiv.org/pdf/2405.15385.pdf' target='_blank'>https://arxiv.org/pdf/2405.15385.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xia Li, Runzhao Yang, Xiangtai Li, Antony Lomax, Ye Zhang, Joachim Buhmann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.15385">CPT-Interp: Continuous sPatial and Temporal Motion Modeling for 4D Medical Image Interpolation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion information from 4D medical imaging offers critical insights into dynamic changes in patient anatomy for clinical assessments and radiotherapy planning and, thereby, enhances the capabilities of 3D image analysis. However, inherent physical and technical constraints of imaging hardware often necessitate a compromise between temporal resolution and image quality. Frame interpolation emerges as a pivotal solution to this challenge. Previous methods often suffer from discretion when they estimate the intermediate motion and execute the forward warping. In this study, we draw inspiration from fluid mechanics to propose a novel approach for continuously modeling patient anatomic motion using implicit neural representation. It ensures both spatial and temporal continuity, effectively bridging Eulerian and Lagrangian specifications together to naturally facilitate continuous frame interpolation. Our experiments across multiple datasets underscore the method's superior accuracy and speed. Furthermore, as a case-specific optimization (training-free) approach, it circumvents the need for extensive datasets and addresses model generalization issues.
<div id='section'>Paperid: <span id='pid'>766, <a href='https://arxiv.org/pdf/2405.00797.pdf' target='_blank'>https://arxiv.org/pdf/2405.00797.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiahui Li, Tianle Shen, Zekai Gu, Jiawei Sun, Chengran Yuan, Yuhang Han, Shuo Sun, Marcelo H. Ang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.00797">ADM: Accelerated Diffusion Model via Estimated Priors for Robust Motion Prediction under Uncertainties</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion prediction is a challenging problem in autonomous driving as it demands the system to comprehend stochastic dynamics and the multi-modal nature of real-world agent interactions. Diffusion models have recently risen to prominence, and have proven particularly effective in pedestrian motion prediction tasks. However, the significant time consumption and sensitivity to noise have limited the real-time predictive capability of diffusion models. In response to these impediments, we propose a novel diffusion-based, acceleratable framework that adeptly predicts future trajectories of agents with enhanced resistance to noise. The core idea of our model is to learn a coarse-grained prior distribution of trajectory, which can skip a large number of denoise steps. This advancement not only boosts sampling efficiency but also maintains the fidelity of prediction accuracy. Our method meets the rigorous real-time operational standards essential for autonomous vehicles, enabling prompt trajectory generation that is vital for secure and efficient navigation. Through extensive experiments, our method speeds up the inference time to 136ms compared to standard diffusion model, and achieves significant improvement in multi-agent motion prediction on the Argoverse 1 motion forecasting dataset.
<div id='section'>Paperid: <span id='pid'>767, <a href='https://arxiv.org/pdf/2405.00430.pdf' target='_blank'>https://arxiv.org/pdf/2405.00430.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xia Li, Runzhao Yang, Muheng Li, Xiangtai Li, Antony J. Lomax, Joachim M. Buhmann, Ye Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.00430">Continuous sPatial-Temporal Deformable Image Registration (CPT-DIR) for motion modelling in radiotherapy: beyond classic voxel-based methods</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deformable image registration (DIR) is a crucial tool in radiotherapy for analyzing anatomical changes and motion patterns. Current DIR implementations rely on discrete volumetric motion representation, which often leads to compromised accuracy and uncertainty when handling significant anatomical changes and sliding boundaries. This limitation affects the reliability of subsequent contour propagation and dose accumulation procedures, particularly in regions with complex anatomical interfaces such as the lung-chest wall boundary. Given that organ motion is inherently a continuous process in both space and time, we aimed to develop a model that preserves these fundamental properties. Drawing inspiration from fluid mechanics, we propose a novel approach using implicit neural representation (INR) for continuous modeling of patient anatomical motion. This approach ensures spatial and temporal continuity while effectively unifying Eulerian and Lagrangian specifications to enable natural continuous motion modeling and frame interpolation. The integration of these specifications provides a more comprehensive understanding of anatomical deformation patterns. By leveraging the continuous representations, the CPT-DIR method significantly enhances registration and interpolation accuracy, automation, and speed. The method demonstrates superior performance in landmark and contour precision, particularly in challenging anatomical regions, representing a substantial advancement over conventional approaches in deformable image registration. The improved efficiency and accuracy of CPT-DIR make it particularly suitable for real-time adaptive radiotherapy applications.
<div id='section'>Paperid: <span id='pid'>768, <a href='https://arxiv.org/pdf/2404.10295.pdf' target='_blank'>https://arxiv.org/pdf/2404.10295.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiawei Sun, Chengran Yuan, Shuo Sun, Shanze Wang, Yuhang Han, Shuailei Ma, Zefan Huang, Anthony Wong, Keng Peng Tee, Marcelo H. Ang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.10295">ControlMTR: Control-Guided Motion Transformer with Scene-Compliant Intention Points for Feasible Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The ability to accurately predict feasible multimodal future trajectories of surrounding traffic participants is crucial for behavior planning in autonomous vehicles. The Motion Transformer (MTR), a state-of-the-art motion prediction method, alleviated mode collapse and instability during training and enhanced overall prediction performance by replacing conventional dense future endpoints with a small set of fixed prior motion intention points. However, the fixed prior intention points make the MTR multi-modal prediction distribution over-scattered and infeasible in many scenarios. In this paper, we propose the ControlMTR framework to tackle the aforementioned issues by generating scene-compliant intention points and additionally predicting driving control commands, which are then converted into trajectories by a simple kinematic model with soft constraints. These control-generated trajectories will guide the directly predicted trajectories by an auxiliary loss function. Together with our proposed scene-compliant intention points, they can effectively restrict the prediction distribution within the road boundaries and suppress infeasible off-road predictions while enhancing prediction performance. Remarkably, without resorting to additional model ensemble techniques, our method surpasses the baseline MTR model across all performance metrics, achieving notable improvements of 5.22% in SoftmAP and a 4.15% reduction in MissRate. Our approach notably results in a 41.85% reduction in the cross-boundary rate of the MTR, effectively ensuring that the prediction distribution is confined within the drivable area.
<div id='section'>Paperid: <span id='pid'>769, <a href='https://arxiv.org/pdf/2403.17610.pdf' target='_blank'>https://arxiv.org/pdf/2403.17610.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>He Zhang, Shenghao Ren, Haolei Yuan, Jianhui Zhao, Fan Li, Shuangpeng Sun, Zhenghao Liang, Tao Yu, Qiu Shen, Xun Cao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.17610">MMVP: A Multimodal MoCap Dataset with Vision and Pressure Sensors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Foot contact is an important cue for human motion capture, understanding, and generation. Existing datasets tend to annotate dense foot contact using visual matching with thresholding or incorporating pressure signals. However, these approaches either suffer from low accuracy or are only designed for small-range and slow motion. There is still a lack of a vision-pressure multimodal dataset with large-range and fast human motion, as well as accurate and dense foot-contact annotation. To fill this gap, we propose a Multimodal MoCap Dataset with Vision and Pressure sensors, named MMVP. MMVP provides accurate and dense plantar pressure signals synchronized with RGBD observations, which is especially useful for both plausible shape estimation, robust pose fitting without foot drifting, and accurate global translation tracking. To validate the dataset, we propose an RGBD-P SMPL fitting method and also a monocular-video-based baseline framework, VP-MoCap, for human motion capture. Experiments demonstrate that our RGBD-P SMPL Fitting results significantly outperform pure visual motion capture. Moreover, VP-MoCap outperforms SOTA methods in foot-contact and global translation estimation accuracy. We believe the configuration of the dataset and the baseline frameworks will stimulate the research in this direction and also provide a good reference for MoCap applications in various domains. Project page: https://metaverse-ai-lab-thu.github.io/MMVP-Dataset/.
<div id='section'>Paperid: <span id='pid'>770, <a href='https://arxiv.org/pdf/2403.07487.pdf' target='_blank'>https://arxiv.org/pdf/2403.07487.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zeyu Zhang, Akide Liu, Ian Reid, Richard Hartley, Bohan Zhuang, Hao Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.07487">Motion Mamba: Efficient and Long Sequence Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion generation stands as a significant pursuit in generative computer vision, while achieving long-sequence and efficient motion generation remains challenging. Recent advancements in state space models (SSMs), notably Mamba, have showcased considerable promise in long sequence modeling with an efficient hardware-aware design, which appears to be a promising direction to build motion generation model upon it. Nevertheless, adapting SSMs to motion generation faces hurdles since the lack of a specialized design architecture to model motion sequence. To address these challenges, we propose Motion Mamba, a simple and efficient approach that presents the pioneering motion generation model utilized SSMs. Specifically, we design a Hierarchical Temporal Mamba (HTM) block to process temporal data by ensemble varying numbers of isolated SSM modules across a symmetric U-Net architecture aimed at preserving motion consistency between frames. We also design a Bidirectional Spatial Mamba (BSM) block to bidirectionally process latent poses, to enhance accurate motion generation within a temporal frame. Our proposed method achieves up to 50% FID improvement and up to 4 times faster on the HumanML3D and KIT-ML datasets compared to the previous best diffusion-based method, which demonstrates strong capabilities of high-quality long sequence motion modeling and real-time human motion generation. See project website https://steve-zeyu-zhang.github.io/MotionMamba/
<div id='section'>Paperid: <span id='pid'>771, <a href='https://arxiv.org/pdf/2402.17434.pdf' target='_blank'>https://arxiv.org/pdf/2402.17434.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tong Hui, Eugenio Cuniato, Michael Pantic, Marco Tognon, Matteo Fumagalli, Roland Siegwart
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.17434">Passive Aligning Physical Interaction of Fully-Actuated Aerial Vehicles for Pushing Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, the utilization of aerial manipulators for performing pushing tasks in non-destructive testing (NDT) applications has seen significant growth. Such operations entail physical interactions between the aerial robotic system and the environment. End-effectors with multiple contact points are often used for placing NDT sensors in contact with a surface to be inspected. Aligning the NDT sensor and the work surface while preserving contact, requires that all available contact points at the end-effector tip are in contact with the work surface. With a standard full-pose controller, attitude errors often occur due to perturbations caused by modeling uncertainties, sensor noise, and environmental uncertainties. Even small attitude errors can cause a loss of contact points between the end-effector tip and the work surface. To preserve full alignment amidst these uncertainties, we propose a control strategy which selectively deactivates angular motion control and enables direct force control in specific directions. In particular, we derive two essential conditions to be met, such that the robot can passively align with flat work surfaces achieving full alignment through the rotation along non-actively controlled axes. Additionally, these conditions serve as hardware design and control guidelines for effectively integrating the proposed control method for practical usage. Real world experiments are conducted to validate both the control design and the guidelines.
<div id='section'>Paperid: <span id='pid'>772, <a href='https://arxiv.org/pdf/2312.08895.pdf' target='_blank'>https://arxiv.org/pdf/2312.08895.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vincent Tao Hu, Wenzhe Yin, Pingchuan Ma, Yunlu Chen, Basura Fernando, Yuki M Asano, Efstratios Gavves, Pascal Mettes, Bjorn Ommer, Cees G. M. Snoek
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.08895">Motion Flow Matching for Human Motion Synthesis and Editing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion synthesis is a fundamental task in computer animation. Recent methods based on diffusion models or GPT structure demonstrate commendable performance but exhibit drawbacks in terms of slow sampling speeds and error accumulation. In this paper, we propose \emph{Motion Flow Matching}, a novel generative model designed for human motion generation featuring efficient sampling and effectiveness in motion editing applications. Our method reduces the sampling complexity from thousand steps in previous diffusion models to just ten steps, while achieving comparable performance in text-to-motion and action-to-motion generation benchmarks. Noticeably, our approach establishes a new state-of-the-art FrÃ©chet Inception Distance on the KIT-ML dataset. What is more, we tailor a straightforward motion editing paradigm named \emph{sampling trajectory rewriting} leveraging the ODE-style generative models and apply it to various editing scenarios including motion prediction, motion in-between prediction, motion interpolation, and upper-body editing. Our code will be released.
<div id='section'>Paperid: <span id='pid'>773, <a href='https://arxiv.org/pdf/2312.02682.pdf' target='_blank'>https://arxiv.org/pdf/2312.02682.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhengyao Jiang, Yingchen Xu, Nolan Wagener, Yicheng Luo, Michael Janner, Edward Grefenstette, Tim RocktÃ¤schel, Yuandong Tian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.02682">H-GAP: Humanoid Control with a Generalist Planner</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humanoid control is an important research challenge offering avenues for integration into human-centric infrastructures and enabling physics-driven humanoid animations. The daunting challenges in this field stem from the difficulty of optimizing in high-dimensional action spaces and the instability introduced by the bipedal morphology of humanoids. However, the extensive collection of human motion-captured data and the derived datasets of humanoid trajectories, such as MoCapAct, paves the way to tackle these challenges. In this context, we present Humanoid Generalist Autoencoding Planner (H-GAP), a state-action trajectory generative model trained on humanoid trajectories derived from human motion-captured data, capable of adeptly handling downstream control tasks with Model Predictive Control (MPC). For 56 degrees of freedom humanoid, we empirically demonstrate that H-GAP learns to represent and generate a wide range of motor behaviours. Further, without any learning from online interactions, it can also flexibly transfer these behaviors to solve novel downstream control tasks via planning. Notably, H-GAP excels established MPC baselines that have access to the ground truth dynamics model, and is superior or comparable to offline RL methods trained for individual tasks. Finally, we do a series of empirical studies on the scaling properties of H-GAP, showing the potential for performance gains via additional data but not computing. Code and videos are available at https://ycxuyingchen.github.io/hgap/.
<div id='section'>Paperid: <span id='pid'>774, <a href='https://arxiv.org/pdf/2310.20249.pdf' target='_blank'>https://arxiv.org/pdf/2310.20249.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qingqing Zhao, Peizhuo Li, Wang Yifan, Olga Sorkine-Hornung, Gordon Wetzstein
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.20249">Pose-to-Motion: Cross-Domain Motion Retargeting with Pose Prior</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Creating believable motions for various characters has long been a goal in computer graphics. Current learning-based motion synthesis methods depend on extensive motion datasets, which are often challenging, if not impossible, to obtain. On the other hand, pose data is more accessible, since static posed characters are easier to create and can even be extracted from images using recent advancements in computer vision. In this paper, we utilize this alternative data source and introduce a neural motion synthesis approach through retargeting. Our method generates plausible motions for characters that have only pose data by transferring motion from an existing motion capture dataset of another character, which can have drastically different skeletons. Our experiments show that our method effectively combines the motion features of the source character with the pose features of the target character, and performs robustly with small or noisy pose data sets, ranging from a few artist-created poses to noisy poses estimated directly from images. Additionally, a conducted user study indicated that a majority of participants found our retargeted motion to be more enjoyable to watch, more lifelike in appearance, and exhibiting fewer artifacts. Project page: https://cyanzhao42.github.io/pose2motion
<div id='section'>Paperid: <span id='pid'>775, <a href='https://arxiv.org/pdf/2309.16838.pdf' target='_blank'>https://arxiv.org/pdf/2309.16838.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Viet-Anh Le, Behdad Chalaki, Vaishnav Tadiparthi, Hossein Nourkhiz Mahjoub, Jovin D'sa, Ehsan Moradi-Pari
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.16838">Social Navigation in Crowded Environments with Model Predictive Control and Deep Learning-Based Human Trajectory Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Crowd navigation has received increasing attention from researchers over the last few decades, resulting in the emergence of numerous approaches aimed at addressing this problem to date. Our proposed approach couples agent motion prediction and planning to avoid the freezing robot problem while simultaneously capturing multi-agent social interactions by utilizing a state-of-the-art trajectory prediction model i.e., social long short-term memory model (Social-LSTM). Leveraging the output of Social-LSTM for the prediction of future trajectories of pedestrians at each time-step given the robot's possible actions, our framework computes the optimal control action using Model Predictive Control (MPC) for the robot to navigate among pedestrians. We demonstrate the effectiveness of our proposed approach in multiple scenarios of simulated crowd navigation and compare it against several state-of-the-art reinforcement learning-based methods.
<div id='section'>Paperid: <span id='pid'>776, <a href='https://arxiv.org/pdf/2308.07207.pdf' target='_blank'>https://arxiv.org/pdf/2308.07207.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mufeng Yao, Jiaqi Wang, Jinlong Peng, Mingmin Chi, Chao Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.07207">FOLT: Fast Multiple Object Tracking from UAV-captured Videos Based on Optical Flow</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multiple object tracking (MOT) has been successfully investigated in computer vision.
  However, MOT for the videos captured by unmanned aerial vehicles (UAV) is still challenging due to small object size, blurred object appearance, and very large and/or irregular motion in both ground objects and UAV platforms.
  In this paper, we propose FOLT to mitigate these problems and reach fast and accurate MOT in UAV view.
  Aiming at speed-accuracy trade-off, FOLT adopts a modern detector and light-weight optical flow extractor to extract object detection features and motion features at a minimum cost.
  Given the extracted flow, the flow-guided feature augmentation is designed to augment the object detection feature based on its optical flow, which improves the detection of small objects.
  Then the flow-guided motion prediction is also proposed to predict the object's position in the next frame, which improves the tracking performance of objects with very large displacements between adjacent frames.
  Finally, the tracker matches the detected objects and predicted objects using a spatially matching scheme to generate tracks for every object.
  Experiments on Visdrone and UAVDT datasets show that our proposed model can successfully track small objects with large and irregular motion and outperform existing state-of-the-art methods in UAV-MOT tasks.
<div id='section'>Paperid: <span id='pid'>777, <a href='https://arxiv.org/pdf/2308.01097.pdf' target='_blank'>https://arxiv.org/pdf/2308.01097.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiexin Wang, Yujie Zhou, Wenwen Qiang, Ying Ba, Bing Su, Ji-Rong Wen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.01097">Spatio-Temporal Branching for Motion Prediction using Motion Increments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion prediction (HMP) has emerged as a popular research topic due to its diverse applications, but it remains a challenging task due to the stochastic and aperiodic nature of future poses. Traditional methods rely on hand-crafted features and machine learning techniques, which often struggle to model the complex dynamics of human motion. Recent deep learning-based methods have achieved success by learning spatio-temporal representations of motion, but these models often overlook the reliability of motion data. Additionally, the temporal and spatial dependencies of skeleton nodes are distinct. The temporal relationship captures motion information over time, while the spatial relationship describes body structure and the relationships between different nodes. In this paper, we propose a novel spatio-temporal branching network using incremental information for HMP, which decouples the learning of temporal-domain and spatial-domain features, extracts more motion information, and achieves complementary cross-domain knowledge learning through knowledge distillation. Our approach effectively reduces noise interference and provides more expressive information for characterizing motion by separately extracting temporal and spatial features. We evaluate our approach on standard HMP benchmarks and outperform state-of-the-art methods in terms of prediction accuracy.
<div id='section'>Paperid: <span id='pid'>778, <a href='https://arxiv.org/pdf/2307.00065.pdf' target='_blank'>https://arxiv.org/pdf/2307.00065.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sariah Mghames, Luca Castri, Marc Hanheide, Nicola Bellotto
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.00065">Qualitative Prediction of Multi-Agent Spatial Interactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deploying service robots in our daily life, whether in restaurants, warehouses or hospitals, calls for the need to reason on the interactions happening in dense and dynamic scenes. In this paper, we present and benchmark three new approaches to model and predict multi-agent interactions in dense scenes, including the use of an intuitive qualitative representation. The proposed solutions take into account static and dynamic context to predict individual interactions. They exploit an input- and a temporal-attention mechanism, and are tested on medium and long-term time horizons. The first two approaches integrate different relations from the so-called Qualitative Trajectory Calculus (QTC) within a state-of-the-art deep neural network to create a symbol-driven neural architecture for predicting spatial interactions. The third approach implements a purely data-driven network for motion prediction, the output of which is post-processed to predict QTC spatial interactions. Experimental results on a popular robot dataset of challenging crowded scenarios show that the purely data-driven prediction approach generally outperforms the other two. The three approaches were further evaluated on a different but related human scenarios to assess their generalisation capability.
<div id='section'>Paperid: <span id='pid'>779, <a href='https://arxiv.org/pdf/2306.17770.pdf' target='_blank'>https://arxiv.org/pdf/2306.17770.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shaoshuai Shi, Li Jiang, Dengxin Dai, Bernt Schiele
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.17770">MTR++: Multi-Agent Motion Prediction with Symmetric Scene Modeling and Guided Intention Querying</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion prediction is crucial for autonomous driving systems to understand complex driving scenarios and make informed decisions. However, this task is challenging due to the diverse behaviors of traffic participants and complex environmental contexts. In this paper, we propose Motion TRansformer (MTR) frameworks to address these challenges. The initial MTR framework utilizes a transformer encoder-decoder structure with learnable intention queries, enabling efficient and accurate prediction of future trajectories. By customizing intention queries for distinct motion modalities, MTR improves multimodal motion prediction while reducing reliance on dense goal candidates. The framework comprises two essential processes: global intention localization, identifying the agent's intent to enhance overall efficiency, and local movement refinement, adaptively refining predicted trajectories for improved accuracy. Moreover, we introduce an advanced MTR++ framework, extending the capability of MTR to simultaneously predict multimodal motion for multiple agents. MTR++ incorporates symmetric context modeling and mutually-guided intention querying modules to facilitate future behavior interaction among multiple agents, resulting in scene-compliant future trajectories. Extensive experimental results demonstrate that the MTR framework achieves state-of-the-art performance on the highly-competitive motion prediction benchmarks, while the MTR++ framework surpasses its precursor, exhibiting enhanced performance and efficiency in predicting accurate multimodal future trajectories for multiple agents.
<div id='section'>Paperid: <span id='pid'>780, <a href='https://arxiv.org/pdf/2306.16530.pdf' target='_blank'>https://arxiv.org/pdf/2306.16530.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guanqi He, Yash Jangir, Junyi Geng, Mohammadreza Mousaei, Dongwei Bai, Sebastian Scherer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.16530">Image-based Visual Servo Control for Aerial Manipulation Using a Fully-Actuated UAV</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Using Unmanned Aerial Vehicles (UAVs) to perform high-altitude manipulation tasks beyond just passive visual application can reduce the time, cost, and risk of human workers. Prior research on aerial manipulation has relied on either ground truth state estimate or GPS/total station with some Simultaneous Localization and Mapping (SLAM) algorithms, which may not be practical for many applications close to infrastructure with degraded GPS signal or featureless environments. Visual servo can avoid the need to estimate robot pose. Existing works on visual servo for aerial manipulation either address solely end-effector position control or rely on precise velocity measurement and pre-defined visual visual marker with known pattern. Furthermore, most of previous work used under-actuated UAVs, resulting in complicated mechanical and hence control design for the end-effector. This paper develops an image-based visual servo control strategy for bridge maintenance using a fully-actuated UAV. The main components are (1) a visual line detection and tracking system, (2) a hybrid impedance force and motion control system. Our approach does not rely on either robot pose/velocity estimation from an external localization system or pre-defined visual markers. The complexity of the mechanical system and controller architecture is also minimized due to the fully-actuated nature. Experiments show that the system can effectively execute motion tracking and force holding using only the visual guidance for the bridge painting. To the best of our knowledge, this is one of the first studies on aerial manipulation using visual servo that is capable of achieving both motion and force control without the need of external pose/velocity information or pre-defined visual guidance.
<div id='section'>Paperid: <span id='pid'>781, <a href='https://arxiv.org/pdf/2306.06363.pdf' target='_blank'>https://arxiv.org/pdf/2306.06363.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Han Gao, Pengying Wu, Yao Su, Kangjie Zhou, Ji Ma, Hangxin Liu, Chang Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.06363">Probabilistic Visibility-Aware Trajectory Planning for Target Tracking in Cluttered Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Target tracking has numerous significant civilian and military applications, and maintaining the visibility of the target plays a vital role in ensuring the success of the tracking task. Existing visibility-aware planners primarily focus on keeping the target within the limited field of view of an onboard sensor and avoiding obstacle occlusion. However, the negative impact of system uncertainty is often neglected, rendering the planners delicate to uncertainties in practice. To bridge the gap, this work proposes a real-time, non-myopic trajectory planner for visibility-aware and safe target tracking in the presence of system uncertainty. For more accurate target motion prediction, we introduce the concept of belief-space probability of detection (BPOD) to measure the predictive visibility of the target under stochastic robot and target states. An Extended Kalman Filter variant incorporating BPOD is developed to predict target belief state under uncertain visibility within the planning horizon. To reach real-time trajectory planning, we propose a computationally efficient algorithm to uniformly calculate both BPOD and the chance-constrained collision risk by utilizing linearized signed distance function (SDF), and then design a two-stage strategy for lightweight calculation of SDF in sequential convex programming. Extensive simulation results with benchmark comparisons show the capacity of the proposed approach to robustly maintain the visibility of the target under high system uncertainty. The practicality of the proposed trajectory planner is validated by real-world experiments.
<div id='section'>Paperid: <span id='pid'>782, <a href='https://arxiv.org/pdf/2305.08953.pdf' target='_blank'>https://arxiv.org/pdf/2305.08953.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mark Endo, Joy Hsu, Jiaman Li, Jiajun Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.08953">Motion Question Answering via Modular Motion Programs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In order to build artificial intelligence systems that can perceive and reason with human behavior in the real world, we must first design models that conduct complex spatio-temporal reasoning over motion sequences. Moving towards this goal, we propose the HumanMotionQA task to evaluate complex, multi-step reasoning abilities of models on long-form human motion sequences. We generate a dataset of question-answer pairs that require detecting motor cues in small portions of motion sequences, reasoning temporally about when events occur, and querying specific motion attributes. In addition, we propose NSPose, a neuro-symbolic method for this task that uses symbolic reasoning and a modular design to ground motion through learning motion concepts, attribute neural operators, and temporal relations. We demonstrate the suitability of NSPose for the HumanMotionQA task, outperforming all baseline methods.
<div id='section'>Paperid: <span id='pid'>783, <a href='https://arxiv.org/pdf/2304.11740.pdf' target='_blank'>https://arxiv.org/pdf/2304.11740.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sariah Mghames, Luca Castri, Marc Hanheide, Nicola Bellotto
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.11740">A Neuro-Symbolic Approach for Enhanced Human Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reasoning on the context of human beings is crucial for many real-world applications especially for those deploying autonomous systems (e.g. robots). In this paper, we present a new approach for context reasoning to further advance the field of human motion prediction. We therefore propose a neuro-symbolic approach for human motion prediction (NeuroSyM), which weights differently the interactions in the neighbourhood by leveraging an intuitive technique for spatial representation called Qualitative Trajectory Calculus (QTC). The proposed approach is experimentally tested on medium and long term time horizons using two architectures from the state of art, one of which is a baseline for human motion prediction and the other is a baseline for generic multivariate time-series prediction. Six datasets of challenging crowded scenarios, collected from both fixed and mobile cameras, were used for testing. Experimental results show that the NeuroSyM approach outperforms in most cases the baseline architectures in terms of prediction accuracy.
<div id='section'>Paperid: <span id='pid'>784, <a href='https://arxiv.org/pdf/2301.09489.pdf' target='_blank'>https://arxiv.org/pdf/2301.09489.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alessandro Flaborea, Guido D'Amely, Stefano D'Arrigo, Marco Aurelio Sterpa, Alessio Sampieri, Fabio Galasso
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.09489">Contracting Skeletal Kinematics for Human-Related Video Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Detecting the anomaly of human behavior is paramount to timely recognizing endangering situations, such as street fights or elderly falls. However, anomaly detection is complex since anomalous events are rare and because it is an open set recognition task, i.e., what is anomalous at inference has not been observed at training. We propose COSKAD, a novel model that encodes skeletal human motion by a graph convolutional network and learns to COntract SKeletal kinematic embeddings onto a latent hypersphere of minimum volume for Video Anomaly Detection. We propose three latent spaces: the commonly-adopted Euclidean and the novel spherical and hyperbolic. All variants outperform the state-of-the-art on the most recent UBnormal dataset, for which we contribute a human-related version with annotated skeletons. COSKAD sets a new state-of-the-art on the human-related versions of ShanghaiTech Campus and CUHK Avenue, with performance comparable to video-based methods. Source code and dataset will be released upon acceptance.
<div id='section'>Paperid: <span id='pid'>785, <a href='https://arxiv.org/pdf/2212.02500.pdf' target='_blank'>https://arxiv.org/pdf/2212.02500.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ye Yuan, Jiaming Song, Umar Iqbal, Arash Vahdat, Jan Kautz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.02500">PhysDiff: Physics-Guided Human Motion Diffusion Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Denoising diffusion models hold great promise for generating diverse and realistic human motions. However, existing motion diffusion models largely disregard the laws of physics in the diffusion process and often generate physically-implausible motions with pronounced artifacts such as floating, foot sliding, and ground penetration. This seriously impacts the quality of generated motions and limits their real-world application. To address this issue, we present a novel physics-guided motion diffusion model (PhysDiff), which incorporates physical constraints into the diffusion process. Specifically, we propose a physics-based motion projection module that uses motion imitation in a physics simulator to project the denoised motion of a diffusion step to a physically-plausible motion. The projected motion is further used in the next diffusion step to guide the denoising diffusion process. Intuitively, the use of physics in our model iteratively pulls the motion toward a physically-plausible space, which cannot be achieved by simple post-processing. Experiments on large-scale human motion datasets show that our approach achieves state-of-the-art motion quality and improves physical plausibility drastically (>78% for all datasets).
<div id='section'>Paperid: <span id='pid'>786, <a href='https://arxiv.org/pdf/2210.01672.pdf' target='_blank'>https://arxiv.org/pdf/2210.01672.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>NoÃ©mie Jaquier, Leonel Rozo, Miguel GonzÃ¡lez-Duque, Viacheslav Borovitskiy, Tamim Asfour
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.01672">Bringing motion taxonomies to continuous domains via GPLVM on hyperbolic manifolds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion taxonomies serve as high-level hierarchical abstractions that classify how humans move and interact with their environment. They have proven useful to analyse grasps, manipulation skills, and whole-body support poses. Despite substantial efforts devoted to design their hierarchy and underlying categories, their use remains limited. This may be attributed to the lack of computational models that fill the gap between the discrete hierarchical structure of the taxonomy and the high-dimensional heterogeneous data associated to its categories. To overcome this problem, we propose to model taxonomy data via hyperbolic embeddings that capture the associated hierarchical structure. We achieve this by formulating a novel Gaussian process hyperbolic latent variable model that incorporates the taxonomy structure through graph-based priors on the latent space and distance-preserving back constraints. We validate our model on three different human motion taxonomies to learn hyperbolic embeddings that faithfully preserve the original graph structure. We show that our model properly encodes unseen data from existing or new taxonomy categories, and outperforms its Euclidean and VAE-based counterparts. Finally, through proof-of-concept experiments, we show that our model may be used to generate realistic trajectories between the learned embeddings.
<div id='section'>Paperid: <span id='pid'>787, <a href='https://arxiv.org/pdf/2206.05949.pdf' target='_blank'>https://arxiv.org/pdf/2206.05949.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Peixi Liu, Guangxu Zhu, Shuai Wang, Wei Jiang, Wu Luo, H. Vincent Poor, Shuguang Cui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2206.05949">Toward Ambient Intelligence: Federated Edge Learning with Task-Oriented Sensing, Computation, and Communication Integration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we address the problem of joint sensing, computation, and communication (SC$^{2}$) resource allocation for federated edge learning (FEEL) via a concrete case study of human motion recognition based on wireless sensing in ambient intelligence. First, by analyzing the wireless sensing process in human motion recognition, we find that there exists a thresholding value for the sensing transmit power, exceeding which yields sensing data samples with approximately the same satisfactory quality. Then, the joint SC$^{2}$ resource allocation problem is cast to maximize the convergence speed of FEEL, under the constraints on training time, energy supply, and sensing quality of each edge device. Solving this problem entails solving two subproblems in order: the first one reduces to determine the joint sensing and communication resource allocation that maximizes the total number of samples that can be sensed during the entire training process; the second one concerns the partition of the attained total number of sensed samples over all the communication rounds to determine the batch size at each round for convergence speed maximization. The first subproblem on joint sensing and communication resource allocation is converted to a single-variable optimization problem by exploiting the derived relation between different control variables (resources), which thus allows an efficient solution via one-dimensional grid search. For the second subproblem, it is found that the number of samples to be sensed (or batch size) at each round is a decreasing function of the loss function value attained at the round. Based on this relationship, the approximate optimal batch size at each communication round is derived in closed-form as a function of the round index. Finally, extensive simulation results are provided to validate the superiority of the proposed joint SC$^{2}$ resource allocation scheme.
<div id='section'>Paperid: <span id='pid'>788, <a href='https://arxiv.org/pdf/2003.03677.pdf' target='_blank'>https://arxiv.org/pdf/2003.03677.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Michael Bowman, Jiucai Zhang, Xiaoli Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2003.03677">An Intent-based Task-aware Shared Control Framework for Intuitive Hands Free Telemanipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Shared control in teleoperation for providing robot assistance to accomplish object manipulation, called telemanipulation, is a new promising yet challenging problem. This has unique challenges--on top of teleoperation challenges in general--due to difficulties of physical discrepancy between human hands and robot hands as well as the fine motion constraints to constitute task success. We present an intuitive shared-control strategy where the focus is on generating robotic grasp poses which are better suited for human perception of successful teleoperated object manipulation and feeling of being in control of the robot, rather than developing objective stable grasp configurations for task success or following the human motion. The former is achieved by understanding human intent and autonomously taking over control on that inference. The latter is achieved by considering human inputs as hard motion constraints which the robot must abide. An arbitration of these two enables a trade-off for the subsequent robot motion to balance accomplishing the inferred task and motion constraints imposed by the operator. The arbitration framework adapts to the level of physical discrepancy between the human and different robot structures, enabling the assistance to indicate and appear to intuitively follow the user. To understand how users perceive good arbitration in object telemanipulation, we have conducted a user study with a hands-free telemanipulation setup to analyze the effect of factors including task predictability, perceived following, and user preference. The hands-free telemanipulation scene is chosen as the validation platform due to its more urgent need of intuitive robotics assistance for task success.
<div id='section'>Paperid: <span id='pid'>789, <a href='https://arxiv.org/pdf/2510.05097.pdf' target='_blank'>https://arxiv.org/pdf/2510.05097.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Robin Courant, Xi Wang, David Loiseaux, Marc Christie, Vicky Kalogeiton
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.05097">Pulp Motion: Framing-aware multimodal camera and human motion generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Treating human motion and camera trajectory generation separately overlooks a core principle of cinematography: the tight interplay between actor performance and camera work in the screen space. In this paper, we are the first to cast this task as a text-conditioned joint generation, aiming to maintain consistent on-screen framing while producing two heterogeneous, yet intrinsically linked, modalities: human motion and camera trajectories. We propose a simple, model-agnostic framework that enforces multimodal coherence via an auxiliary modality: the on-screen framing induced by projecting human joints onto the camera. This on-screen framing provides a natural and effective bridge between modalities, promoting consistency and leading to more precise joint distribution. We first design a joint autoencoder that learns a shared latent space, together with a lightweight linear transform from the human and camera latents to a framing latent. We then introduce auxiliary sampling, which exploits this linear transform to steer generation toward a coherent framing modality. To support this task, we also introduce the PulpMotion dataset, a human-motion and camera-trajectory dataset with rich captions, and high-quality human motions. Extensive experiments across DiT- and MAR-based architectures show the generality and effectiveness of our method in generating on-frame coherent human-camera motions, while also achieving gains on textual alignment for both modalities. Our qualitative results yield more cinematographically meaningful framings setting the new state of the art for this task. Code, models and data are available in our \href{https://www.lix.polytechnique.fr/vista/projects/2025_pulpmotion_courant/}{project page}.
<div id='section'>Paperid: <span id='pid'>790, <a href='https://arxiv.org/pdf/2510.03909.pdf' target='_blank'>https://arxiv.org/pdf/2510.03909.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hyelin Nam, Hyojun Go, Byeongjun Park, Byung-Hoon Kim, Hyungjin Chung
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03909">Generating Human Motion Videos using a Cascaded Text-to-Video Framework</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human video generation is becoming an increasingly important task with broad applications in graphics, entertainment, and embodied AI. Despite the rapid progress of video diffusion models (VDMs), their use for general-purpose human video generation remains underexplored, with most works constrained to image-to-video setups or narrow domains like dance videos. In this work, we propose CAMEO, a cascaded framework for general human motion video generation. It seamlessly bridges Text-to-Motion (T2M) models and conditional VDMs, mitigating suboptimal factors that may arise in this process across both training and inference through carefully designed components. Specifically, we analyze and prepare both textual prompts and visual conditions to effectively train the VDM, ensuring robust alignment between motion descriptions, conditioning signals, and the generated videos. Furthermore, we introduce a camera-aware conditioning module that connects the two stages, automatically selecting viewpoints aligned with the input text to enhance coherence and reduce manual intervention. We demonstrate the effectiveness of our approach on both the MovieGen benchmark and a newly introduced benchmark tailored to the T2M-VDM combination, while highlighting its versatility across diverse use cases.
<div id='section'>Paperid: <span id='pid'>791, <a href='https://arxiv.org/pdf/2509.25600.pdf' target='_blank'>https://arxiv.org/pdf/2509.25600.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wontaek Kim, Tianyu Li, Sehoon Ha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25600">MoReFlow: Motion Retargeting Learning through Unsupervised Flow Matching</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion retargeting holds a premise of offering a larger set of motion data for characters and robots with different morphologies. Many prior works have approached this problem via either handcrafted constraints or paired motion datasets, limiting their applicability to humanoid characters or narrow behaviors such as locomotion. Moreover, they often assume a fixed notion of retargeting, overlooking domain-specific objectives like style preservation in animation or task-space alignment in robotics. In this work, we propose MoReFlow, Motion Retargeting via Flow Matching, an unsupervised framework that learns correspondences between characters' motion embedding spaces. Our method consists of two stages. First, we train tokenized motion embeddings for each character using a VQ-VAE, yielding compact latent representations. Then, we employ flow matching with conditional coupling to align the latent spaces across characters, which simultaneously learns conditioned and unconditioned matching to achieve robust but flexible retargeting. Once trained, MoReFlow enables flexible and reversible retargeting without requiring paired data. Experiments demonstrate that MoReFlow produces high-quality motions across diverse characters and tasks, offering improved controllability, generalization, and motion realism compared to the baselines.
<div id='section'>Paperid: <span id='pid'>792, <a href='https://arxiv.org/pdf/2509.19259.pdf' target='_blank'>https://arxiv.org/pdf/2509.19259.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Markos Diomataris, Berat Mert Albaba, Giorgio Becherini, Partha Ghosh, Omid Taheri, Michael J. Black
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.19259">Moving by Looking: Towards Vision-Driven Avatar Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The way we perceive the world fundamentally shapes how we move, whether it is how we navigate in a room or how we interact with other humans. Current human motion generation methods, neglect this interdependency and use task-specific ``perception'' that differs radically from that of humans. We argue that the generation of human-like avatar behavior requires human-like perception. Consequently, in this work we present CLOPS, the first human avatar that solely uses egocentric vision to perceive its surroundings and navigate. Using vision as the primary driver of motion however, gives rise to a significant challenge for training avatars: existing datasets have either isolated human motion, without the context of a scene, or lack scale. We overcome this challenge by decoupling the learning of low-level motion skills from learning of high-level control that maps visual input to motion. First, we train a motion prior model on a large motion capture dataset. Then, a policy is trained using Q-learning to map egocentric visual inputs to high-level control commands for the motion prior. Our experiments empirically demonstrate that egocentric vision can give rise to human-like motion characteristics in our avatars. For example, the avatars walk such that they avoid obstacles present in their visual field. These findings suggest that equipping avatars with human-like sensors, particularly egocentric vision, holds promise for training avatars that behave like humans.
<div id='section'>Paperid: <span id='pid'>793, <a href='https://arxiv.org/pdf/2508.21556.pdf' target='_blank'>https://arxiv.org/pdf/2508.21556.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ilya A. Petrov, Vladimir Guzov, Riccardo Marin, Emre Aksan, Xu Chen, Daniel Cremers, Thabo Beeler, Gerard Pons-Moll
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.21556">ECHO: Ego-Centric modeling of Human-Object interactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modeling human-object interactions (HOI) from an egocentric perspective is a largely unexplored yet important problem due to the increasing adoption of wearable devices, such as smart glasses and watches. We investigate how much information about interaction can be recovered from only head and wrists tracking. Our answer is ECHO (Ego-Centric modeling of Human-Object interactions), which, for the first time, proposes a unified framework to recover three modalities: human pose, object motion, and contact from such minimal observation. ECHO employs a Diffusion Transformer architecture and a unique three-variate diffusion process, which jointly models human motion, object trajectory, and contact sequence, allowing for flexible input configurations. Our method operates in a head-centric canonical space, enhancing robustness to global orientation. We propose a conveyor-based inference, which progressively increases the diffusion timestamp with the frame position, allowing us to process sequences of any length. Through extensive evaluation, we demonstrate that ECHO outperforms existing methods that do not offer the same flexibility, setting a state-of-the-art in egocentric HOI reconstruction.
<div id='section'>Paperid: <span id='pid'>794, <a href='https://arxiv.org/pdf/2508.21556.pdf' target='_blank'>https://arxiv.org/pdf/2508.21556.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ilya A. Petrov, Vladimir Guzov, Riccardo Marin, Emre Aksan, Xu Chen, Daniel Cremers, Thabo Beeler, Gerard Pons-Moll
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.21556">ECHO: Ego-Centric modeling of Human-Object interactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modeling human-object interactions (HOI) from an egocentric perspective is a largely unexplored yet important problem due to the increasing adoption of wearable devices, such as smart glasses and watches. We investigate how much information about interaction can be recovered from only head and wrists tracking. Our answer is ECHO (Ego-Centric modeling of Human-Object interactions), which, for the first time, proposes a unified framework to recover three modalities: human pose, object motion, and contact from such minimal observation. ECHO employs a Diffusion Transformer architecture and a unique three-variate diffusion process, which jointly models human motion, object trajectory, and contact sequence, allowing for flexible input configurations. Our method operates in a head-centric canonical space, enhancing robustness to global orientation. We propose a conveyor-based inference, which progressively increases the diffusion timestamp with the frame position, allowing us to process sequences of any length. Through extensive evaluation, we demonstrate that ECHO outperforms existing methods that do not offer the same flexibility, setting a state-of-the-art in egocentric HOI reconstruction.
<div id='section'>Paperid: <span id='pid'>795, <a href='https://arxiv.org/pdf/2506.23552.pdf' target='_blank'>https://arxiv.org/pdf/2506.23552.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingi Kwon, Joonghyuk Shin, Jaeseok Jung, Jaesik Park, Youngjung Uh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.23552">JAM-Flow: Joint Audio-Motion Synthesis with Flow Matching</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The intrinsic link between facial motion and speech is often overlooked in generative modeling, where talking head synthesis and text-to-speech (TTS) are typically addressed as separate tasks. This paper introduces JAM-Flow, a unified framework to simultaneously synthesize and condition on both facial motion and speech. Our approach leverages flow matching and a novel Multi-Modal Diffusion Transformer (MM-DiT) architecture, integrating specialized Motion-DiT and Audio-DiT modules. These are coupled via selective joint attention layers and incorporate key architectural choices, such as temporally aligned positional embeddings and localized joint attention masking, to enable effective cross-modal interaction while preserving modality-specific strengths. Trained with an inpainting-style objective, JAM-Flow supports a wide array of conditioning inputs-including text, reference audio, and reference motion-facilitating tasks such as synchronized talking head generation from text, audio-driven animation, and much more, within a single, coherent model. JAM-Flow significantly advances multi-modal generative modeling by providing a practical solution for holistic audio-visual synthesis. project page: https://joonghyuk.com/jamflow-web
<div id='section'>Paperid: <span id='pid'>796, <a href='https://arxiv.org/pdf/2506.23236.pdf' target='_blank'>https://arxiv.org/pdf/2506.23236.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Marko Mihajlovic, Siwei Zhang, Gen Li, Kaifeng Zhao, Lea MÃ¼ller, Siyu Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.23236">VolumetricSMPL: A Neural Volumetric Body Model for Efficient Interactions, Contacts, and Collisions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Parametric human body models play a crucial role in computer graphics and vision, enabling applications ranging from human motion analysis to understanding human-environment interactions. Traditionally, these models use surface meshes, which pose challenges in efficiently handling interactions with other geometric entities, such as objects and scenes, typically represented as meshes or point clouds. To address this limitation, recent research has explored volumetric neural implicit body models. However, existing works are either insufficiently robust for complex human articulations or impose high computational and memory costs, limiting their widespread use. To this end, we introduce VolumetricSMPL, a neural volumetric body model that leverages Neural Blend Weights (NBW) to generate compact, yet efficient MLP decoders. Unlike prior approaches that rely on large MLPs, NBW dynamically blends a small set of learned weight matrices using predicted shape- and pose-dependent coefficients, significantly improving computational efficiency while preserving expressiveness. VolumetricSMPL outperforms prior volumetric occupancy model COAP with 10x faster inference, 6x lower GPU memory usage, enhanced accuracy, and a Signed Distance Function (SDF) for efficient and differentiable contact modeling. We demonstrate VolumetricSMPL's strengths across four challenging tasks: (1) reconstructing human-object interactions from in-the-wild images, (2) recovering human meshes in 3D scenes from egocentric views, (3) scene-constrained motion synthesis, and (4) resolving self-intersections. Our results highlight its broad applicability and significant performance and efficiency gains.
<div id='section'>Paperid: <span id='pid'>797, <a href='https://arxiv.org/pdf/2506.08541.pdf' target='_blank'>https://arxiv.org/pdf/2506.08541.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qi Yan, Brian Zhang, Yutong Zhang, Daniel Yang, Joshua White, Di Chen, Jiachao Liu, Langechuan Liu, Binnan Zhuang, Shaoshuai Shi, Renjie Liao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.08541">TrajFlow: Multi-modal Motion Prediction via Flow Matching</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Efficient and accurate motion prediction is crucial for ensuring safety and informed decision-making in autonomous driving, particularly under dynamic real-world conditions that necessitate multi-modal forecasts. We introduce TrajFlow, a novel flow matching-based motion prediction framework that addresses the scalability and efficiency challenges of existing generative trajectory prediction methods. Unlike conventional generative approaches that employ i.i.d. sampling and require multiple inference passes to capture diverse outcomes, TrajFlow predicts multiple plausible future trajectories in a single pass, significantly reducing computational overhead while maintaining coherence across predictions. Moreover, we propose a ranking loss based on the Plackett-Luce distribution to improve uncertainty estimation of predicted trajectories. Additionally, we design a self-conditioning training technique that reuses the model's own predictions to construct noisy inputs during a second forward pass, thereby improving generalization and accelerating inference. Extensive experiments on the large-scale Waymo Open Motion Dataset (WOMD) demonstrate that TrajFlow achieves state-of-the-art performance across various key metrics, underscoring its effectiveness for safety-critical autonomous driving applications. The code and other details are available on the project website https://traj-flow.github.io/.
<div id='section'>Paperid: <span id='pid'>798, <a href='https://arxiv.org/pdf/2504.08366.pdf' target='_blank'>https://arxiv.org/pdf/2504.08366.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sauradip Nag, Daniel Cohen-Or, Hao Zhang, Ali Mahdavi-Amiri
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.08366">In-2-4D: Inbetweening from Two Single-View Images to 4D Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We pose a new problem, In-2-4D, for generative 4D (i.e., 3D + motion) inbetweening to interpolate two single-view images. In contrast to video/4D generation from only text or a single image, our interpolative task can leverage more precise motion control to better constrain the generation. Given two monocular RGB images representing the start and end states of an object in motion, our goal is to generate and reconstruct the motion in 4D, without making assumptions on the object category, motion type, length, or complexity. To handle such arbitrary and diverse motions, we utilize a foundational video interpolation model for motion prediction. However, large frame-to-frame motion gaps can lead to ambiguous interpretations. To this end, we employ a hierarchical approach to identify keyframes that are visually close to the input states while exhibiting significant motions, then generate smooth fragments between them. For each fragment, we construct a 3D representation of the keyframe using Gaussian Splatting (3DGS). The temporal frames within the fragment guide the motion, enabling their transformation into dynamic 3DGS through a deformation field. To improve temporal consistency and refine the 3D motion, we expand the self-attention of multi-view diffusion across timesteps and apply rigid transformation regularization. Finally, we merge the independently generated 3D motion segments by interpolating boundary deformation fields and optimizing them to align with the guiding video, ensuring smooth and flicker-free transitions. Through extensive qualitative and quantitive experiments as well as a user study, we demonstrate the effectiveness of our method and design choices.
<div id='section'>Paperid: <span id='pid'>799, <a href='https://arxiv.org/pdf/2504.02478.pdf' target='_blank'>https://arxiv.org/pdf/2504.02478.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bizhu Wu, Jinheng Xie, Keming Shen, Zhe Kong, Jianfeng Ren, Ruibin Bai, Rong Qu, Linlin Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.02478">MG-MotionLLM: A Unified Framework for Motion Comprehension and Generation across Multiple Granularities</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent motion-aware large language models have demonstrated promising potential in unifying motion comprehension and generation. However, existing approaches primarily focus on coarse-grained motion-text modeling, where text describes the overall semantics of an entire motion sequence in just a few words. This limits their ability to handle fine-grained motion-relevant tasks, such as understanding and controlling the movements of specific body parts. To overcome this limitation, we pioneer MG-MotionLLM, a unified motion-language model for multi-granular motion comprehension and generation. We further introduce a comprehensive multi-granularity training scheme by incorporating a set of novel auxiliary tasks, such as localizing temporal boundaries of motion segments via detailed text as well as motion detailed captioning, to facilitate mutual reinforcement for motion-text modeling across various levels of granularity. Extensive experiments show that our MG-MotionLLM achieves superior performance on classical text-to-motion and motion-to-text tasks, and exhibits potential in novel fine-grained motion comprehension and editing tasks. Project page: CVI-SZU/MG-MotionLLM
<div id='section'>Paperid: <span id='pid'>800, <a href='https://arxiv.org/pdf/2503.13229.pdf' target='_blank'>https://arxiv.org/pdf/2503.13229.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yongkang Cheng, Shaoli Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.13229">HoloGest: Decoupled Diffusion and Motion Priors for Generating Holisticly Expressive Co-speech Gestures</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Animating virtual characters with holistic co-speech gestures is a challenging but critical task. Previous systems have primarily focused on the weak correlation between audio and gestures, leading to physically unnatural outcomes that degrade the user experience. To address this problem, we introduce HoleGest, a novel neural network framework based on decoupled diffusion and motion priors for the automatic generation of high-quality, expressive co-speech gestures. Our system leverages large-scale human motion datasets to learn a robust prior with low audio dependency and high motion reliance, enabling stable global motion and detailed finger movements. To improve the generation efficiency of diffusion-based models, we integrate implicit joint constraints with explicit geometric and conditional constraints, capturing complex motion distributions between large strides. This integration significantly enhances generation speed while maintaining high-quality motion. Furthermore, we design a shared embedding space for gesture-transcription text alignment, enabling the generation of semantically correct gesture actions. Extensive experiments and user feedback demonstrate the effectiveness and potential applications of our model, with our method achieving a level of realism close to the ground truth, providing an immersive user experience. Our code, model, and demo are are available at https://cyk990422.github.io/HoloGest.github.io/.
<div id='section'>Paperid: <span id='pid'>801, <a href='https://arxiv.org/pdf/2503.09950.pdf' target='_blank'>https://arxiv.org/pdf/2503.09950.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxiang Fu, Qi Yan, Lele Wang, Ke Li, Renjie Liao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.09950">MoFlow: One-Step Flow Matching for Human Trajectory Forecasting via Implicit Maximum Likelihood Estimation based Distillation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we address the problem of human trajectory forecasting, which aims to predict the inherently multi-modal future movements of humans based on their past trajectories and other contextual cues. We propose a novel motion prediction conditional flow matching model, termed MoFlow, to predict K-shot future trajectories for all agents in a given scene. We design a novel flow matching loss function that not only ensures at least one of the $K$ sets of future trajectories is accurate but also encourages all $K$ sets of future trajectories to be diverse and plausible. Furthermore, by leveraging the implicit maximum likelihood estimation (IMLE), we propose a novel distillation method for flow models that only requires samples from the teacher model. Extensive experiments on the real-world datasets, including SportVU NBA games, ETH-UCY, and SDD, demonstrate that both our teacher flow model and the IMLE-distilled student model achieve state-of-the-art performance. These models can generate diverse trajectories that are physically and socially plausible. Moreover, our one-step student model is $\textbf{100}$ times faster than the teacher flow model during sampling. The code, model, and data are available at our project page: https://moflow-imle.github.io
<div id='section'>Paperid: <span id='pid'>802, <a href='https://arxiv.org/pdf/2502.17834.pdf' target='_blank'>https://arxiv.org/pdf/2502.17834.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Parag Khanna, MÃ¥rten BjÃ¶rkman, Christian Smith
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.17834">Impact of Object Weight in Handovers: Inspiring Robotic Grip Release and Motion from Human Handovers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work explores the effect of object weight on human motion and grip release during handovers to enhance the naturalness, safety, and efficiency of robot-human interactions. We introduce adaptive robotic strategies based on the analysis of human handover behavior with varying object weights. The key contributions of this work includes the development of an adaptive grip-release strategy for robots, a detailed analysis of how object weight influences human motion to guide robotic motion adaptations, and the creation of handover-datasets incorporating various object weights, including the YCB handover dataset. By aligning robotic grip release and motion with human behavior, this work aims to improve robot-human handovers for different weighted objects. We also evaluate these human-inspired adaptive robotic strategies in robot-to-human handovers to assess their effectiveness and performance and demonstrate that they outperform the baseline approaches in terms of naturalness, efficiency, and user perception.
<div id='section'>Paperid: <span id='pid'>803, <a href='https://arxiv.org/pdf/2502.12546.pdf' target='_blank'>https://arxiv.org/pdf/2502.12546.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sang-Eun Lee, Ko Nishino, Shohei Nobuhara
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.12546">Spatiotemporal Multi-Camera Calibration using Freely Moving People</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a novel method for spatiotemporal multi-camera calibration using freely moving people in multiview videos. Since calibrating multiple cameras and finding matches across their views are inherently interdependent, performing both in a unified framework poses a significant challenge. We address these issues as a single registration problem of matching two sets of 3D points, leveraging human motion in dynamic multi-person scenes. To this end, we utilize 3D human poses obtained from an off-the-shelf monocular 3D human pose estimator and transform them into 3D points on a unit sphere, to solve the rotation, time offset, and the association alternatingly. We employ a probabilistic approach that can jointly solve both problems of aligning spatiotemporal data and establishing correspondences through soft assignment between two views. The translation is determined by applying coplanarity constraints. The pairwise registration results are integrated into a multiview setup, and then a nonlinear optimization method is used to improve the accuracy of the camera poses, temporal offsets, and multi-person associations. Extensive experiments on synthetic and real data demonstrate the effectiveness and flexibility of the proposed method as a practical marker-free calibration tool.
<div id='section'>Paperid: <span id='pid'>804, <a href='https://arxiv.org/pdf/2502.07531.pdf' target='_blank'>https://arxiv.org/pdf/2502.07531.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sixiao Zheng, Zimian Peng, Yanpeng Zhou, Yi Zhu, Hang Xu, Xiangru Huang, Yanwei Fu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.07531">VidCRAFT3: Camera, Object, and Lighting Control for Image-to-Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Controllable image-to-video (I2V) generation transforms a reference image into a coherent video guided by user-specified control signals. In content creation workflows, precise and simultaneous control over camera motion, object motion, and lighting direction enhances both accuracy and flexibility. However, existing approaches typically treat these control signals separately, largely due to the scarcity of datasets with high-quality joint annotations and mismatched control spaces across modalities. We present VidCRAFT3, a unified and flexible I2V framework that supports both independent and joint control over camera motion, object motion, and lighting direction by integrating three core components. Image2Cloud reconstructs a 3D point cloud from the reference image to enable precise camera motion control. ObjMotionNet encodes sparse object trajectories into multi-scale optical flow features to guide object motion. The Spatial Triple-Attention Transformer integrates lighting direction embeddings via parallel cross-attention. To address the scarcity of jointly annotated data, we curate the VideoLightingDirection (VLD) dataset of synthetic static-scene video clips with per-frame lighting-direction labels, and adopt a three-stage training strategy that enables robust learning without fully joint annotations. Extensive experiments show that VidCRAFT3 outperforms existing methods in control precision and visual coherence. Code and data will be released. Project page: https://sixiaozheng.github.io/VidCRAFT3/.
<div id='section'>Paperid: <span id='pid'>805, <a href='https://arxiv.org/pdf/2502.07358.pdf' target='_blank'>https://arxiv.org/pdf/2502.07358.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoran Chen, Yiteng Xu, Yiming Ren, Yaoqin Ye, Xinran Li, Ning Ding, Yuxuan Wu, Yaoze Liu, Peishan Cong, Ziyi Wang, Bushi Liu, Yuhan Chen, Zhiyang Dou, Xiaokun Leng, Manyi Li, Yuexin Ma, Changhe Tu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.07358">SymBridge: A Human-in-the-Loop Cyber-Physical Interactive System for Adaptive Human-Robot Symbiosis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The development of intelligent robots seeks to seamlessly integrate them into the human world, providing assistance and companionship in daily life and work, with the ultimate goal of achieving human-robot symbiosis. This requires robots with intelligent interaction abilities to work naturally and effectively with humans. However, current robotic simulators fail to support real human participation, limiting their ability to provide authentic interaction experiences and gather valuable human feedback essential for enhancing robotic capabilities. In this paper, we introduce SymBridge, the first human-in-the-loop cyber-physical interactive system designed to enable the safe and efficient development, evaluation, and optimization of human-robot interaction methods. Specifically, we employ augmented reality technology to enable real humans to interact with virtual robots in physical environments, creating an authentic interactive experience. Building on this, we propose a novel robotic interaction model that generates responsive, precise robot actions in real time through continuous human behavior observation. The model incorporates multi-resolution human motion features and environmental affordances, ensuring contextually adaptive robotic responses. Additionally, SymBridge enables continuous robot learning by collecting human feedback and dynamically adapting the robotic interaction model. By leveraging a carefully designed system architecture and modules, SymBridge builds a bridge between humans and robots, as well as between cyber and physical spaces, providing a natural and realistic online interaction experience while facilitating the continuous evolution of robotic intelligence. Extensive experiments, user studies, and real robot testing demonstrate the promising performance of the system and highlight its potential to significantly advance research on human-robot symbiosis.
<div id='section'>Paperid: <span id='pid'>806, <a href='https://arxiv.org/pdf/2412.19089.pdf' target='_blank'>https://arxiv.org/pdf/2412.19089.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Changwoon Choi, Jeongjun Kim, Geonho Cha, Minkwan Kim, Dongyoon Wee, Young Min Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.19089">Humans as a Calibration Pattern: Dynamic 3D Scene Reconstruction from Unsynchronized and Uncalibrated Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent works on dynamic 3D neural field reconstruction assume the input from synchronized multi-view videos whose poses are known. The input constraints are often not satisfied in real-world setups, making the approach impractical. We show that unsynchronized videos from unknown poses can generate dynamic neural fields as long as the videos capture human motion. Humans are one of the most common dynamic subjects captured in videos, and their shapes and poses can be estimated using state-of-the-art libraries. While noisy, the estimated human shape and pose parameters provide a decent initialization point to start the highly non-convex and under-constrained problem of training a consistent dynamic neural representation. Given the shape and pose parameters of humans in individual frames, we formulate methods to calculate the time offsets between videos, followed by camera pose estimations that analyze the 3D joint positions. Then, we train the dynamic neural fields employing multiresolution grids while we concurrently refine both time offsets and camera poses. The setup still involves optimizing many parameters; therefore, we introduce a robust progressive learning strategy to stabilize the process. Experiments show that our approach achieves accurate spatio-temporal calibration and high-quality scene reconstruction in challenging conditions.
<div id='section'>Paperid: <span id='pid'>807, <a href='https://arxiv.org/pdf/2411.17765.pdf' target='_blank'>https://arxiv.org/pdf/2411.17765.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wanquan Feng, Tianhao Qi, Jiawei Liu, Mingzhen Sun, Pengqi Tu, Tianxiang Ma, Fei Dai, Songtao Zhao, Siyu Zhou, Qian He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.17765">I2VControl: Disentangled and Unified Video Motion Synthesis Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion controllability is crucial in video synthesis. However, most previous methods are limited to single control types, and combining them often results in logical conflicts. In this paper, we propose a disentangled and unified framework, namely I2VControl, to overcome the logical conflicts. We rethink camera control, object dragging, and motion brush, reformulating all tasks into a consistent representation based on point trajectories, each managed by a dedicated formulation. Accordingly, we propose a spatial partitioning strategy, where each unit is assigned to a concomitant control category, enabling diverse control types to be dynamically orchestrated within a single synthesis pipeline without conflicts. Furthermore, we design an adapter structure that functions as a plug-in for pre-trained models and is agnostic to specific model architectures. We conduct extensive experiments, achieving excellent performance on various control tasks, and our method further facilitates user-driven creative combinations, enhancing innovation and creativity. Project page: https://wanquanf.github.io/I2VControl .
<div id='section'>Paperid: <span id='pid'>808, <a href='https://arxiv.org/pdf/2410.20358.pdf' target='_blank'>https://arxiv.org/pdf/2410.20358.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingjiang Liang, Yongkang Cheng, Hualin Liang, Shaoli Huang, Wei Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.20358">RopeTP: Global Human Motion Recovery via Integrating Robust Pose Estimation with Diffusion Trajectory Prior</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present RopeTP, a novel framework that combines Robust pose estimation with a diffusion Trajectory Prior to reconstruct global human motion from videos. At the heart of RopeTP is a hierarchical attention mechanism that significantly improves context awareness, which is essential for accurately inferring the posture of occluded body parts. This is achieved by exploiting the relationships with visible anatomical structures, enhancing the accuracy of local pose estimations. The improved robustness of these local estimations allows for the reconstruction of precise and stable global trajectories. Additionally, RopeTP incorporates a diffusion trajectory model that predicts realistic human motion from local pose sequences. This model ensures that the generated trajectories are not only consistent with observed local actions but also unfold naturally over time, thereby improving the realism and stability of 3D human motion reconstruction. Extensive experimental validation shows that RopeTP surpasses current methods on two benchmark datasets, particularly excelling in scenarios with occlusions. It also outperforms methods that rely on SLAM for initial camera estimates and extensive optimization, delivering more accurate and realistic trajectories.
<div id='section'>Paperid: <span id='pid'>809, <a href='https://arxiv.org/pdf/2410.15154.pdf' target='_blank'>https://arxiv.org/pdf/2410.15154.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yin Li, Liangwei Wang, Shiyuan Piao, Boo-Ho Yang, Ziyue Li, Wei Zeng, Fugee Tsung
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.15154">MCCoder: Streamlining Motion Control with LLM-Assisted Code Generation and Rigorous Verification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs) have demonstrated significant potential in code generation. However, in the factory automation sector, particularly motion control, manual programming, alongside inefficient and unsafe debugging practices, remains prevalent. This stems from the complex interplay of mechanical and electrical systems and stringent safety requirements. Moreover, most current AI-assisted motion control programming efforts focus on PLCs, with little attention given to high-level languages and function libraries. To address these challenges, we introduce MCCoder, an LLM-powered system tailored for generating motion control code, integrated with a soft-motion controller. MCCoder improves code generation through a structured workflow that combines multitask decomposition, hybrid retrieval-augmented generation (RAG), and iterative self-correction, utilizing a well-established motion library. Additionally, it integrates a 3D simulator for intuitive motion validation and logs of full motion trajectories for data verification, significantly enhancing accuracy and safety. In the absence of benchmark datasets and metrics tailored for evaluating motion control code generation, we propose MCEVAL, a dataset spanning motion tasks of varying complexity. Experiments show that MCCoder outperforms baseline models using Advanced RAG, achieving an overall performance gain of 33.09% and a 131.77% improvement on complex tasks in the MCEVAL dataset.
<div id='section'>Paperid: <span id='pid'>810, <a href='https://arxiv.org/pdf/2410.14508.pdf' target='_blank'>https://arxiv.org/pdf/2410.14508.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nefeli Andreou, Xi Wang, Victoria FernÃ¡ndez Abrevaya, Marie-Paule Cani, Yiorgos Chrysanthou, Vicky Kalogeiton
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.14508">LEAD: Latent Realignment for Human Motion Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Our goal is to generate realistic human motion from natural language. Modern methods often face a trade-off between model expressiveness and text-to-motion alignment. Some align text and motion latent spaces but sacrifice expressiveness; others rely on diffusion models producing impressive motions, but lacking semantic meaning in their latent space. This may compromise realism, diversity, and applicability. Here, we address this by combining latent diffusion with a realignment mechanism, producing a novel, semantically structured space that encodes the semantics of language. Leveraging this capability, we introduce the task of textual motion inversion to capture novel motion concepts from a few examples. For motion synthesis, we evaluate LEAD on HumanML3D and KIT-ML and show comparable performance to the state-of-the-art in terms of realism, diversity, and text-motion consistency. Our qualitative analysis and user study reveal that our synthesized motions are sharper, more human-like and comply better with the text compared to modern methods. For motion textual inversion, our method demonstrates improved capacity in capturing out-of-distribution characteristics in comparison to traditional VAEs.
<div id='section'>Paperid: <span id='pid'>811, <a href='https://arxiv.org/pdf/2410.07296.pdf' target='_blank'>https://arxiv.org/pdf/2410.07296.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gaoge Han, Mingjiang Liang, Jinglei Tang, Yongkang Cheng, Wei Liu, Shaoli Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.07296">ReinDiffuse: Crafting Physically Plausible Motions with Reinforced Diffusion Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating human motion from textual descriptions is a challenging task. Existing methods either struggle with physical credibility or are limited by the complexities of physics simulations. In this paper, we present \emph{ReinDiffuse} that combines reinforcement learning with motion diffusion model to generate physically credible human motions that align with textual descriptions. Our method adapts Motion Diffusion Model to output a parameterized distribution of actions, making them compatible with reinforcement learning paradigms. We employ reinforcement learning with the objective of maximizing physically plausible rewards to optimize motion generation for physical fidelity. Our approach outperforms existing state-of-the-art models on two major datasets, HumanML3D and KIT-ML, achieving significant improvements in physical plausibility and motion quality. Project: https://reindiffuse.github.io/
<div id='section'>Paperid: <span id='pid'>812, <a href='https://arxiv.org/pdf/2410.01968.pdf' target='_blank'>https://arxiv.org/pdf/2410.01968.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenshuai Zhao, Yi Zhao, Joni Pajarinen, Michael Muehlebach
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.01968">Bi-Level Motion Imitation for Humanoid Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Imitation learning from human motion capture (MoCap) data provides a promising way to train humanoid robots. However, due to differences in morphology, such as varying degrees of joint freedom and force limits, exact replication of human behaviors may not be feasible for humanoid robots. Consequently, incorporating physically infeasible MoCap data in training datasets can adversely affect the performance of the robot policy. To address this issue, we propose a bi-level optimization-based imitation learning framework that alternates between optimizing both the robot policy and the target MoCap data. Specifically, we first develop a generative latent dynamics model using a novel self-consistent auto-encoder, which learns sparse and structured motion representations while capturing desired motion patterns in the dataset. The dynamics model is then utilized to generate reference motions while the latent representation regularizes the bi-level motion imitation process. Simulations conducted with a realistic model of a humanoid robot demonstrate that our method enhances the robot policy by modifying reference motions to be physically consistent.
<div id='section'>Paperid: <span id='pid'>813, <a href='https://arxiv.org/pdf/2409.03944.pdf' target='_blank'>https://arxiv.org/pdf/2409.03944.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shashank Tripathi, Omid Taheri, Christoph Lassner, Michael J. Black, Daniel Holden, Carsten Stoll
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.03944">HUMOS: Human Motion Model Conditioned on Body Shape</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating realistic human motion is essential for many computer vision and graphics applications. The wide variety of human body shapes and sizes greatly impacts how people move. However, most existing motion models ignore these differences, relying on a standardized, average body. This leads to uniform motion across different body types, where movements don't match their physical characteristics, limiting diversity. To solve this, we introduce a new approach to develop a generative motion model based on body shape. We show that it's possible to train this model using unpaired data by applying cycle consistency, intuitive physics, and stability constraints, which capture the relationship between identity and movement. The resulting model generates diverse, physically plausible, and dynamically stable human motions that are both quantitatively and qualitatively more realistic than current state-of-the-art methods. More details are available on our project page https://CarstenEpic.github.io/humos/.
<div id='section'>Paperid: <span id='pid'>814, <a href='https://arxiv.org/pdf/2409.02324.pdf' target='_blank'>https://arxiv.org/pdf/2409.02324.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lina MarÃ­a Amaya-MejÃ­a, Mohamed Ghita, Jan Dentler, Miguel Olivares-Mendez, Carol Martinez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.02324">Visual Servoing for Robotic On-Orbit Servicing: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>On-orbit servicing (OOS) activities will power the next big step for sustainable exploration and commercialization of space. Developing robotic capabilities for autonomous OOS operations is a priority for the space industry. Visual Servoing (VS) enables robots to achieve the precise manoeuvres needed for critical OOS missions by utilizing visual information for motion control. This article presents an overview of existing VS approaches for autonomous OOS operations with space manipulator systems (SMS). We divide the approaches according to their contribution to the typical phases of a robotic OOS mission: a) Recognition, b) Approach, and c) Contact. We also present a discussion on the reviewed VS approaches, identifying current trends. Finally, we highlight the challenges and areas for future research on VS techniques for robotic OOS.
<div id='section'>Paperid: <span id='pid'>815, <a href='https://arxiv.org/pdf/2408.16659.pdf' target='_blank'>https://arxiv.org/pdf/2408.16659.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xingjian Han, Yu Jiang, Weiming Wang, Guoxin Fang, Simeon Gill, Zhiqiang Zhang, Shengfa Wang, Jun Saito, Deepak Kumar, Zhongxuan Luo, Emily Whiting, Charlie C. L. Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.16659">Motion-Driven Neural Optimizer for Prophylactic Braces Made by Distributed Microstructures</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Joint injuries, and their long-term consequences, present a substantial global health burden. Wearable prophylactic braces are an attractive potential solution to reduce the incidence of joint injuries by limiting joint movements that are related to injury risk. Given human motion and ground reaction forces, we present a computational framework that enables the design of personalized braces by optimizing the distribution of microstructures and elasticity. As varied brace designs yield different reaction forces that influence kinematics and kinetics analysis outcomes, the optimization process is formulated as a differentiable end-to-end pipeline in which the design domain of microstructure distribution is parameterized onto a neural network. The optimized distribution of microstructures is obtained via a self-learning process to determine the network coefficients according to a carefully designed set of losses and the integrated biomechanical and physical analyses. Since knees and ankles are the most commonly injured joints, we demonstrate the effectiveness of our pipeline by designing, fabricating, and testing prophylactic braces for the knee and ankle to prevent potentially harmful joint movements.
<div id='section'>Paperid: <span id='pid'>816, <a href='https://arxiv.org/pdf/2407.17502.pdf' target='_blank'>https://arxiv.org/pdf/2407.17502.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fatemeh Zargarbashi, Fabrizio Di Giuro, Jin Cheng, Dongho Kang, Bhavya Sukhija, Stelian Coros
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.17502">MetaLoco: Universal Quadrupedal Locomotion with Meta-Reinforcement Learning and Motion Imitation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work presents a meta-reinforcement learning approach to develop a universal locomotion control policy capable of zero-shot generalization across diverse quadrupedal platforms. The proposed method trains an RL agent equipped with a memory unit to imitate reference motions using a small set of procedurally generated quadruped robots. Through comprehensive simulation and real-world hardware experiments, we demonstrate the efficacy of our approach in achieving locomotion across various robots without requiring robot-specific fine-tuning. Furthermore, we highlight the critical role of the memory unit in enabling generalization, facilitating rapid adaptation to changes in the robot properties, and improving sample efficiency.
<div id='section'>Paperid: <span id='pid'>817, <a href='https://arxiv.org/pdf/2407.02104.pdf' target='_blank'>https://arxiv.org/pdf/2407.02104.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nicola Messina, Jan Sedmidubsky, Fabrizio Falchi, TomÃ¡Å¡ Rebok
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.02104">Joint-Dataset Learning and Cross-Consistent Regularization for Text-to-Motion Retrieval</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pose-estimation methods enable extracting human motion from common videos in the structured form of 3D skeleton sequences. Despite great application opportunities, effective content-based access to such spatio-temporal motion data is a challenging problem. In this paper, we focus on the recently introduced text-motion retrieval tasks, which aim to search for database motions that are the most relevant to a specified natural-language textual description (text-to-motion) and vice-versa (motion-to-text). Despite recent efforts to explore these promising avenues, a primary challenge remains the insufficient data available to train robust text-motion models effectively. To address this issue, we propose to investigate joint-dataset learning - where we train on multiple text-motion datasets simultaneously - together with the introduction of a Cross-Consistent Contrastive Loss function (CCCL), which regularizes the learned text-motion common space by imposing uni-modal constraints that augment the representation ability of the trained network. To learn a proper motion representation, we also introduce a transformer-based motion encoder, called MoT++, which employs spatio-temporal attention to process sequences of skeleton data. We demonstrate the benefits of the proposed approaches on the widely-used KIT Motion-Language and HumanML3D datasets. We perform detailed experimentation on joint-dataset learning and cross-dataset scenarios, showing the effectiveness of each introduced module in a carefully conducted ablation study and, in turn, pointing out the limitations of state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>818, <a href='https://arxiv.org/pdf/2405.04963.pdf' target='_blank'>https://arxiv.org/pdf/2405.04963.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yitong Jin, Zhiping Qiu, Yi Shi, Shuangpeng Sun, Chongwu Wang, Donghao Pan, Jiachen Zhao, Zhenghao Liang, Yuan Wang, Xiaobing Li, Feng Yu, Tao Yu, Qionghai Dai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.04963">Audio Matters Too! Enhancing Markerless Motion Capture with Audio Signals for String Performance Capture</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we touch on the problem of markerless multi-modal human motion capture especially for string performance capture which involves inherently subtle hand-string contacts and intricate movements. To fulfill this goal, we first collect a dataset, named String Performance Dataset (SPD), featuring cello and violin performances. The dataset includes videos captured from up to 23 different views, audio signals, and detailed 3D motion annotations of the body, hands, instrument, and bow. Moreover, to acquire the detailed motion annotations, we propose an audio-guided multi-modal motion capture framework that explicitly incorporates hand-string contacts detected from the audio signals for solving detailed hand poses. This framework serves as a baseline for string performance capture in a completely markerless manner without imposing any external devices on performers, eliminating the potential of introducing distortion in such delicate movements. We argue that the movements of performers, particularly the sound-producing gestures, contain subtle information often elusive to visual methods but can be inferred and retrieved from audio cues. Consequently, we refine the vision-based motion capture results through our innovative audio-guided approach, simultaneously clarifying the contact relationship between the performer and the instrument, as deduced from the audio. We validate the proposed framework and conduct ablation studies to demonstrate its efficacy. Our results outperform current state-of-the-art vision-based algorithms, underscoring the feasibility of augmenting visual motion capture with audio modality. To the best of our knowledge, SPD is the first dataset for musical instrument performance, covering fine-grained hand motion details in a multi-modal, large-scale collection.
<div id='section'>Paperid: <span id='pid'>819, <a href='https://arxiv.org/pdf/2403.13900.pdf' target='_blank'>https://arxiv.org/pdf/2403.13900.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiming Huang, Weilin Wan, Yue Yang, Chris Callison-Burch, Mark Yatskar, Lingjie Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.13900">CoMo: Controllable Motion Generation through Language Guided Pose Code Editing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-to-motion models excel at efficient human motion generation, but existing approaches lack fine-grained controllability over the generation process. Consequently, modifying subtle postures within a motion or inserting new actions at specific moments remains a challenge, limiting the applicability of these methods in diverse scenarios. In light of these challenges, we introduce CoMo, a Controllable Motion generation model, adept at accurately generating and editing motions by leveraging the knowledge priors of large language models (LLMs). Specifically, CoMo decomposes motions into discrete and semantically meaningful pose codes, with each code encapsulating the semantics of a body part, representing elementary information such as "left knee slightly bent". Given textual inputs, CoMo autoregressively generates sequences of pose codes, which are then decoded into 3D motions. Leveraging pose codes as interpretable representations, an LLM can directly intervene in motion editing by adjusting the pose codes according to editing instructions. Experiments demonstrate that CoMo achieves competitive performance in motion generation compared to state-of-the-art models while, in human studies, CoMo substantially surpasses previous work in motion editing abilities.
<div id='section'>Paperid: <span id='pid'>820, <a href='https://arxiv.org/pdf/2401.06146.pdf' target='_blank'>https://arxiv.org/pdf/2401.06146.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianyu Li, Calvin Qiao, Guanqiao Ren, KangKang Yin, Sehoon Ha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.06146">AAMDM: Accelerated Auto-regressive Motion Diffusion Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Interactive motion synthesis is essential in creating immersive experiences in entertainment applications, such as video games and virtual reality. However, generating animations that are both high-quality and contextually responsive remains a challenge. Traditional techniques in the game industry can produce high-fidelity animations but suffer from high computational costs and poor scalability. Trained neural network models alleviate the memory and speed issues, yet fall short on generating diverse motions. Diffusion models offer diverse motion synthesis with low memory usage, but require expensive reverse diffusion processes. This paper introduces the Accelerated Auto-regressive Motion Diffusion Model (AAMDM), a novel motion synthesis framework designed to achieve quality, diversity, and efficiency all together. AAMDM integrates Denoising Diffusion GANs as a fast Generation Module, and an Auto-regressive Diffusion Model as a Polishing Module. Furthermore, AAMDM operates in a lower-dimensional embedded space rather than the full-dimensional pose space, which reduces the training complexity as well as further improves the performance. We show that AAMDM outperforms existing methods in motion quality, diversity, and runtime efficiency, through comprehensive quantitative analyses and visual comparisons. We also demonstrate the effectiveness of each algorithmic component through ablation studies.
<div id='section'>Paperid: <span id='pid'>821, <a href='https://arxiv.org/pdf/2311.18303.pdf' target='_blank'>https://arxiv.org/pdf/2311.18303.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhangsihao Yang, Mingyuan Zhou, Mengyi Shan, Bingbing Wen, Ziwei Xuan, Mitch Hill, Junjie Bai, Guo-Jun Qi, Yalin Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.18303">OmniMotionGPT: Animal Motion Generation with Limited Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Our paper aims to generate diverse and realistic animal motion sequences from textual descriptions, without a large-scale animal text-motion dataset. While the task of text-driven human motion synthesis is already extensively studied and benchmarked, it remains challenging to transfer this success to other skeleton structures with limited data. In this work, we design a model architecture that imitates Generative Pretraining Transformer (GPT), utilizing prior knowledge learned from human data to the animal domain. We jointly train motion autoencoders for both animal and human motions and at the same time optimize through the similarity scores among human motion encoding, animal motion encoding, and text CLIP embedding. Presenting the first solution to this problem, we are able to generate animal motions with high diversity and fidelity, quantitatively and qualitatively outperforming the results of training human motion generation baselines on animal data. Additionally, we introduce AnimalML3D, the first text-animal motion dataset with 1240 animation sequences spanning 36 different animal identities. We hope this dataset would mediate the data scarcity problem in text-driven animal motion generation, providing a new playground for the research community.
<div id='section'>Paperid: <span id='pid'>822, <a href='https://arxiv.org/pdf/2309.17046.pdf' target='_blank'>https://arxiv.org/pdf/2309.17046.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianyu Li, Hyunyoung Jung, Matthew Gombolay, Yong Kwon Cho, Sehoon Ha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.17046">CrossLoco: Human Motion Driven Control of Legged Robots via Guided Unsupervised Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion driven control (HMDC) is an effective approach for generating natural and compelling robot motions while preserving high-level semantics. However, establishing the correspondence between humans and robots with different body structures is not straightforward due to the mismatches in kinematics and dynamics properties, which causes intrinsic ambiguity to the problem. Many previous algorithms approach this motion retargeting problem with unsupervised learning, which requires the prerequisite skill sets. However, it will be extremely costly to learn all the skills without understanding the given human motions, particularly for high-dimensional robots. In this work, we introduce CrossLoco, a guided unsupervised reinforcement learning framework that simultaneously learns robot skills and their correspondence to human motions. Our key innovation is to introduce a cycle-consistency-based reward term designed to maximize the mutual information between human motions and robot states. We demonstrate that the proposed framework can generate compelling robot motions by translating diverse human motions, such as running, hopping, and dancing. We quantitatively compare our CrossLoco against the manually engineered and unsupervised baseline algorithms along with the ablated versions of our framework and demonstrate that our method translates human motions with better accuracy, diversity, and user preference. We also showcase its utility in other applications, such as synthesizing robot movements from language input and enabling interactive robot control.
<div id='section'>Paperid: <span id='pid'>823, <a href='https://arxiv.org/pdf/2309.09969.pdf' target='_blank'>https://arxiv.org/pdf/2309.09969.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yen-Jen Wang, Bike Zhang, Jianyu Chen, Koushil Sreenath
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.09969">Prompt a Robot to Walk with Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models (LLMs) pre-trained on vast internet-scale data have showcased remarkable capabilities across diverse domains. Recently, there has been escalating interest in deploying LLMs for robotics, aiming to harness the power of foundation models in real-world settings. However, this approach faces significant challenges, particularly in grounding these models in the physical world and in generating dynamic robot motions. To address these issues, we introduce a novel paradigm in which we use few-shot prompts collected from the physical environment, enabling the LLM to autoregressively generate low-level control commands for robots without task-specific fine-tuning. Experiments across various robots and environments validate that our method can effectively prompt a robot to walk. We thus illustrate how LLMs can proficiently function as low-level feedback controllers for dynamic motion control even in high-dimensional robotic systems. The project website and source code can be found at: https://prompt2walk.github.io/ .
<div id='section'>Paperid: <span id='pid'>824, <a href='https://arxiv.org/pdf/2309.09163.pdf' target='_blank'>https://arxiv.org/pdf/2309.09163.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Abdullah Altawaitan, Jason Stanley, Sambaran Ghosal, Thai Duong, Nikolay Atanasov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.09163">Hamiltonian Dynamics Learning from Point Cloud Observations for Nonholonomic Mobile Robot Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reliable autonomous navigation requires adapting the control policy of a mobile robot in response to dynamics changes in different operational conditions. Hand-designed dynamics models may struggle to capture model variations due to a limited set of parameters. Data-driven dynamics learning approaches offer higher model capacity and better generalization but require large amounts of state-labeled data. This paper develops an approach for learning robot dynamics directly from point-cloud observations, removing the need and associated errors of state estimation, while embedding Hamiltonian structure in the dynamics model to improve data efficiency. We design an observation-space loss that relates motion prediction from the dynamics model with motion prediction from point-cloud registration to train a Hamiltonian neural ordinary differential equation. The learned Hamiltonian model enables the design of an energy-shaping model-based tracking controller for rigid-body robots. We demonstrate dynamics learning and tracking control on a real nonholonomic wheeled robot.
<div id='section'>Paperid: <span id='pid'>825, <a href='https://arxiv.org/pdf/2309.01236.pdf' target='_blank'>https://arxiv.org/pdf/2309.01236.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dorian F. Henning, Christopher Choi, Simon Schaefer, Stefan Leutenegger
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.01236">BodySLAM++: Fast and Tightly-Coupled Visual-Inertial Camera and Human Motion Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robust, fast, and accurate human state - 6D pose and posture - estimation remains a challenging problem. For real-world applications, the ability to estimate the human state in real-time is highly desirable. In this paper, we present BodySLAM++, a fast, efficient, and accurate human and camera state estimation framework relying on visual-inertial data. BodySLAM++ extends an existing visual-inertial state estimation framework, OKVIS2, to solve the dual task of estimating camera and human states simultaneously. Our system improves the accuracy of both human and camera state estimation with respect to baseline methods by 26% and 12%, respectively, and achieves real-time performance at 15+ frames per second on an Intel i7-model CPU. Experiments were conducted on a custom dataset containing both ground truth human and camera poses collected with an indoor motion tracking system.
<div id='section'>Paperid: <span id='pid'>826, <a href='https://arxiv.org/pdf/2305.14792.pdf' target='_blank'>https://arxiv.org/pdf/2305.14792.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianyu Li, Jungdam Won, Alexander Clegg, Jeonghwan Kim, Akshara Rai, Sehoon Ha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.14792">ACE: Adversarial Correspondence Embedding for Cross Morphology Motion Retargeting from Human to Nonhuman Characters</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion retargeting is a promising approach for generating natural and compelling animations for nonhuman characters. However, it is challenging to translate human movements into semantically equivalent motions for target characters with different morphologies due to the ambiguous nature of the problem. This work presents a novel learning-based motion retargeting framework, Adversarial Correspondence Embedding (ACE), to retarget human motions onto target characters with different body dimensions and structures. Our framework is designed to produce natural and feasible robot motions by leveraging generative-adversarial networks (GANs) while preserving high-level motion semantics by introducing an additional feature loss. In addition, we pretrain a robot motion prior that can be controlled in a latent embedding space and seek to establish a compact correspondence. We demonstrate that the proposed framework can produce retargeted motions for three different characters -- a quadrupedal robot with a manipulator, a crab character, and a wheeled manipulator. We further validate the design choices of our framework by conducting baseline comparisons and a user study. We also showcase sim-to-real transfer of the retargeted motions by transferring them to a real Spot robot.
<div id='section'>Paperid: <span id='pid'>827, <a href='https://arxiv.org/pdf/2304.05131.pdf' target='_blank'>https://arxiv.org/pdf/2304.05131.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaobing Dai, Huanzhuo Wu, Siyi Wang, Junjie Jiao, Giang T. Nguyen, Frank H. P. Fitzek, Sandra Hirche
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.05131">Fast IMU-based Dual Estimation of Human Motion and Kinematic Parameters via Progressive In-Network Computing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Many applications involve humans in the loop, where continuous and accurate human motion monitoring provides valuable information for safe and intuitive human-machine interaction. Portable devices such as inertial measurement units (IMUs) are applicable to monitor human motions, while in practice often limited computational power is available locally. The human motion in task space coordinates requires not only the human joint motion but also the nonlinear coordinate transformation depending on the parameters such as human limb length. In most applications, measuring these kinematics parameters for each individual requires undesirably high effort. Therefore, it is desirable to estimate both, the human motion and kinematic parameters from IMUs. In this work, we propose a novel computational framework for dual estimation in real-time exploiting in-network computational resources. We adopt the concept of field Kalman filtering, where the dual estimation problem is decomposed into a fast state estimation process and a computationally expensive parameter estimation process. In order to further accelerate the convergence, the parameter estimation is progressively computed on multiple networked computational nodes. The superiority of our proposed method is demonstrated by a simulation of a human arm, where the estimation accuracy is shown to converge faster than with conventional approaches.
<div id='section'>Paperid: <span id='pid'>828, <a href='https://arxiv.org/pdf/2210.11940.pdf' target='_blank'>https://arxiv.org/pdf/2210.11940.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Edward Vendrow, Duy Tho Le, Jianfei Cai, Hamid Rezatofighi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.11940">JRDB-Pose: A Large-scale Dataset for Multi-Person Pose Estimation and Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous robotic systems operating in human environments must understand their surroundings to make accurate and safe decisions. In crowded human scenes with close-up human-robot interaction and robot navigation, a deep understanding requires reasoning about human motion and body dynamics over time with human body pose estimation and tracking. However, existing datasets either do not provide pose annotations or include scene types unrelated to robotic applications. Many datasets also lack the diversity of poses and occlusions found in crowded human scenes. To address this limitation we introduce JRDB-Pose, a large-scale dataset and benchmark for multi-person pose estimation and tracking using videos captured from a social navigation robot. The dataset contains challenge scenes with crowded indoor and outdoor locations and a diverse range of scales and occlusion types. JRDB-Pose provides human pose annotations with per-keypoint occlusion labels and track IDs consistent across the scene. A public evaluation server is made available for fair evaluation on a held-out test set. JRDB-Pose is available at https://jrdb.erc.monash.edu/ .
<div id='section'>Paperid: <span id='pid'>829, <a href='https://arxiv.org/pdf/2209.15406.pdf' target='_blank'>https://arxiv.org/pdf/2209.15406.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohatashem Reyaz Makhdoomi, Vivek Muralidharan, Kuldeep R. Barad, Juan Sandoval, Miguel Olivares-Mendez, Carol Martinez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.15406">Emulating On-Orbit Interactions Using Forward Dynamics Based Cartesian Motion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>On-orbit operations such as servicing and assembly are considered a priority for the future space industry. Ground-based facilities that emulate on-orbit interactions are key tools for developing and testing space technology. This paper presents a control framework to emulate on-orbit operations using on-ground robotic manipulators. It combines Virtual Forward Dynamics Models (VFDM) for Cartesian motion control of robotic manipulators with an Orbital Dynamics Simulator (ODS) based on the Clohessy Wiltshire (CW) Model. The VFDM-based Inverse Kinematics (IK) solver is known to have better motion tracking, path accuracy, and solver convergency than traditional IK solvers. Thus, it provides a stable Cartesian motion for manipulators based on orbit emulations, even at singular or near singular configurations. The framework is tested at the ZeroG-Lab robotic facility of the SnT by emulating two scenarios: free-floating satellite motion and free-floating interaction (collision). Results show fidelity between the simulated motion commanded by the ODS and the one executed by the robot-mounted mockups.
<div id='section'>Paperid: <span id='pid'>830, <a href='https://arxiv.org/pdf/2205.02830.pdf' target='_blank'>https://arxiv.org/pdf/2205.02830.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vladimir Guzov, Julian Chibane, Riccardo Marin, Yannan He, Yunus Saracoglu, Torsten Sattler, Gerard Pons-Moll
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2205.02830">Interaction Replica: Tracking Human-Object Interaction and Scene Changes From Human Motion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Our world is not static and humans naturally cause changes in their environments through interactions, e.g., opening doors or moving furniture. Modeling changes caused by humans is essential for building digital twins, e.g., in the context of shared physical-virtual spaces (metaverses) and robotics. In order for widespread adoption of such emerging applications, the sensor setup used to capture the interactions needs to be inexpensive and easy-to-use for non-expert users. I.e., interactions should be captured and modeled by simple ego-centric sensors such as a combination of cameras and IMU sensors, not relying on any external cameras or object trackers. Yet, to the best of our knowledge, no work tackling the challenging problem of modeling human-scene interactions via such an ego-centric sensor setup exists. This paper closes this gap in the literature by developing a novel approach that combines visual localization of humans in the scene with contact-based reasoning about human-scene interactions from IMU data. Interestingly, we can show that even without visual observations of the interactions, human-scene contacts and interactions can be realistically predicted from human pose sequences. Our method, iReplica (Interaction Replica), is an essential first step towards the egocentric capture of human interactions and modeling of dynamic scenes, which is required for future AR/VR applications in immersive virtual universes and for training machines to behave like humans. Our code, data and model are available on our project page at http://virtualhumans.mpi-inf.mpg.de/ireplica/
<div id='section'>Paperid: <span id='pid'>831, <a href='https://arxiv.org/pdf/2202.13427.pdf' target='_blank'>https://arxiv.org/pdf/2202.13427.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aamir Hasan, Pranav Sriram, Katherine Driggs-Campbell
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2202.13427">Meta-path Analysis on Spatio-Temporal Graphs for Pedestrian Trajectory Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Spatio-temporal graphs (ST-graphs) have been used to model time series tasks such as traffic forecasting, human motion modeling, and action recognition. The high-level structure and corresponding features from ST-graphs have led to improved performance over traditional architectures. However, current methods tend to be limited by simple features, despite the rich information provided by the full graph structure, which leads to inefficiencies and suboptimal performance in downstream tasks. We propose the use of features derived from meta-paths, walks across different types of edges, in ST-graphs to improve the performance of Structural Recurrent Neural Network. In this paper, we present the Meta-path Enhanced Structural Recurrent Neural Network (MESRNN), a generic framework that can be applied to any spatio-temporal task in a simple and scalable manner. We employ MESRNN for pedestrian trajectory prediction, utilizing these meta-path based features to capture the relationships between the trajectories of pedestrians at different points in time and space. We compare our MESRNN against state-of-the-art ST-graph methods on standard datasets to show the performance boost provided by meta-path information. The proposed model consistently outperforms the baselines in trajectory prediction over long time horizons by over 32\%, and produces more socially compliant trajectories in dense crowds. For more information please refer to the project website at https://sites.google.com/illinois.edu/mesrnn/home.
<div id='section'>Paperid: <span id='pid'>832, <a href='https://arxiv.org/pdf/2510.08260.pdf' target='_blank'>https://arxiv.org/pdf/2510.08260.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mu Li, Yin Wang, Zhiying Leng, Jiapeng Liu, Frederick W. B. Li, Xiaohui Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.08260">Fine-grained text-driven dual-human motion generation via dynamic hierarchical interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human interaction is inherently dynamic and hierarchical, where the dynamic refers to the motion changes with distance, and the hierarchy is from individual to inter-individual and ultimately to overall motion. Exploiting these properties is vital for dual-human motion generation, while existing methods almost model human interaction temporally invariantly, ignoring distance and hierarchy. To address it, we propose a fine-grained dual-human motion generation method, namely FineDual, a tri-stage method to model the dynamic hierarchical interaction from individual to inter-individual. The first stage, Self-Learning Stage, divides the dual-human overall text into individual texts through a Large Language Model, aligning text features and motion features at the individual level. The second stage, Adaptive Adjustment Stage, predicts interaction distance by an interaction distance predictor, modeling human interactions dynamically at the inter-individual level by an interaction-aware graph network. The last stage, Teacher-Guided Refinement Stage, utilizes overall text features as guidance to refine motion features at the overall level, generating fine-grained and high-quality dual-human motion. Extensive quantitative and qualitative evaluations on dual-human motion datasets demonstrate that our proposed FineDual outperforms existing approaches, effectively modeling dynamic hierarchical human interaction.
<div id='section'>Paperid: <span id='pid'>833, <a href='https://arxiv.org/pdf/2510.07345.pdf' target='_blank'>https://arxiv.org/pdf/2510.07345.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Danush Kumar Venkatesh, Adam Schmidt, Muhammad Abdullah Jamal, Omid Mohareri
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07345">Mitigating Surgical Data Imbalance with Dual-Prediction Video Diffusion Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Surgical video datasets are essential for scene understanding, enabling procedural modeling and intra-operative support. However, these datasets are often heavily imbalanced, with rare actions and tools under-represented, which limits the robustness of downstream models. We address this challenge with $SurgiFlowVid$, a sparse and controllable video diffusion framework for generating surgical videos of under-represented classes. Our approach introduces a dual-prediction diffusion module that jointly denoises RGB frames and optical flow, providing temporal inductive biases to improve motion modeling from limited samples. In addition, a sparse visual encoder conditions the generation process on lightweight signals (e.g., sparse segmentation masks or RGB frames), enabling controllability without dense annotations. We validate our approach on three surgical datasets across tasks including action recognition, tool presence detection, and laparoscope motion prediction. Synthetic data generated by our method yields consistent gains of 10-20% over competitive baselines, establishing $SurgiFlowVid$ as a promising strategy to mitigate data imbalance and advance surgical video understanding methods.
<div id='section'>Paperid: <span id='pid'>834, <a href='https://arxiv.org/pdf/2510.02566.pdf' target='_blank'>https://arxiv.org/pdf/2510.02566.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiao Feng, Yiming Huang, Yufu Wang, Jiatao Gu, Lingjie Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.02566">PhysHMR: Learning Humanoid Control Policies from Vision for Physically Plausible Human Motion Reconstruction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reconstructing physically plausible human motion from monocular videos remains a challenging problem in computer vision and graphics. Existing methods primarily focus on kinematics-based pose estimation, often leading to unrealistic results due to the lack of physical constraints. To address such artifacts, prior methods have typically relied on physics-based post-processing following the initial kinematics-based motion estimation. However, this two-stage design introduces error accumulation, ultimately limiting the overall reconstruction quality. In this paper, we present PhysHMR, a unified framework that directly learns a visual-to-action policy for humanoid control in a physics-based simulator, enabling motion reconstruction that is both physically grounded and visually aligned with the input video. A key component of our approach is the pixel-as-ray strategy, which lifts 2D keypoints into 3D spatial rays and transforms them into global space. These rays are incorporated as policy inputs, providing robust global pose guidance without depending on noisy 3D root predictions. This soft global grounding, combined with local visual features from a pretrained encoder, allows the policy to reason over both detailed pose and global positioning. To overcome the sample inefficiency of reinforcement learning, we further introduce a distillation scheme that transfers motion knowledge from a mocap-trained expert to the vision-conditioned policy, which is then refined using physically motivated reinforcement learning rewards. Extensive experiments demonstrate that PhysHMR produces high-fidelity, physically plausible motion across diverse scenarios, outperforming prior approaches in both visual accuracy and physical realism.
<div id='section'>Paperid: <span id='pid'>835, <a href='https://arxiv.org/pdf/2509.14353.pdf' target='_blank'>https://arxiv.org/pdf/2509.14353.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dvij Kalaria, Sudarshan S Harithas, Pushkal Katara, Sangkyung Kwak, Sarthak Bhagat, Shankar Sastry, Srinath Sridhar, Sai Vemprala, Ashish Kapoor, Jonathan Chung-Kuan Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14353">DreamControl: Human-Inspired Whole-Body Humanoid Control for Scene Interaction via Guided Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce DreamControl, a novel methodology for learning autonomous whole-body humanoid skills. DreamControl leverages the strengths of diffusion models and Reinforcement Learning (RL): our core innovation is the use of a diffusion prior trained on human motion data, which subsequently guides an RL policy in simulation to complete specific tasks of interest (e.g., opening a drawer or picking up an object). We demonstrate that this human motion-informed prior allows RL to discover solutions unattainable by direct RL, and that diffusion models inherently promote natural looking motions, aiding in sim-to-real transfer. We validate DreamControl's effectiveness on a Unitree G1 robot across a diverse set of challenging tasks involving simultaneous lower and upper body control and object interaction.
<div id='section'>Paperid: <span id='pid'>836, <a href='https://arxiv.org/pdf/2509.14353.pdf' target='_blank'>https://arxiv.org/pdf/2509.14353.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dvij Kalaria, Sudarshan S Harithas, Pushkal Katara, Sangkyung Kwak, Sarthak Bhagat, Shankar Sastry, Srinath Sridhar, Sai Vemprala, Ashish Kapoor, Jonathan Chung-Kuan Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14353">DreamControl: Human-Inspired Whole-Body Humanoid Control for Scene Interaction via Guided Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce DreamControl, a novel methodology for learning autonomous whole-body humanoid skills. DreamControl leverages the strengths of diffusion models and Reinforcement Learning (RL): our core innovation is the use of a diffusion prior trained on human motion data, which subsequently guides an RL policy in simulation to complete specific tasks of interest (e.g., opening a drawer or picking up an object). We demonstrate that this human motion-informed prior allows RL to discover solutions unattainable by direct RL, and that diffusion models inherently promote natural looking motions, aiding in sim-to-real transfer. We validate DreamControl's effectiveness on a Unitree G1 robot across a diverse set of challenging tasks involving simultaneous lower and upper body control and object interaction. Project website at https://genrobo.github.io/DreamControl/
<div id='section'>Paperid: <span id='pid'>837, <a href='https://arxiv.org/pdf/2508.12644.pdf' target='_blank'>https://arxiv.org/pdf/2508.12644.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Wen, Hongbo Kang, Jian Ma, Jing Huang, Yuanwang Yang, Haozhe Lin, Yu-Kun Lai, Kun Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.12644">DyCrowd: Towards Dynamic Crowd Reconstruction from a Large-scene Video</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D reconstruction of dynamic crowds in large scenes has become increasingly important for applications such as city surveillance and crowd analysis. However, current works attempt to reconstruct 3D crowds from a static image, causing a lack of temporal consistency and inability to alleviate the typical impact caused by occlusions. In this paper, we propose DyCrowd, the first framework for spatio-temporally consistent 3D reconstruction of hundreds of individuals' poses, positions and shapes from a large-scene video. We design a coarse-to-fine group-guided motion optimization strategy for occlusion-robust crowd reconstruction in large scenes. To address temporal instability and severe occlusions, we further incorporate a VAE (Variational Autoencoder)-based human motion prior along with a segment-level group-guided optimization. The core of our strategy leverages collective crowd behavior to address long-term dynamic occlusions. By jointly optimizing the motion sequences of individuals with similar motion segments and combining this with the proposed Asynchronous Motion Consistency (AMC) loss, we enable high-quality unoccluded motion segments to guide the motion recovery of occluded ones, ensuring robust and plausible motion recovery even in the presence of temporal desynchronization and rhythmic inconsistencies. Additionally, in order to fill the gap of no existing well-annotated large-scene video dataset, we contribute a virtual benchmark dataset, VirtualCrowd, for evaluating dynamic crowd reconstruction from large-scene videos. Experimental results demonstrate that the proposed method achieves state-of-the-art performance in the large-scene dynamic crowd reconstruction task. The code and dataset will be available for research purposes.
<div id='section'>Paperid: <span id='pid'>838, <a href='https://arxiv.org/pdf/2508.06093.pdf' target='_blank'>https://arxiv.org/pdf/2508.06093.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chen Zhu, Buzhen Huang, Zijing Wu, Binghui Zuo, Yangang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.06093">E-React: Towards Emotionally Controlled Synthesis of Human Reactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Emotion serves as an essential component in daily human interactions. Existing human motion generation frameworks do not consider the impact of emotions, which reduces naturalness and limits their application in interactive tasks, such as human reaction synthesis. In this work, we introduce a novel task: generating diverse reaction motions in response to different emotional cues. However, learning emotion representation from limited motion data and incorporating it into a motion generation framework remains a challenging problem. To address the above obstacles, we introduce a semi-supervised emotion prior in an actor-reactor diffusion model to facilitate emotion-driven reaction synthesis. Specifically, based on the observation that motion clips within a short sequence tend to share the same emotion, we first devise a semi-supervised learning framework to train an emotion prior. With this prior, we further train an actor-reactor diffusion model to generate reactions by considering both spatial interaction and emotional response. Finally, given a motion sequence of an actor, our approach can generate realistic reactions under various emotional conditions. Experimental results demonstrate that our model outperforms existing reaction generation methods. The code and data will be made publicly available at https://ereact.github.io/
<div id='section'>Paperid: <span id='pid'>839, <a href='https://arxiv.org/pdf/2508.03313.pdf' target='_blank'>https://arxiv.org/pdf/2508.03313.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Libo Zhang, Xinyu Yi, Feng Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.03313">BaroPoser: Real-time Human Motion Tracking from IMUs and Barometers in Everyday Devices</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, tracking human motion using IMUs from everyday devices such as smartphones and smartwatches has gained increasing popularity. However, due to the sparsity of sensor measurements and the lack of datasets capturing human motion over uneven terrain, existing methods often struggle with pose estimation accuracy and are typically limited to recovering movements on flat terrain only. To this end, we present BaroPoser, the first method that combines IMU and barometric data recorded by a smartphone and a smartwatch to estimate human pose and global translation in real time. By leveraging barometric readings, we estimate sensor height changes, which provide valuable cues for both improving the accuracy of human pose estimation and predicting global translation on non-flat terrain. Furthermore, we propose a local thigh coordinate frame to disentangle local and global motion input for better pose representation learning. We evaluate our method on both public benchmark datasets and real-world recordings. Quantitative and qualitative results demonstrate that our approach outperforms the state-of-the-art (SOTA) methods that use IMUs only with the same hardware configuration.
<div id='section'>Paperid: <span id='pid'>840, <a href='https://arxiv.org/pdf/2507.14694.pdf' target='_blank'>https://arxiv.org/pdf/2507.14694.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yue Ma, Kanglei Zhou, Fuyang Yu, Frederick W. B. Li, Xiaohui Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.14694">Uncertainty-aware Probabilistic 3D Human Motion Forecasting via Invertible Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D human motion forecasting aims to enable autonomous applications. Estimating uncertainty for each prediction (i.e., confidence based on probability density or quantile) is essential for safety-critical contexts like human-robot collaboration to minimize risks. However, existing diverse motion forecasting approaches struggle with uncertainty quantification due to implicit probabilistic representations hindering uncertainty modeling. We propose ProbHMI, which introduces invertible networks to parameterize poses in a disentangled latent space, enabling probabilistic dynamics modeling. A forecasting module then explicitly predicts future latent distributions, allowing effective uncertainty quantification. Evaluated on benchmarks, ProbHMI achieves strong performance for both deterministic and diverse prediction while validating uncertainty calibration, critical for risk-aware decision making.
<div id='section'>Paperid: <span id='pid'>841, <a href='https://arxiv.org/pdf/2507.11464.pdf' target='_blank'>https://arxiv.org/pdf/2507.11464.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ajay Shankar, Keisuke Okumura, Amanda Prorok
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.11464">LF: Online Multi-Robot Path Planning Meets Optimal Trajectory Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a multi-robot control paradigm to solve point-to-point navigation tasks for a team of holonomic robots with access to the full environment information. The framework invokes two processes asynchronously at high frequency: (i) a centralized, discrete, and full-horizon planner for computing collision- and deadlock-free paths rapidly, leveraging recent advances in multi-agent pathfinding (MAPF), and (ii) dynamics-aware, robot-wise optimal trajectory controllers that ensure all robots independently follow their assigned paths reliably. This hierarchical shift in planning representation from (i) discrete and coupled to (ii) continuous and decoupled domains enables the framework to maintain long-term scalable motion synthesis. As an instantiation of this idea, we present LF, which combines a fast state-of-the-art MAPF solver (LaCAM), and a robust feedback control stack (Freyja) for executing agile robot maneuvers. LF provides a robust and versatile mechanism for lifelong multi-robot navigation even under asynchronous and partial goal updates, and adapts to dynamic workspaces simply by quick replanning. We present various multirotor and ground robot demonstrations, including the deployment of 15 real multirotors with random, consecutive target updates while a person walks through the operational workspace.
<div id='section'>Paperid: <span id='pid'>842, <a href='https://arxiv.org/pdf/2507.06590.pdf' target='_blank'>https://arxiv.org/pdf/2507.06590.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yin Wang, Mu li, Zhiying Leng, Frederick W. B. Li, Xiaohui Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06590">MOST: Motion Diffusion Model for Rare Text via Temporal Clip Banzhaf Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce MOST, a novel motion diffusion model via temporal clip Banzhaf interaction, aimed at addressing the persistent challenge of generating human motion from rare language prompts. While previous approaches struggle with coarse-grained matching and overlook important semantic cues due to motion redundancy, our key insight lies in leveraging fine-grained clip relationships to mitigate these issues. MOST's retrieval stage presents the first formulation of its kind - temporal clip Banzhaf interaction - which precisely quantifies textual-motion coherence at the clip level. This facilitates direct, fine-grained text-to-motion clip matching and eliminates prevalent redundancy. In the generation stage, a motion prompt module effectively utilizes retrieved motion clips to produce semantically consistent movements. Extensive evaluations confirm that MOST achieves state-of-the-art text-to-motion retrieval and generation performance by comprehensively addressing previous challenges, as demonstrated through quantitative and qualitative results highlighting its effectiveness, especially for rare prompts.
<div id='section'>Paperid: <span id='pid'>843, <a href='https://arxiv.org/pdf/2507.05963.pdf' target='_blank'>https://arxiv.org/pdf/2507.05963.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenghao Zhang, Junchao Liao, Xiangyu Meng, Long Qin, Weizhi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.05963">Tora2: Motion and Appearance Customized Diffusion Transformer for Multi-Entity Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in diffusion transformer models for motion-guided video generation, such as Tora, have shown significant progress. In this paper, we present Tora2, an enhanced version of Tora, which introduces several design improvements to expand its capabilities in both appearance and motion customization. Specifically, we introduce a decoupled personalization extractor that generates comprehensive personalization embeddings for multiple open-set entities, better preserving fine-grained visual details compared to previous methods. Building on this, we design a gated self-attention mechanism to integrate trajectory, textual description, and visual information for each entity. This innovation significantly reduces misalignment in multimodal conditioning during training. Moreover, we introduce a contrastive loss that jointly optimizes trajectory dynamics and entity consistency through explicit mapping between motion and personalization embeddings. Tora2 is, to our best knowledge, the first method to achieve simultaneous multi-entity customization of appearance and motion for video generation. Experimental results demonstrate that Tora2 achieves competitive performance with state-of-the-art customization methods while providing advanced motion control capabilities, which marks a critical advancement in multi-condition video generation. Project page: https://ali-videoai.github.io/Tora2_page/.
<div id='section'>Paperid: <span id='pid'>844, <a href='https://arxiv.org/pdf/2507.00677.pdf' target='_blank'>https://arxiv.org/pdf/2507.00677.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongho Kang, Jin Cheng, Fatemeh Zargarbashi, Taerim Yoon, Sungjoon Choi, Stelian Coros
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.00677">Learning Steerable Imitation Controllers from Unstructured Animal Motions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a control framework for legged robots that leverages unstructured real-world animal motion data to generate animal-like and user-steerable behaviors. Our framework learns to follow velocity commands while reproducing the diverse gait patterns in the original dataset. To begin with, animal motion data is transformed into a robot-compatible database using constrained inverse kinematics and model predictive control, bridging the morphological and physical gap between the animal and the robot. Subsequently, a variational autoencoder-based motion synthesis module captures the diverse locomotion patterns in the motion database and generates smooth transitions between them in response to velocity commands. The resulting kinematic motions serve as references for a reinforcement learning-based feedback controller deployed on physical robots. We show that this approach enables a quadruped robot to adaptively switch gaits and accurately track user velocity commands while maintaining the stylistic coherence of the motion data. Additionally, we provide component-wise evaluations to analyze the system's behavior in depth and demonstrate the efficacy of our method for more accurate and reliable motion imitation.
<div id='section'>Paperid: <span id='pid'>845, <a href='https://arxiv.org/pdf/2506.23114.pdf' target='_blank'>https://arxiv.org/pdf/2506.23114.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhanxiang Cao, Buqing Nie, Yang Zhang, Yue Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.23114">Minimizing Acoustic Noise: Enhancing Quiet Locomotion for Quadruped Robots in Indoor Applications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in quadruped robot research have significantly improved their ability to traverse complex and unstructured outdoor environments. However, the issue of noise generated during locomotion is generally overlooked, which is critically important in noise-sensitive indoor environments, such as service and healthcare settings, where maintaining low noise levels is essential. This study aims to optimize the acoustic noise generated by quadruped robots during locomotion through the development of advanced motion control algorithms. To achieve this, we propose a novel approach that minimizes noise emissions by integrating optimized gait design with tailored control strategies. This method achieves an average noise reduction of approximately 8 dBA during movement, thereby enhancing the suitability of quadruped robots for deployment in noise-sensitive indoor environments. Experimental results demonstrate the effectiveness of this approach across various indoor settings, highlighting the potential of quadruped robots for quiet operation in noise-sensitive environments.
<div id='section'>Paperid: <span id='pid'>846, <a href='https://arxiv.org/pdf/2506.22907.pdf' target='_blank'>https://arxiv.org/pdf/2506.22907.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunzhe Shao, Xinyu Yi, Lu Yin, Shihui Guo, Junhai Yong, Feng Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.22907">MagShield: Towards Better Robustness in Sparse Inertial Motion Capture Under Magnetic Disturbances</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper proposes a novel method called MagShield, designed to address the issue of magnetic interference in sparse inertial motion capture (MoCap) systems. Existing Inertial Measurement Unit (IMU) systems are prone to orientation estimation errors in magnetically disturbed environments, limiting their practical application in real-world scenarios. To address this problem, MagShield employs a "detect-then-correct" strategy, first detecting magnetic disturbances through multi-IMU joint analysis, and then correcting orientation errors using human motion priors. MagShield can be integrated with most existing sparse inertial MoCap systems, improving their performance in magnetically disturbed environments. Experimental results demonstrate that MagShield significantly enhances the accuracy of motion capture under magnetic interference and exhibits good compatibility across different sparse inertial MoCap systems.
<div id='section'>Paperid: <span id='pid'>847, <a href='https://arxiv.org/pdf/2506.22554.pdf' target='_blank'>https://arxiv.org/pdf/2506.22554.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vasu Agrawal, Akinniyi Akinyemi, Kathryn Alvero, Morteza Behrooz, Julia Buffalini, Fabio Maria Carlucci, Joy Chen, Junming Chen, Zhang Chen, Shiyang Cheng, Praveen Chowdary, Joe Chuang, Antony D'Avirro, Jon Daly, Ning Dong, Mark Duppenthaler, Cynthia Gao, Jeff Girard, Martin Gleize, Sahir Gomez, Hongyu Gong, Srivathsan Govindarajan, Brandon Han, Sen He, Denise Hernandez, Yordan Hristov, Rongjie Huang, Hirofumi Inaguma, Somya Jain, Raj Janardhan, Qingyao Jia, Christopher Klaiber, Dejan Kovachev, Moneish Kumar, Hang Li, Yilei Li, Pavel Litvin, Wei Liu, Guangyao Ma, Jing Ma, Martin Ma, Xutai Ma, Lucas Mantovani, Sagar Miglani, Sreyas Mohan, Louis-Philippe Morency, Evonne Ng, Kam-Woh Ng, Tu Anh Nguyen, Amia Oberai, Benjamin Peloquin, Juan Pino, Jovan Popovic, Omid Poursaeed, Fabian Prada, Alice Rakotoarison, Rakesh Ranjan, Alexander Richard, Christophe Ropers, Safiyyah Saleem, Vasu Sharma, Alex Shcherbyna, Jia Shen, Jie Shen, Anastasis Stathopoulos, Anna Sun, Paden Tomasello, Tuan Tran, Arina Turkatenko, Bo Wan, Chao Wang, Jeff Wang, Mary Williamson, Carleigh Wood, Tao Xiang, Yilin Yang, Julien Yao, Chen Zhang, Jiemin Zhang, Xinyue Zhang, Jason Zheng, Pavlo Zhyzheria, Jan Zikes, Michael Zollhoefer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.22554">Seamless Interaction: Dyadic Audiovisual Motion Modeling and Large-Scale Dataset</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human communication involves a complex interplay of verbal and nonverbal signals, essential for conveying meaning and achieving interpersonal goals. To develop socially intelligent AI technologies, it is crucial to develop models that can both comprehend and generate dyadic behavioral dynamics. To this end, we introduce the Seamless Interaction Dataset, a large-scale collection of over 4,000 hours of face-to-face interaction footage from over 4,000 participants in diverse contexts. This dataset enables the development of AI technologies that understand dyadic embodied dynamics, unlocking breakthroughs in virtual agents, telepresence experiences, and multimodal content analysis tools. We also develop a suite of models that utilize the dataset to generate dyadic motion gestures and facial expressions aligned with human speech. These models can take as input both the speech and visual behavior of their interlocutors. We present a variant with speech from an LLM model and integrations with 2D and 3D rendering methods, bringing us closer to interactive virtual agents. Additionally, we describe controllable variants of our motion models that can adapt emotional responses and expressivity levels, as well as generating more semantically-relevant gestures. Finally, we discuss methods for assessing the quality of these dyadic motion models, which are demonstrating the potential for more intuitive and responsive human-AI interactions.
<div id='section'>Paperid: <span id='pid'>848, <a href='https://arxiv.org/pdf/2506.21912.pdf' target='_blank'>https://arxiv.org/pdf/2506.21912.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinghan Wang, Kun Xu, Fei Li, Cao Sheng, Jiazhong Yu, Yadong Mu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.21912">Generating Attribute-Aware Human Motions from Textual Prompt</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-driven human motion generation has recently attracted considerable attention, allowing models to generate human motions based on textual descriptions. However, current methods neglect the influence of human attributes (such as age, gender, weight, and height) which are key factors shaping human motion patterns. This work represents a pilot exploration for bridging this gap. We conceptualize each motion as comprising both attribute information and action semantics, where textual descriptions align exclusively with action semantics. To achieve this, a new framework inspired by Structural Causal Models is proposed to decouple action semantics from human attributes, enabling text-to-semantics prediction and attribute-controlled generation. The resulting model is capable of generating realistic, attribute-aware motion aligned with the user's text and attribute inputs. For evaluation, we introduce HumanAttr, a comprehensive dataset containing attribute annotations for text-motion pairs, setting the first benchmark for attribute-aware text-to-motion generation. Extensive experiments on the new dataset validate our model's effectiveness.
<div id='section'>Paperid: <span id='pid'>849, <a href='https://arxiv.org/pdf/2506.20668.pdf' target='_blank'>https://arxiv.org/pdf/2506.20668.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sungjae Park, Homanga Bharadhwaj, Shubham Tulsiani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.20668">DemoDiffusion: One-Shot Human Imitation using pre-trained Diffusion Policy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose DemoDiffusion, a simple and scalable method for enabling robots to perform manipulation tasks in natural environments by imitating a single human demonstration. Our approach is based on two key insights. First, the hand motion in a human demonstration provides a useful prior for the robot's end-effector trajectory, which we can convert into a rough open-loop robot motion trajectory via kinematic retargeting. Second, while this retargeted motion captures the overall structure of the task, it may not align well with plausible robot actions in-context. To address this, we leverage a pre-trained generalist diffusion policy to modify the trajectory, ensuring it both follows the human motion and remains within the distribution of plausible robot actions. Our approach avoids the need for online reinforcement learning or paired human-robot data, enabling robust adaptation to new tasks and scenes with minimal manual effort. Experiments in both simulation and real-world settings show that DemoDiffusion outperforms both the base policy and the retargeted trajectory, enabling the robot to succeed even on tasks where the pre-trained generalist policy fails entirely. Project page: https://demodiffusion.github.io/
<div id='section'>Paperid: <span id='pid'>850, <a href='https://arxiv.org/pdf/2506.19851.pdf' target='_blank'>https://arxiv.org/pdf/2506.19851.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zehuan Huang, Haoran Feng, Yangtian Sun, Yuanchen Guo, Yanpei Cao, Lu Sheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.19851">AnimaX: Animating the Inanimate in 3D with Joint Video-Pose Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present AnimaX, a feed-forward 3D animation framework that bridges the motion priors of video diffusion models with the controllable structure of skeleton-based animation. Traditional motion synthesis methods are either restricted to fixed skeletal topologies or require costly optimization in high-dimensional deformation spaces. In contrast, AnimaX effectively transfers video-based motion knowledge to the 3D domain, supporting diverse articulated meshes with arbitrary skeletons. Our method represents 3D motion as multi-view, multi-frame 2D pose maps, and enables joint video-pose diffusion conditioned on template renderings and a textual motion prompt. We introduce shared positional encodings and modality-aware embeddings to ensure spatial-temporal alignment between video and pose sequences, effectively transferring video priors to motion generation task. The resulting multi-view pose sequences are triangulated into 3D joint positions and converted into mesh animation via inverse kinematics. Trained on a newly curated dataset of 160,000 rigged sequences, AnimaX achieves state-of-the-art results on VBench in generalization, motion fidelity, and efficiency, offering a scalable solution for category-agnostic 3D animation. Project page: \href{https://anima-x.github.io/}{https://anima-x.github.io/}.
<div id='section'>Paperid: <span id='pid'>851, <a href='https://arxiv.org/pdf/2506.14233.pdf' target='_blank'>https://arxiv.org/pdf/2506.14233.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Amirreza Payandeh, Anuj Pokhrel, Daeun Song, Marcos Zampieri, Xuesu Xiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.14233">Narrate2Nav: Real-Time Visual Navigation with Implicit Language Reasoning in Human-Centric Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Vision-Language Models (VLMs) have demonstrated potential in enhancing mobile robot navigation in human-centric environments by understanding contextual cues, human intentions, and social dynamics while exhibiting reasoning capabilities. However, their computational complexity and limited sensitivity to continuous numerical data impede real-time performance and precise motion control. To this end, we propose Narrate2Nav, a novel real-time vision-action model that leverages a novel self-supervised learning framework based on the Barlow Twins redundancy reduction loss to embed implicit natural language reasoning, social cues, and human intentions within a visual encoder-enabling reasoning in the model's latent space rather than token space. The model combines RGB inputs, motion commands, and textual signals of scene context during training to bridge from robot observations to low-level motion commands for short-horizon point-goal navigation during deployment. Extensive evaluation of Narrate2Nav across various challenging scenarios in both offline unseen dataset and real-world experiments demonstrates an overall improvement of 52.94 percent and 41.67 percent, respectively, over the next best baseline. Additionally, qualitative comparative analysis of Narrate2Nav's visual encoder attention map against four other baselines demonstrates enhanced attention to navigation-critical scene elements, underscoring its effectiveness in human-centric navigation tasks.
<div id='section'>Paperid: <span id='pid'>852, <a href='https://arxiv.org/pdf/2505.10239.pdf' target='_blank'>https://arxiv.org/pdf/2505.10239.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gokhan Solak, Gustavo J. G. Lahr, Idil Ozdamar, Arash Ajoudani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.10239">Context-aware collaborative pushing of heavy objects using skeleton-based intention prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In physical human-robot interaction, force feedback has been the most common sensing modality to convey the human intention to the robot. It is widely used in admittance control to allow the human to direct the robot. However, it cannot be used in scenarios where direct force feedback is not available since manipulated objects are not always equipped with a force sensor. In this work, we study one such scenario: the collaborative pushing and pulling of heavy objects on frictional surfaces, a prevalent task in industrial settings. When humans do it, they communicate through verbal and non-verbal cues, where body poses, and movements often convey more than words. We propose a novel context-aware approach using Directed Graph Neural Networks to analyze spatio-temporal human posture data to predict human motion intention for non-verbal collaborative physical manipulation. Our experiments demonstrate that robot assistance significantly reduces human effort and improves task efficiency. The results indicate that incorporating posture-based context recognition, either together with or as an alternative to force sensing, enhances robot decision-making and control efficiency.
<div id='section'>Paperid: <span id='pid'>853, <a href='https://arxiv.org/pdf/2505.06584.pdf' target='_blank'>https://arxiv.org/pdf/2505.06584.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziluo Ding, Haobin Jiang, Yuxuan Wang, Zhenguo Sun, Yu Zhang, Xiaojie Niu, Ming Yang, Weishuai Zeng, Xinrun Xu, Zongqing Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.06584">JAEGER: Dual-Level Humanoid Whole-Body Controller</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents JAEGER, a dual-level whole-body controller for humanoid robots that addresses the challenges of training a more robust and versatile policy. Unlike traditional single-controller approaches, JAEGER separates the control of the upper and lower bodies into two independent controllers, so that they can better focus on their distinct tasks. This separation alleviates the dimensionality curse and improves fault tolerance. JAEGER supports both root velocity tracking (coarse-grained control) and local joint angle tracking (fine-grained control), enabling versatile and stable movements. To train the controller, we utilize a human motion dataset (AMASS), retargeting human poses to humanoid poses through an efficient retargeting network, and employ a curriculum learning approach. This method performs supervised learning for initialization, followed by reinforcement learning for further exploration. We conduct our experiments on two humanoid platforms and demonstrate the superiority of our approach against state-of-the-art methods in both simulation and real environments.
<div id='section'>Paperid: <span id='pid'>854, <a href='https://arxiv.org/pdf/2505.05010.pdf' target='_blank'>https://arxiv.org/pdf/2505.05010.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinyu Yi, Shaohua Pan, Feng Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.05010">Improving Global Motion Estimation in Sparse IMU-based Motion Capture with Physics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>By learning human motion priors, motion capture can be achieved by 6 inertial measurement units (IMUs) in recent years with the development of deep learning techniques, even though the sensor inputs are sparse and noisy. However, human global motions are still challenging to be reconstructed by IMUs. This paper aims to solve this problem by involving physics. It proposes a physical optimization scheme based on multiple contacts to enable physically plausible translation estimation in the full 3D space where the z-directional motion is usually challenging for previous works. It also considers gravity in local pose estimation which well constrains human global orientations and refines local pose estimation in a joint estimation manner. Experiments demonstrate that our method achieves more accurate motion capture for both local poses and global motions. Furthermore, by deeply integrating physics, we can also estimate 3D contact, contact forces, joint torques, and interacting proxy surfaces.
<div id='section'>Paperid: <span id='pid'>855, <a href='https://arxiv.org/pdf/2505.03779.pdf' target='_blank'>https://arxiv.org/pdf/2505.03779.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tao Liu, Tianyu Zhang, Yongxue Chen, Weiming Wang, Yu Jiang, Yuming Huang, Charlie C. L. Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.03779">Neural Co-Optimization of Structural Topology, Manufacturable Layers, and Path Orientations for Fiber-Reinforced Composites</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a neural network-based computational framework for the simultaneous optimization of structural topology, curved layers, and path orientations to achieve strong anisotropic strength in fiber-reinforced thermoplastic composites while ensuring manufacturability. Our framework employs three implicit neural fields to represent geometric shape, layer sequence, and fiber orientation. This enables the direct formulation of both design and manufacturability objectives - such as anisotropic strength, structural volume, machine motion control, layer curvature, and layer thickness - into an integrated and differentiable optimization process. By incorporating these objectives as loss functions, the framework ensures that the resultant composites exhibit optimized mechanical strength while remaining its manufacturability for filament-based multi-axis 3D printing across diverse hardware platforms. Physical experiments demonstrate that the composites generated by our co-optimization method can achieve an improvement of up to 33.1% in failure loads compared to composites with sequentially optimized structures and manufacturing sequences.
<div id='section'>Paperid: <span id='pid'>856, <a href='https://arxiv.org/pdf/2504.04634.pdf' target='_blank'>https://arxiv.org/pdf/2504.04634.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Foram Niravbhai Shah, Parshwa Shah, Muhammad Usama Saleem, Ekkasit Pinyoanuntapong, Pu Wang, Hongfei Xue, Ahmed Helmy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.04634">DanceMosaic: High-Fidelity Dance Generation with Multimodal Editability</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in dance generation have enabled automatic synthesis of 3D dance motions. However, existing methods still struggle to produce high-fidelity dance sequences that simultaneously deliver exceptional realism, precise dance-music synchronization, high motion diversity, and physical plausibility. Moreover, existing methods lack the flexibility to edit dance sequences according to diverse guidance signals, such as musical prompts, pose constraints, action labels, and genre descriptions, significantly restricting their creative utility and adaptability. Unlike the existing approaches, DanceMosaic enables fast and high-fidelity dance generation, while allowing multimodal motion editing. Specifically, we propose a multimodal masked motion model that fuses the text-to-motion model with music and pose adapters to learn probabilistic mapping from diverse guidance signals to high-quality dance motion sequences via progressive generative masking training. To further enhance the motion generation quality, we propose multimodal classifier-free guidance and inference-time optimization mechanism that further enforce the alignment between the generated motions and the multimodal guidance. Extensive experiments demonstrate that our method establishes a new state-of-the-art performance in dance generation, significantly advancing the quality and editability achieved by existing approaches.
<div id='section'>Paperid: <span id='pid'>857, <a href='https://arxiv.org/pdf/2504.01019.pdf' target='_blank'>https://arxiv.org/pdf/2504.01019.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pablo Ruiz-Ponce, German Barquero, Cristina Palmero, Sergio Escalera, JosÃ© GarcÃ­a-RodrÃ­guez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.01019">MixerMDM: Learnable Composition of Human Motion Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating human motion guided by conditions such as textual descriptions is challenging due to the need for datasets with pairs of high-quality motion and their corresponding conditions. The difficulty increases when aiming for finer control in the generation. To that end, prior works have proposed to combine several motion diffusion models pre-trained on datasets with different types of conditions, thus allowing control with multiple conditions. However, the proposed merging strategies overlook that the optimal way to combine the generation processes might depend on the particularities of each pre-trained generative model and also the specific textual descriptions. In this context, we introduce MixerMDM, the first learnable model composition technique for combining pre-trained text-conditioned human motion diffusion models. Unlike previous approaches, MixerMDM provides a dynamic mixing strategy that is trained in an adversarial fashion to learn to combine the denoising process of each model depending on the set of conditions driving the generation. By using MixerMDM to combine single- and multi-person motion diffusion models, we achieve fine-grained control on the dynamics of every person individually, and also on the overall interaction. Furthermore, we propose a new evaluation technique that, for the first time in this task, measures the interaction and individual quality by computing the alignment between the mixed generated motions and their conditions as well as the capabilities of MixerMDM to adapt the mixing throughout the denoising process depending on the motions to mix.
<div id='section'>Paperid: <span id='pid'>858, <a href='https://arxiv.org/pdf/2503.05540.pdf' target='_blank'>https://arxiv.org/pdf/2503.05540.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Leonel Rozo, Miguel GonzÃ¡lez-Duque, NoÃ©mie Jaquier, SÃ¸ren Hauberg
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.05540">Riemann$^2$: Learning Riemannian Submanifolds from Riemannian Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Latent variable models are powerful tools for learning low-dimensional manifolds from high-dimensional data. However, when dealing with constrained data such as unit-norm vectors or symmetric positive-definite matrices, existing approaches ignore the underlying geometric constraints or fail to provide meaningful metrics in the latent space. To address these limitations, we propose to learn Riemannian latent representations of such geometric data. To do so, we estimate the pullback metric induced by a Wrapped Gaussian Process Latent Variable Model, which explicitly accounts for the data geometry. This enables us to define geometry-aware notions of distance and shortest paths in the latent space, while ensuring that our model only assigns probability mass to the data manifold. This generalizes previous work and allows us to handle complex tasks in various domains, including robot motion synthesis and analysis of brain connectomes.
<div id='section'>Paperid: <span id='pid'>859, <a href='https://arxiv.org/pdf/2503.02353.pdf' target='_blank'>https://arxiv.org/pdf/2503.02353.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Luobin Wang, Hongzhan Yu, Chenning Yu, Sicun Gao, Henrik Christensen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.02353">Controllable Motion Generation via Diffusion Modal Coupling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion models have recently gained significant attention in robotics due to their ability to generate multi-modal distributions of system states and behaviors. However, a key challenge remains: ensuring precise control over the generated outcomes without compromising realism. This is crucial for applications such as motion planning or trajectory forecasting, where adherence to physical constraints and task-specific objectives is essential. We propose a novel framework that enhances controllability in diffusion models by leveraging multi-modal prior distributions and enforcing strong modal coupling. This allows us to initiate the denoising process directly from distinct prior modes that correspond to different possible system behaviors, ensuring sampling to align with the training distribution. We evaluate our approach on motion prediction using the Waymo dataset and multi-task control in Maze2D environments. Experimental results show that our framework outperforms both guidance-based techniques and conditioned models with unimodal priors, achieving superior fidelity, diversity, and controllability, even in the absence of explicit conditioning. Overall, our approach provides a more reliable and scalable solution for controllable motion generation in robotics.
<div id='section'>Paperid: <span id='pid'>860, <a href='https://arxiv.org/pdf/2503.02353.pdf' target='_blank'>https://arxiv.org/pdf/2503.02353.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Luobin Wang, Hongzhan Yu, Chenning Yu, Sicun Gao, Henrik Christensen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.02353">Controllable Motion Generation via Diffusion Modal Coupling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion models have recently gained significant attention in robotics due to their ability to generate multi-modal distributions of system states and behaviors. However, a key challenge remains: ensuring precise control over the generated outcomes without compromising realism. This is crucial for applications such as motion planning or trajectory forecasting, where adherence to physical constraints and task-specific objectives is essential. We propose a novel framework that enhances controllability in diffusion models by leveraging multi-modal prior distributions and enforcing strong modal coupling. This allows us to initiate the denoising process directly from distinct prior modes that correspond to different possible system behaviors, ensuring sampling to align with the training distribution. We evaluate our approach on motion prediction using the Waymo dataset and multi-task control in Maze2D environments. Experimental results show that our framework outperforms both guidance-based techniques and conditioned models with unimodal priors, achieving superior fidelity, diversity, and controllability, even in the absence of explicit conditioning. Overall, our approach provides a more reliable and scalable solution for controllable motion generation in robotics.
<div id='section'>Paperid: <span id='pid'>861, <a href='https://arxiv.org/pdf/2502.05534.pdf' target='_blank'>https://arxiv.org/pdf/2502.05534.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yin Wang, Mu Li, Jiapeng Liu, Zhiying Leng, Frederick W. B. Li, Ziyao Zhang, Xiaohui Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.05534">Fg-T2M++: LLMs-Augmented Fine-Grained Text Driven Human Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We address the challenging problem of fine-grained text-driven human motion generation. Existing works generate imprecise motions that fail to accurately capture relationships specified in text due to: (1) lack of effective text parsing for detailed semantic cues regarding body parts, (2) not fully modeling linguistic structures between words to comprehend text comprehensively. To tackle these limitations, we propose a novel fine-grained framework Fg-T2M++ that consists of: (1) an LLMs semantic parsing module to extract body part descriptions and semantics from text, (2) a hyperbolic text representation module to encode relational information between text units by embedding the syntactic dependency graph into hyperbolic space, and (3) a multi-modal fusion module to hierarchically fuse text and motion features. Extensive experiments on HumanML3D and KIT-ML datasets demonstrate that Fg-T2M++ outperforms SOTA methods, validating its ability to accurately generate motions adhering to comprehensive text semantics.
<div id='section'>Paperid: <span id='pid'>862, <a href='https://arxiv.org/pdf/2501.16753.pdf' target='_blank'>https://arxiv.org/pdf/2501.16753.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hy Nguyen, Srikanth Thudumu, Hung Du, Rajesh Vasa, Kon Mouzakis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.16753">Overcoming Semantic Dilution in Transformer-Based Next Frame Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Next-frame prediction in videos is crucial for applications such as autonomous driving, object tracking, and motion prediction. The primary challenge in next-frame prediction lies in effectively capturing and processing both spatial and temporal information from previous video sequences. The transformer architecture, known for its prowess in handling sequence data, has made remarkable progress in this domain. However, transformer-based next-frame prediction models face notable issues: (a) The multi-head self-attention (MHSA) mechanism requires the input embedding to be split into $N$ chunks, where $N$ is the number of heads. Each segment captures only a fraction of the original embeddings information, which distorts the representation of the embedding in the latent space, resulting in a semantic dilution problem; (b) These models predict the embeddings of the next frames rather than the frames themselves, but the loss function based on the errors of the reconstructed frames, not the predicted embeddings -- this creates a discrepancy between the training objective and the model output. We propose a Semantic Concentration Multi-Head Self-Attention (SCMHSA) architecture, which effectively mitigates semantic dilution in transformer-based next-frame prediction. Additionally, we introduce a loss function that optimizes SCMHSA in the latent space, aligning the training objective more closely with the model output. Our method demonstrates superior performance compared to the original transformer-based predictors.
<div id='section'>Paperid: <span id='pid'>863, <a href='https://arxiv.org/pdf/2501.06035.pdf' target='_blank'>https://arxiv.org/pdf/2501.06035.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cecilia Curreli, Dominik Muhle, Abhishek Saroha, Zhenzhang Ye, Riccardo Marin, Daniel Cremers
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.06035">Nonisotropic Gaussian Diffusion for Realistic 3D Human Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Probabilistic human motion prediction aims to forecast multiple possible future movements from past observations. While current approaches report high diversity and realism, they often generate motions with undetected limb stretching and jitter. To address this, we introduce SkeletonDiffusion, a latent diffusion model that embeds an explicit inductive bias on the human body within its architecture and training. Our model is trained with a novel nonisotropic Gaussian diffusion formulation that aligns with the natural kinematic structure of the human skeleton. Results show that our approach outperforms conventional isotropic alternatives, consistently generating realistic predictions while avoiding artifacts such as limb distortion. Additionally, we identify a limitation in commonly used diversity metrics, which may inadvertently favor models that produce inconsistent limb lengths within the same sequence. SkeletonDiffusion sets a new benchmark on real-world datasets, outperforming various baselines across multiple evaluation metrics. Visit our project page at https://ceveloper.github.io/publications/skeletondiffusion/ .
<div id='section'>Paperid: <span id='pid'>864, <a href='https://arxiv.org/pdf/2412.20104.pdf' target='_blank'>https://arxiv.org/pdf/2412.20104.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenkun He, Yun Liu, Ruitao Liu, Li Yi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.20104">SyncDiff: Synchronized Motion Diffusion for Multi-Body Human-Object Interaction Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Synthesizing realistic human-object interaction motions is a critical problem in VR/AR and human animation. Unlike the commonly studied scenarios involving a single human or hand interacting with one object, we address a more generic multi-body setting with arbitrary numbers of humans, hands, and objects. This complexity introduces significant challenges in synchronizing motions due to the high correlations and mutual influences among bodies. To address these challenges, we introduce SyncDiff, a novel method for multi-body interaction synthesis using a synchronized motion diffusion strategy. SyncDiff employs a single diffusion model to capture the joint distribution of multi-body motions. To enhance motion fidelity, we propose a frequency-domain motion decomposition scheme. Additionally, we introduce a new set of alignment scores to emphasize the synchronization of different body motions. SyncDiff jointly optimizes both data sample likelihood and alignment likelihood through an explicit synchronization strategy. Extensive experiments across four datasets with various multi-body configurations demonstrate the superiority of SyncDiff over existing state-of-the-art motion synthesis methods.
<div id='section'>Paperid: <span id='pid'>865, <a href='https://arxiv.org/pdf/2412.13185.pdf' target='_blank'>https://arxiv.org/pdf/2412.13185.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hsin-Ping Huang, Yang Zhou, Jui-Hsien Wang, Difan Liu, Feng Liu, Ming-Hsuan Yang, Zhan Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.13185">Move-in-2D: 2D-Conditioned Human Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating realistic human videos remains a challenging task, with the most effective methods currently relying on a human motion sequence as a control signal. Existing approaches often use existing motion extracted from other videos, which restricts applications to specific motion types and global scene matching. We propose Move-in-2D, a novel approach to generate human motion sequences conditioned on a scene image, allowing for diverse motion that adapts to different scenes. Our approach utilizes a diffusion model that accepts both a scene image and text prompt as inputs, producing a motion sequence tailored to the scene. To train this model, we collect a large-scale video dataset featuring single-human activities, annotating each video with the corresponding human motion as the target output. Experiments demonstrate that our method effectively predicts human motion that aligns with the scene image after projection. Furthermore, we show that the generated motion sequence improves human motion quality in video synthesis tasks.
<div id='section'>Paperid: <span id='pid'>866, <a href='https://arxiv.org/pdf/2412.01747.pdf' target='_blank'>https://arxiv.org/pdf/2412.01747.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyun Wang, Ruijun Zhang, Zi-Yan Liu, Yufu Wang, Kostas Daniilidis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.01747">Continuous-Time Human Motion Field from Events</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper addresses the challenges of estimating a continuous-time human motion field from a stream of events. Existing Human Mesh Recovery (HMR) methods rely predominantly on frame-based approaches, which are prone to aliasing and inaccuracies due to limited temporal resolution and motion blur. In this work, we predict a continuous-time human motion field directly from events by leveraging a recurrent feed-forward neural network to predict human motion in the latent space of possible human motions. Prior state-of-the-art event-based methods rely on computationally intensive optimization across a fixed number of poses at high frame rates, which becomes prohibitively expensive as we increase the temporal resolution. In comparison, we present the first work that replaces traditional discrete-time predictions with a continuous human motion field represented as a time-implicit function, enabling parallel pose queries at arbitrary temporal resolutions. Despite the promises of event cameras, few benchmarks have tested the limit of high-speed human motion estimation. We introduce Beam-splitter Event Agile Human Motion Dataset-a hardware-synchronized high-speed human dataset to fill this gap. On this new data, our method improves joint errors by 23.8% compared to previous event human methods while reducing the computational time by 69%.
<div id='section'>Paperid: <span id='pid'>867, <a href='https://arxiv.org/pdf/2411.19324.pdf' target='_blank'>https://arxiv.org/pdf/2411.19324.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zeqi Xiao, Wenqi Ouyang, Yifan Zhou, Shuai Yang, Lei Yang, Jianlou Si, Xingang Pan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.19324">Trajectory Attention for Fine-grained Video Motion Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in video generation have been greatly driven by video diffusion models, with camera motion control emerging as a crucial challenge in creating view-customized visual content. This paper introduces trajectory attention, a novel approach that performs attention along available pixel trajectories for fine-grained camera motion control. Unlike existing methods that often yield imprecise outputs or neglect temporal correlations, our approach possesses a stronger inductive bias that seamlessly injects trajectory information into the video generation process. Importantly, our approach models trajectory attention as an auxiliary branch alongside traditional temporal attention. This design enables the original temporal attention and the trajectory attention to work in synergy, ensuring both precise motion control and new content generation capability, which is critical when the trajectory is only partially available. Experiments on camera motion control for images and videos demonstrate significant improvements in precision and long-range consistency while maintaining high-quality generation. Furthermore, we show that our approach can be extended to other video motion control tasks, such as first-frame-guided video editing, where it excels in maintaining content consistency over large spatial and temporal ranges.
<div id='section'>Paperid: <span id='pid'>868, <a href='https://arxiv.org/pdf/2411.16273.pdf' target='_blank'>https://arxiv.org/pdf/2411.16273.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Silas Ruhrberg EstÃ©vez, JosÃ©e Mallah, Dominika Kazieczko, Chenyu Tang, Luigi G. Occhipinti
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.16273">Deep Learning for Motion Classification in Ankle Exoskeletons Using Surface EMG and IMU Signals</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Ankle exoskeletons have garnered considerable interest for their potential to enhance mobility and reduce fall risks, particularly among the aging population. The efficacy of these devices relies on accurate real-time prediction of the user's intended movements through sensor-based inputs. This paper presents a novel motion prediction framework that integrates three Inertial Measurement Units (IMUs) and eight surface Electromyography (sEMG) sensors to capture both kinematic and muscular activity data. A comprehensive set of activities, representative of everyday movements in barrier-free environments, was recorded for the purpose. Our findings reveal that Convolutional Neural Networks (CNNs) slightly outperform Long Short-Term Memory (LSTM) networks on a dataset of five motion tasks, achieving classification accuracies of $96.5 \pm 0.8 \%$ and $87.5 \pm 2.9 \%$, respectively. Furthermore, we demonstrate the system's proficiency in transfer learning, enabling accurate motion classification for new subjects using just ten samples per class for finetuning. The robustness of the model is demonstrated by its resilience to sensor failures resulting in absent signals, maintaining reliable performance in real-world scenarios. These results underscore the potential of deep learning algorithms to enhance the functionality and safety of ankle exoskeletons, ultimately improving their usability in daily life.
<div id='section'>Paperid: <span id='pid'>869, <a href='https://arxiv.org/pdf/2410.15797.pdf' target='_blank'>https://arxiv.org/pdf/2410.15797.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Julien Mellet, Andrea Berra, Achilleas Santi Seisa, Viswa Sankaranarayanan, Udayanga G. W. K. N. Gamage, Miguel Angel Trujillo Soto, Guillermo Heredia, George Nikolakopoulos, Vincenzo Lippiello, Fabio Ruggiero
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.15797">Design of a Flexible Robot Arm for Safe Aerial Physical Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces a novel compliant mechanism combining lightweight and energy dissipation for aerial physical interaction. Weighting 400~g at take-off, the mechanism is actuated in the forward body direction, enabling precise position control for force interaction and various other aerial manipulation tasks. The robotic arm, structured as a closed-loop kinematic chain, employs two deported servomotors. Each joint is actuated with a single tendon for active motion control in compression of the arm at the end-effector. Its elasto-mechanical design reduces weight and provides flexibility, allowing passive-compliant interactions without impacting the motors' integrity. Notably, the arm's damping can be adjusted based on the proposed inner frictional bulges. Experimental applications showcase the aerial system performance in both free-flight and physical interaction. The presented work may open safer applications for \ac{MAV} in real environments subject to perturbations during interaction.
<div id='section'>Paperid: <span id='pid'>870, <a href='https://arxiv.org/pdf/2409.10308.pdf' target='_blank'>https://arxiv.org/pdf/2409.10308.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Esteve Valls Mascaro, Dongheui Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.10308">Know your limits! Optimize the robot's behavior through self-awareness</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As humanoid robots transition from labs to real-world environments, it is essential to democratize robot control for non-expert users. Recent human-robot imitation algorithms focus on following a reference human motion with high precision, but they are susceptible to the quality of the reference motion and require the human operator to simplify its movements to match the robot's capabilities. Instead, we consider that the robot should understand and adapt the reference motion to its own abilities, facilitating the operator's task. For that, we introduce a deep-learning model that anticipates the robot's performance when imitating a given reference. Then, our system can generate multiple references given a high-level task command, assign a score to each of them, and select the best reference to achieve the desired robot behavior. Our Self-AWare model (SAW) ranks potential robot behaviors based on various criteria, such as fall likelihood, adherence to the reference motion, and smoothness. We integrate advanced motion generation, robot control, and SAW in one unique system, ensuring optimal robot behavior for any task command. For instance, SAW can anticipate falls with 99.29% accuracy. For more information check our project page: https://evm7.github.io/Self-AWare
<div id='section'>Paperid: <span id='pid'>871, <a href='https://arxiv.org/pdf/2409.04440.pdf' target='_blank'>https://arxiv.org/pdf/2409.04440.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vongani Maluleke, Lea MÃ¼ller, Jathushan Rajasegaran, Georgios Pavlakos, Shiry Ginosar, Angjoo Kanazawa, Jitendra Malik
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.04440">Synergy and Synchrony in Couple Dances</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper asks to what extent social interaction influences one's behavior. We study this in the setting of two dancers dancing as a couple. We first consider a baseline in which we predict a dancer's future moves conditioned only on their past motion without regard to their partner. We then investigate the advantage of taking social information into account by conditioning also on the motion of their dancing partner. We focus our analysis on Swing, a dance genre with tight physical coupling for which we present an in-the-wild video dataset. We demonstrate that single-person future motion prediction in this context is challenging. Instead, we observe that prediction greatly benefits from considering the interaction partners' behavior, resulting in surprisingly compelling couple dance synthesis results (see supp. video). Our contributions are a demonstration of the advantages of socially conditioned future motion prediction and an in-the-wild, couple dance video dataset to enable future research in this direction. Video results are available on the project website: https://von31.github.io/synNsync
<div id='section'>Paperid: <span id='pid'>872, <a href='https://arxiv.org/pdf/2409.02638.pdf' target='_blank'>https://arxiv.org/pdf/2409.02638.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junyi Ma, Xieyuanli Chen, Wentao Bao, Jingyi Xu, Hesheng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.02638">MADiff: Motion-Aware Mamba Diffusion Models for Hand Trajectory Prediction on Egocentric Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding human intentions and actions through egocentric videos is important on the path to embodied artificial intelligence. As a branch of egocentric vision techniques, hand trajectory prediction plays a vital role in comprehending human motion patterns, benefiting downstream tasks in extended reality and robot manipulation. However, capturing high-level human intentions consistent with reasonable temporal causality is challenging when only egocentric videos are available. This difficulty is exacerbated under camera egomotion interference and the absence of affordance labels to explicitly guide the optimization of hand waypoint distribution. In this work, we propose a novel hand trajectory prediction method dubbed MADiff, which forecasts future hand waypoints with diffusion models. The devised denoising operation in the latent space is achieved by our proposed motion-aware Mamba, where the camera wearer's egomotion is integrated to achieve motion-driven selective scan (MDSS). To discern the relationship between hands and scenarios without explicit affordance supervision, we leverage a foundation model that fuses visual and language features to capture high-level semantics from video clips. Comprehensive experiments conducted on five public datasets with the existing and our proposed new evaluation metrics demonstrate that MADiff predicts comparably reasonable hand trajectories compared to the state-of-the-art baselines, and achieves real-time performance. We will release our code and pretrained models of MADiff at the project page: https://irmvlab.github.io/madiff.github.io.
<div id='section'>Paperid: <span id='pid'>873, <a href='https://arxiv.org/pdf/2408.03302.pdf' target='_blank'>https://arxiv.org/pdf/2408.03302.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Siyuan Fan, Bo Du, Xiantao Cai, Bo Peng, Longling Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.03302">TextIM: Part-aware Interactive Motion Synthesis from Text</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we propose TextIM, a novel framework for synthesizing TEXT-driven human Interactive Motions, with a focus on the precise alignment of part-level semantics. Existing methods often overlook the critical roles of interactive body parts and fail to adequately capture and align part-level semantics, resulting in inaccuracies and even erroneous movement outcomes. To address these issues, TextIM utilizes a decoupled conditional diffusion framework to enhance the detailed alignment between interactive movements and corresponding semantic intents from textual descriptions. Our approach leverages large language models, functioning as a human brain, to identify interacting human body parts and to comprehend interaction semantics to generate complicated and subtle interactive motion. Guided by the refined movements of the interacting parts, TextIM further extends these movements into a coherent whole-body motion. We design a spatial coherence module to complement the entire body movements while maintaining consistency and harmony across body parts using a part graph convolutional network. For training and evaluation, we carefully selected and re-labeled interactive motions from HUMANML3D to develop a specialized dataset. Experimental results demonstrate that TextIM produces semantically accurate human interactive motions, significantly enhancing the realism and applicability of synthesized interactive motions in diverse scenarios, even including interactions with deformable and dynamically changing objects.
<div id='section'>Paperid: <span id='pid'>874, <a href='https://arxiv.org/pdf/2407.19071.pdf' target='_blank'>https://arxiv.org/pdf/2407.19071.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Minjun Sung, Hunmin Kim, Naira Hovakimyan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.19071">Addressing Behavior Model Inaccuracies for Safe Motion Control in Uncertain Dynamic Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Uncertainties in the environment and behavior model inaccuracies compromise the state estimation of a dynamic obstacle and its trajectory predictions, introducing biases in estimation and shifts in predictive distributions. Addressing these challenges is crucial to safely control an autonomous system. In this paper, we propose a novel algorithm SIED-MPC, which synergistically integrates Simultaneous State and Input Estimation (SSIE) and Distributionally Robust Model Predictive Control (DR-MPC) using model confidence evaluation. The SSIE process produces unbiased state estimates and optimal input gap estimates to assess the confidence of the behavior model, defining the ambiguity radius for DR-MPC to handle predictive distribution shifts. This systematic confidence evaluation leads to producing safe inputs with an adequate level of conservatism. Our algorithm demonstrated a reduced collision rate in autonomous driving simulations through improved state estimation, with a 54% shorter average computation time.
<div id='section'>Paperid: <span id='pid'>875, <a href='https://arxiv.org/pdf/2406.19353.pdf' target='_blank'>https://arxiv.org/pdf/2406.19353.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yun Liu, Chengwen Zhang, Ruofan Xing, Bingda Tang, Bowen Yang, Li Yi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.19353">CORE4D: A 4D Human-Object-Human Interaction Dataset for Collaborative Object REarrangement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding how humans cooperatively rearrange household objects is critical for VR/AR and human-robot interaction. However, in-depth studies on modeling these behaviors are under-researched due to the lack of relevant datasets. We fill this gap by presenting CORE4D, a novel large-scale 4D human-object-human interaction dataset focusing on collaborative object rearrangement, which encompasses diverse compositions of various object geometries, collaboration modes, and 3D scenes. With 1K human-object-human motion sequences captured in the real world, we enrich CORE4D by contributing an iterative collaboration retargeting strategy to augment motions to a variety of novel objects. Leveraging this approach, CORE4D comprises a total of 11K collaboration sequences spanning 3K real and virtual object shapes. Benefiting from extensive motion patterns provided by CORE4D, we benchmark two tasks aiming at generating human-object interaction: human-object motion forecasting and interaction synthesis. Extensive experiments demonstrate the effectiveness of our collaboration retargeting strategy and indicate that CORE4D has posed new challenges to existing human-object interaction generation methodologies.
<div id='section'>Paperid: <span id='pid'>876, <a href='https://arxiv.org/pdf/2405.17817.pdf' target='_blank'>https://arxiv.org/pdf/2405.17817.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vida Adeli, Soroush Mehraban, Irene Ballester, Yasamin Zarghami, Andrea Sabo, Andrea Iaboni, Babak Taati
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.17817">Benchmarking Skeleton-based Motion Encoder Models for Clinical Applications: Estimating Parkinson's Disease Severity in Walking Sequences</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study investigates the application of general human motion encoders trained on large-scale human motion datasets for analyzing gait patterns in PD patients. Although these models have learned a wealth of human biomechanical knowledge, their effectiveness in analyzing pathological movements, such as parkinsonian gait, has yet to be fully validated. We propose a comparative framework and evaluate six pre-trained state-of-the-art human motion encoder models on their ability to predict the Movement Disorder Society - Unified Parkinson's Disease Rating Scale (MDS-UPDRS-III) gait scores from motion capture data. We compare these against a traditional gait feature-based predictive model in a recently released large public PD dataset, including PD patients on and off medication. The feature-based model currently shows higher weighted average accuracy, precision, recall, and F1-score. Motion encoder models with closely comparable results demonstrate promise for scalability and efficiency in clinical settings. This potential is underscored by the enhanced performance of the encoder model upon fine-tuning on PD training set. Four of the six human motion models examined provided prediction scores that were significantly different between on- and off-medication states. This finding reveals the sensitivity of motion encoder models to nuanced clinical changes. It also underscores the necessity for continued customization of these models to better capture disease-specific features, thereby reducing the reliance on labor-intensive feature engineering. Lastly, we establish a benchmark for the analysis of skeleton-based motion encoder models in clinical settings. To the best of our knowledge, this is the first study to provide a benchmark that enables state-of-the-art models to be tested and compete in a clinical context. Codes and benchmark leaderboard are available at code.
<div id='section'>Paperid: <span id='pid'>877, <a href='https://arxiv.org/pdf/2405.15439.pdf' target='_blank'>https://arxiv.org/pdf/2405.15439.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zichen Geng, Caren Han, Zeeshan Hayder, Jian Liu, Mubarak Shah, Ajmal Mian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.15439">Text-guided 3D Human Motion Generation with Keyframe-based Parallel Skip Transformer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-driven human motion generation is an emerging task in animation and humanoid robot design. Existing algorithms directly generate the full sequence which is computationally expensive and prone to errors as it does not pay special attention to key poses, a process that has been the cornerstone of animation for decades. We propose KeyMotion, that generates plausible human motion sequences corresponding to input text by first generating keyframes followed by in-filling. We use a Variational Autoencoder (VAE) with Kullback-Leibler regularization to project the keyframes into a latent space to reduce dimensionality and further accelerate the subsequent diffusion process. For the reverse diffusion, we propose a novel Parallel Skip Transformer that performs cross-modal attention between the keyframe latents and text condition. To complete the motion sequence, we propose a text-guided Transformer designed to perform motion-in-filling, ensuring the preservation of both fidelity and adherence to the physical constraints of human motion. Experiments show that our method achieves state-of-theart results on the HumanML3D dataset outperforming others on all R-precision metrics and MultiModal Distance. KeyMotion also achieves competitive performance on the KIT dataset, achieving the best results on Top3 R-precision, FID, and Diversity metrics.
<div id='section'>Paperid: <span id='pid'>878, <a href='https://arxiv.org/pdf/2405.14864.pdf' target='_blank'>https://arxiv.org/pdf/2405.14864.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zeqi Xiao, Yifan Zhou, Shuai Yang, Xingang Pan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.14864">Video Diffusion Models are Training-free Motion Interpreter and Controller</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video generation primarily aims to model authentic and customized motion across frames, making understanding and controlling the motion a crucial topic. Most diffusion-based studies on video motion focus on motion customization with training-based paradigms, which, however, demands substantial training resources and necessitates retraining for diverse models. Crucially, these approaches do not explore how video diffusion models encode cross-frame motion information in their features, lacking interpretability and transparency in their effectiveness. To answer this question, this paper introduces a novel perspective to understand, localize, and manipulate motion-aware features in video diffusion models. Through analysis using Principal Component Analysis (PCA), our work discloses that robust motion-aware feature already exists in video diffusion models. We present a new MOtion FeaTure (MOFT) by eliminating content correlation information and filtering motion channels. MOFT provides a distinct set of benefits, including the ability to encode comprehensive motion information with clear interpretability, extraction without the need for training, and generalizability across diverse architectures. Leveraging MOFT, we propose a novel training-free video motion control framework. Our method demonstrates competitive performance in generating natural and faithful motion, providing architecture-agnostic insights and applicability in a variety of downstream tasks.
<div id='section'>Paperid: <span id='pid'>879, <a href='https://arxiv.org/pdf/2405.14017.pdf' target='_blank'>https://arxiv.org/pdf/2405.14017.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Zhang, Di Chang, Fang Li, Mohammad Soleymani, Narendra Ahuja
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.14017">MagicPose4D: Crafting Articulated Models with Appearance and Motion Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the success of 2D and 3D visual generative models, there is growing interest in generating 4D content. Existing methods primarily rely on text prompts to produce 4D content, but they often fall short of accurately defining complex or rare motions. To address this limitation, we propose MagicPose4D, a novel framework for refined control over both appearance and motion in 4D generation. Unlike current 4D generation methods, MagicPose4D accepts monocular videos or mesh sequences as motion prompts, enabling precise and customizable motion control. MagicPose4D comprises two key modules: (i) Dual-Phase 4D Reconstruction Module, which operates in two phases. The first phase focuses on capturing the model's shape using accurate 2D supervision and less accurate but geometrically informative 3D pseudo-supervision without imposing skeleton constraints. The second phase extracts the 3D motion (skeleton poses) using more accurate pseudo-3D supervision, obtained in the first phase and introduces kinematic chain-based skeleton constraints to ensure physical plausibility. Additionally, we propose a Global-local Chamfer loss that aligns the overall distribution of predicted mesh vertices with the supervision while maintaining part-level alignment without extra annotations. (ii) Cross-category Motion Transfer Module, which leverages the extracted motion from the 4D reconstruction module and uses a kinematic-chain-based skeleton to achieve cross-category motion transfer. It ensures smooth transitions between frames through dynamic rigidity, facilitating robust generalization without additional training. Through extensive experiments, we demonstrate that MagicPose4D significantly improves the accuracy and consistency of 4D content generation, outperforming existing methods in various benchmarks.
<div id='section'>Paperid: <span id='pid'>880, <a href='https://arxiv.org/pdf/2405.08726.pdf' target='_blank'>https://arxiv.org/pdf/2405.08726.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yashuai Yan, Esteve Valls Mascaro, Tobias Egle, Dongheui Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.08726">I-CTRL: Imitation to Control Humanoid Robots Through Constrained Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humanoid robots have the potential to mimic human motions with high visual fidelity, yet translating these motions into practical, physical execution remains a significant challenge. Existing techniques in the graphics community often prioritize visual fidelity over physics-based feasibility, posing a significant challenge for deploying bipedal systems in practical applications. This paper addresses these issues through bounded residual reinforcement learning to produce physics-based high-quality motion imitation onto legged humanoid robots that enhance motion resemblance while successfully following the reference human trajectory. Our framework, Imitation to Control Humanoid Robots Through Bounded Residual Reinforcement Learning (I-CTRL), reformulates motion imitation as a constrained refinement over non-physics-based retargeted motions. I-CTRL excels in motion imitation with simple and unique rewards that generalize across five robots. Moreover, our framework introduces an automatic priority scheduler to manage large-scale motion datasets when efficiently training a unified RL policy across diverse motions. The proposed approach signifies a crucial step forward in advancing the control of bipedal robots, emphasizing the importance of aligning visual and physical realism for successful motion imitation.
<div id='section'>Paperid: <span id='pid'>881, <a href='https://arxiv.org/pdf/2404.19619.pdf' target='_blank'>https://arxiv.org/pdf/2404.19619.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinyu Yi, Yuxiao Zhou, Feng Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.19619">Physical Non-inertial Poser (PNP): Modeling Non-inertial Effects in Sparse-inertial Human Motion Capture</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing inertial motion capture techniques use the human root coordinate frame to estimate local poses and treat it as an inertial frame by default. We argue that when the root has linear acceleration or rotation, the root frame should be considered non-inertial theoretically. In this paper, we model the fictitious forces that are non-neglectable in a non-inertial frame by an auto-regressive estimator delicately designed following physics. With the fictitious forces, the force-related IMU measurement (accelerations) can be correctly compensated in the non-inertial frame and thus Newton's laws of motion are satisfied. In this case, the relationship between the accelerations and body motions is deterministic and learnable, and we train a neural network to model it for better motion capture. Furthermore, to train the neural network with synthetic data, we develop an IMU synthesis by simulation strategy to better model the noise model of IMU hardware and allow parameter tuning to fit different hardware. This strategy not only establishes the network training with synthetic data but also enables calibration error modeling to handle bad motion capture calibration, increasing the robustness of the system. Code is available at https://xinyu-yi.github.io/PNP/.
<div id='section'>Paperid: <span id='pid'>882, <a href='https://arxiv.org/pdf/2404.12942.pdf' target='_blank'>https://arxiv.org/pdf/2404.12942.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nicolas Ugrinovic, Thomas Lucas, Fabien Baradel, Philippe Weinzaepfel, Gregory Rogez, Francesc Moreno-Noguer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.12942">Purposer: Putting Human Motion Generation in Context</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a novel method to generate human motion to populate 3D indoor scenes. It can be controlled with various combinations of conditioning signals such as a path in a scene, target poses, past motions, and scenes represented as 3D point clouds. State-of-the-art methods are either models specialized to one single setting, require vast amounts of high-quality and diverse training data, or are unconditional models that do not integrate scene or other contextual information. As a consequence, they have limited applicability and rely on costly training data. To address these limitations, we propose a new method ,dubbed Purposer, based on neural discrete representation learning. Our model is capable of exploiting, in a flexible manner, different types of information already present in open access large-scale datasets such as AMASS. First, we encode unconditional human motion into a discrete latent space. Second, an autoregressive generative model, conditioned with key contextual information, either with prompting or additive tokens, and trained for next-step prediction in this space, synthesizes sequences of latent indices. We further design a novel conditioning block to handle future conditioning information in such a causal model by using a network with two branches to compute separate stacks of features. In this manner, Purposer can generate realistic motion sequences in diverse test scenes. Through exhaustive evaluation, we demonstrate that our multi-contextual solution outperforms existing specialized approaches for specific contextual information, both in terms of quality and diversity. Our model is trained with short sequences, but a byproduct of being able to use various conditioning signals is that at test time different combinations can be used to chain short sequences together and generate long motions within a context scene.
<div id='section'>Paperid: <span id='pid'>883, <a href='https://arxiv.org/pdf/2404.11557.pdf' target='_blank'>https://arxiv.org/pdf/2404.11557.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Taerim Yoon, Dongho Kang, Seungmin Kim, Jin Cheng, Minsung Ahn, Stelian Coros, Sungjoon Choi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.11557">Spatio-Temporal Motion Retargeting for Quadruped Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work presents a motion retargeting approach for legged robots, aimed at transferring the dynamic and agile movements to robots from source motions. In particular, we guide the imitation learning procedures by transferring motions from source to target, effectively bridging the morphological disparities while ensuring the physical feasibility of the target system. In the first stage, we focus on motion retargeting at the kinematic level by generating kinematically feasible whole-body motions from keypoint trajectories. Following this, we refine the motion at the dynamic level by adjusting it in the temporal domain while adhering to physical constraints. This process facilitates policy training via reinforcement learning, enabling precise and robust motion tracking. We demonstrate that our approach successfully transforms noisy motion sources, such as hand-held camera videos, into robot-specific motions that align with the morphology and physical properties of the target robots. Moreover, we demonstrate terrain-aware motion retargeting to perform BackFlip on top of a box. We successfully deployed these skills to four robots with different dimensions and physical properties in the real world through hardware experiments.
<div id='section'>Paperid: <span id='pid'>884, <a href='https://arxiv.org/pdf/2404.09988.pdf' target='_blank'>https://arxiv.org/pdf/2404.09988.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pablo Ruiz Ponce, German Barquero, Cristina Palmero, Sergio Escalera, Jose Garcia-Rodriguez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.09988">in2IN: Leveraging individual Information to Generate Human INteractions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating human-human motion interactions conditioned on textual descriptions is a very useful application in many areas such as robotics, gaming, animation, and the metaverse. Alongside this utility also comes a great difficulty in modeling the highly dimensional inter-personal dynamics. In addition, properly capturing the intra-personal diversity of interactions has a lot of challenges. Current methods generate interactions with limited diversity of intra-person dynamics due to the limitations of the available datasets and conditioning strategies. For this, we introduce in2IN, a novel diffusion model for human-human motion generation which is conditioned not only on the textual description of the overall interaction but also on the individual descriptions of the actions performed by each person involved in the interaction. To train this model, we use a large language model to extend the InterHuman dataset with individual descriptions. As a result, in2IN achieves state-of-the-art performance in the InterHuman dataset. Furthermore, in order to increase the intra-personal diversity on the existing interaction datasets, we propose DualMDM, a model composition technique that combines the motions generated with in2IN and the motions generated by a single-person motion prior pre-trained on HumanML3D. As a result, DualMDM generates motions with higher individual diversity and improves control over the intra-person dynamics while maintaining inter-personal coherence.
<div id='section'>Paperid: <span id='pid'>885, <a href='https://arxiv.org/pdf/2404.07505.pdf' target='_blank'>https://arxiv.org/pdf/2404.07505.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Thies Oelerich, Christian Hartl-Nesic, Andreas Kugi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.07505">Model Predictive Trajectory Planning for Human-Robot Handovers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work develops a novel trajectory planner for human-robot handovers. The handover requirements can naturally be handled by a path-following-based model predictive controller, where the path progress serves as a progress measure of the handover. Moreover, the deviations from the path are used to follow human motion by adapting the path deviation bounds with a handover location prediction. A Gaussian process regression model, which is trained on known handover trajectories, is employed for this prediction. Experiments with a collaborative 7-DoF robotic manipulator show the effectiveness and versatility of the proposed approach.
<div id='section'>Paperid: <span id='pid'>886, <a href='https://arxiv.org/pdf/2404.01596.pdf' target='_blank'>https://arxiv.org/pdf/2404.01596.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhipeng Zhao, Bowen Li, Yi Du, Taimeng Fu, Chen Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.01596">PhysORD: A Neuro-Symbolic Approach for Physics-infused Motion Prediction in Off-road Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion prediction is critical for autonomous off-road driving, however, it presents significantly more challenges than on-road driving because of the complex interaction between the vehicle and the terrain. Traditional physics-based approaches encounter difficulties in accurately modeling dynamic systems and external disturbance. In contrast, data-driven neural networks require extensive datasets and struggle with explicitly capturing the fundamental physical laws, which can easily lead to poor generalization. By merging the advantages of both methods, neuro-symbolic approaches present a promising direction. These methods embed physical laws into neural models, potentially significantly improving generalization capabilities. However, no prior works were evaluated in real-world settings for off-road driving. To bridge this gap, we present PhysORD, a neural-symbolic approach integrating the conservation law, i.e., the Euler-Lagrange equation, into data-driven neural models for motion prediction in off-road driving. Our experiments showed that PhysORD can accurately predict vehicle motion and tolerate external disturbance by modeling uncertainties. The learned dynamics model achieves 46.7% higher accuracy using only 3.1% of the parameters compared to data-driven methods, demonstrating the data efficiency and superior generalization ability of our neural-symbolic method.
<div id='section'>Paperid: <span id='pid'>887, <a href='https://arxiv.org/pdf/2403.19435.pdf' target='_blank'>https://arxiv.org/pdf/2403.19435.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ekkasit Pinyoanuntapong, Muhammad Usama Saleem, Pu Wang, Minwoo Lee, Srijan Das, Chen Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.19435">BAMM: Bidirectional Autoregressive Motion Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating human motion from text has been dominated by denoising motion models either through diffusion or generative masking process. However, these models face great limitations in usability by requiring prior knowledge of the motion length. Conversely, autoregressive motion models address this limitation by adaptively predicting motion endpoints, at the cost of degraded generation quality and editing capabilities. To address these challenges, we propose Bidirectional Autoregressive Motion Model (BAMM), a novel text-to-motion generation framework. BAMM consists of two key components: (1) a motion tokenizer that transforms 3D human motion into discrete tokens in latent space, and (2) a masked self-attention transformer that autoregressively predicts randomly masked tokens via a hybrid attention masking strategy. By unifying generative masked modeling and autoregressive modeling, BAMM captures rich and bidirectional dependencies among motion tokens, while learning the probabilistic mapping from textual inputs to motion outputs with dynamically-adjusted motion sequence length. This feature enables BAMM to simultaneously achieving high-quality motion generation with enhanced usability and built-in motion editability. Extensive experiments on HumanML3D and KIT-ML datasets demonstrate that BAMM surpasses current state-of-the-art methods in both qualitative and quantitative measures. Our project page is available at https://exitudio.github.io/BAMM-page
<div id='section'>Paperid: <span id='pid'>888, <a href='https://arxiv.org/pdf/2403.16080.pdf' target='_blank'>https://arxiv.org/pdf/2403.16080.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoyun Zheng, Liwei Liao, Xufeng Li, Jianbo Jiao, Rongjie Wang, Feng Gao, Shiqi Wang, Ronggang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.16080">PKU-DyMVHumans: A Multi-View Video Benchmark for High-Fidelity Dynamic Human Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>High-quality human reconstruction and photo-realistic rendering of a dynamic scene is a long-standing problem in computer vision and graphics. Despite considerable efforts invested in developing various capture systems and reconstruction algorithms, recent advancements still struggle with loose or oversized clothing and overly complex poses. In part, this is due to the challenges of acquiring high-quality human datasets. To facilitate the development of these fields, in this paper, we present PKU-DyMVHumans, a versatile human-centric dataset for high-fidelity reconstruction and rendering of dynamic human scenarios from dense multi-view videos. It comprises 8.2 million frames captured by more than 56 synchronized cameras across diverse scenarios. These sequences comprise 32 human subjects across 45 different scenarios, each with a high-detailed appearance and realistic human motion. Inspired by recent advancements in neural radiance field (NeRF)-based scene representations, we carefully set up an off-the-shelf framework that is easy to provide those state-of-the-art NeRF-based implementations and benchmark on PKU-DyMVHumans dataset. It is paving the way for various applications like fine-grained foreground/background decomposition, high-quality human reconstruction and photo-realistic novel view synthesis of a dynamic scene. Extensive studies are performed on the benchmark, demonstrating new observations and challenges that emerge from using such high-fidelity dynamic data.
<div id='section'>Paperid: <span id='pid'>889, <a href='https://arxiv.org/pdf/2403.13238.pdf' target='_blank'>https://arxiv.org/pdf/2403.13238.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qitong Yang, Mingtao Feng, Zijie Wu, Shijie Sun, Weisheng Dong, Yaonan Wang, Ajmal Mian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.13238">Learning Coherent Matrixized Representation in Latent Space for Volumetric 4D Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Directly learning to model 4D content, including shape, color, and motion, is challenging. Existing methods rely on pose priors for motion control, resulting in limited motion diversity and continuity in details. To address this, we propose a framework that generates volumetric 4D sequences, where 3D shapes are animated under given conditions (text-image guidance) with dynamic evolution in shape and color across spatial and temporal dimensions, allowing for free navigation and rendering from any direction. We first use a coherent 3D shape and color modeling to encode the shape and color of each detailed 3D geometry frame into a latent space. Then we propose a matrixized 4D sequence representation allowing efficient diffusion model operation. Finally, we introduce spatio-temporal diffusion for 4D volumetric generation under given images and text prompts. Extensive experiments on the ShapeNet, 3DBiCar, DeformingThings4D and Objaverse datasets for several tasks demonstrate that our method effectively learns to generate high quality 3D shapes with consistent color and coherent mesh animations, improving over the current methods. Our code will be publicly available.
<div id='section'>Paperid: <span id='pid'>890, <a href='https://arxiv.org/pdf/2403.11237.pdf' target='_blank'>https://arxiv.org/pdf/2403.11237.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaohan Zhang, Bharat Lal Bhatnagar, Sebastian Starke, Ilya Petrov, Vladimir Guzov, Helisa Dhamo, Eduardo PÃ©rez-Pellitero, Gerard Pons-Moll
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.11237">FORCE: Physics-aware Human-object Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Interactions between human and objects are influenced not only by the object's pose and shape, but also by physical attributes such as object mass and surface friction. They introduce important motion nuances that are essential for diversity and realism. Despite advancements in recent human-object interaction methods, this aspect has been overlooked. Generating nuanced human motion presents two challenges. First, it is non-trivial to learn from multi-modal human and object information derived from both the physical and non-physical attributes. Second, there exists no dataset capturing nuanced human interactions with objects of varying physical properties, hampering model development. This work addresses the gap by introducing the FORCE model, an approach for synthesizing diverse, nuanced human-object interactions by modeling physical attributes. Our key insight is that human motion is dictated by the interrelation between the force exerted by the human and the perceived resistance. Guided by a novel intuitive physics encoding, the model captures the interplay between human force and resistance. Experiments also demonstrate incorporating human force facilitates learning multi-class motion. Accompanying our model, we contribute a dataset, which features diverse, different-styled motion through interactions with varying resistances.
<div id='section'>Paperid: <span id='pid'>891, <a href='https://arxiv.org/pdf/2402.15509.pdf' target='_blank'>https://arxiv.org/pdf/2402.15509.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>German Barquero, Sergio Escalera, Cristina Palmero
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.15509">Seamless Human Motion Composition with Blended Positional Encodings</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Conditional human motion generation is an important topic with many applications in virtual reality, gaming, and robotics. While prior works have focused on generating motion guided by text, music, or scenes, these typically result in isolated motions confined to short durations. Instead, we address the generation of long, continuous sequences guided by a series of varying textual descriptions. In this context, we introduce FlowMDM, the first diffusion-based model that generates seamless Human Motion Compositions (HMC) without any postprocessing or redundant denoising steps. For this, we introduce the Blended Positional Encodings, a technique that leverages both absolute and relative positional encodings in the denoising chain. More specifically, global motion coherence is recovered at the absolute stage, whereas smooth and realistic transitions are built at the relative stage. As a result, we achieve state-of-the-art results in terms of accuracy, realism, and smoothness on the Babel and HumanML3D datasets. FlowMDM excels when trained with only a single description per motion sequence thanks to its Pose-Centric Cross-ATtention, which makes it robust against varying text descriptions at inference time. Finally, to address the limitations of existing HMC metrics, we propose two new metrics: the Peak Jerk and the Area Under the Jerk, to detect abrupt transitions.
<div id='section'>Paperid: <span id='pid'>892, <a href='https://arxiv.org/pdf/2402.14780.pdf' target='_blank'>https://arxiv.org/pdf/2402.14780.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yixuan Ren, Yang Zhou, Jimei Yang, Jing Shi, Difan Liu, Feng Liu, Mingi Kwon, Abhinav Shrivastava
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.14780">Customize-A-Video: One-Shot Motion Customization of Text-to-Video Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image customization has been extensively studied in text-to-image (T2I) diffusion models, leading to impressive outcomes and applications. With the emergence of text-to-video (T2V) diffusion models, its temporal counterpart, motion customization, has not yet been well investigated. To address the challenge of one-shot video motion customization, we propose Customize-A-Video that models the motion from a single reference video and adapts it to new subjects and scenes with both spatial and temporal varieties. It leverages low-rank adaptation (LoRA) on temporal attention layers to tailor the pre-trained T2V diffusion model for specific motion modeling. To disentangle the spatial and temporal information during training, we introduce a novel concept of appearance absorbers that detach the original appearance from the reference video prior to motion learning. The proposed modules are trained in a staged pipeline and inferred in a plug-and-play fashion, enabling easy extensions to various downstream tasks such as custom video generation and editing, video appearance customization and multiple motion combination. Our project page can be found at https://customize-a-video.github.io.
<div id='section'>Paperid: <span id='pid'>893, <a href='https://arxiv.org/pdf/2402.12099.pdf' target='_blank'>https://arxiv.org/pdf/2402.12099.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haiming Zhu, Yangyang Xu, Shengfeng He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.12099">Human Video Translation via Query Warping</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we present QueryWarp, a novel framework for temporally coherent human motion video translation. Existing diffusion-based video editing approaches that rely solely on key and value tokens to ensure temporal consistency, which scarifies the preservation of local and structural regions. In contrast, we aim to consider complementary query priors by constructing the temporal correlations among query tokens from different frames. Initially, we extract appearance flows from source poses to capture continuous human foreground motion. Subsequently, during the denoising process of the diffusion model, we employ appearance flows to warp the previous frame's query token, aligning it with the current frame's query. This query warping imposes explicit constraints on the outputs of self-attention layers, effectively guaranteeing temporally coherent translation. We perform experiments on various human motion video translation tasks, and the results demonstrate that our QueryWarp framework surpasses state-of-the-art methods both qualitatively and quantitatively.
<div id='section'>Paperid: <span id='pid'>894, <a href='https://arxiv.org/pdf/2401.16189.pdf' target='_blank'>https://arxiv.org/pdf/2401.16189.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sungmin Woo, Minjung Kim, Donghyeong Kim, Sungjun Jang, Sangyoun Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.16189">FIMP: Future Interaction Modeling for Multi-Agent Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent motion prediction is a crucial concern in autonomous driving, yet it remains a challenge owing to the ambiguous intentions of dynamic agents and their intricate interactions. Existing studies have attempted to capture interactions between road entities by using the definite data in history timesteps, as future information is not available and involves high uncertainty. However, without sufficient guidance for capturing future states of interacting agents, they frequently produce unrealistic trajectory overlaps. In this work, we propose Future Interaction modeling for Motion Prediction (FIMP), which captures potential future interactions in an end-to-end manner. FIMP adopts a future decoder that implicitly extracts the potential future information in an intermediate feature-level, and identifies the interacting entity pairs through future affinity learning and top-k filtering strategy. Experiments show that our future interaction modeling improves the performance remarkably, leading to superior performance on the Argoverse motion forecasting benchmark.
<div id='section'>Paperid: <span id='pid'>895, <a href='https://arxiv.org/pdf/2312.10960.pdf' target='_blank'>https://arxiv.org/pdf/2312.10960.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenyu Xie, Yang Wu, Xuehao Gao, Zhongqian Sun, Wei Yang, Xiaodan Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.10960">Towards Detailed Text-to-Motion Synthesis via Basic-to-Advanced Hierarchical Diffusion Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-guided motion synthesis aims to generate 3D human motion that not only precisely reflects the textual description but reveals the motion details as much as possible. Pioneering methods explore the diffusion model for text-to-motion synthesis and obtain significant superiority. However, these methods conduct diffusion processes either on the raw data distribution or the low-dimensional latent space, which typically suffer from the problem of modality inconsistency or detail-scarce. To tackle this problem, we propose a novel Basic-to-Advanced Hierarchical Diffusion Model, named B2A-HDM, to collaboratively exploit low-dimensional and high-dimensional diffusion models for high quality detailed motion synthesis. Specifically, the basic diffusion model in low-dimensional latent space provides the intermediate denoising result that to be consistent with the textual description, while the advanced diffusion model in high-dimensional latent space focuses on the following detail-enhancing denoising process. Besides, we introduce a multi-denoiser framework for the advanced diffusion model to ease the learning of high-dimensional model and fully explore the generative potential of the diffusion model. Quantitative and qualitative experiment results on two text-to-motion benchmarks (HumanML3D and KIT-ML) demonstrate that B2A-HDM can outperform existing state-of-the-art methods in terms of fidelity, modality consistency, and diversity.
<div id='section'>Paperid: <span id='pid'>896, <a href='https://arxiv.org/pdf/2312.03596.pdf' target='_blank'>https://arxiv.org/pdf/2312.03596.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ekkasit Pinyoanuntapong, Pu Wang, Minwoo Lee, Chen Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.03596">MMM: Generative Masked Motion Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in text-to-motion generation using diffusion and autoregressive models have shown promising results. However, these models often suffer from a trade-off between real-time performance, high fidelity, and motion editability. To address this gap, we introduce MMM, a novel yet simple motion generation paradigm based on Masked Motion Model. MMM consists of two key components: (1) a motion tokenizer that transforms 3D human motion into a sequence of discrete tokens in latent space, and (2) a conditional masked motion transformer that learns to predict randomly masked motion tokens, conditioned on the pre-computed text tokens. By attending to motion and text tokens in all directions, MMM explicitly captures inherent dependency among motion tokens and semantic mapping between motion and text tokens. During inference, this allows parallel and iterative decoding of multiple motion tokens that are highly consistent with fine-grained text descriptions, therefore simultaneously achieving high-fidelity and high-speed motion generation. In addition, MMM has innate motion editability. By simply placing mask tokens in the place that needs editing, MMM automatically fills the gaps while guaranteeing smooth transitions between editing and non-editing parts. Extensive experiments on the HumanML3D and KIT-ML datasets demonstrate that MMM surpasses current leading methods in generating high-quality motion (evidenced by superior FID scores of 0.08 and 0.429), while offering advanced editing features such as body-part modification, motion in-betweening, and the synthesis of long motion sequences. In addition, MMM is two orders of magnitude faster on a single mid-range GPU than editable motion diffusion models. Our project page is available at \url{https://exitudio.github.io/MMM-page}.
<div id='section'>Paperid: <span id='pid'>897, <a href='https://arxiv.org/pdf/2312.00113.pdf' target='_blank'>https://arxiv.org/pdf/2312.00113.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyun Wang, Friedhelm Hamann, Kenneth Chaney, Wen Jiang, Guillermo Gallego, Kostas Daniilidis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.00113">Event-based Continuous Color Video Decompression from Single Frames</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present ContinuityCam, a novel approach to generate a continuous video from a single static RGB image and an event camera stream. Conventional cameras struggle with high-speed motion capture due to bandwidth and dynamic range limitations. Event cameras are ideal sensors to solve this problem because they encode compressed change information at high temporal resolution. In this work, we tackle the problem of event-based continuous color video decompression, pairing single static color frames and event data to reconstruct temporally continuous videos. Our approach combines continuous long-range motion modeling with a neural synthesis model, enabling frame prediction at arbitrary times within the events. Our method only requires an initial image, thus increasing the robustness to sudden motions, light changes, minimizing the prediction latency, and decreasing bandwidth usage. We also introduce a novel single-lens beamsplitter setup that acquires aligned images and events, and a novel and challenging Event Extreme Decompression Dataset (E2D2) that tests the method in various lighting and motion profiles. We thoroughly evaluate our method by benchmarking color frame reconstruction, outperforming the baseline methods by 3.61 dB in PSNR and by 33% decrease in LPIPS, as well as showing superior results on two downstream tasks.
<div id='section'>Paperid: <span id='pid'>898, <a href='https://arxiv.org/pdf/2311.09104.pdf' target='_blank'>https://arxiv.org/pdf/2311.09104.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Matthieu Armando, Salma Galaaoui, Fabien Baradel, Thomas Lucas, Vincent Leroy, Romain BrÃ©gier, Philippe Weinzaepfel, GrÃ©gory Rogez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.09104">Cross-view and Cross-pose Completion for 3D Human Understanding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human perception and understanding is a major domain of computer vision which, like many other vision subdomains recently, stands to gain from the use of large models pre-trained on large datasets. We hypothesize that the most common pre-training strategy of relying on general purpose, object-centric image datasets such as ImageNet, is limited by an important domain shift. On the other hand, collecting domain-specific ground truth such as 2D or 3D labels does not scale well. Therefore, we propose a pre-training approach based on self-supervised learning that works on human-centric data using only images. Our method uses pairs of images of humans: the first is partially masked and the model is trained to reconstruct the masked parts given the visible ones and a second image. It relies on both stereoscopic (cross-view) pairs, and temporal (cross-pose) pairs taken from videos, in order to learn priors about 3D as well as human motion. We pre-train a model for body-centric tasks and one for hand-centric tasks. With a generic transformer architecture, these models outperform existing self-supervised pre-training methods on a wide set of human-centric downstream tasks, and obtain state-of-the-art performance for instance when fine-tuning for model-based and model-free human mesh recovery.
<div id='section'>Paperid: <span id='pid'>899, <a href='https://arxiv.org/pdf/2310.14907.pdf' target='_blank'>https://arxiv.org/pdf/2310.14907.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chunzhi Gu, Chao Zhang, Shigeru Kuriyama
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.14907">Orientation-Aware Leg Movement Learning for Action-Driven Human Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The task of action-driven human motion prediction aims to forecast future human motion based on the observed sequence while respecting the given action label. It requires modeling not only the stochasticity within human motion but the smooth yet realistic transition between multiple action labels. However, the fact that most datasets do not contain such transition data complicates this task. Existing work tackles this issue by learning a smoothness prior to simply promote smooth transitions, yet doing so can result in unnatural transitions especially when the history and predicted motions differ significantly in orientations. In this paper, we argue that valid human motion transitions should incorporate realistic leg movements to handle orientation changes, and cast it as an action-conditioned in-betweening (ACB) learning task to encourage transition naturalness. Because modeling all possible transitions is virtually unreasonable, our ACB is only performed on very few selected action classes with active gait motions, such as Walk or Run. Specifically, we follow a two-stage forecasting strategy by first employing the motion diffusion model to generate the target motion with a specified future action, and then producing the in-betweening to smoothly connect the observation and prediction to eventually address motion prediction. Our method is completely free from the labeled motion transition data during training. To show the robustness of our approach, we generalize our trained in-betweening learning model on one dataset to two unseen large-scale motion datasets to produce natural transitions. Extensive experimental evaluations on three benchmark datasets demonstrate that our method yields the state-of-the-art performance in terms of visual quality, prediction accuracy, and action faithfulness.
<div id='section'>Paperid: <span id='pid'>900, <a href='https://arxiv.org/pdf/2309.06284.pdf' target='_blank'>https://arxiv.org/pdf/2309.06284.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yin Wang, Zhiying Leng, Frederick W. B. Li, Shun-Cheng Wu, Xiaohui Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.06284">Fg-T2M: Fine-Grained Text-Driven Human Motion Generation via Diffusion Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-driven human motion generation in computer vision is both significant and challenging. However, current methods are limited to producing either deterministic or imprecise motion sequences, failing to effectively control the temporal and spatial relationships required to conform to a given text description. In this work, we propose a fine-grained method for generating high-quality, conditional human motion sequences supporting precise text description. Our approach consists of two key components: 1) a linguistics-structure assisted module that constructs accurate and complete language feature to fully utilize text information; and 2) a context-aware progressive reasoning module that learns neighborhood and overall semantic linguistics features from shallow and deep graph neural networks to achieve a multi-step inference. Experiments show that our approach outperforms text-driven motion generation methods on HumanML3D and KIT test sets and generates better visually confirmed motion to the text conditions.
<div id='section'>Paperid: <span id='pid'>901, <a href='https://arxiv.org/pdf/2309.05310.pdf' target='_blank'>https://arxiv.org/pdf/2309.05310.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yashuai Yan, Esteve Valls Mascaro, Dongheui Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.05310">ImitationNet: Unsupervised Human-to-Robot Motion Retargeting via Shared Latent Space</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces a novel deep-learning approach for human-to-robot motion retargeting, enabling robots to mimic human poses accurately. Contrary to prior deep-learning-based works, our method does not require paired human-to-robot data, which facilitates its translation to new robots. First, we construct a shared latent space between humans and robots via adaptive contrastive learning that takes advantage of a proposed cross-domain similarity metric between the human and robot poses. Additionally, we propose a consistency term to build a common latent space that captures the similarity of the poses with precision while allowing direct robot motion control from the latent space. For instance, we can generate in-between motion through simple linear interpolation between two projected human poses. We conduct a comprehensive evaluation of robot control from diverse modalities (i.e., texts, RGB videos, and key poses), which facilitates robot control for non-expert users. Our model outperforms existing works regarding human-to-robot retargeting in terms of efficiency and precision. Finally, we implemented our method in a real robot with self-collision avoidance through a whole-body controller to showcase the effectiveness of our approach. More information on our website https://evm7.github.io/UnsH2R/
<div id='section'>Paperid: <span id='pid'>902, <a href='https://arxiv.org/pdf/2309.00310.pdf' target='_blank'>https://arxiv.org/pdf/2309.00310.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shaohua Pan, Qi Ma, Xinyu Yi, Weifeng Hu, Xiong Wang, Xingkang Zhou, Jijunnan Li, Feng Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.00310">Fusing Monocular Images and Sparse IMU Signals for Real-time Human Motion Capture</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Either RGB images or inertial signals have been used for the task of motion capture (mocap), but combining them together is a new and interesting topic. We believe that the combination is complementary and able to solve the inherent difficulties of using one modality input, including occlusions, extreme lighting/texture, and out-of-view for visual mocap and global drifts for inertial mocap. To this end, we propose a method that fuses monocular images and sparse IMUs for real-time human motion capture. Our method contains a dual coordinate strategy to fully explore the IMU signals with different goals in motion capture. To be specific, besides one branch transforming the IMU signals to the camera coordinate system to combine with the image information, there is another branch to learn from the IMU signals in the body root coordinate system to better estimate body poses. Furthermore, a hidden state feedback mechanism is proposed for both two branches to compensate for their own drawbacks in extreme input cases. Thus our method can easily switch between the two kinds of signals or combine them in different cases to achieve a robust mocap. %The two divided parts can help each other for better mocap results under different conditions. Quantitative and qualitative results demonstrate that by delicately designing the fusion method, our technique significantly outperforms the state-of-the-art vision, IMU, and combined methods on both global orientation and local pose estimation. Our codes are available for research at https://shaohua-pan.github.io/robustcap-page/.
<div id='section'>Paperid: <span id='pid'>903, <a href='https://arxiv.org/pdf/2308.15153.pdf' target='_blank'>https://arxiv.org/pdf/2308.15153.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ali Hammoud, Valerio Belcamino, Alessandro Carfi, Veronique Perdereau, Fulvio Mastrogiovanni
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.15153">In-hand manipulation planning using human motion dictionary</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dexterous in-hand manipulation is a peculiar and useful human skill. This ability requires the coordination of many senses and hand motion to adhere to many constraints. These constraints vary and can be influenced by the object characteristics or the specific application. One of the key elements for a robotic platform to implement reliable inhand manipulation skills is to be able to integrate those constraints in their motion generations. These constraints can be implicitly modelled, learned through experience or human demonstrations. We propose a method based on motion primitives dictionaries to learn and reproduce in-hand manipulation skills. In particular, we focused on fingertip motions during the manipulation, and we defined an optimization process to combine motion primitives to reach specific fingertip configurations. The results of this work show that the proposed approach can generate manipulation motion coherent with the human one and that manipulation constraints are inherited even without an explicit formalization.
<div id='section'>Paperid: <span id='pid'>904, <a href='https://arxiv.org/pdf/2308.07301.pdf' target='_blank'>https://arxiv.org/pdf/2308.07301.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Esteve Valls Mascaro, Hyemin Ahn, Dongheui Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.07301">A Unified Masked Autoencoder with Patchified Skeletons for Motion Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The synthesis of human motion has traditionally been addressed through task-dependent models that focus on specific challenges, such as predicting future motions or filling in intermediate poses conditioned on known key-poses. In this paper, we present a novel task-independent model called UNIMASK-M, which can effectively address these challenges using a unified architecture. Our model obtains comparable or better performance than the state-of-the-art in each field. Inspired by Vision Transformers (ViTs), our UNIMASK-M model decomposes a human pose into body parts to leverage the spatio-temporal relationships existing in human motion. Moreover, we reformulate various pose-conditioned motion synthesis tasks as a reconstruction problem with different masking patterns given as input. By explicitly informing our model about the masked joints, our UNIMASK-M becomes more robust to occlusions. Experimental results show that our model successfully forecasts human motion on the Human3.6M dataset. Moreover, it achieves state-of-the-art results in motion inbetweening on the LaFAN1 dataset, particularly in long transition periods. More information can be found on the project website https://evm7.github.io/UNIMASKM-page/
<div id='section'>Paperid: <span id='pid'>905, <a href='https://arxiv.org/pdf/2308.03975.pdf' target='_blank'>https://arxiv.org/pdf/2308.03975.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiahang Zhang, Lilang Lin, Jiaying Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.03975">Prompted Contrast with Masked Motion Modeling: Towards Versatile 3D Action Representation Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Self-supervised learning has proved effective for skeleton-based human action understanding, which is an important yet challenging topic. Previous works mainly rely on contrastive learning or masked motion modeling paradigm to model the skeleton relations. However, the sequence-level and joint-level representation learning cannot be effectively and simultaneously handled by these methods. As a result, the learned representations fail to generalize to different downstream tasks. Moreover, combining these two paradigms in a naive manner leaves the synergy between them untapped and can lead to interference in training. To address these problems, we propose Prompted Contrast with Masked Motion Modeling, PCM$^{\rm 3}$, for versatile 3D action representation learning. Our method integrates the contrastive learning and masked prediction tasks in a mutually beneficial manner, which substantially boosts the generalization capacity for various downstream tasks. Specifically, masked prediction provides novel training views for contrastive learning, which in turn guides the masked prediction training with high-level semantic information. Moreover, we propose a dual-prompted multi-task pretraining strategy, which further improves model representations by reducing the interference caused by learning the two different pretext tasks. Extensive experiments on five downstream tasks under three large-scale datasets are conducted, demonstrating the superior generalization capacity of PCM$^{\rm 3}$ compared to the state-of-the-art works. Our project is publicly available at: https://jhang2020.github.io/Projects/PCM3/PCM3.html .
<div id='section'>Paperid: <span id='pid'>906, <a href='https://arxiv.org/pdf/2307.12729.pdf' target='_blank'>https://arxiv.org/pdf/2307.12729.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hung Tran, Vuong Le, Svetha Venkatesh, Truyen Tran
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.12729">Persistent-Transient Duality: A Multi-mechanism Approach for Modeling Human-Object Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humans are highly adaptable, swiftly switching between different modes to progressively handle different tasks, situations and contexts. In Human-object interaction (HOI) activities, these modes can be attributed to two mechanisms: (1) the large-scale consistent plan for the whole activity and (2) the small-scale children interactive actions that start and end along the timeline. While neuroscience and cognitive science have confirmed this multi-mechanism nature of human behavior, machine modeling approaches for human motion are trailing behind. While attempted to use gradually morphing structures (e.g., graph attention networks) to model the dynamic HOI patterns, they miss the expeditious and discrete mode-switching nature of the human motion. To bridge that gap, this work proposes to model two concurrent mechanisms that jointly control human motion: the Persistent process that runs continually on the global scale, and the Transient sub-processes that operate intermittently on the local context of the human while interacting with objects. These two mechanisms form an interactive Persistent-Transient Duality that synergistically governs the activity sequences. We model this conceptual duality by a parent-child neural network of Persistent and Transient channels with a dedicated neural module for dynamic mechanism switching. The framework is trialed on HOI motion forecasting. On two rich datasets and a wide variety of settings, the model consistently delivers superior performances, proving its suitability for the challenge.
<div id='section'>Paperid: <span id='pid'>907, <a href='https://arxiv.org/pdf/2306.12279.pdf' target='_blank'>https://arxiv.org/pdf/2306.12279.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Raj Desai, Marko CvetkoviÄ, Junda Wu, Georgios Papaioannou, Riender Happee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.12279">Computationally efficient human body modelling for real time motion comfort assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Due to the complexity of the human body and its neuromuscular stabilization, it has been challenging to efficiently and accurately predict human motion and capture posture while being driven. Existing simple models of the seated human body are mostly two-dimensional and developed in the mid-sagittal plane ex-posed to in-plane excitation. Such models capture fore-aft and vertical motion but not the more complex 3D motions due to lateral loading. Advanced 3D full-body active human models (AHMs), such as in MADYMO, can be used for comfort analysis and to investigate how vibrations influence the human body while being driven. However, such AHMs are very time-consuming due to their complexity. To effectively analyze motion comfort, a computationally efficient and accurate three dimensional (3D) human model, which runs faster than real-time, is presented. The model's postural stabilization parameters are tuned using available 3D vibration data for head, trunk and pelvis translation and rotation. A comparison between AHM and EHM is conducted regarding human body kinematics. According to the results, the EHM model configuration with two neck joints, two torso bending joints, and a spinal compression joint accurately predicts body kinematics.
<div id='section'>Paperid: <span id='pid'>908, <a href='https://arxiv.org/pdf/2306.03877.pdf' target='_blank'>https://arxiv.org/pdf/2306.03877.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Violetta Rostobaya, Yue Guan, James Berneburg, Michael Dorothy, Daigo Shishika
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.03877">The Eater and the Mover Game</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper studies the idea of ``deception by motion'' through a two-player dynamic game played between a Mover who must retrieve resources at a goal location, and an Eater who can consume resources at two candidate goals. The Mover seeks to minimize the resource consumption at the true goal, and the Eater tries to maximize it. While the Mover has the knowledge about the true goal, the Eater cannot differentiate between the two candidates. Unlike existing works on deceptive motion control that measures the deceptiveness through the quality of inference made by a distant observer (an estimator), we incorporate their actions to directly measure the efficacy of deception through the outcome of the game. An equilibrium concept is then proposed without the notion of an estimator. We further identify a pair of equilibrium strategies and demonstrate that if the Eater optimizes for the worst-case scenario, hiding the intention (deception by ambiguity) is still effective, whereas trying to fake the true goal (deception by exaggeration) is not.
<div id='section'>Paperid: <span id='pid'>909, <a href='https://arxiv.org/pdf/2306.02585.pdf' target='_blank'>https://arxiv.org/pdf/2306.02585.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Changcheng Xiao, Qiong Cao, Yujie Zhong, Long Lan, Xiang Zhang, Zhigang Luo, Dacheng Tao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.02585">MotionTrack: Learning Motion Predictor for Multiple Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Significant progress has been achieved in multi-object tracking (MOT) through the evolution of detection and re-identification (ReID) techniques. Despite these advancements, accurately tracking objects in scenarios with homogeneous appearance and heterogeneous motion remains a challenge. This challenge arises from two main factors: the insufficient discriminability of ReID features and the predominant utilization of linear motion models in MOT. In this context, we introduce a novel motion-based tracker, MotionTrack, centered around a learnable motion predictor that relies solely on object trajectory information. This predictor comprehensively integrates two levels of granularity in motion features to enhance the modeling of temporal dynamics and facilitate precise future motion prediction for individual objects. Specifically, the proposed approach adopts a self-attention mechanism to capture token-level information and a Dynamic MLP layer to model channel-level features. MotionTrack is a simple, online tracking approach. Our experimental results demonstrate that MotionTrack yields state-of-the-art performance on datasets such as Dancetrack and SportsMOT, characterized by highly complex object motion.
<div id='section'>Paperid: <span id='pid'>910, <a href='https://arxiv.org/pdf/2304.03312.pdf' target='_blank'>https://arxiv.org/pdf/2304.03312.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rodrigo A. GonzÃ¡lez, Koen Tiels, Tom Oomen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.03312">Identifying Lebesgue-sampled Continuous-time Impulse Response Models: A Kernel-based Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Control applications are increasingly sampled non-equidistantly in time, including in motion control, networked control, resource-aware control, and event-triggered control. Some of these applications use measurement devices that sample equidistantly in the amplitude domain. The aim of this paper is to develop a non-parametric estimator of the impulse response of continuous-time systems based on such sampling strategy, known as Lebesgue-sampling. To this end, kernel methods are developed to formulate an algorithm that adequately takes into account the output intersample behavior, which ultimately leads to more accurate models and more efficient output sampling compared to the standard approach. The efficacy of this method is demonstrated through a mass-spring damper case study.
<div id='section'>Paperid: <span id='pid'>911, <a href='https://arxiv.org/pdf/2304.01199.pdf' target='_blank'>https://arxiv.org/pdf/2304.01199.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jathushan Rajasegaran, Georgios Pavlakos, Angjoo Kanazawa, Christoph Feichtenhofer, Jitendra Malik
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.01199">On the Benefits of 3D Pose and Tracking for Human Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work we study the benefits of using tracking and 3D poses for action recognition. To achieve this, we take the Lagrangian view on analysing actions over a trajectory of human motion rather than at a fixed point in space. Taking this stand allows us to use the tracklets of people to predict their actions. In this spirit, first we show the benefits of using 3D pose to infer actions, and study person-person interactions. Subsequently, we propose a Lagrangian Action Recognition model by fusing 3D pose and contextualized appearance over tracklets. To this end, our method achieves state-of-the-art performance on the AVA v2.2 dataset on both pose only settings and on standard benchmark settings. When reasoning about the action using only pose cues, our pose model achieves +10.0 mAP gain over the corresponding state-of-the-art while our fused model has a gain of +2.8 mAP over the best state-of-the-art model. Code and results are available at: https://brjathu.github.io/LART
<div id='section'>Paperid: <span id='pid'>912, <a href='https://arxiv.org/pdf/2303.16479.pdf' target='_blank'>https://arxiv.org/pdf/2303.16479.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xianghui Xie, Bharat Lal Bhatnagar, Gerard Pons-Moll
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.16479">Visibility Aware Human-Object Interaction Tracking from Single RGB Camera</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Capturing the interactions between humans and their environment in 3D is important for many applications in robotics, graphics, and vision. Recent works to reconstruct the 3D human and object from a single RGB image do not have consistent relative translation across frames because they assume a fixed depth. Moreover, their performance drops significantly when the object is occluded. In this work, we propose a novel method to track the 3D human, object, contacts between them, and their relative translation across frames from a single RGB camera, while being robust to heavy occlusions. Our method is built on two key insights. First, we condition our neural field reconstructions for human and object on per-frame SMPL model estimates obtained by pre-fitting SMPL to a video sequence. This improves neural reconstruction accuracy and produces coherent relative translation across frames. Second, human and object motion from visible frames provides valuable information to infer the occluded object. We propose a novel transformer-based neural network that explicitly uses object visibility and human motion to leverage neighbouring frames to make predictions for the occluded frames. Building on these insights, our method is able to track both human and object robustly even under occlusions. Experiments on two datasets show that our method significantly improves over the state-of-the-art methods. Our code and pretrained models are available at: https://virtualhumans.mpi-inf.mpg.de/VisTracker
<div id='section'>Paperid: <span id='pid'>913, <a href='https://arxiv.org/pdf/2303.10532.pdf' target='_blank'>https://arxiv.org/pdf/2303.10532.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>James F. O'Brien, Robert E. Bodenheimer, Gabriel J. Brostow, Jessica K. Hodgins
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.10532">Automatic Joint Parameter Estimation from Magnetic Motion Capture Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper describes a technique for using magnetic motion capture data to determine the joint parameters of an articulated hierarchy. This technique makes it possible to determine limb lengths, joint locations, and sensor placement for a human subject without external measurements. Instead, the joint parameters are inferred with high accuracy from the motion data acquired during the capture session. The parameters are computed by performing a linear least squares fit of a rotary joint model to the input data. A hierarchical structure for the articulated model can also be determined in situations where the topology of the model is not known. Once the system topology and joint parameters have been recovered, the resulting model can be used to perform forward and inverse kinematic procedures. We present the results of using the algorithm on human motion capture data, as well as validation results obtained with data from a simulation and a wooden linkage of known dimensions.
<div id='section'>Paperid: <span id='pid'>914, <a href='https://arxiv.org/pdf/2303.06045.pdf' target='_blank'>https://arxiv.org/pdf/2303.06045.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rodrigo A. GonzÃ¡lez, Koen Tiels, Tom Oomen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.06045">Kernel-based identification using Lebesgue-sampled data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Sampling in control applications is increasingly done non-equidistantly in time. This includes applications in motion control, networked control, resource-aware control, and event-based control. Some of these applications, like the ones where displacement is tracked using incremental encoders, are driven by signals that are only measured when their values cross fixed thresholds in the amplitude domain. This paper introduces a non-parametric estimator of the impulse response and transfer function of continuous-time systems based on such amplitude-equidistant sampling strategy, known as Lebesgue sampling. To this end, kernel methods are developed to formulate an algorithm that adequately takes into account the bounded output uncertainty between the event timestamps, which ultimately leads to more accurate models and more efficient output sampling compared to the equidistantly-sampled kernel-based approach. The efficacy of our proposed method is demonstrated through a mass-spring damper example with encoder measurements and extensive Monte Carlo simulation studies on system benchmarks.
<div id='section'>Paperid: <span id='pid'>915, <a href='https://arxiv.org/pdf/2302.14503.pdf' target='_blank'>https://arxiv.org/pdf/2302.14503.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hyemin Ahn, Esteve Valls Mascaro, Dongheui Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.14503">Can We Use Diffusion Probabilistic Models for 3D Motion Prediction?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>After many researchers observed fruitfulness from the recent diffusion probabilistic model, its effectiveness in image generation is actively studied these days. In this paper, our objective is to evaluate the potential of diffusion probabilistic models for 3D human motion-related tasks. To this end, this paper presents a study of employing diffusion probabilistic models to predict future 3D human motion(s) from the previously observed motion. Based on the Human 3.6M and HumanEva-I datasets, our results show that diffusion probabilistic models are competitive for both single (deterministic) and multiple (stochastic) 3D motion prediction tasks, after finishing a single training process. In addition, we find out that diffusion probabilistic models can offer an attractive compromise, since they can strike the right balance between the likelihood and diversity of the predicted future motions. Our code is publicly available on the project website: https://sites.google.com/view/diffusion-motion-prediction.
<div id='section'>Paperid: <span id='pid'>916, <a href='https://arxiv.org/pdf/2302.12827.pdf' target='_blank'>https://arxiv.org/pdf/2302.12827.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vickie Ye, Georgios Pavlakos, Jitendra Malik, Angjoo Kanazawa
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.12827">Decoupling Human and Camera Motion from Videos in the Wild</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a method to reconstruct global human trajectories from videos in the wild. Our optimization method decouples the camera and human motion, which allows us to place people in the same world coordinate frame. Most existing methods do not model the camera motion; methods that rely on the background pixels to infer 3D human motion usually require a full scene reconstruction, which is often not possible for in-the-wild videos. However, even when existing SLAM systems cannot recover accurate scene reconstructions, the background pixel motion still provides enough signal to constrain the camera motion. We show that relative camera estimates along with data-driven human motion priors can resolve the scene scale ambiguity and recover global human trajectories. Our method robustly recovers the global 3D trajectories of people in challenging in-the-wild videos, such as PoseTrack. We quantify our improvement over existing methods on 3D human dataset Egobody. We further demonstrate that our recovered camera scale allows us to reason about motion of multiple people in a shared coordinate frame, which improves performance of downstream tracking in PoseTrack. Code and video results can be found at https://vye16.github.io/slahmr.
<div id='section'>Paperid: <span id='pid'>917, <a href='https://arxiv.org/pdf/2302.08274.pdf' target='_blank'>https://arxiv.org/pdf/2302.08274.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Esteve Valls Mascaro, Shuo Ma, Hyemin Ahn, Dongheui Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.08274">Robust Human Motion Forecasting using Transformer-based Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Comprehending human motion is a fundamental challenge for developing Human-Robot Collaborative applications. Computer vision researchers have addressed this field by only focusing on reducing error in predictions, but not taking into account the requirements to facilitate its implementation in robots. In this paper, we propose a new model based on Transformer that simultaneously deals with the real time 3D human motion forecasting in the short and long term. Our 2-Channel Transformer (2CH-TR) is able to efficiently exploit the spatio-temporal information of a shortly observed sequence (400ms) and generates a competitive accuracy against the current state-of-the-art. 2CH-TR stands out for the efficient performance of the Transformer, being lighter and faster than its competitors. In addition, our model is tested in conditions where the human motion is severely occluded, demonstrating its robustness in reconstructing and predicting 3D human motion in a highly noisy environment. Our experiment results show that the proposed 2CH-TR outperforms the ST-Transformer, which is another state-of-the-art model based on the Transformer, in terms of reconstruction and prediction under the same conditions of input prefix. Our model reduces in 8.89% the mean squared error of ST-Transformer in short-term prediction, and 2.57% in long-term prediction in Human3.6M dataset with 400ms input prefix. Webpage: https://evm7.github.io/2CHTR-page/
<div id='section'>Paperid: <span id='pid'>918, <a href='https://arxiv.org/pdf/2302.07489.pdf' target='_blank'>https://arxiv.org/pdf/2302.07489.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jessica K. Hodgins, James F. O'Brien, Jack Tumblin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.07489">Perception of Human Motion with Different Geometric Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human figures have been animated using a variety of geometric models including stick figures, polygonal models, and NURBS-based models with muscles, flexible skin, or clothing. This paper reports on experimental results indicating that a viewer's perception of motion characteristics is affected by the geometric model used for rendering. Subjects were shown a series of paired motion sequences and asked if the two motions in each pair were the same or different. The motion sequences in each pair were rendered using the same geometric model. For the three types of motion variation tested, sensitivity scores indicate that subjects were better able to observe changes with the polygonal model than they were with the stick figure model.
<div id='section'>Paperid: <span id='pid'>919, <a href='https://arxiv.org/pdf/2301.04421.pdf' target='_blank'>https://arxiv.org/pdf/2301.04421.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenbo Shao, Yanchao Xu, Liang Peng, Jun Li, Hong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.04421">Failure Detection for Motion Prediction of Autonomous Driving: An Uncertainty Perspective</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion prediction is essential for safe and efficient autonomous driving. However, the inexplicability and uncertainty of complex artificial intelligence models may lead to unpredictable failures of the motion prediction module, which may mislead the system to make unsafe decisions. Therefore, it is necessary to develop methods to guarantee reliable autonomous driving, where failure detection is a potential direction. Uncertainty estimates can be used to quantify the degree of confidence a model has in its predictions and may be valuable for failure detection. We propose a framework of failure detection for motion prediction from the uncertainty perspective, considering both motion uncertainty and model uncertainty, and formulate various uncertainty scores according to different prediction stages. The proposed approach is evaluated based on different motion prediction algorithms, uncertainty estimation methods, uncertainty scores, etc., and the results show that uncertainty is promising for failure detection for motion prediction but should be used with caution.
<div id='section'>Paperid: <span id='pid'>920, <a href='https://arxiv.org/pdf/2212.04420.pdf' target='_blank'>https://arxiv.org/pdf/2212.04420.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongwei Yi, Hualin Liang, Yifei Liu, Qiong Cao, Yandong Wen, Timo Bolkart, Dacheng Tao, Michael J. Black
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.04420">Generating Holistic 3D Human Motion from Speech</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work addresses the problem of generating 3D holistic body motions from human speech. Given a speech recording, we synthesize sequences of 3D body poses, hand gestures, and facial expressions that are realistic and diverse. To achieve this, we first build a high-quality dataset of 3D holistic body meshes with synchronous speech. We then define a novel speech-to-motion generation framework in which the face, body, and hands are modeled separately. The separated modeling stems from the fact that face articulation strongly correlates with human speech, while body poses and hand gestures are less correlated. Specifically, we employ an autoencoder for face motions, and a compositional vector-quantized variational autoencoder (VQ-VAE) for the body and hand motions. The compositional VQ-VAE is key to generating diverse results. Additionally, we propose a cross-conditional autoregressive model that generates body poses and hand gestures, leading to coherent and realistic motions. Extensive experiments and user studies demonstrate that our proposed approach achieves state-of-the-art performance both qualitatively and quantitatively. Our novel dataset and code will be released for research purposes at https://talkshow.is.tue.mpg.de.
<div id='section'>Paperid: <span id='pid'>921, <a href='https://arxiv.org/pdf/2211.14304.pdf' target='_blank'>https://arxiv.org/pdf/2211.14304.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>German Barquero, Sergio Escalera, Cristina Palmero
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.14304">BeLFusion: Latent Diffusion for Behavior-Driven Human Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Stochastic human motion prediction (HMP) has generally been tackled with generative adversarial networks and variational autoencoders. Most prior works aim at predicting highly diverse movements in terms of the skeleton joints' dispersion. This has led to methods predicting fast and motion-divergent movements, which are often unrealistic and incoherent with past motion. Such methods also neglect contexts that need to anticipate diverse low-range behaviors, or actions, with subtle joint displacements. To address these issues, we present BeLFusion, a model that, for the first time, leverages latent diffusion models in HMP to sample from a latent space where behavior is disentangled from pose and motion. As a result, diversity is encouraged from a behavioral perspective. Thanks to our behavior coupler's ability to transfer sampled behavior to ongoing motion, BeLFusion's predictions display a variety of behaviors that are significantly more realistic than the state of the art. To support it, we introduce two metrics, the Area of the Cumulative Motion Distribution, and the Average Pairwise Distance Error, which are correlated to our definition of realism according to a qualitative study with 126 participants. Finally, we prove BeLFusion's generalization power in a new cross-dataset scenario for stochastic HMP.
<div id='section'>Paperid: <span id='pid'>922, <a href='https://arxiv.org/pdf/2509.24469.pdf' target='_blank'>https://arxiv.org/pdf/2509.24469.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Heechang Kim, Gwanghyun Kim, Se Young Chun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24469">LaMoGen: Laban Movement-Guided Diffusion for Text-to-Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diverse human motion generation is an increasingly important task, having various applications in computer vision, human-computer interaction and animation. While text-to-motion synthesis using diffusion models has shown success in generating high-quality motions, achieving fine-grained expressive motion control remains a significant challenge. This is due to the lack of motion style diversity in datasets and the difficulty of expressing quantitative characteristics in natural language. Laban movement analysis has been widely used by dance experts to express the details of motion including motion quality as consistent as possible. Inspired by that, this work aims for interpretable and expressive control of human motion generation by seamlessly integrating the quantification methods of Laban Effort and Shape components into the text-guided motion generation models. Our proposed zero-shot, inference-time optimization method guides the motion generation model to have desired Laban Effort and Shape components without any additional motion data by updating the text embedding of pretrained diffusion models during the sampling step. We demonstrate that our approach yields diverse expressive motion qualities while preserving motion identity by successfully manipulating motion attributes according to target Laban tags.
<div id='section'>Paperid: <span id='pid'>923, <a href='https://arxiv.org/pdf/2509.21888.pdf' target='_blank'>https://arxiv.org/pdf/2509.21888.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Minjun Kang, Inkyu Shin, Taeyeop Lee, In So Kweon, Kuk-Jin Yoon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21888">Drag4D: Align Your Motion with Text-Driven 3D Scene Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce Drag4D, an interactive framework that integrates object motion control within text-driven 3D scene generation. This framework enables users to define 3D trajectories for the 3D objects generated from a single image, seamlessly integrating them into a high-quality 3D background. Our Drag4D pipeline consists of three stages. First, we enhance text-to-3D background generation by applying 2D Gaussian Splatting with panoramic images and inpainted novel views, resulting in dense and visually complete 3D reconstructions. In the second stage, given a reference image of the target object, we introduce a 3D copy-and-paste approach: the target instance is extracted in a full 3D mesh using an off-the-shelf image-to-3D model and seamlessly composited into the generated 3D scene. The object mesh is then positioned within the 3D scene via our physics-aware object position learning, ensuring precise spatial alignment. Lastly, the spatially aligned object is temporally animated along a user-defined 3D trajectory. To mitigate motion hallucination and ensure view-consistent temporal alignment, we develop a part-augmented, motion-conditioned video diffusion model that processes multiview image pairs together with their projected 2D trajectories. We demonstrate the effectiveness of our unified architecture through evaluations at each stage and in the final results, showcasing the harmonized alignment of user-controlled object motion within a high-quality 3D background.
<div id='section'>Paperid: <span id='pid'>924, <a href='https://arxiv.org/pdf/2509.21006.pdf' target='_blank'>https://arxiv.org/pdf/2509.21006.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Konstantin Gubernatorov, Artem Voronov, Roman Voronov, Sergei Pasynkov, Stepan Perminov, Ziang Guo, Dzmitry Tsetserukou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21006">AnywhereVLA: Language-Conditioned Exploration and Mobile Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We address natural language pick-and-place in unseen, unpredictable indoor environments with AnywhereVLA, a modular framework for mobile manipulation. A user text prompt serves as an entry point and is parsed into a structured task graph that conditions classical SLAM with LiDAR and cameras, metric semantic mapping, and a task-aware frontier exploration policy. An approach planner then selects visibility and reachability aware pre grasp base poses. For interaction, a compact SmolVLA manipulation head is fine tuned on platform pick and place trajectories for the SO-101 by TheRobotStudio, grounding local visual context and sub-goals into grasp and place proposals. The full system runs fully onboard on consumer-level hardware, with Jetson Orin NX for perception and VLA and an Intel NUC for SLAM, exploration, and control, sustaining real-time operation. We evaluated AnywhereVLA in a multi-room lab under static scenes and normal human motion. In this setting, the system achieves a $46\%$ overall task success rate while maintaining throughput on embedded compute. By combining a classical stack with a fine-tuned VLA manipulation, the system inherits the reliability of geometry-based navigation with the agility and task generalization of language-conditioned manipulation.
<div id='section'>Paperid: <span id='pid'>925, <a href='https://arxiv.org/pdf/2509.13116.pdf' target='_blank'>https://arxiv.org/pdf/2509.13116.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruibo Li, Hanyu Shi, Zhe Wang, Guosheng Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.13116">Weakly and Self-Supervised Class-Agnostic Motion Prediction for Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding motion in dynamic environments is critical for autonomous driving, thereby motivating research on class-agnostic motion prediction. In this work, we investigate weakly and self-supervised class-agnostic motion prediction from LiDAR point clouds. Outdoor scenes typically consist of mobile foregrounds and static backgrounds, allowing motion understanding to be associated with scene parsing. Based on this observation, we propose a novel weakly supervised paradigm that replaces motion annotations with fully or partially annotated (1%, 0.1%) foreground/background masks for supervision. To this end, we develop a weakly supervised approach utilizing foreground/background cues to guide the self-supervised learning of motion prediction models. Since foreground motion generally occurs in non-ground regions, non-ground/ground masks can serve as an alternative to foreground/background masks, further reducing annotation effort. Leveraging non-ground/ground cues, we propose two additional approaches: a weakly supervised method requiring fewer (0.01%) foreground/background annotations, and a self-supervised method without annotations. Furthermore, we design a Robust Consistency-aware Chamfer Distance loss that incorporates multi-frame information and robust penalty functions to suppress outliers in self-supervised learning. Experiments show that our weakly and self-supervised models outperform existing self-supervised counterparts, and our weakly supervised models even rival some supervised ones. This demonstrates that our approaches effectively balance annotation effort and performance.
<div id='section'>Paperid: <span id='pid'>926, <a href='https://arxiv.org/pdf/2509.12880.pdf' target='_blank'>https://arxiv.org/pdf/2509.12880.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anna Deichler, Siyang Wang, Simon Alexanderson, Jonas Beskow
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.12880">Towards Context-Aware Human-like Pointing Gestures with RL Motion Imitation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pointing is a key mode of interaction with robots, yet most prior work has focused on recognition rather than generation. We present a motion capture dataset of human pointing gestures covering diverse styles, handedness, and spatial targets. Using reinforcement learning with motion imitation, we train policies that reproduce human-like pointing while maximizing precision. Results show our approach enables context-aware pointing behaviors in simulation, balancing task performance with natural dynamics.
<div id='section'>Paperid: <span id='pid'>927, <a href='https://arxiv.org/pdf/2509.12430.pdf' target='_blank'>https://arxiv.org/pdf/2509.12430.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mayank Patel, Rahul Jain, Asim Unmesh, Karthik Ramani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.12430">DYNAMO: Dependency-Aware Deep Learning Framework for Articulated Assembly Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding the motion of articulated mechanical assemblies from static geometry remains a core challenge in 3D perception and design automation. Prior work on everyday articulated objects such as doors and laptops typically assumes simplified kinematic structures or relies on joint annotations. However, in mechanical assemblies like gears, motion arises from geometric coupling, through meshing teeth or aligned axes, making it difficult for existing methods to reason about relational motion from geometry alone. To address this gap, we introduce MechBench, a benchmark dataset of 693 diverse synthetic gear assemblies with part-wise ground-truth motion trajectories. MechBench provides a structured setting to study coupled motion, where part dynamics are induced by contact and transmission rather than predefined joints. Building on this, we propose DYNAMO, a dependency-aware neural model that predicts per-part SE(3) motion trajectories directly from segmented CAD point clouds. Experiments show that DYNAMO outperforms strong baselines, achieving accurate and temporally consistent predictions across varied gear configurations. Together, MechBench and DYNAMO establish a novel systematic framework for data-driven learning of coupled mechanical motion in CAD assemblies.
<div id='section'>Paperid: <span id='pid'>928, <a href='https://arxiv.org/pdf/2509.10096.pdf' target='_blank'>https://arxiv.org/pdf/2509.10096.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Saeed Saadatnejad, Reyhaneh Hosseininejad, Jose Barreiros, Katherine M. Tsui, Alexandre Alahi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.10096">HHI-Assist: A Dataset and Benchmark of Human-Human Interaction in Physical Assistance Scenario</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The increasing labor shortage and aging population underline the need for assistive robots to support human care recipients. To enable safe and responsive assistance, robots require accurate human motion prediction in physical interaction scenarios. However, this remains a challenging task due to the variability of assistive settings and the complexity of coupled dynamics in physical interactions. In this work, we address these challenges through two key contributions: (1) HHI-Assist, a dataset comprising motion capture clips of human-human interactions in assistive tasks; and (2) a conditional Transformer-based denoising diffusion model for predicting the poses of interacting agents. Our model effectively captures the coupled dynamics between caregivers and care receivers, demonstrating improvements over baselines and strong generalization to unseen scenarios. By advancing interaction-aware motion prediction and introducing a new dataset, our work has the potential to significantly enhance robotic assistance policies. The dataset and code are available at: https://sites.google.com/view/hhi-assist/home
<div id='section'>Paperid: <span id='pid'>929, <a href='https://arxiv.org/pdf/2509.00339.pdf' target='_blank'>https://arxiv.org/pdf/2509.00339.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Md. Taherul Islam Shawon, Yuan Li, Yincai Cai, Junjie Niu, Ting Peng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.00339">Autonomous Aggregate Sorting in Construction and Mining via Computer Vision-Aided Robotic Arm Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Traditional aggregate sorting methods, whether manual or mechanical, often suffer from low precision, limited flexibility, and poor adaptability to diverse material properties such as size, shape, and lithology. To address these limitations, this study presents a computer vision-aided robotic arm system designed for autonomous aggregate sorting in construction and mining applications. The system integrates a six-degree-of-freedom robotic arm, a binocular stereo camera for 3D perception, and a ROS-based control framework. Core techniques include an attention-augmented YOLOv8 model for aggregate detection, stereo matching for 3D localization, Denavit-Hartenberg kinematic modeling for arm motion control, minimum enclosing rectangle analysis for size estimation, and hand-eye calibration for precise coordinate alignment. Experimental validation with four aggregate types achieved an average grasping and sorting success rate of 97.5%, with comparable classification accuracy. Remaining challenges include the reliable handling of small aggregates and texture-based misclassification. Overall, the proposed system demonstrates significant potential to enhance productivity, reduce operational costs, and improve safety in aggregate handling, while providing a scalable framework for advancing smart automation in construction, mining, and recycling industries.
<div id='section'>Paperid: <span id='pid'>930, <a href='https://arxiv.org/pdf/2508.10567.pdf' target='_blank'>https://arxiv.org/pdf/2508.10567.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Philipp Wolters, Johannes Gilg, Torben Teepe, Gerhard Rigoll
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10567">SpaRC-AD: A Baseline for Radar-Camera Fusion in End-to-End Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>End-to-end autonomous driving systems promise stronger performance through unified optimization of perception, motion forecasting, and planning. However, vision-based approaches face fundamental limitations in adverse weather conditions, partial occlusions, and precise velocity estimation - critical challenges in safety-sensitive scenarios where accurate motion understanding and long-horizon trajectory prediction are essential for collision avoidance. To address these limitations, we propose SpaRC-AD, a query-based end-to-end camera-radar fusion framework for planning-oriented autonomous driving. Through sparse 3D feature alignment, and doppler-based velocity estimation, we achieve strong 3D scene representations for refinement of agent anchors, map polylines and motion modelling. Our method achieves strong improvements over the state-of-the-art vision-only baselines across multiple autonomous driving tasks, including 3D detection (+4.8% mAP), multi-object tracking (+8.3% AMOTA), online mapping (+1.8% mAP), motion prediction (-4.0% mADE), and trajectory planning (-0.1m L2 and -9% TPC). We achieve both spatial coherence and temporal consistency on multiple challenging benchmarks, including real-world open-loop nuScenes, long-horizon T-nuScenes, and closed-loop simulator Bench2Drive. We show the effectiveness of radar-based fusion in safety-critical scenarios where accurate motion understanding and long-horizon trajectory prediction are essential for collision avoidance. The source code of all experiments is available at https://phi-wol.github.io/sparcad/
<div id='section'>Paperid: <span id='pid'>931, <a href='https://arxiv.org/pdf/2508.08179.pdf' target='_blank'>https://arxiv.org/pdf/2508.08179.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sihan Zhao, Zixuan Wang, Tianyu Luan, Jia Jia, Wentao Zhu, Jiebo Luo, Junsong Yuan, Nan Xi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08179">PP-Motion: Physical-Perceptual Fidelity Evaluation for Human Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion generation has found widespread applications in AR/VR, film, sports, and medical rehabilitation, offering a cost-effective alternative to traditional motion capture systems. However, evaluating the fidelity of such generated motions is a crucial, multifaceted task. Although previous approaches have attempted at motion fidelity evaluation using human perception or physical constraints, there remains an inherent gap between human-perceived fidelity and physical feasibility. Moreover, the subjective and coarse binary labeling of human perception further undermines the development of a robust data-driven metric. We address these issues by introducing a physical labeling method. This method evaluates motion fidelity by calculating the minimum modifications needed for a motion to align with physical laws. With this approach, we are able to produce fine-grained, continuous physical alignment annotations that serve as objective ground truth. With these annotations, we propose PP-Motion, a novel data-driven metric to evaluate both physical and perceptual fidelity of human motion. To effectively capture underlying physical priors, we employ Pearson's correlation loss for the training of our metric. Additionally, by incorporating a human-based perceptual fidelity loss, our metric can capture fidelity that simultaneously considers both human perception and physical alignment. Experimental results demonstrate that our metric, PP-Motion, not only aligns with physical laws but also aligns better with human perception of motion fidelity than previous work.
<div id='section'>Paperid: <span id='pid'>932, <a href='https://arxiv.org/pdf/2508.02264.pdf' target='_blank'>https://arxiv.org/pdf/2508.02264.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Markus Buchholz, Ignacio Carlucho, Michele Grimaldi, Yvan R. Petillot
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02264">Tethered Multi-Robot Systems in Marine Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces a novel simulation framework for evaluating motion control in tethered multi-robot systems within dynamic marine environments. Specifically, it focuses on the coordinated operation of an Autonomous Underwater Vehicle (AUV) and an Autonomous Surface Vehicle(ASV). The framework leverages GazeboSim, enhanced with realistic marine environment plugins and ArduPilots SoftwareIn-The-Loop (SITL) mode, to provide a high-fidelity simulation platform. A detailed tether model, combining catenary equations and physical simulation, is integrated to accurately represent the dynamic interactions between the vehicles and the environment. This setup facilitates the development and testing of advanced control strategies under realistic conditions, demonstrating the frameworks capability to analyze complex tether interactions and their impact on system performance.
<div id='section'>Paperid: <span id='pid'>933, <a href='https://arxiv.org/pdf/2508.01730.pdf' target='_blank'>https://arxiv.org/pdf/2508.01730.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianbo Ma, Hui Luo, Qi Chen, Yuankai Qi, Yumei Sun, Amin Beheshti, Jianlin Zhang, Ming-Hsuan Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01730">Tracking the Unstable: Appearance-Guided Motion Modeling for Robust Multi-Object Tracking in UAV-Captured Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) aims to track multiple objects while maintaining consistent identities across frames of a given video. In unmanned aerial vehicle (UAV) recorded videos, frequent viewpoint changes and complex UAV-ground relative motion dynamics pose significant challenges, which often lead to unstable affinity measurement and ambiguous association. Existing methods typically model motion and appearance cues separately, overlooking their spatio-temporal interplay and resulting in suboptimal tracking performance. In this work, we propose AMOT, which jointly exploits appearance and motion cues through two key components: an Appearance-Motion Consistency (AMC) matrix and a Motion-aware Track Continuation (MTC) module. Specifically, the AMC matrix computes bi-directional spatial consistency under the guidance of appearance features, enabling more reliable and context-aware identity association. The MTC module complements AMC by reactivating unmatched tracks through appearance-guided predictions that align with Kalman-based predictions, thereby reducing broken trajectories caused by missed detections. Extensive experiments on three UAV benchmarks, including VisDrone2019, UAVDT, and VT-MOT-UAV, demonstrate that our AMOT outperforms current state-of-the-art methods and generalizes well in a plug-and-play and training-free manner.
<div id='section'>Paperid: <span id='pid'>934, <a href='https://arxiv.org/pdf/2508.00917.pdf' target='_blank'>https://arxiv.org/pdf/2508.00917.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiayuan Wang, Farhad Pourpanah, Q. M. Jonathan Wu, Ning Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.00917">A Survey on Deep Multi-Task Learning in Connected Autonomous Vehicles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Connected autonomous vehicles (CAVs) must simultaneously perform multiple tasks, such as object detection, semantic segmentation, depth estimation, trajectory prediction, motion prediction, and behaviour prediction, to ensure safe and reliable navigation in complex environments. Vehicle-to-everything (V2X) communication enables cooperative driving among CAVs, thereby mitigating the limitations of individual sensors, reducing occlusions, and improving perception over long distances. Traditionally, these tasks are addressed using distinct models, which leads to high deployment costs, increased computational overhead, and challenges in achieving real-time performance. Multi-task learning (MTL) has recently emerged as a promising solution that enables the joint learning of multiple tasks within a single unified model. This offers improved efficiency and resource utilization. To the best of our knowledge, this survey is the first comprehensive review focused on MTL in the context of CAVs. We begin with an overview of CAVs and MTL to provide foundational background. We then explore the application of MTL across key functional modules, including perception, prediction, planning, control, and multi-agent collaboration. Finally, we discuss the strengths and limitations of existing methods, identify key research gaps, and provide directions for future research aimed at advancing MTL methodologies for CAV systems.
<div id='section'>Paperid: <span id='pid'>935, <a href='https://arxiv.org/pdf/2507.07356.pdf' target='_blank'>https://arxiv.org/pdf/2507.07356.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kangning Yin, Weishuai Zeng, Ke Fan, Minyue Dai, Zirui Wang, Qiang Zhang, Zheng Tian, Jingbo Wang, Jiangmiao Pang, Weinan Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07356">UniTracker: Learning Universal Whole-Body Motion Tracker for Humanoid Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Achieving expressive and generalizable whole-body motion control is essential for deploying humanoid robots in real-world environments. In this work, we propose UniTracker, a three-stage training framework that enables robust and scalable motion tracking across a wide range of human behaviors. In the first stage, we train a teacher policy with privileged observations to generate high-quality actions. In the second stage, we introduce a Conditional Variational Autoencoder (CVAE) to model a universal student policy that can be deployed directly on real hardware. The CVAE structure allows the policy to learn a global latent representation of motion, enhancing generalization to unseen behaviors and addressing the limitations of standard MLP-based policies under partial observations. Unlike pure MLPs that suffer from drift in global attributes like orientation, our CVAE-student policy incorporates global intent during training by aligning a partial-observation prior to the full-observation encoder. In the third stage, we introduce a fast adaptation module that fine-tunes the universal policy on harder motion sequences that are difficult to track directly. This adaptation can be performed both for single sequences and in batch mode, further showcasing the flexibility and scalability of our approach. We evaluate UniTracker in both simulation and real-world settings using a Unitree G1 humanoid, demonstrating strong performance in motion diversity, tracking accuracy, and deployment robustness.
<div id='section'>Paperid: <span id='pid'>936, <a href='https://arxiv.org/pdf/2507.05906.pdf' target='_blank'>https://arxiv.org/pdf/2507.05906.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenhao Li, Marco Hutter, Andreas Krause
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.05906">Feature-Based vs. GAN-Based Learning from Demonstrations: When and Why</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This survey provides a comparative analysis of feature-based and GAN-based approaches to learning from demonstrations, with a focus on the structure of reward functions and their implications for policy learning. Feature-based methods offer dense, interpretable rewards that excel at high-fidelity motion imitation, yet often require sophisticated representations of references and struggle with generalization in unstructured settings. GAN-based methods, in contrast, use implicit, distributional supervision that enables scalability and adaptation flexibility, but are prone to training instability and coarse reward signals. Recent advancements in both paradigms converge on the importance of structured motion representations, which enable smoother transitions, controllable synthesis, and improved task integration. We argue that the dichotomy between feature-based and GAN-based methods is increasingly nuanced: rather than one paradigm dominating the other, the choice should be guided by task-specific priorities such as fidelity, diversity, interpretability, and adaptability. This work outlines the algorithmic trade-offs and design considerations that underlie method selection, offering a framework for principled decision-making in learning from demonstrations.
<div id='section'>Paperid: <span id='pid'>937, <a href='https://arxiv.org/pdf/2507.05254.pdf' target='_blank'>https://arxiv.org/pdf/2507.05254.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fabian Konstantinidis, Ariel Dallari Guerreiro, Raphael Trumpp, Moritz Sackmann, Ulrich Hofmann, Marco Caccamo, Christoph Stiller
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.05254">From Marginal to Joint Predictions: Evaluating Scene-Consistent Trajectory Prediction Approaches for Automated Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate motion prediction of surrounding traffic participants is crucial for the safe and efficient operation of automated vehicles in dynamic environments. Marginal prediction models commonly forecast each agent's future trajectories independently, often leading to sub-optimal planning decisions for an automated vehicle. In contrast, joint prediction models explicitly account for the interactions between agents, yielding socially and physically consistent predictions on a scene level. However, existing approaches differ not only in their problem formulation but also in the model architectures and implementation details used, making it difficult to compare them. In this work, we systematically investigate different approaches to joint motion prediction, including post-processing of the marginal predictions, explicitly training the model for joint predictions, and framing the problem as a generative task. We evaluate each approach in terms of prediction accuracy, multi-modality, and inference efficiency, offering a comprehensive analysis of the strengths and limitations of each approach. Several prediction examples are available at https://frommarginaltojointpred.github.io/.
<div id='section'>Paperid: <span id='pid'>938, <a href='https://arxiv.org/pdf/2507.04522.pdf' target='_blank'>https://arxiv.org/pdf/2507.04522.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anna Deichler, Jim O'Regan, Teo Guichoux, David Johansson, Jonas Beskow
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.04522">Grounded Gesture Generation: Language, Motion, and Space</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion generation has advanced rapidly in recent years, yet the critical problem of creating spatially grounded, context-aware gestures has been largely overlooked. Existing models typically specialize either in descriptive motion generation, such as locomotion and object interaction, or in isolated co-speech gesture synthesis aligned with utterance semantics. However, both lines of work often treat motion and environmental grounding separately, limiting advances toward embodied, communicative agents. To address this gap, our work introduces a multimodal dataset and framework for grounded gesture generation, combining two key resources: (1) a synthetic dataset of spatially grounded referential gestures, and (2) MM-Conv, a VR-based dataset capturing two-party dialogues. Together, they provide over 7.7 hours of synchronized motion, speech, and 3D scene information, standardized in the HumanML3D format. Our framework further connects to a physics-based simulator, enabling synthetic data generation and situated evaluation. By bridging gesture modeling and spatial grounding, our contribution establishes a foundation for advancing research in situated gesture generation and grounded multimodal interaction.
  Project page: https://groundedgestures.github.io/
<div id='section'>Paperid: <span id='pid'>939, <a href='https://arxiv.org/pdf/2506.10574.pdf' target='_blank'>https://arxiv.org/pdf/2506.10574.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qing Wang, Xiaohang Yang, Yilan Dong, Naveen Raj Govindaraj, Gregory Slabaugh, Shanxin Yuan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.10574">DanceChat: Large Language Model-Guided Music-to-Dance Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Music-to-dance generation aims to synthesize human dance motion conditioned on musical input. Despite recent progress, significant challenges remain due to the semantic gap between music and dance motion, as music offers only abstract cues, such as melody, groove, and emotion, without explicitly specifying the physical movements. Moreover, a single piece of music can produce multiple plausible dance interpretations. This one-to-many mapping demands additional guidance, as music alone provides limited information for generating diverse dance movements. The challenge is further amplified by the scarcity of paired music and dance data, which restricts the modelÃ¢ÄÅ¹s ability to learn diverse dance patterns. In this paper, we introduce DanceChat, a Large Language Model (LLM)-guided music-to-dance generation approach. We use an LLM as a choreographer that provides textual motion instructions, offering explicit, high-level guidance for dance generation. This approach goes beyond implicit learning from music alone, enabling the model to generate dance that is both more diverse and better aligned with musical styles. Our approach consists of three components: (1) an LLM-based pseudo instruction generation module that produces textual dance guidance based on music style and structure, (2) a multi-modal feature extraction and fusion module that integrates music, rhythm, and textual guidance into a shared representation, and (3) a diffusion-based motion synthesis module together with a multi-modal alignment loss, which ensures that the generated dance is aligned with both musical and textual cues. Extensive experiments on AIST++ and human evaluations show that DanceChat outperforms state-of-the-art methods both qualitatively and quantitatively.
<div id='section'>Paperid: <span id='pid'>940, <a href='https://arxiv.org/pdf/2506.00173.pdf' target='_blank'>https://arxiv.org/pdf/2506.00173.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingyi Shi, Wei Liu, Jidong Mei, Wangpok Tse, Rui Chen, Xuelin Chen, Taku Komura
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.00173">MotionPersona: Characteristics-aware Locomotion Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present MotionPersona, a novel real-time character controller that allows users to characterize a character by specifying attributes such as physical traits, mental states, and demographics, and projects these properties into the generated motions for animating the character. In contrast to existing deep learning-based controllers, which typically produce homogeneous animations tailored to a single, predefined character, MotionPersona accounts for the impact of various traits on human motion as observed in the real world. To achieve this, we develop a block autoregressive motion diffusion model conditioned on SMPLX parameters, textual prompts, and user-defined locomotion control signals. We also curate a comprehensive dataset featuring a wide range of locomotion types and actor traits to enable the training of this characteristic-aware controller. Unlike prior work, MotionPersona is the first method capable of generating motion that faithfully reflects user-specified characteristics (e.g., an elderly person's shuffling gait) while responding in real time to dynamic control inputs. Additionally, we introduce a few-shot characterization technique as a complementary conditioning mechanism, enabling customization via short motion clips when language prompts fall short. Through extensive experiments, we demonstrate that MotionPersona outperforms existing methods in characteristics-aware locomotion control, achieving superior motion quality and diversity. Results, code, and demo can be found at: https://motionpersona25.github.io/.
<div id='section'>Paperid: <span id='pid'>941, <a href='https://arxiv.org/pdf/2506.00043.pdf' target='_blank'>https://arxiv.org/pdf/2506.00043.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jusheng Zhang, Jinzhou Tang, Sidi Liu, Mingyan Li, Sheng Zhang, Jian Wang, Keze Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.00043">From Motion to Behavior: Hierarchical Modeling of Humanoid Generative Behavior Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion generative modeling or synthesis aims to characterize complicated human motions of daily activities in diverse real-world environments. However, current research predominantly focuses on either low-level, short-period motions or high-level action planning, without taking into account the hierarchical goal-oriented nature of human activities. In this work, we take a step forward from human motion generation to human behavior modeling, which is inspired by cognitive science. We present a unified framework, dubbed Generative Behavior Control (GBC), to model diverse human motions driven by various high-level intentions by aligning motions with hierarchical behavior plans generated by large language models (LLMs). Our insight is that human motions can be jointly controlled by task and motion planning in robotics, but guided by LLMs to achieve improved motion diversity and physical fidelity. Meanwhile, to overcome the limitations of existing benchmarks, i.e., lack of behavioral plans, we propose GBC-100K dataset annotated with a hierarchical granularity of semantic and motion plans driven by target goals. Our experiments demonstrate that GBC can generate more diverse and purposeful high-quality human motions with 10* longer horizons compared with existing methods when trained on GBC-100K, laying a foundation for future research on behavioral modeling of human motions. Our dataset and source code will be made publicly available.
<div id='section'>Paperid: <span id='pid'>942, <a href='https://arxiv.org/pdf/2505.22974.pdf' target='_blank'>https://arxiv.org/pdf/2505.22974.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuntao Ma, Andrei Cramariuc, Farbod Farshidian, Marco Hutter
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.22974">Learning coordinated badminton skills for legged manipulators</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Coordinating the motion between lower and upper limbs and aligning limb control with perception are substantial challenges in robotics, particularly in dynamic environments. To this end, we introduce an approach for enabling legged mobile manipulators to play badminton, a task that requires precise coordination of perception, locomotion, and arm swinging. We propose a unified reinforcement learning-based control policy for whole-body visuomotor skills involving all degrees of freedom to achieve effective shuttlecock tracking and striking. This policy is informed by a perception noise model that utilizes real-world camera data, allowing for consistent perception error levels between simulation and deployment and encouraging learned active perception behaviors. Our method includes a shuttlecock prediction model, constrained reinforcement learning for robust motion control, and integrated system identification techniques to enhance deployment readiness. Extensive experimental results in a variety of environments validate the robot's capability to predict shuttlecock trajectories, navigate the service area effectively, and execute precise strikes against human players, demonstrating the feasibility of using legged mobile manipulators in complex and dynamic sports scenarios.
<div id='section'>Paperid: <span id='pid'>943, <a href='https://arxiv.org/pdf/2505.21864.pdf' target='_blank'>https://arxiv.org/pdf/2505.21864.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mengda Xu, Han Zhang, Yifan Hou, Zhenjia Xu, Linxi Fan, Manuela Veloso, Shuran Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.21864">DexUMI: Using Human Hand as the Universal Manipulation Interface for Dexterous Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present DexUMI - a data collection and policy learning framework that uses the human hand as the natural interface to transfer dexterous manipulation skills to various robot hands. DexUMI includes hardware and software adaptations to minimize the embodiment gap between the human hand and various robot hands. The hardware adaptation bridges the kinematics gap using a wearable hand exoskeleton. It allows direct haptic feedback in manipulation data collection and adapts human motion to feasible robot hand motion. The software adaptation bridges the visual gap by replacing the human hand in video data with high-fidelity robot hand inpainting. We demonstrate DexUMI's capabilities through comprehensive real-world experiments on two different dexterous robot hand hardware platforms, achieving an average task success rate of 86%.
<div id='section'>Paperid: <span id='pid'>944, <a href='https://arxiv.org/pdf/2505.21864.pdf' target='_blank'>https://arxiv.org/pdf/2505.21864.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mengda Xu, Han Zhang, Yifan Hou, Zhenjia Xu, Linxi Fan, Manuela Veloso, Shuran Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.21864">DexUMI: Using Human Hand as the Universal Manipulation Interface for Dexterous Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present DexUMI - a data collection and policy learning framework that uses the human hand as the natural interface to transfer dexterous manipulation skills to various robot hands. DexUMI includes hardware and software adaptations to minimize the embodiment gap between the human hand and various robot hands. The hardware adaptation bridges the kinematics gap using a wearable hand exoskeleton. It allows direct haptic feedback in manipulation data collection and adapts human motion to feasible robot hand motion. The software adaptation bridges the visual gap by replacing the human hand in video data with high-fidelity robot hand inpainting. We demonstrate DexUMI's capabilities through comprehensive real-world experiments on two different dexterous robot hand hardware platforms, achieving an average task success rate of 86%.
<div id='section'>Paperid: <span id='pid'>945, <a href='https://arxiv.org/pdf/2505.02108.pdf' target='_blank'>https://arxiv.org/pdf/2505.02108.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maksym Ivashechkin, Oscar Mendez, Richard Bowden
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.02108">SignSplat: Rendering Sign Language via Gaussian Splatting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>State-of-the-art approaches for conditional human body rendering via Gaussian splatting typically focus on simple body motions captured from many views. This is often in the context of dancing or walking. However, for more complex use cases, such as sign language, we care less about large body motion and more about subtle and complex motions of the hands and face. The problems of building high fidelity models are compounded by the complexity of capturing multi-view data of sign. The solution is to make better use of sequence data, ensuring that we can overcome the limited information from only a few views by exploiting temporal variability. Nevertheless, learning from sequence-level data requires extremely accurate and consistent model fitting to ensure that appearance is consistent across complex motions. We focus on how to achieve this, constraining mesh parameters to build an accurate Gaussian splatting framework from few views capable of modelling subtle human motion. We leverage regularization techniques on the Gaussian parameters to mitigate overfitting and rendering artifacts. Additionally, we propose a new adaptive control method to densify Gaussians and prune splat points on the mesh surface. To demonstrate the accuracy of our approach, we render novel sequences of sign language video, building on neural machine translation approaches to sign stitching. On benchmark datasets, our approach achieves state-of-the-art performance; and on highly articulated and complex sign language motion, we significantly outperform competing approaches.
<div id='section'>Paperid: <span id='pid'>946, <a href='https://arxiv.org/pdf/2504.20685.pdf' target='_blank'>https://arxiv.org/pdf/2504.20685.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zesheng Wang, Alexandre Bruckert, Patrick Le Callet, Guangtao Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.20685">Efficient Listener: Dyadic Facial Motion Synthesis via Action Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating realistic listener facial motions in dyadic conversations remains challenging due to the high-dimensional action space and temporal dependency requirements. Existing approaches usually consider extracting 3D Morphable Model (3DMM) coefficients and modeling in the 3DMM space. However, this makes the computational speed of the 3DMM a bottleneck, making it difficult to achieve real-time interactive responses. To tackle this problem, we propose Facial Action Diffusion (FAD), which introduces the diffusion methods from the field of image generation to achieve efficient facial action generation. We further build the Efficient Listener Network (ELNet) specially designed to accommodate both the visual and audio information of the speaker as input. Considering of FAD and ELNet, the proposed method learns effective listener facial motion representations and leads to improvements of performance over the state-of-the-art methods while reducing 99% computational time.
<div id='section'>Paperid: <span id='pid'>947, <a href='https://arxiv.org/pdf/2504.17371.pdf' target='_blank'>https://arxiv.org/pdf/2504.17371.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Oussema Dhaouadi, Johannes Meier, Luca Wahl, Jacques Kaiser, Luca Scalerandi, Nick Wandelburg, Zhuolun Zhou, Nijanthan Berinpanathan, Holger Banzhaf, Daniel Cremers
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.17371">Highly Accurate and Diverse Traffic Data: The DeepScenario Open 3D Dataset</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate 3D trajectory data is crucial for advancing autonomous driving. Yet, traditional datasets are usually captured by fixed sensors mounted on a car and are susceptible to occlusion. Additionally, such an approach can precisely reconstruct the dynamic environment in the close vicinity of the measurement vehicle only, while neglecting objects that are further away. In this paper, we introduce the DeepScenario Open 3D Dataset (DSC3D), a high-quality, occlusion-free dataset of 6 degrees of freedom bounding box trajectories acquired through a novel monocular camera drone tracking pipeline. Our dataset includes more than 175,000 trajectories of 14 types of traffic participants and significantly exceeds existing datasets in terms of diversity and scale, containing many unprecedented scenarios such as complex vehicle-pedestrian interaction on highly populated urban streets and comprehensive parking maneuvers from entry to exit. DSC3D dataset was captured in five various locations in Europe and the United States and include: a parking lot, a crowded inner-city, a steep urban intersection, a federal highway, and a suburban intersection. Our 3D trajectory dataset aims to enhance autonomous driving systems by providing detailed environmental 3D representations, which could lead to improved obstacle interactions and safety. We demonstrate its utility across multiple applications including motion prediction, motion planning, scenario mining, and generative reactive traffic agents. Our interactive online visualization platform and the complete dataset are publicly available at https://app.deepscenario.com, facilitating research in motion prediction, behavior modeling, and safety validation.
<div id='section'>Paperid: <span id='pid'>948, <a href='https://arxiv.org/pdf/2504.09209.pdf' target='_blank'>https://arxiv.org/pdf/2504.09209.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiangyue Zhang, Jianfang Li, Jiaxu Zhang, Jianqiang Ren, Liefeng Bo, Zhigang Tu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.09209">EchoMask: Speech-Queried Attention-based Mask Modeling for Holistic Co-Speech Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Masked modeling framework has shown promise in co-speech motion generation. However, it struggles to identify semantically significant frames for effective motion masking. In this work, we propose a speech-queried attention-based mask modeling framework for co-speech motion generation. Our key insight is to leverage motion-aligned speech features to guide the masked motion modeling process, selectively masking rhythm-related and semantically expressive motion frames. Specifically, we first propose a motion-audio alignment module (MAM) to construct a latent motion-audio joint space. In this space, both low-level and high-level speech features are projected, enabling motion-aligned speech representation using learnable speech queries. Then, a speech-queried attention mechanism (SQA) is introduced to compute frame-level attention scores through interactions between motion keys and speech queries, guiding selective masking toward motion frames with high attention scores. Finally, the motion-aligned speech features are also injected into the generation network to facilitate co-speech motion generation. Qualitative and quantitative evaluations confirm that our method outperforms existing state-of-the-art approaches, successfully producing high-quality co-speech motion.
<div id='section'>Paperid: <span id='pid'>949, <a href='https://arxiv.org/pdf/2503.23381.pdf' target='_blank'>https://arxiv.org/pdf/2503.23381.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiexin Wang, Wenwen Qiang, Zhao Yang, Bing Su
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.23381">Enhancing Human Motion Prediction via Multi-range Decoupling Decoding with Gating-adjusting Aggregation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Expressive representation of pose sequences is crucial for accurate motion modeling in human motion prediction (HMP). While recent deep learning-based methods have shown promise in learning motion representations, these methods tend to overlook the varying relevance and dependencies between historical information and future moments, with a stronger correlation for short-term predictions and weaker for distant future predictions. This limits the learning of motion representation and then hampers prediction performance. In this paper, we propose a novel approach called multi-range decoupling decoding with gating-adjusting aggregation ($MD2GA$), which leverages the temporal correlations to refine motion representation learning. This approach employs a two-stage strategy for HMP. In the first stage, a multi-range decoupling decoding adeptly adjusts feature learning by decoding the shared features into distinct future lengths, where different decoders offer diverse insights into motion patterns. In the second stage, a gating-adjusting aggregation dynamically combines the diverse insights guided by input motion data. Extensive experiments demonstrate that the proposed method can be easily integrated into other motion prediction methods and enhance their prediction performance.
<div id='section'>Paperid: <span id='pid'>950, <a href='https://arxiv.org/pdf/2503.22204.pdf' target='_blank'>https://arxiv.org/pdf/2503.22204.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiren Lu, Yunlai Zhou, Yiran Qiao, Chaoda Song, Tuo Liang, Jing Ma, Yu Yin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.22204">Segment then Splat: A Unified Approach for 3D Open-Vocabulary Segmentation based on Gaussian Splatting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Open-vocabulary querying in 3D space is crucial for enabling more intelligent perception in applications such as robotics, autonomous systems, and augmented reality. However, most existing methods rely on 2D pixel-level parsing, leading to multi-view inconsistencies and poor 3D object retrieval. Moreover, they are limited to static scenes and struggle with dynamic scenes due to the complexities of motion modeling. In this paper, we propose Segment then Splat, a 3D-aware open vocabulary segmentation approach for both static and dynamic scenes based on Gaussian Splatting. Segment then Splat reverses the long established approach of "segmentation after reconstruction" by dividing Gaussians into distinct object sets before reconstruction. Once the reconstruction is complete, the scene is naturally segmented into individual objects, achieving true 3D segmentation. This approach not only eliminates Gaussian-object misalignment issues in dynamic scenes but also accelerates the optimization process, as it eliminates the need for learning a separate language field. After optimization, a CLIP embedding is assigned to each object to enable open-vocabulary querying. Extensive experiments on various datasets demonstrate the effectiveness of our proposed method in both static and dynamic scenarios.
<div id='section'>Paperid: <span id='pid'>951, <a href='https://arxiv.org/pdf/2503.20218.pdf' target='_blank'>https://arxiv.org/pdf/2503.20218.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haiyang Liu, Zhan Xu, Fa-Ting Hong, Hsin-Ping Huang, Yi Zhou, Yang Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.20218">Video Motion Graphs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present Video Motion Graphs, a system designed to generate realistic human motion videos. Using a reference video and conditional signals such as music or motion tags, the system synthesizes new videos by first retrieving video clips with gestures matching the conditions and then generating interpolation frames to seamlessly connect clip boundaries. The core of our approach is HMInterp, a robust Video Frame Interpolation (VFI) model that enables seamless interpolation of discontinuous frames, even for complex motion scenarios like dancing. HMInterp i) employs a dual-branch interpolation approach, combining a Motion Diffusion Model for human skeleton motion interpolation with a diffusion-based video frame interpolation model for final frame generation. ii) adopts condition progressive training to effectively leverage identity strong and weak conditions, such as images and pose. These designs ensure both high video texture quality and accurate motion trajectory. Results show that our Video Motion Graphs outperforms existing generative- and retrieval-based methods for multi-modal conditioned human motion video generation. Project page can be found at https://h-liu1997.github.io/Video-Motion-Graphs/
<div id='section'>Paperid: <span id='pid'>952, <a href='https://arxiv.org/pdf/2503.19557.pdf' target='_blank'>https://arxiv.org/pdf/2503.19557.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haim Sawdayee, Chuan Guo, Guy Tevet, Bing Zhou, Jian Wang, Amit H. Bermano
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.19557">Dance Like a Chicken: Low-Rank Stylization for Human Motion Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-to-motion generative models span a wide range of 3D human actions but struggle with nuanced stylistic attributes such as a "Chicken" style. Due to the scarcity of style-specific data, existing approaches pull the generative prior towards a reference style, which often results in out-of-distribution low quality generations. In this work, we introduce LoRA-MDM, a lightweight framework for motion stylization that generalizes to complex actions while maintaining editability. Our key insight is that adapting the generative prior to include the style, while preserving its overall distribution, is more effective than modifying each individual motion during generation. Building on this idea, LoRA-MDM learns to adapt the prior to include the reference style using only a few samples. The style can then be used in the context of different textual prompts for generation. The low-rank adaptation shifts the motion manifold in a semantically meaningful way, enabling realistic style infusion even for actions not present in the reference samples. Moreover, preserving the distribution structure enables advanced operations such as style blending and motion editing. We compare LoRA-MDM to state-of-the-art stylized motion generation methods and demonstrate a favorable balance between text fidelity and style consistency.
<div id='section'>Paperid: <span id='pid'>953, <a href='https://arxiv.org/pdf/2503.14736.pdf' target='_blank'>https://arxiv.org/pdf/2503.14736.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yilan Dong, Haohe Liu, Qing Wang, Jiahao Yang, Wenqing Wang, Gregory Slabaugh, Shanxin Yuan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.14736">HandSplat: Embedding-Driven Gaussian Splatting for High-Fidelity Hand Rendering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing 3D Gaussian Splatting (3DGS) methods for hand rendering rely on rigid skeletal motion with an oversimplified non-rigid motion model, which fails to capture fine geometric and appearance details. Additionally, they perform densification based solely on per-point gradients and process poses independently, ignoring spatial and temporal correlations. These limitations lead to geometric detail loss, temporal instability, and inefficient point distribution. To address these issues, we propose HandSplat, a novel Gaussian Splatting-based framework that enhances both fidelity and stability for hand rendering. To improve fidelity, we extend standard 3DGS attributes with implicit geometry and appearance embeddings for finer non-rigid motion modeling while preserving the static hand characteristic modeled by original 3DGS attributes. Additionally, we introduce a local gradient-aware densification strategy that dynamically refines Gaussian density in high-variation regions. To improve stability, we incorporate pose-conditioned attribute regularization to encourage attribute consistency across similar poses, mitigating temporal artifacts. Extensive experiments on InterHand2.6M demonstrate that HandSplat surpasses existing methods in fidelity and stability while achieving real-time performance. We will release the code and pre-trained models upon acceptance.
<div id='section'>Paperid: <span id='pid'>954, <a href='https://arxiv.org/pdf/2503.11038.pdf' target='_blank'>https://arxiv.org/pdf/2503.11038.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingjie Wei, Xuemei Xie, Guangming Shi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.11038">ACMo: Attribute Controllable Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Attributes such as style, fine-grained text, and trajectory are specific conditions for describing motion. However, existing methods often lack precise user control over motion attributes and suffer from limited generalizability to unseen motions. This work introduces an Attribute Controllable Motion generation architecture, to address these challenges via decouple any conditions and control them separately. Firstly, we explored the Attribute Diffusion Model to imporve text-to-motion performance via decouple text and motion learning, as the controllable model relies heavily on the pre-trained model. Then, we introduce Motion Adpater to quickly finetune previously unseen motion patterns. Its motion prompts inputs achieve multimodal text-to-motion generation that captures user-specified styles. Finally, we propose a LLM Planner to bridge the gap between unseen attributes and dataset-specific texts via local knowledage for user-friendly interaction. Our approach introduces the capability for motion prompts for stylize generation, enabling fine-grained and user-friendly attribute control while providing performance comparable to state-of-the-art methods. Project page: https://mjwei3d.github.io/ACMo/
<div id='section'>Paperid: <span id='pid'>955, <a href='https://arxiv.org/pdf/2503.07597.pdf' target='_blank'>https://arxiv.org/pdf/2503.07597.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuhong Zhang, Guanlin Wu, Ling-Hao Chen, Zhuokai Zhao, Jing Lin, Xiaoke Jiang, Jiamin Wu, Zhuoheng Li, Hao Frank Yang, Haoqian Wang, Lei Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.07597">HumanMM: Global Human Motion Recovery from Multi-shot Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we present a novel framework designed to reconstruct long-sequence 3D human motion in the world coordinates from in-the-wild videos with multiple shot transitions. Such long-sequence in-the-wild motions are highly valuable to applications such as motion generation and motion understanding, but are of great challenge to be recovered due to abrupt shot transitions, partial occlusions, and dynamic backgrounds presented in such videos. Existing methods primarily focus on single-shot videos, where continuity is maintained within a single camera view, or simplify multi-shot alignment in camera space only. In this work, we tackle the challenges by integrating an enhanced camera pose estimation with Human Motion Recovery (HMR) by incorporating a shot transition detector and a robust alignment module for accurate pose and orientation continuity across shots. By leveraging a custom motion integrator, we effectively mitigate the problem of foot sliding and ensure temporal consistency in human pose. Extensive evaluations on our created multi-shot dataset from public 3D human datasets demonstrate the robustness of our method in reconstructing realistic human motion in world coordinates.
<div id='section'>Paperid: <span id='pid'>956, <a href='https://arxiv.org/pdf/2503.01291.pdf' target='_blank'>https://arxiv.org/pdf/2503.01291.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Peishan Cong, Ziyi Wang, Yuexin Ma, Xiangyu Yue
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.01291">SemGeoMo: Dynamic Contextual Human Motion Generation with Semantic and Geometric Guidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating reasonable and high-quality human interactive motions in a given dynamic environment is crucial for understanding, modeling, transferring, and applying human behaviors to both virtual and physical robots. In this paper, we introduce an effective method, SemGeoMo, for dynamic contextual human motion generation, which fully leverages the text-affordance-joint multi-level semantic and geometric guidance in the generation process, improving the semantic rationality and geometric correctness of generative motions. Our method achieves state-of-the-art performance on three datasets and demonstrates superior generalization capability for diverse interaction scenarios. The project page and code can be found at https://4dvlab.github.io/project_page/semgeomo/.
<div id='section'>Paperid: <span id='pid'>957, <a href='https://arxiv.org/pdf/2503.01178.pdf' target='_blank'>https://arxiv.org/pdf/2503.01178.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoyuan Zhang, Xinyan Cai, Bo Liu, Weidong Huang, Song-Chun Zhu, Siyuan Qi, Yaodong Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.01178">Differentiable Information Enhanced Model-Based Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Differentiable environments have heralded new possibilities for learning control policies by offering rich differentiable information that facilitates gradient-based methods. In comparison to prevailing model-free reinforcement learning approaches, model-based reinforcement learning (MBRL) methods exhibit the potential to effectively harness the power of differentiable information for recovering the underlying physical dynamics. However, this presents two primary challenges: effectively utilizing differentiable information to 1) construct models with more accurate dynamic prediction and 2) enhance the stability of policy training. In this paper, we propose a Differentiable Information Enhanced MBRL method, MB-MIX, to address both challenges. Firstly, we adopt a Sobolev model training approach that penalizes incorrect model gradient outputs, enhancing prediction accuracy and yielding more precise models that faithfully capture system dynamics. Secondly, we introduce mixing lengths of truncated learning windows to reduce the variance in policy gradient estimation, resulting in improved stability during policy learning. To validate the effectiveness of our approach in differentiable environments, we provide theoretical analysis and empirical results. Notably, our approach outperforms previous model-based and model-free methods, in multiple challenging tasks involving controllable rigid robots such as humanoid robots' motion control and deformable object manipulation.
<div id='section'>Paperid: <span id='pid'>958, <a href='https://arxiv.org/pdf/2502.11370.pdf' target='_blank'>https://arxiv.org/pdf/2502.11370.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pengming Zhu, Zongtan Zhou, Weijia Yao, Wei Dai, Zhiwen Zeng, Huimin Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.11370">HI-GVF: Shared Control based on Human-Influenced Guiding Vector Fields for Human-multi-robot Cooperation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human-multi-robot shared control leverages human decision-making and robotic autonomy to enhance human-robot collaboration. While widely studied, existing systems often adopt a leader-follower model, limiting robot autonomy to some extent. Besides, a human is required to directly participate in the motion control of robots through teleoperation, which significantly burdens the operator. To alleviate these two issues, we propose a layered shared control computing framework using human-influenced guiding vector fields (HI-GVF) for human-robot collaboration. HI-GVF guides the multi-robot system along a desired path specified by the human. Then, an intention field is designed to merge the human and robot intentions, accelerating the propagation of the human intention within the multi-robot system. Moreover, we give the stability analysis of the proposed model and use collision avoidance based on safety barrier certificates to fine-tune the velocity. Eventually, considering the firefighting task as an example scenario, we conduct simulations and experiments using multiple human-robot interfaces (brain-computer interface, myoelectric wristband, eye-tracking), and the results demonstrate that our proposed approach boosts the effectiveness and performance of the task.
<div id='section'>Paperid: <span id='pid'>959, <a href='https://arxiv.org/pdf/2502.09533.pdf' target='_blank'>https://arxiv.org/pdf/2502.09533.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fei Shen, Cong Wang, Junyao Gao, Qin Guo, Jisheng Dang, Jinhui Tang, Tat-Seng Chua
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.09533">Long-Term TalkingFace Generation via Motion-Prior Conditional Diffusion Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in conditional diffusion models have shown promise for generating realistic TalkingFace videos, yet challenges persist in achieving consistent head movement, synchronized facial expressions, and accurate lip synchronization over extended generations. To address these, we introduce the \textbf{M}otion-priors \textbf{C}onditional \textbf{D}iffusion \textbf{M}odel (\textbf{MCDM}), which utilizes both archived and current clip motion priors to enhance motion prediction and ensure temporal consistency. The model consists of three key elements: (1) an archived-clip motion-prior that incorporates historical frames and a reference frame to preserve identity and context; (2) a present-clip motion-prior diffusion model that captures multimodal causality for accurate predictions of head movements, lip sync, and expressions; and (3) a memory-efficient temporal attention mechanism that mitigates error accumulation by dynamically storing and updating motion features. We also release the \textbf{TalkingFace-Wild} dataset, a multilingual collection of over 200 hours of footage across 10 languages. Experimental results demonstrate the effectiveness of MCDM in maintaining identity and motion continuity for long-term TalkingFace generation. Code, models, and datasets will be publicly available.
<div id='section'>Paperid: <span id='pid'>960, <a href='https://arxiv.org/pdf/2502.04299.pdf' target='_blank'>https://arxiv.org/pdf/2502.04299.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinbo Xing, Long Mai, Cusuh Ham, Jiahui Huang, Aniruddha Mahapatra, Chi-Wing Fu, Tien-Tsin Wong, Feng Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.04299">MotionCanvas: Cinematic Shot Design with Controllable Image-to-Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a method that allows users to design cinematic video shots in the context of image-to-video generation. Shot design, a critical aspect of filmmaking, involves meticulously planning both camera movements and object motions in a scene. However, enabling intuitive shot design in modern image-to-video generation systems presents two main challenges: first, effectively capturing user intentions on the motion design, where both camera movements and scene-space object motions must be specified jointly; and second, representing motion information that can be effectively utilized by a video diffusion model to synthesize the image animations. To address these challenges, we introduce MotionCanvas, a method that integrates user-driven controls into image-to-video (I2V) generation models, allowing users to control both object and camera motions in a scene-aware manner. By connecting insights from classical computer graphics and contemporary video generation techniques, we demonstrate the ability to achieve 3D-aware motion control in I2V synthesis without requiring costly 3D-related training data. MotionCanvas enables users to intuitively depict scene-space motion intentions, and translates them into spatiotemporal motion-conditioning signals for video diffusion models. We demonstrate the effectiveness of our method on a wide range of real-world image content and shot-design scenarios, highlighting its potential to enhance the creative workflows in digital content creation and adapt to various image and video editing applications.
<div id='section'>Paperid: <span id='pid'>961, <a href='https://arxiv.org/pdf/2502.02492.pdf' target='_blank'>https://arxiv.org/pdf/2502.02492.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hila Chefer, Uriel Singer, Amit Zohar, Yuval Kirstain, Adam Polyak, Yaniv Taigman, Lior Wolf, Shelly Sheynin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.02492">VideoJAM: Joint Appearance-Motion Representations for Enhanced Motion Generation in Video Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite tremendous recent progress, generative video models still struggle to capture real-world motion, dynamics, and physics. We show that this limitation arises from the conventional pixel reconstruction objective, which biases models toward appearance fidelity at the expense of motion coherence. To address this, we introduce VideoJAM, a novel framework that instills an effective motion prior to video generators, by encouraging the model to learn a joint appearance-motion representation. VideoJAM is composed of two complementary units. During training, we extend the objective to predict both the generated pixels and their corresponding motion from a single learned representation. During inference, we introduce Inner-Guidance, a mechanism that steers the generation toward coherent motion by leveraging the model's own evolving motion prediction as a dynamic guidance signal. Notably, our framework can be applied to any video model with minimal adaptations, requiring no modifications to the training data or scaling of the model. VideoJAM achieves state-of-the-art performance in motion coherence, surpassing highly competitive proprietary models while also enhancing the perceived visual quality of the generations. These findings emphasize that appearance and motion can be complementary and, when effectively integrated, enhance both the visual quality and the coherence of video generation. Project website: https://hila-chefer.github.io/videojam-paper.github.io/
<div id='section'>Paperid: <span id='pid'>962, <a href='https://arxiv.org/pdf/2412.05095.pdf' target='_blank'>https://arxiv.org/pdf/2412.05095.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaofeng Tan, Hongsong Wang, Xin Geng, Pan Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.05095">SoPo: Text-to-Motion Generation Using Semi-Online Preference Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-to-motion generation is essential for advancing the creative industry but often presents challenges in producing consistent, realistic motions. To address this, we focus on fine-tuning text-to-motion models to consistently favor high-quality, human-preferred motions, a critical yet largely unexplored problem. In this work, we theoretically investigate the DPO under both online and offline settings, and reveal their respective limitation: overfitting in offline DPO, and biased sampling in online DPO. Building on our theoretical insights, we introduce Semi-online Preference Optimization (SoPo), a DPO-based method for training text-to-motion models using "semi-online" data pair, consisting of unpreferred motion from online distribution and preferred motion in offline datasets. This method leverages both online and offline DPO, allowing each to compensate for the other's limitations. Extensive experiments demonstrate that SoPo outperforms other preference alignment methods, with an MM-Dist of 3.25% (vs e.g. 0.76% of MoDiPO) on the MLD model, 2.91% (vs e.g. 0.66% of MoDiPO) on MDM model, respectively. Additionally, the MLD model fine-tuned by our SoPo surpasses the SoTA model in terms of R-precision and MM Dist. Visualization results also show the efficacy of our SoPo in preference alignment. Project page: https://xiaofeng-tan.github.io/projects/SoPo/ .
<div id='section'>Paperid: <span id='pid'>963, <a href='https://arxiv.org/pdf/2412.02419.pdf' target='_blank'>https://arxiv.org/pdf/2412.02419.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingyi Shi, Dafei Qin, Leo Ho, Zhouyingcheng Liao, Yinghao Huang, Junichi Yamagishi, Taku Komura
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.02419">It Takes Two: Real-time Co-Speech Two-person's Interaction Generation via Reactive Auto-regressive Diffusion Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Conversational scenarios are very common in real-world settings, yet existing co-speech motion synthesis approaches often fall short in these contexts, where one person's audio and gestures will influence the other's responses. Additionally, most existing methods rely on offline sequence-to-sequence frameworks, which are unsuitable for online applications. In this work, we introduce an audio-driven, auto-regressive system designed to synthesize dynamic movements for two characters during a conversation. At the core of our approach is a diffusion-based full-body motion synthesis model, which is conditioned on the past states of both characters, speech audio, and a task-oriented motion trajectory input, allowing for flexible spatial control. To enhance the model's ability to learn diverse interactions, we have enriched existing two-person conversational motion datasets with more dynamic and interactive motions. We evaluate our system through multiple experiments to show it outperforms across a variety of tasks, including single and two-person co-speech motion generation, as well as interactive motion generation. To the best of our knowledge, this is the first system capable of generating interactive full-body motions for two characters from speech in an online manner.
<div id='section'>Paperid: <span id='pid'>964, <a href='https://arxiv.org/pdf/2411.12773.pdf' target='_blank'>https://arxiv.org/pdf/2411.12773.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Youyuan Zhang, Zehua Liu, Zenan Li, Zhaoyu Li, James J. Clark, Xujie Si
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.12773">Decoupling Training-Free Guided Diffusion by ADMM</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we consider the conditional generation problem by guiding off-the-shelf unconditional diffusion models with differentiable loss functions in a plug-and-play fashion. While previous research has primarily focused on balancing the unconditional diffusion model and the guided loss through a tuned weight hyperparameter, we propose a novel framework that distinctly decouples these two components. Specifically, we introduce two variables ${x}$ and ${z}$, to represent the generated samples governed by the unconditional generation model and the guidance function, respectively. This decoupling reformulates conditional generation into two manageable subproblems, unified by the constraint ${x} = {z}$. Leveraging this setup, we develop a new algorithm based on the Alternating Direction Method of Multipliers (ADMM) to adaptively balance these components. Additionally, we establish the equivalence between the diffusion reverse step and the proximal operator of ADMM and provide a detailed convergence analysis of our algorithm under certain mild assumptions. Our experiments demonstrate that our proposed method ADMMDiff consistently generates high-quality samples while ensuring strong adherence to the conditioning criteria. It outperforms existing methods across a range of conditional generation tasks, including image generation with various guidance and controllable motion synthesis.
<div id='section'>Paperid: <span id='pid'>965, <a href='https://arxiv.org/pdf/2410.10802.pdf' target='_blank'>https://arxiv.org/pdf/2410.10802.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Soon Yau Cheong, Duygu Ceylan, Armin Mustafa, Andrew Gilbert, Chun-Hao Paul Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.10802">Boosting Camera Motion Control for Video Diffusion Transformers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in diffusion models have significantly enhanced the quality of video generation. However, fine-grained control over camera pose remains a challenge. While U-Net-based models have shown promising results for camera control, transformer-based diffusion models (DiT)-the preferred architecture for large-scale video generation - suffer from severe degradation in camera motion accuracy. In this paper, we investigate the underlying causes of this issue and propose solutions tailored to DiT architectures. Our study reveals that camera control performance depends heavily on the choice of conditioning methods rather than camera pose representations that is commonly believed. To address the persistent motion degradation in DiT, we introduce Camera Motion Guidance (CMG), based on classifier-free guidance, which boosts camera control by over 400%. Additionally, we present a sparse camera control pipeline, significantly simplifying the process of specifying camera poses for long videos. Our method universally applies to both U-Net and DiT models, offering improved camera control for video generation tasks.
<div id='section'>Paperid: <span id='pid'>966, <a href='https://arxiv.org/pdf/2410.06437.pdf' target='_blank'>https://arxiv.org/pdf/2410.06437.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kojiro Takeyama, Yimeng Liu, Misha Sra
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.06437">LocoVR: Multiuser Indoor Locomotion Dataset in Virtual Reality</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding human locomotion is crucial for AI agents such as robots, particularly in complex indoor home environments. Modeling human trajectories in these spaces requires insight into how individuals maneuver around physical obstacles and manage social navigation dynamics. These dynamics include subtle behaviors influenced by proxemics - the social use of space, such as stepping aside to allow others to pass or choosing longer routes to avoid collisions. Previous research has developed datasets of human motion in indoor scenes, but these are often limited in scale and lack the nuanced social navigation dynamics common in home environments. To address this, we present LocoVR, a dataset of 7000+ two-person trajectories captured in virtual reality from over 130 different indoor home environments. LocoVR provides accurate trajectory data and precise spatial information, along with rich examples of socially-motivated movement behaviors. For example, the dataset captures instances of individuals navigating around each other in narrow spaces, adjusting paths to respect personal boundaries in living areas, and coordinating movements in high-traffic zones like entryways and kitchens. Our evaluation shows that LocoVR significantly enhances model performance in three practical indoor tasks utilizing human trajectories, and demonstrates predicting socially-aware navigation patterns in home environments.
<div id='section'>Paperid: <span id='pid'>967, <a href='https://arxiv.org/pdf/2410.05628.pdf' target='_blank'>https://arxiv.org/pdf/2410.05628.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jeongeun Park, Sungjoon Choi, Sangdoo Yun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.05628">A Unified Framework for Motion Reasoning and Generation in Human Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in large language models (LLMs) have significantly improved their ability to generate natural and contextually relevant text, enabling more human-like AI interactions. However, generating and understanding interactive human-like motion, where multiple individuals engage in coordinated movements, remains challenging due to the complexity of modeling these interactions. Additionally, a unified and versatile model is needed to handle diverse interactive scenarios, such as chat systems that dynamically adapt to user instructions and assigned roles. To address these challenges, we introduce VIM, the Versatile Interactive Motion-language model, which integrates both language and motion modalities to effectively understand, generate, and control interactive motions in multi-turn conversational contexts. Unlike previous studies that primarily focus on uni-directional tasks such as text-to-motion or motion-to-text, VIM employs a unified architecture capable of simultaneously understanding and generating both motion and text modalities. Given the absence of an appropriate dataset to support this task, we introduce Inter-MT2, a large-scale instruction-tuning dataset containing 82.7K multi-turn interactive motion instructions, covering 153K interactive motion samples. Inter-MT2 spans diverse instructional scenarios, including motion editing, question answering, and story generation, leveraging off-the-shelf large language models and motion diffusion models to construct a broad set of interactive motion instructions. We extensively evaluate the versatility of VIM across multiple interactive motion-related tasks, including motion-to-text, text-to-motion, reaction generation, motion editing, and reasoning about motion sequences.
<div id='section'>Paperid: <span id='pid'>968, <a href='https://arxiv.org/pdf/2410.03441.pdf' target='_blank'>https://arxiv.org/pdf/2410.03441.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guy Tevet, Sigal Raab, Setareh Cohan, Daniele Reda, Zhengyi Luo, Xue Bin Peng, Amit H. Bermano, Michiel van de Panne
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.03441">CLoSD: Closing the Loop between Simulation and Diffusion for multi-task character control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion diffusion models and Reinforcement Learning (RL) based control for physics-based simulations have complementary strengths for human motion generation. The former is capable of generating a wide variety of motions, adhering to intuitive control such as text, while the latter offers physically plausible motion and direct interaction with the environment. In this work, we present a method that combines their respective strengths. CLoSD is a text-driven RL physics-based controller, guided by diffusion generation for various tasks. Our key insight is that motion diffusion can serve as an on-the-fly universal planner for a robust RL controller. To this end, CLoSD maintains a closed-loop interaction between two modules -- a Diffusion Planner (DiP), and a tracking controller. DiP is a fast-responding autoregressive diffusion model, controlled by textual prompts and target locations, and the controller is a simple and robust motion imitator that continuously receives motion plans from DiP and provides feedback from the environment. CLoSD is capable of seamlessly performing a sequence of different tasks, including navigation to a goal location, striking an object with a hand or foot as specified in a text prompt, sitting down, and getting up. https://guytevet.github.io/CLoSD-page/
<div id='section'>Paperid: <span id='pid'>969, <a href='https://arxiv.org/pdf/2409.16666.pdf' target='_blank'>https://arxiv.org/pdf/2409.16666.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aggelina Chatziagapi, Bindita Chaudhuri, Amit Kumar, Rakesh Ranjan, Dimitris Samaras, Nikolaos Sarafianos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.16666">TalkinNeRF: Animatable Neural Fields for Full-Body Talking Humans</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a novel framework that learns a dynamic neural radiance field (NeRF) for full-body talking humans from monocular videos. Prior work represents only the body pose or the face. However, humans communicate with their full body, combining body pose, hand gestures, as well as facial expressions. In this work, we propose TalkinNeRF, a unified NeRF-based network that represents the holistic 4D human motion. Given a monocular video of a subject, we learn corresponding modules for the body, face, and hands, that are combined together to generate the final result. To capture complex finger articulation, we learn an additional deformation field for the hands. Our multi-identity representation enables simultaneous training for multiple subjects, as well as robust animation under completely unseen poses. It can also generalize to novel identities, given only a short video as input. We demonstrate state-of-the-art performance for animating full-body talking humans, with fine-grained hand articulation and facial expressions.
<div id='section'>Paperid: <span id='pid'>970, <a href='https://arxiv.org/pdf/2409.00343.pdf' target='_blank'>https://arxiv.org/pdf/2409.00343.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bonan Liu, Handi Yin, Manuel Kaufmann, Jinhao He, Sammy Christen, Jie Song, Pan Hui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.00343">EgoHDM: An Online Egocentric-Inertial Human Motion Capture, Localization, and Dense Mapping System</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present EgoHDM, an online egocentric-inertial human motion capture (mocap), localization, and dense mapping system. Our system uses 6 inertial measurement units (IMUs) and a commodity head-mounted RGB camera. EgoHDM is the first human mocap system that offers dense scene mapping in near real-time. Further, it is fast and robust to initialize and fully closes the loop between physically plausible map-aware global human motion estimation and mocap-aware 3D scene reconstruction. Our key idea is integrating camera localization and mapping information with inertial human motion capture bidirectionally in our system. To achieve this, we design a tightly coupled mocap-aware dense bundle adjustment and physics-based body pose correction module leveraging a local body-centric elevation map. The latter introduces a novel terrain-aware contact PD controller, which enables characters to physically contact the given local elevation map thereby reducing human floating or penetration. We demonstrate the performance of our system on established synthetic and real-world benchmarks. The results show that our method reduces human localization, camera pose, and mapping accuracy error by 41%, 71%, 46%, respectively, compared to the state of the art. Our qualitative evaluations on newly captured data further demonstrate that EgoHDM can cover challenging scenarios in non-flat terrain including stepping over stairs and outdoor scenes in the wild.
<div id='section'>Paperid: <span id='pid'>971, <a href='https://arxiv.org/pdf/2407.16591.pdf' target='_blank'>https://arxiv.org/pdf/2407.16591.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kan Chen, Zhen Meng, Xiangmin Xu, Changyang She, Philip G. Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.16591">Real-Time Interactions Between Human Controllers and Remote Devices in Metaverse</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Supporting real-time interactions between human controllers and remote devices remains a challenging goal in the Metaverse due to the stringent requirements on computing workload, communication throughput, and round-trip latency. In this paper, we establish a novel framework for real-time interactions through the virtual models in the Metaverse. Specifically, we jointly predict the motion of the human controller for 1) proactive rendering in the Metaverse and 2) generating control commands to the real-world remote device in advance. The virtual model is decoupled into two components for rendering and control, respectively. To dynamically adjust the prediction horizons for rendering and control, we develop a two-step human-in-the-loop continuous reinforcement learning approach and use an expert policy to improve the training efficiency. An experimental prototype is built to verify our algorithm with different communication latencies. Compared with the baseline policy without prediction, our proposed method can reduce 1) the Motion-To-Photon (MTP) latency between human motion and rendering feedback and 2) the root mean squared error (RMSE) between human motion and real-world remote devices significantly.
<div id='section'>Paperid: <span id='pid'>972, <a href='https://arxiv.org/pdf/2407.16341.pdf' target='_blank'>https://arxiv.org/pdf/2407.16341.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaodong Chen, Wu Liu, Qian Bao, Xinchen Liu, Quanwei Yang, Ruoli Dai, Tao Mei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.16341">Motion Capture from Inertial and Vision Sensors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion capture is the foundation for many computer vision and graphics tasks. While industrial motion capture systems with complex camera arrays or expensive wearable sensors have been widely adopted in movie and game production, consumer-affordable and easy-to-use solutions for personal applications are still far from mature. To utilize a mixture of a monocular camera and very few inertial measurement units (IMUs) for accurate multi-modal human motion capture in daily life, we contribute MINIONS in this paper, a large-scale Motion capture dataset collected from INertial and visION Sensors. MINIONS has several featured properties: 1) large scale of over five million frames and 400 minutes duration; 2) multi-modality data of IMUs signals and RGB videos labeled with joint positions, joint rotations, SMPL parameters, etc.; 3) a diverse set of 146 fine-grained single and interactive actions with textual descriptions. With the proposed MINIONS, we conduct experiments on multi-modal motion capture and explore the possibilities of consumer-affordable motion capture using a monocular camera and very few IMUs. The experiment results emphasize the unique advantages of inertial and vision sensors, showcasing the promise of consumer-affordable multi-modal motion capture and providing a valuable resource for further research and development.
<div id='section'>Paperid: <span id='pid'>973, <a href='https://arxiv.org/pdf/2407.16341.pdf' target='_blank'>https://arxiv.org/pdf/2407.16341.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaodong Chen, Wu Liu, Qian Bao, Xinchen Liu, Quanwei Yang, Ruoli Dai, Tao Mei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.16341">Motion Capture from Inertial and Vision Sensors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion capture is the foundation for many computer vision and graphics tasks. While industrial motion capture systems with complex camera arrays or expensive wearable sensors have been widely adopted in movie and game production, consumer-affordable and easy-to-use solutions for personal applications are still far from mature. To utilize a mixture of a monocular camera and very few inertial measurement units (IMUs) for accurate multi-modal human motion capture in daily life, we contribute MINIONS in this paper, a large-scale Motion capture dataset collected from INertial and visION Sensors. MINIONS has several featured properties: 1) large scale of over five million frames and 400 minutes duration; 2) multi-modality data of IMUs signals and RGB videos labeled with joint positions, joint rotations, SMPL parameters, etc.; 3) a diverse set of 146 fine-grained single and interactive actions with textual descriptions. With the proposed MINIONS dataset, we propose a SparseNet framework to capture human motion from IMUs and videos by discovering their supplementary features and exploring the possibilities of consumer-affordable motion capture using a monocular camera and very few IMUs. The experiment results emphasize the unique advantages of inertial and vision sensors, showcasing the promise of consumer-affordable multi-modal motion capture and providing a valuable resource for further research and development.
<div id='section'>Paperid: <span id='pid'>974, <a href='https://arxiv.org/pdf/2407.14220.pdf' target='_blank'>https://arxiv.org/pdf/2407.14220.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunfan Gao, Florian Messerer, Niels van Duijkeren, Moritz Diehl
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.14220">Stochastic Model Predictive Control with Optimal Linear Feedback for Mobile Robots in Dynamic Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robot navigation around humans can be a challenging problem since human movements are hard to predict. Stochastic model predictive control (MPC) can account for such uncertainties and approximately bound the probability of a collision to take place. In this paper, to counteract the rapidly growing human motion uncertainty over time, we incorporate state feedback in the stochastic MPC. This allows the robot to more closely track reference trajectories. To this end the feedback policy is left as a degree of freedom in the optimal control problem. The stochastic MPC with feedback is validated in simulation experiments and is compared against nominal MPC and stochastic MPC without feedback. The added computation time can be limited by reducing the number of additional variables for the feedback law with a small compromise in control performance.
<div id='section'>Paperid: <span id='pid'>975, <a href='https://arxiv.org/pdf/2407.08428.pdf' target='_blank'>https://arxiv.org/pdf/2407.08428.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wentao Lei, Jinting Wang, Fengji Ma, Guanjie Huang, Li Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.08428">A Comprehensive Survey on Human Video Generation: Challenges, Methods, and Insights</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human video generation is a dynamic and rapidly evolving task that aims to synthesize 2D human body video sequences with generative models given control conditions such as text, audio, and pose. With the potential for wide-ranging applications in film, gaming, and virtual communication, the ability to generate natural and realistic human video is critical. Recent advancements in generative models have laid a solid foundation for the growing interest in this area. Despite the significant progress, the task of human video generation remains challenging due to the consistency of characters, the complexity of human motion, and difficulties in their relationship with the environment. This survey provides a comprehensive review of the current state of human video generation, marking, to the best of our knowledge, the first extensive literature review in this domain. We start with an introduction to the fundamentals of human video generation and the evolution of generative models that have facilitated the field's growth. We then examine the main methods employed for three key sub-tasks within human video generation: text-driven, audio-driven, and pose-driven motion generation. These areas are explored concerning the conditions that guide the generation process. Furthermore, we offer a collection of the most commonly utilized datasets and the evaluation metrics that are crucial in assessing the quality and realism of generated videos. The survey concludes with a discussion of the current challenges in the field and suggests possible directions for future research. The goal of this survey is to offer the research community a clear and holistic view of the advancements in human video generation, highlighting the milestones achieved and the challenges that lie ahead.
<div id='section'>Paperid: <span id='pid'>976, <a href='https://arxiv.org/pdf/2407.00955.pdf' target='_blank'>https://arxiv.org/pdf/2407.00955.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiang Jiao, Dingzhu Wen, Guangxu Zhu, Wei Jiang, Wu Luo, Yuanming Shi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.00955">Task-oriented Over-the-air Computation for Edge-device Co-inference with Balanced Classification Accuracy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Edge-device co-inference, which concerns the cooperation between edge devices and an edge server for completing inference tasks over wireless networks, has been a promising technique for enabling various kinds of intelligent services at the network edge, e.g., auto-driving. In this paradigm, the concerned design objective of the network shifts from the traditional communication throughput to the effective and efficient execution of the inference task underpinned by the network, measured by, e.g., the inference accuracy and latency. In this paper, a task-oriented over-the-air computation scheme is proposed for a multidevice artificial intelligence system. Particularly, a novel tractable inference accuracy metric is proposed for classification tasks, which is called minimum pair-wise discriminant gain. Unlike prior work measuring the average of all class pairs in feature space, it measures the minimum distance of all class pairs. By maximizing the minimum pair-wise discriminant gain instead of its average counterpart, any pair of classes can be better separated in the feature space, and thus leading to a balanced and improved inference accuracy for all classes. Besides, this paper jointly optimizes the minimum discriminant gain of all feature elements instead of separately maximizing that of each element in the existing designs. As a result, the transmit power can be adaptively allocated to the feature elements according to their different contributions to the inference accuracy, opening an extra degree of freedom to improve inference performance. Extensive experiments are conducted using a concrete use case of human motion recognition to verify the superiority of the proposed design over the benchmarking scheme.
<div id='section'>Paperid: <span id='pid'>977, <a href='https://arxiv.org/pdf/2406.10454.pdf' target='_blank'>https://arxiv.org/pdf/2406.10454.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zipeng Fu, Qingqing Zhao, Qi Wu, Gordon Wetzstein, Chelsea Finn
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.10454">HumanPlus: Humanoid Shadowing and Imitation from Humans</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>One of the key arguments for building robots that have similar form factors to human beings is that we can leverage the massive human data for training. Yet, doing so has remained challenging in practice due to the complexities in humanoid perception and control, lingering physical gaps between humanoids and humans in morphologies and actuation, and lack of a data pipeline for humanoids to learn autonomous skills from egocentric vision. In this paper, we introduce a full-stack system for humanoids to learn motion and autonomous skills from human data. We first train a low-level policy in simulation via reinforcement learning using existing 40-hour human motion datasets. This policy transfers to the real world and allows humanoid robots to follow human body and hand motion in real time using only a RGB camera, i.e. shadowing. Through shadowing, human operators can teleoperate humanoids to collect whole-body data for learning different tasks in the real world. Using the data collected, we then perform supervised behavior cloning to train skill policies using egocentric vision, allowing humanoids to complete different tasks autonomously by imitating human skills. We demonstrate the system on our customized 33-DoF 180cm humanoid, autonomously completing tasks such as wearing a shoe to stand up and walk, unloading objects from warehouse racks, folding a sweatshirt, rearranging objects, typing, and greeting another robot with 60-100% success rates using up to 40 demonstrations. Project website: https://humanoid-ai.github.io/
<div id='section'>Paperid: <span id='pid'>978, <a href='https://arxiv.org/pdf/2406.05613.pdf' target='_blank'>https://arxiv.org/pdf/2406.05613.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenhang Liu, Meng Ren, Kun Song, Gaoming Chen, Michael Yu Wang, Zhenhua Xiong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.05613">Distributed Motion Control of Multiple Mobile Manipulators for Reducing Interaction Wrench in Object Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In real-world cooperative manipulation of objects, multiple mobile manipulator systems may suffer from disturbances and asynchrony, leading to excessive interaction wrenches and potentially causing object damage or emergency stops. Existing methods often rely on torque control and dynamic models, which are uncommon in many industrial robots and settings. Additionally, dynamic models often neglect joint friction forces and are not accurate. These methods are challenging to implement and validate in physical systems. To address the problems, this paper presents a novel distributed motion control approach aimed at reducing these unnecessary interaction wrenches. The control law is only based on local information and joint velocity control to enhance practical applicability. The communication delays within the distributed architecture are considered. The stability of the control law is rigorously proven by the Lyapunov theorem. In the simulations, the effectiveness is shown, and the impact of communication graph connectivity and communication delays has been studied. A comparison with other methods shows the advantages of the proposed control law in terms of convergence speed and robustness. Finally, the control law has been validated in physical experiments. It does not require dynamic modeling or torque control, and thus is more user-friendly for physical robots.
<div id='section'>Paperid: <span id='pid'>979, <a href='https://arxiv.org/pdf/2405.19283.pdf' target='_blank'>https://arxiv.org/pdf/2405.19283.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hanchao Liu, Xiaohang Zhan, Shaoli Huang, Tai-Jiang Mu, Ying Shan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.19283">Programmable Motion Generation for Open-Set Motion Control Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Character animation in real-world scenarios necessitates a variety of constraints, such as trajectories, key-frames, interactions, etc. Existing methodologies typically treat single or a finite set of these constraint(s) as separate control tasks. They are often specialized, and the tasks they address are rarely extendable or customizable. We categorize these as solutions to the close-set motion control problem. In response to the complexity of practical motion control, we propose and attempt to solve the open-set motion control problem. This problem is characterized by an open and fully customizable set of motion control tasks. To address this, we introduce a new paradigm, programmable motion generation. In this paradigm, any given motion control task is broken down into a combination of atomic constraints. These constraints are then programmed into an error function that quantifies the degree to which a motion sequence adheres to them. We utilize a pre-trained motion generation model and optimize its latent code to minimize the error function of the generated motion. Consequently, the generated motion not only inherits the prior of the generative model but also satisfies the required constraints. Experiments show that we can generate high-quality motions when addressing a wide range of unseen tasks. These tasks encompass motion control by motion dynamics, geometric constraints, physical laws, interactions with scenes, objects or the character own body parts, etc. All of these are achieved in a unified approach, without the need for ad-hoc paired training data collection or specialized network designs. During the programming of novel tasks, we observed the emergence of new skills beyond those of the prior model. With the assistance of large language models, we also achieved automatic programming. We hope that this work will pave the way for the motion control of general AI agents.
<div id='section'>Paperid: <span id='pid'>980, <a href='https://arxiv.org/pdf/2405.03803.pdf' target='_blank'>https://arxiv.org/pdf/2405.03803.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Massimiliano Pappa, Luca Collorone, Giovanni Ficarra, Indro Spinelli, Fabio Galasso
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.03803">MoDiPO: text-to-motion alignment via AI-feedback-driven Direct Preference Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion Models have revolutionized the field of human motion generation by offering exceptional generation quality and fine-grained controllability through natural language conditioning. Their inherent stochasticity, that is the ability to generate various outputs from a single input, is key to their success. However, this diversity should not be unrestricted, as it may lead to unlikely generations. Instead, it should be confined within the boundaries of text-aligned and realistic generations. To address this issue, we propose MoDiPO (Motion Diffusion DPO), a novel methodology that leverages Direct Preference Optimization (DPO) to align text-to-motion models. We streamline the laborious and expensive process of gathering human preferences needed in DPO by leveraging AI feedback instead. This enables us to experiment with novel DPO strategies, using both online and offline generated motion-preference pairs. To foster future research we contribute with a motion-preference dataset which we dub Pick-a-Move. We demonstrate, both qualitatively and quantitatively, that our proposed method yields significantly more realistic motions. In particular, MoDiPO substantially improves Frechet Inception Distance (FID) while retaining the same RPrecision and Multi-Modality performances.
<div id='section'>Paperid: <span id='pid'>981, <a href='https://arxiv.org/pdf/2404.19531.pdf' target='_blank'>https://arxiv.org/pdf/2404.19531.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Norman Mu, Jingwei Ji, Zhenpei Yang, Nate Harada, Haotian Tang, Kan Chen, Charles R. Qi, Runzhou Ge, Kratarth Goel, Zoey Yang, Scott Ettinger, Rami Al-Rfou, Dragomir Anguelov, Yin Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.19531">MoST: Multi-modality Scene Tokenization for Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Many existing motion prediction approaches rely on symbolic perception outputs to generate agent trajectories, such as bounding boxes, road graph information and traffic lights. This symbolic representation is a high-level abstraction of the real world, which may render the motion prediction model vulnerable to perception errors (e.g., failures in detecting open-vocabulary obstacles) while missing salient information from the scene context (e.g., poor road conditions). An alternative paradigm is end-to-end learning from raw sensors. However, this approach suffers from the lack of interpretability and requires significantly more training resources. In this work, we propose tokenizing the visual world into a compact set of scene elements and then leveraging pre-trained image foundation models and LiDAR neural networks to encode all the scene elements in an open-vocabulary manner. The image foundation model enables our scene tokens to encode the general knowledge of the open world while the LiDAR neural network encodes geometry information. Our proposed representation can efficiently encode the multi-frame multi-modality observations with a few hundred tokens and is compatible with most transformer-based architectures. To evaluate our method, we have augmented Waymo Open Motion Dataset with camera embeddings. Experiments over Waymo Open Motion Dataset show that our approach leads to significant performance improvements over the state-of-the-art.
<div id='section'>Paperid: <span id='pid'>982, <a href='https://arxiv.org/pdf/2404.06171.pdf' target='_blank'>https://arxiv.org/pdf/2404.06171.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Oxana Shamilyan, Ievgen Kabin, Zoya Dyka, Oleksandr Sudakov, Andrii Cherninskyi, Marcin Brzozowski, Peter Langendoerfer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.06171">Intelligence and Motion Models of Continuum Robots: an Overview</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Many technical solutions are bio-inspired. Octopus-inspired robotic arms belong to continuum robots which are used in minimally invasive surgery or for technical system restoration in areas difficult-toaccess. Continuum robot missions are bounded with their motions, whereby the motion of the robots is controlled by humans via wireless communication. In case of a lost connection, robot autonomy is required. Distributed control and distributed decision-making mechanisms based on artificial intelligence approaches can be a promising solution to achieve autonomy of technical systems and to increase their resilience. However these methods are not well investigated yet. Octopuses are the living example of natural distributed intelligence but their learning and decision-making mechanisms are also not fully investigated and understood yet. Our major interest is investigating mechanisms of Distributed Artificial Intelligence as a basis for improving resilience of complex systems. We decided to use a physical continuum robot prototype that is able to perform some basic movements for our research. The idea is to research how a technical system can be empowered to combine movements into sequences of motions by itself. For the experimental investigations a suitable physical prototype has to be selected, its motion control has to be implemented and automated. In this paper, we give an overview combining different fields of research, such as Distributed Artificial Intelligence and continuum robots based on 98 publications. We provide a detailed description of the basic motion control models of continuum robots based on the literature reviewed, discuss different aspects of autonomy and give an overview of physical prototypes of continuum robots.
<div id='section'>Paperid: <span id='pid'>983, <a href='https://arxiv.org/pdf/2403.09805.pdf' target='_blank'>https://arxiv.org/pdf/2403.09805.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Md Salman Shamil, Dibyadip Chatterjee, Fadime Sener, Shugao Ma, Angela Yao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.09805">On the Utility of 3D Hand Poses for Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D hand pose is an underexplored modality for action recognition. Poses are compact yet informative and can greatly benefit applications with limited compute budgets. However, poses alone offer an incomplete understanding of actions, as they cannot fully capture objects and environments with which humans interact. We propose HandFormer, a novel multimodal transformer, to efficiently model hand-object interactions. HandFormer combines 3D hand poses at a high temporal resolution for fine-grained motion modeling with sparsely sampled RGB frames for encoding scene semantics. Observing the unique characteristics of hand poses, we temporally factorize hand modeling and represent each joint by its short-term trajectories. This factorized pose representation combined with sparse RGB samples is remarkably efficient and highly accurate. Unimodal HandFormer with only hand poses outperforms existing skeleton-based methods at 5x fewer FLOPs. With RGB, we achieve new state-of-the-art performance on Assembly101 and H2O with significant improvements in egocentric action recognition.
<div id='section'>Paperid: <span id='pid'>984, <a href='https://arxiv.org/pdf/2402.02624.pdf' target='_blank'>https://arxiv.org/pdf/2402.02624.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Baha Zarrouki, Marios Spanakakis, Johannes Betz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.02624">A Safe Reinforcement Learning driven Weights-varying Model Predictive Control for Autonomous Vehicle Motion Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Determining the optimal cost function parameters of Model Predictive Control (MPC) to optimize multiple control objectives is a challenging and time-consuming task. Multiobjective Bayesian Optimization (BO) techniques solve this problem by determining a Pareto optimal parameter set for an MPC with static weights. However, a single parameter set may not deliver the most optimal closed-loop control performance when the context of the MPC operating conditions changes during its operation, urging the need to adapt the cost function weights at runtime. Deep Reinforcement Learning (RL) algorithms can automatically learn context-dependent optimal parameter sets and dynamically adapt for a Weightsvarying MPC (WMPC). However, learning cost function weights from scratch in a continuous action space may lead to unsafe operating states. To solve this, we propose a novel approach limiting the RL actions within a safe learning space representing a catalog of pre-optimized BO Pareto-optimal weight sets. We conceive a RL agent not to learn in a continuous space but to proactively anticipate upcoming control tasks and to choose the most optimal discrete actions, each corresponding to a single set of Pareto optimal weights, context-dependent. Hence, even an untrained RL agent guarantees a safe and optimal performance. Experimental results demonstrate that an untrained RL-WMPC shows Pareto-optimal closed-loop behavior and training the RL-WMPC helps exhibit a performance beyond the Pareto-front.
<div id='section'>Paperid: <span id='pid'>985, <a href='https://arxiv.org/pdf/2401.13441.pdf' target='_blank'>https://arxiv.org/pdf/2401.13441.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maximilian Stölzle, Sonal Santosh Baberwal, Daniela Rus, Shirley Coyle, Cosimo Della Santina
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.13441">Guiding Soft Robots with Motor-Imagery Brain Signals and Impedance Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Integrating Brain-Machine Interfaces into non-clinical applications like robot motion control remains difficult - despite remarkable advancements in clinical settings. Specifically, EEG-based motor imagery systems are still error-prone, posing safety risks when rigid robots operate near humans. This work presents an alternative pathway towards safe and effective operation by combining wearable EEG with physically embodied safety in soft robots. We introduce and test a pipeline that allows a user to move a soft robot's end effector in real time via brain waves that are measured by as few as three EEG channels. A robust motor imagery algorithm interprets the user's intentions to move the position of a virtual attractor to which the end effector is attracted, thanks to a new Cartesian impedance controller. We specifically focus here on planar soft robot-based architected metamaterials, which require the development of a novel control architecture to deal with the peculiar nonlinearities - e.g., non-affinity in control. We preliminarily but quantitatively evaluate the approach on the task of setpoint regulation. We observe that the user reaches the proximity of the setpoint in 66% of steps and that for successful steps, the average response time is 21.5s. We also demonstrate the execution of simple real-world tasks involving interaction with the environment, which would be extremely hard to perform if it were not for the robot's softness.
<div id='section'>Paperid: <span id='pid'>986, <a href='https://arxiv.org/pdf/2401.03599.pdf' target='_blank'>https://arxiv.org/pdf/2401.03599.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Victoria M. Dax, Jiachen Li, Enna Sachdeva, Nakul Agarwal, Mykel J. Kochenderfer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.03599">Disentangled Neural Relational Inference for Interpretable Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Effective interaction modeling and behavior prediction of dynamic agents play a significant role in interactive motion planning for autonomous robots. Although existing methods have improved prediction accuracy, few research efforts have been devoted to enhancing prediction model interpretability and out-of-distribution (OOD) generalizability. This work addresses these two challenging aspects by designing a variational auto-encoder framework that integrates graph-based representations and time-sequence models to efficiently capture spatio-temporal relations between interactive agents and predict their dynamics. Our model infers dynamic interaction graphs in a latent space augmented with interpretable edge features that characterize the interactions. Moreover, we aim to enhance model interpretability and performance in OOD scenarios by disentangling the latent space of edge features, thereby strengthening model versatility and robustness. We validate our approach through extensive experiments on both simulated and real-world datasets. The results show superior performance compared to existing methods in modeling spatio-temporal relations, motion prediction, and identifying time-invariant latent features.
<div id='section'>Paperid: <span id='pid'>987, <a href='https://arxiv.org/pdf/2312.15881.pdf' target='_blank'>https://arxiv.org/pdf/2312.15881.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yao Liu, Binghao Li, Xianzhi Wang, Claude Sammut, Lina Yao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.15881">Attention-aware Social Graph Transformer Networks for Stochastic Trajectory Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Trajectory prediction is fundamental to various intelligent technologies, such as autonomous driving and robotics. The motion prediction of pedestrians and vehicles helps emergency braking, reduces collisions, and improves traffic safety. Current trajectory prediction research faces problems of complex social interactions, high dynamics and multi-modality. Especially, it still has limitations in long-time prediction. We propose Attention-aware Social Graph Transformer Networks for multi-modal trajectory prediction. We combine Graph Convolutional Networks and Transformer Networks by generating stable resolution pseudo-images from Spatio-temporal graphs through a designed stacking and interception method. Furthermore, we design the attention-aware module to handle social interaction information in scenarios involving mixed pedestrian-vehicle traffic. Thus, we maintain the advantages of the Graph and Transformer, i.e., the ability to aggregate information over an arbitrary number of neighbors and the ability to perform complex time-dependent data processing. We conduct experiments on datasets involving pedestrian, vehicle, and mixed trajectories, respectively. Our results demonstrate that our model minimizes displacement errors across various metrics and significantly reduces the likelihood of collisions. It is worth noting that our model effectively reduces the final displacement error, illustrating the ability of our model to predict for a long time.
<div id='section'>Paperid: <span id='pid'>988, <a href='https://arxiv.org/pdf/2312.06653.pdf' target='_blank'>https://arxiv.org/pdf/2312.06653.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Neerja Thakkar, Karttikeya Mangalam, Andrea Bajcsy, Jitendra Malik
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.06653">Adaptive Human Trajectory Prediction via Latent Corridors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human trajectory prediction is typically posed as a zero-shot generalization problem: a predictor is learnt on a dataset of human motion in training scenes, and then deployed on unseen test scenes. While this paradigm has yielded tremendous progress, it fundamentally assumes that trends in human behavior within the deployment scene are constant over time. As such, current prediction models are unable to adapt to scene-specific transient human behaviors, such as crowds temporarily gathering to see buskers, pedestrians hurrying through the rain and avoiding puddles, or a protest breaking out. We formalize the problem of scene-specific adaptive trajectory prediction and propose a new adaptation approach inspired by prompt tuning called latent corridors. By augmenting the input of any pre-trained human trajectory predictor with learnable image prompts, the predictor can improve in the deployment scene by inferring trends from extremely small amounts of new data (e.g., 2 humans observed for 30 seconds). With less than 0.1% additional model parameters, we see up to 23.9% ADE improvement in MOTSynth simulated data and 16.4% ADE in MOT and Wildtrack real pedestrian data. Qualitatively, we observe that latent corridors imbue predictors with an awareness of scene geometry and scene-specific human behaviors that non-adaptive predictors struggle to capture. The project website can be found at https://neerja.me/atp_latent_corridors/.
<div id='section'>Paperid: <span id='pid'>989, <a href='https://arxiv.org/pdf/2312.06571.pdf' target='_blank'>https://arxiv.org/pdf/2312.06571.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Takahide Yoshida, Atsushi Masumori, Takashi Ikegami
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.06571">From Text to Motion: Grounding GPT-4 in a Humanoid Robot "Alter3"</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We report the development of Alter3, a humanoid robot capable of generating spontaneous motion using a Large Language Model (LLM), specifically GPT-4. This achievement was realized by integrating GPT-4 into our proprietary android, Alter3, thereby effectively grounding the LLM with Alter's bodily movement. Typically, low-level robot control is hardware-dependent and falls outside the scope of LLM corpora, presenting challenges for direct LLM-based robot control. However, in the case of humanoid robots like Alter3, direct control is feasible by mapping the linguistic expressions of human actions onto the robot's body through program code. Remarkably, this approach enables Alter3 to adopt various poses, such as a 'selfie' stance or 'pretending to be a ghost,' and generate sequences of actions over time without explicit programming for each body part. This demonstrates the robot's zero-shot learning capabilities. Additionally, verbal feedback can adjust poses, obviating the need for fine-tuning. A video of Alter3's generated motions is available at https://tnoinkwms.github.io/ALTER-LLM/
<div id='section'>Paperid: <span id='pid'>990, <a href='https://arxiv.org/pdf/2311.06420.pdf' target='_blank'>https://arxiv.org/pdf/2311.06420.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Baha Zarrouki, JoÃ£o Nunes, Johannes Betz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.06420">R$^2$NMPC: A Real-Time Reduced Robustified Nonlinear Model Predictive Control with Ellipsoidal Uncertainty Sets for Autonomous Vehicle Motion Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we present a novel Reduced Robustified NMPC (R$^2$NMPC) algorithm that has the same complexity as an equivalent nominal NMPC while enhancing it with robustified constraints based on the dynamics of ellipsoidal uncertainty sets. This promises both a closed-loop- and constraint satisfaction performance equivalent to common Robustified NMPC approaches, while drastically reducing the computational complexity. The main idea lies in approximating the ellipsoidal uncertainty sets propagation over the prediction horizon with the system dynamics' sensitivities inferred from the last optimal control problem (OCP) solution, and similarly for the gradients to robustify the constraints. Thus, we do not require the decision variables related to the uncertainty propagation within the OCP, rendering it computationally tractable. Next, we illustrate the real-time control capabilities of our algorithm in handling a complex, high-dimensional, and highly nonlinear system, namely the trajectory following of an autonomous passenger vehicle modeled with a dynamic nonlinear single-track model. Our experimental findings, alongside a comparative assessment against other Robust NMPC approaches, affirm the robustness of our method in effectively tracking an optimal racetrack trajectory while satisfying the nonlinear constraints. This performance is achieved while fully utilizing the vehicle's interface limits, even at high speeds of up to 37.5m/s, and successfully managing state estimation disturbances. Remarkably, our approach maintains a mean solving frequency of 144Hz.
<div id='section'>Paperid: <span id='pid'>991, <a href='https://arxiv.org/pdf/2311.04303.pdf' target='_blank'>https://arxiv.org/pdf/2311.04303.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Baha Zarrouki, Chenyang Wang, Johannes Betz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.04303">Adaptive Stochastic Nonlinear Model Predictive Control with Look-ahead Deep Reinforcement Learning for Autonomous Vehicle Motion Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we present a Deep Reinforcement Learning (RL)-driven Adaptive Stochastic Nonlinear Model Predictive Control (SNMPC) to optimize uncertainty handling, constraints robustification, feasibility, and closed-loop performance. To this end, we conceive an RL agent to proactively anticipate upcoming control tasks and to dynamically determine the most suitable combination of key SNMPC parameters - foremost the robustification factor $Îº$ and the Uncertainty Propagation Horizon (UPH) $T_u$. We analyze the trained RL agent's decision-making process and highlight its ability to learn context-dependent optimal parameters. One key finding is that adapting the constraints robustification factor with the learned policy reduces conservatism and improves closed-loop performance while adapting UPH renders previously infeasible SNMPC problems feasible when faced with severe disturbances. We showcase the enhanced robustness and feasibility of our Adaptive SNMPC (aSNMPC) through the real-time motion control task of an autonomous passenger vehicle to follow an optimal race line when confronted with significant time-variant disturbances. Experimental findings demonstrate that our look-ahead RL-driven aSNMPC outperforms its Static SNMPC (sSNMPC) counterpart in minimizing the lateral deviation both with accurate and inaccurate disturbance assumptions and even when driving in previously unexplored environments.
<div id='section'>Paperid: <span id='pid'>992, <a href='https://arxiv.org/pdf/2310.18753.pdf' target='_blank'>https://arxiv.org/pdf/2310.18753.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Baha Zarrouki, Chenyang Wang, Johannes Betz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.18753">A Stochastic Nonlinear Model Predictive Control with an Uncertainty Propagation Horizon for Autonomous Vehicle Motion Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Employing Stochastic Nonlinear Model Predictive Control (SNMPC) for real-time applications is challenging due to the complex task of propagating uncertainties through nonlinear systems. This difficulty becomes more pronounced in high-dimensional systems with extended prediction horizons, such as autonomous vehicles. To enhance closed-loop performance in and feasibility in SNMPCs, we introduce the concept of the Uncertainty Propagation Horizon (UPH). The UPH limits the time for uncertainty propagation through system dynamics, preventing trajectory divergence, optimizing feedback loop advantages, and reducing computational overhead. Our SNMPC approach utilizes Polynomial Chaos Expansion (PCE) to propagate uncertainties and incorporates nonlinear hard constraints on state expectations and nonlinear probabilistic constraints. We transform the probabilistic constraints into deterministic constraints by estimating the nonlinear constraints' expectation and variance. We then showcase our algorithm's effectiveness in real-time control of a high-dimensional, highly nonlinear system-the trajectory following of an autonomous passenger vehicle, modeled with a dynamic nonlinear single-track model. Experimental results demonstrate our approach's robust capability to follow an optimal racetrack trajectory at speeds of up to 37.5m/s while dealing with state estimation disturbances, achieving a minimum solving frequency of 97Hz. Additionally, our experiments illustrate that limiting the UPH renders previously infeasible SNMPC problems feasible, even when incorrect uncertainty assumptions or strong disturbances are present.
<div id='section'>Paperid: <span id='pid'>993, <a href='https://arxiv.org/pdf/2310.10029.pdf' target='_blank'>https://arxiv.org/pdf/2310.10029.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xin Zhang, Pietro Balatti, Mattia Leonori, Arash Ajoudani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.10029">A Human Motion Compensation Framework for a Supernumerary Robotic Arm</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Supernumerary robotic arms (SRAs) can be used as the third arm to complement and augment the abilities of human users. The user carrying a SRA forms a connected kinodynamic chain, which can be viewed as a special class of floating-base robot systems. However, unlike general floating-base robot systems, human users are the bases of SRAs and they have their subjective behaviors/motions. This implies that human body motions can unintentionally affect the SRA's end-effector movements. To address this challenge, we propose a framework to compensate for the human whole-body motions that interfere with the SRA's end-effector trajectories. The SRA system in this study consists of a 6-degree-of-freedom lightweight arm and a wearable interface. The wearable interface allows users to adjust the installation position of the SRA to fit different body shapes. An inertial measurement unit (IMU)-based sensory interface can provide the body skeleton motion feedback of the human user in real time. By simplifying the floating-base kinematics model, we design an effective motion planner by reconstructing the Jacobian matrix of the SRA. Under the proposed framework, the performance of the reconstructed Jacobian method is assessed by comparing the results obtained with the classical nullspace-based method through two sets of experiments.
<div id='section'>Paperid: <span id='pid'>994, <a href='https://arxiv.org/pdf/2309.05455.pdf' target='_blank'>https://arxiv.org/pdf/2309.05455.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anna Deichler, Shivam Mehta, Simon Alexanderson, Jonas Beskow
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.05455">Diffusion-Based Co-Speech Gesture Generation Using Joint Text and Audio Representation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper describes a system developed for the GENEA (Generation and Evaluation of Non-verbal Behaviour for Embodied Agents) Challenge 2023. Our solution builds on an existing diffusion-based motion synthesis model. We propose a contrastive speech and motion pretraining (CSMP) module, which learns a joint embedding for speech and gesture with the aim to learn a semantic coupling between these modalities. The output of the CSMP module is used as a conditioning signal in the diffusion-based gesture synthesis model in order to achieve semantically-aware co-speech gesture generation. Our entry achieved highest human-likeness and highest speech appropriateness rating among the submitted entries. This indicates that our system is a promising approach to achieve human-like co-speech gestures in agents that carry semantic meaning.
<div id='section'>Paperid: <span id='pid'>995, <a href='https://arxiv.org/pdf/2309.04750.pdf' target='_blank'>https://arxiv.org/pdf/2309.04750.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Daniel Ajisafe, James Tang, Shih-Yang Su, Bastian Wandt, Helge Rhodin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.04750">Mirror-Aware Neural Humans</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion capture either requires multi-camera systems or is unreliable when using single-view input due to depth ambiguities. Meanwhile, mirrors are readily available in urban environments and form an affordable alternative by recording two views with only a single camera. However, the mirror setting poses the additional challenge of handling occlusions of real and mirror image. Going beyond existing mirror approaches for 3D human pose estimation, we utilize mirrors for learning a complete body model, including shape and dense appearance. Our main contributions are extending articulated neural radiance fields to include a notion of a mirror, making it sample-efficient over potential occlusion regions. Together, our contributions realize a consumer-level 3D motion capture system that starts from off-the-shelf 2D poses by automatically calibrating the camera, estimating mirror orientation, and subsequently lifting 2D keypoint detections to 3D skeleton pose that is used to condition the mirror-aware NeRF. We empirically demonstrate the benefit of learning a body model and accounting for occlusion in challenging mirror scenes.
<div id='section'>Paperid: <span id='pid'>996, <a href='https://arxiv.org/pdf/2308.14480.pdf' target='_blank'>https://arxiv.org/pdf/2308.14480.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hanyang Kong, Kehong Gong, Dongze Lian, Michael Bi Mi, Xinchao Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.14480">Priority-Centric Human Motion Generation in Discrete Latent Space</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-to-motion generation is a formidable task, aiming to produce human motions that align with the input text while also adhering to human capabilities and physical laws. While there have been advancements in diffusion models, their application in discrete spaces remains underexplored. Current methods often overlook the varying significance of different motions, treating them uniformly. It is essential to recognize that not all motions hold the same relevance to a particular textual description. Some motions, being more salient and informative, should be given precedence during generation. In response, we introduce a Priority-Centric Motion Discrete Diffusion Model (M2DM), which utilizes a Transformer-based VQ-VAE to derive a concise, discrete motion representation, incorporating a global self-attention mechanism and a regularization term to counteract code collapse. We also present a motion discrete diffusion model that employs an innovative noise schedule, determined by the significance of each motion token within the entire motion sequence. This approach retains the most salient motions during the reverse diffusion process, leading to more semantically rich and varied motions. Additionally, we formulate two strategies to gauge the importance of motion tokens, drawing from both textual and visual indicators. Comprehensive experiments on the HumanML3D and KIT-ML datasets confirm that our model surpasses existing techniques in fidelity and diversity, particularly for intricate textual descriptions.
<div id='section'>Paperid: <span id='pid'>997, <a href='https://arxiv.org/pdf/2308.11951.pdf' target='_blank'>https://arxiv.org/pdf/2308.11951.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chunjin Song, Bastian Wandt, Helge Rhodin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.11951">Pose Modulated Avatars from Video</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>It is now possible to reconstruct dynamic human motion and shape from a sparse set of cameras using Neural Radiance Fields (NeRF) driven by an underlying skeleton. However, a challenge remains to model the deformation of cloth and skin in relation to skeleton pose. Unlike existing avatar models that are learned implicitly or rely on a proxy surface, our approach is motivated by the observation that different poses necessitate unique frequency assignments. Neglecting this distinction yields noisy artifacts in smooth areas or blurs fine-grained texture and shape details in sharp regions. We develop a two-branch neural network that is adaptive and explicit in the frequency domain. The first branch is a graph neural network that models correlations among body parts locally, taking skeleton pose as input. The second branch combines these correlation features to a set of global frequencies and then modulates the feature encoding. Our experiments demonstrate that our network outperforms state-of-the-art methods in terms of preserving details and generalization capabilities.
<div id='section'>Paperid: <span id='pid'>998, <a href='https://arxiv.org/pdf/2308.09507.pdf' target='_blank'>https://arxiv.org/pdf/2308.09507.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jon Arrizabalaga, Markus Ryll
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.09507">Pose-Following with Dual Quaternions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work focuses on pose-following, a variant of path-following in which the goal is to steer the system's position and attitude along a path with a moving frame attached to it. Full body motion control, while accounting for the additional freedom to self-regulate the progress along the path, is an appealing trade-off. Towards this end, we extend the well-established dual quaternion-based pose-tracking method into a pose-following control law. Specifically, we derive the equations of motion for the full pose error between the geometric reference and the rigid body in the form of a dual quaternion and dual twist. Subsequently, we formulate an almost globally asymptotically stable control law. The global attractivity of the presented approach is validated in a spatial example, while its benefits over pose-tracking are showcased through a planar case-study.
<div id='section'>Paperid: <span id='pid'>999, <a href='https://arxiv.org/pdf/2308.08380.pdf' target='_blank'>https://arxiv.org/pdf/2308.08380.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaxin Pan, Changyao Zhou, Mariia Gladkova, Qadeer Khan, Daniel Cremers
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.08380">Robust Autonomous Vehicle Pursuit without Expert Steering Labels</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we present a learning method for lateral and longitudinal motion control of an ego-vehicle for vehicle pursuit. The car being controlled does not have a pre-defined route, rather it reactively adapts to follow a target vehicle while maintaining a safety distance. To train our model, we do not rely on steering labels recorded from an expert driver but effectively leverage a classical controller as an offline label generation tool. In addition, we account for the errors in the predicted control values, which can lead to a loss of tracking and catastrophic crashes of the controlled vehicle. To this end, we propose an effective data augmentation approach, which allows to train a network capable of handling different views of the target vehicle. During the pursuit, the target vehicle is firstly localized using a Convolutional Neural Network. The network takes a single RGB image along with cars' velocities and estimates the target vehicle's pose with respect to the ego-vehicle. This information is then fed to a Multi-Layer Perceptron, which regresses the control commands for the ego-vehicle, namely throttle and steering angle. We extensively validate our approach using the CARLA simulator on a wide range of terrains. Our method demonstrates real-time performance and robustness to different scenarios including unseen trajectories and high route completion. The project page containing code and multimedia can be publicly accessed here: https://changyaozhou.github.io/Autonomous-Vehicle-Pursuit/.
<div id='section'>Paperid: <span id='pid'>1000, <a href='https://arxiv.org/pdf/2308.06503.pdf' target='_blank'>https://arxiv.org/pdf/2308.06503.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zeming Zhuang, Dingzhu Wen, Yuanming Shi, Guangxu Zhu, Sheng Wu, Dusit Niyato
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.06503">Integrated Sensing-Communication-Computation for Over-the-Air Edge AI Inference</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Edge-device co-inference refers to deploying well-trained artificial intelligent (AI) models at the network edge under the cooperation of devices and edge servers for providing ambient intelligent services. For enhancing the utilization of limited network resources in edge-device co-inference tasks from a systematic view, we propose a task-oriented scheme of integrated sensing, computation and communication (ISCC) in this work. In this system, all devices sense a target from the same wide view to obtain homogeneous noise-corrupted sensory data, from which the local feature vectors are extracted. All local feature vectors are aggregated at the server using over-the-air computation (AirComp) in a broadband channel with the orthogonal-frequency-division-multiplexing technique for suppressing the sensing and channel noise. The aggregated denoised global feature vector is further input to a server-side AI model for completing the downstream inference task. A novel task-oriented design criterion, called maximum minimum pair-wise discriminant gain, is adopted for classification tasks. It extends the distance of the closest class pair in the feature space, leading to a balanced and enhanced inference accuracy. Under this criterion, a problem of joint sensing power assignment, transmit precoding and receive beamforming is formulated. The challenge lies in three aspects: the coupling between sensing and AirComp, the joint optimization of all feature dimensions' AirComp aggregation over a broadband channel, and the complicated form of the maximum minimum pair-wise discriminant gain. To solve this problem, a task-oriented ISCC scheme with AirComp is proposed. Experiments based on a human motion recognition task are conducted to verify the advantages of the proposed scheme over the existing scheme and a baseline.
<div id='section'>Paperid: <span id='pid'>1001, <a href='https://arxiv.org/pdf/2307.10743.pdf' target='_blank'>https://arxiv.org/pdf/2307.10743.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Paolo Franceschi, Fabio Bertini, Francesco Braghin, Loris Roveda, Nicola Pedrocchi, Manuel Beschi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.10743">Predicting human motion intention for pHRI assistive control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work addresses human intention identification during physical Human-Robot Interaction (pHRI) tasks to include this information in an assistive controller. To this purpose, human intention is defined as the desired trajectory that the human wants to follow over a finite rolling prediction horizon so that the robot can assist in pursuing it. This work investigates a Recurrent Neural Network (RNN), specifically, Long-Short Term Memory (LSTM) cascaded with a Fully Connected layer. In particular, we propose an iterative training procedure to adapt the model. Such an iterative procedure is powerful in reducing the prediction error. Still, it has the drawback that it is time-consuming and does not generalize to different users or different co-manipulated objects. To overcome this issue, Transfer Learning (TL) adapts the pre-trained model to new trajectories, users, and co-manipulated objects by freezing the LSTM layer and fine-tuning the last FC layer, which makes the procedure faster. Experiments show that the iterative procedure adapts the model and reduces prediction error. Experiments also show that TL adapts to different users and to the co-manipulation of a large object. Finally, to check the utility of adopting the proposed method, we compare the proposed controller enhanced by the intention prediction with the other two standard controllers of pHRI.
<div id='section'>Paperid: <span id='pid'>1002, <a href='https://arxiv.org/pdf/2307.02858.pdf' target='_blank'>https://arxiv.org/pdf/2307.02858.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Usman Muhammad, Md Ziaul Hoque, Mourad Oussalah, Jorma Laaksonen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.02858">Deep Ensemble Learning with Frame Skipping for Face Anti-Spoofing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Face presentation attacks (PA), also known as spoofing attacks, pose a substantial threat to biometric systems that rely on facial recognition systems, such as access control systems, mobile payments, and identity verification systems. To mitigate the spoofing risk, several video-based methods have been presented in the literature that analyze facial motion in successive video frames. However, estimating the motion between adjacent frames is a challenging task and requires high computational cost. In this paper, we rephrase the face anti-spoofing task as a motion prediction problem and introduce a deep ensemble learning model with a frame skipping mechanism. In particular, the proposed frame skipping adopts a uniform sampling approach by dividing the original video into video clips of fixed size. By doing so, every nth frame of the clip is selected to ensure that the temporal patterns can easily be perceived during the training of three different recurrent neural networks (RNNs). Motivated by the performance of individual RNNs, a meta-model is developed to improve the overall detection performance by combining the prediction of individual RNNs. Extensive experiments were performed on four datasets, and state-of-the-art performance is reported on MSU-MFSD (3.12%), Replay-Attack (11.19%), and OULU-NPU (12.23%) databases by using half total error rates (HTERs) in the most challenging cross-dataset testing scenario.
<div id='section'>Paperid: <span id='pid'>1003, <a href='https://arxiv.org/pdf/2306.05846.pdf' target='_blank'>https://arxiv.org/pdf/2306.05846.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>GuÃ©nolÃ© Fiche, Simon Leglaive, Xavier Alameda-Pineda, Renaud SÃ©guier
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.05846">Motion-DVAE: Unsupervised learning for fast human motion denoising</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pose and motion priors are crucial for recovering realistic and accurate human motion from noisy observations. Substantial progress has been made on pose and shape estimation from images, and recent works showed impressive results using priors to refine frame-wise predictions. However, a lot of motion priors only model transitions between consecutive poses and are used in time-consuming optimization procedures, which is problematic for many applications requiring real-time motion capture. We introduce Motion-DVAE, a motion prior to capture the short-term dependencies of human motion. As part of the dynamical variational autoencoder (DVAE) models family, Motion-DVAE combines the generative capability of VAE models and the temporal modeling of recurrent architectures. Together with Motion-DVAE, we introduce an unsupervised learned denoising method unifying regression- and optimization-based approaches in a single framework for real-time 3D human pose estimation. Experiments show that the proposed approach reaches competitive performance with state-of-the-art methods while being much faster.
<div id='section'>Paperid: <span id='pid'>1004, <a href='https://arxiv.org/pdf/2306.03988.pdf' target='_blank'>https://arxiv.org/pdf/2306.03988.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aram Davtyan, Paolo Favaro
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.03988">Learn the Force We Can: Enabling Sparse Motion Control in Multi-Object Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a novel unsupervised method to autoregressively generate videos from a single frame and a sparse motion input. Our trained model can generate unseen realistic object-to-object interactions. Although our model has never been given the explicit segmentation and motion of each object in the scene during training, it is able to implicitly separate their dynamics and extents. Key components in our method are the randomized conditioning scheme, the encoding of the input motion control, and the randomized and sparse sampling to enable generalization to out of distribution but realistic correlations. Our model, which we call YODA, has therefore the ability to move objects without physically touching them. Through extensive qualitative and quantitative evaluations on several datasets, we show that YODA is on par with or better than state of the art video generation prior work in terms of both controllability and video quality.
<div id='section'>Paperid: <span id='pid'>1005, <a href='https://arxiv.org/pdf/2306.03083.pdf' target='_blank'>https://arxiv.org/pdf/2306.03083.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chiyu Max Jiang, Andre Cornman, Cheolho Park, Ben Sapp, Yin Zhou, Dragomir Anguelov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.03083">MotionDiffuser: Controllable Multi-Agent Motion Prediction using Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present MotionDiffuser, a diffusion based representation for the joint distribution of future trajectories over multiple agents. Such representation has several key advantages: first, our model learns a highly multimodal distribution that captures diverse future outcomes. Second, the simple predictor design requires only a single L2 loss training objective, and does not depend on trajectory anchors. Third, our model is capable of learning the joint distribution for the motion of multiple agents in a permutation-invariant manner. Furthermore, we utilize a compressed trajectory representation via PCA, which improves model performance and allows for efficient computation of the exact sample log probability. Subsequently, we propose a general constrained sampling framework that enables controlled trajectory sampling based on differentiable cost functions. This strategy enables a host of applications such as enforcing rules and physical priors, or creating tailored simulation scenarios. MotionDiffuser can be combined with existing backbone architectures to achieve top motion forecasting results. We obtain state-of-the-art results for multi-agent motion prediction on the Waymo Open Motion Dataset.
<div id='section'>Paperid: <span id='pid'>1006, <a href='https://arxiv.org/pdf/2306.02990.pdf' target='_blank'>https://arxiv.org/pdf/2306.02990.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yao Tang, Guangxu Zhu, Wei Xu, Man Hon Cheung, Tat-Ming Lok, Shuguang Cui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.02990">Integrated Sensing, Computation, and Communication for UAV-assisted Federated Edge Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Federated edge learning (FEEL) enables privacy-preserving model training through periodic communication between edge devices and the server. Unmanned Aerial Vehicle (UAV)-mounted edge devices are particularly advantageous for FEEL due to their flexibility and mobility in efficient data collection. In UAV-assisted FEEL, sensing, computation, and communication are coupled and compete for limited onboard resources, and UAV deployment also affects sensing and communication performance. Therefore, the joint design of UAV deployment and resource allocation is crucial to achieving the optimal training performance. In this paper, we address the problem of joint UAV deployment design and resource allocation for FEEL via a concrete case study of human motion recognition based on wireless sensing. We first analyze the impact of UAV deployment on the sensing quality and identify a threshold value for the sensing elevation angle that guarantees a satisfactory quality of data samples. Due to the non-ideal sensing channels, we consider the probabilistic sensing model, where the successful sensing probability of each UAV is determined by its position. Then, we derive the upper bound of the FEEL training loss as a function of the sensing probability. Theoretical results suggest that the convergence rate can be improved if UAVs have a uniform successful sensing probability. Based on this analysis, we formulate a training time minimization problem by jointly optimizing UAV deployment, integrated sensing, computation, and communication (ISCC) resources under a desirable optimality gap constraint. To solve this challenging mixed-integer non-convex problem, we apply the alternating optimization technique, and propose the bandwidth, batch size, and position optimization (BBPO) scheme to optimize these three decision variables alternately.
<div id='section'>Paperid: <span id='pid'>1007, <a href='https://arxiv.org/pdf/2306.00559.pdf' target='_blank'>https://arxiv.org/pdf/2306.00559.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rishubh Parihar, Raghav Magazine, Piyush Tiwari, R. Venkatesh Babu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.00559">We never go out of Style: Motion Disentanglement by Subspace Decomposition of Latent Space</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Real-world objects perform complex motions that involve multiple independent motion components. For example, while talking, a person continuously changes their expressions, head, and body pose. In this work, we propose a novel method to decompose motion in videos by using a pretrained image GAN model. We discover disentangled motion subspaces in the latent space of widely used style-based GAN models that are semantically meaningful and control a single explainable motion component. The proposed method uses only a few $(\approx10)$ ground truth video sequences to obtain such subspaces. We extensively evaluate the disentanglement properties of motion subspaces on face and car datasets, quantitatively and qualitatively. Further, we present results for multiple downstream tasks such as motion editing, and selective motion transfer, e.g. transferring only facial expressions without training for it.
<div id='section'>Paperid: <span id='pid'>1008, <a href='https://arxiv.org/pdf/2305.16725.pdf' target='_blank'>https://arxiv.org/pdf/2305.16725.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ehsan Sabouni, H. M. Sabbir Ahmad, Christos G. Cassandras, Wenchao Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.16725">Merging control in mixed traffic with safety guarantees: a safe sequencing policy with optimal motion control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We address the problem of merging traffic from two roadways consisting of both Connected Autonomous Vehicles (CAVs) and Human Driven Vehicles (HDVs). Guaranteeing safe merging in such mixed traffic settings is challenging due to the unpredictability of possibly uncooperative HDVs. We develop a hierarchical controller where at each discrete time step first a coordinator determines the best possible Safe Sequence (SS) which can be realized without any knowledge of human driving behavior. Then, a lower-level decentralized motion controller for each CAV jointly minimizes travel time and energy over a prediction horizon, subject to hard safety constraints dependent on the given safe sequence. This is accomplished using a Model Predictive Controller (MPC) subject to constraints based on Control Barrier Functions (CBFs) which render it computationally efficient. Extensive simulation results are included showing that this hierarchical controller outperforms the commonly adopted Shortest Distance First (SDF) passing sequence over the full range of CAV penetration rates, while also providing safe merging guarantees.
<div id='section'>Paperid: <span id='pid'>1009, <a href='https://arxiv.org/pdf/2304.14404.pdf' target='_blank'>https://arxiv.org/pdf/2304.14404.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tsai-Shien Chen, Chieh Hubert Lin, Hung-Yu Tseng, Tsung-Yi Lin, Ming-Hsuan Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.14404">Motion-Conditioned Diffusion Model for Controllable Video Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in diffusion models have greatly improved the quality and diversity of synthesized content. To harness the expressive power of diffusion models, researchers have explored various controllable mechanisms that allow users to intuitively guide the content synthesis process. Although the latest efforts have primarily focused on video synthesis, there has been a lack of effective methods for controlling and describing desired content and motion. In response to this gap, we introduce MCDiff, a conditional diffusion model that generates a video from a starting image frame and a set of strokes, which allow users to specify the intended content and dynamics for synthesis. To tackle the ambiguity of sparse motion inputs and achieve better synthesis quality, MCDiff first utilizes a flow completion model to predict the dense video motion based on the semantic understanding of the video frame and the sparse motion control. Then, the diffusion model synthesizes high-quality future frames to form the output video. We qualitatively and quantitatively show that MCDiff achieves the state-the-of-art visual quality in stroke-guided controllable video synthesis. Additional experiments on MPII Human Pose further exhibit the capability of our model on diverse content and motion synthesis.
<div id='section'>Paperid: <span id='pid'>1010, <a href='https://arxiv.org/pdf/2304.14389.pdf' target='_blank'>https://arxiv.org/pdf/2304.14389.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>John Z. Zhang, Shuo Yang, Gengshan Yang, Arun L. Bishop, Deva Ramanan, Zachary Manchester
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.14389">SLoMo: A General System for Legged Robot Motion Imitation from Casual Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present SLoMo: a first-of-its-kind framework for transferring skilled motions from casually captured "in the wild" video footage of humans and animals to legged robots. SLoMo works in three stages: 1) synthesize a physically plausible reconstructed key-point trajectory from monocular videos; 2) optimize a dynamically feasible reference trajectory for the robot offline that includes body and foot motion, as well as contact sequences that closely tracks the key points; 3) track the reference trajectory online using a general-purpose model-predictive controller on robot hardware. Traditional motion imitation for legged motor skills often requires expert animators, collaborative demonstrations, and/or expensive motion capture equipment, all of which limits scalability. Instead, SLoMo only relies on easy-to-obtain monocular video footage, readily available in online repositories such as YouTube. It converts videos into motion primitives that can be executed reliably by real-world robots. We demonstrate our approach by transferring the motions of cats, dogs, and humans to example robots including a quadruped (on hardware) and a humanoid (in simulation). To the best knowledge of the authors, this is the first attempt at a general-purpose motion transfer framework that imitates animal and human motions on legged robots directly from casual videos without artificial markers or labels.
<div id='section'>Paperid: <span id='pid'>1011, <a href='https://arxiv.org/pdf/2304.02419.pdf' target='_blank'>https://arxiv.org/pdf/2304.02419.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kehong Gong, Dongze Lian, Heng Chang, Chuan Guo, Zihang Jiang, Xinxin Zuo, Michael Bi Mi, Xinchao Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.02419">TM2D: Bimodality Driven 3D Dance Generation via Music-Text Integration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a novel task for generating 3D dance movements that simultaneously incorporate both text and music modalities. Unlike existing works that generate dance movements using a single modality such as music, our goal is to produce richer dance movements guided by the instructive information provided by the text. However, the lack of paired motion data with both music and text modalities limits the ability to generate dance movements that integrate both. To alleviate this challenge, we propose to utilize a 3D human motion VQ-VAE to project the motions of the two datasets into a latent space consisting of quantized vectors, which effectively mix the motion tokens from the two datasets with different distributions for training. Additionally, we propose a cross-modal transformer to integrate text instructions into motion generation architecture for generating 3D dance movements without degrading the performance of music-conditioned dance generation. To better evaluate the quality of the generated motion, we introduce two novel metrics, namely Motion Prediction Distance (MPD) and Freezing Score (FS), to measure the coherence and freezing percentage of the generated motion. Extensive experiments show that our approach can generate realistic and coherent dance movements conditioned on both text and music while maintaining comparable performance with the two single modalities. Code is available at https://garfield-kh.github.io/TM2D/.
<div id='section'>Paperid: <span id='pid'>1012, <a href='https://arxiv.org/pdf/2303.10941.pdf' target='_blank'>https://arxiv.org/pdf/2303.10941.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoyu Wang, Shaoli Huang, Fang Zhao, Chun Yuan, Ying Shan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.10941">HMC: Hierarchical Mesh Coarsening for Skeleton-free Motion Retargeting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a simple yet effective method for skeleton-free motion retargeting. Previous methods transfer motion between high-resolution meshes, failing to preserve the inherent local-part motions in the mesh. Addressing this issue, our proposed method learns the correspondence in a coarse-to-fine fashion by integrating the retargeting process with a mesh-coarsening pipeline. First, we propose a mesh-coarsening module that coarsens the mesh representations for better motion transfer. This module improves the ability to handle small-part motion and preserves the local motion interdependence between neighboring mesh vertices. Furthermore, we leverage a hierarchical refinement procedure to complement missing mesh details by gradually improving the low-resolution mesh output with a higher-resolution one. We evaluate our method on several well-known 3D character datasets, and it yields an average improvement of 25% on point-wise mesh euclidean distance (PMD) against the start-of-art method. Moreover, our qualitative results show that our method is significantly helpful in preserving the moving consistency of different body parts on the target character due to disentangling body-part structures and mesh details in a hierarchical way.
<div id='section'>Paperid: <span id='pid'>1013, <a href='https://arxiv.org/pdf/2303.10677.pdf' target='_blank'>https://arxiv.org/pdf/2303.10677.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vishnu Pandi Chellapandi, Liangqi Yuan, Stanislaw H /. Zak, Ziran Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.10677">A Survey of Federated Learning for Connected and Automated Vehicles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Connected and Automated Vehicles (CAVs) are one of the emerging technologies in the automotive domain that has the potential to alleviate the issues of accidents, traffic congestion, and pollutant emissions, leading to a safe, efficient, and sustainable transportation system. Machine learning-based methods are widely used in CAVs for crucial tasks like perception, motion planning, and motion control, where machine learning models in CAVs are solely trained using the local vehicle data, and the performance is not certain when exposed to new environments or unseen conditions. Federated learning (FL) is an effective solution for CAVs that enables a collaborative model development with multiple vehicles in a distributed learning framework. FL enables CAVs to learn from a wide range of driving environments and improve their overall performance while ensuring the privacy and security of local vehicle data. In this paper, we review the progress accomplished by researchers in applying FL to CAVs. A broader view of the various data modalities and algorithms that have been implemented on CAVs is provided. Specific applications of FL are reviewed in detail, and an analysis of the challenges and future scope of research are presented.
<div id='section'>Paperid: <span id='pid'>1014, <a href='https://arxiv.org/pdf/2303.01418.pdf' target='_blank'>https://arxiv.org/pdf/2303.01418.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yonatan Shafir, Guy Tevet, Roy Kapon, Amit H. Bermano
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.01418">Human Motion Diffusion as a Generative Prior</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent work has demonstrated the significant potential of denoising diffusion models for generating human motion, including text-to-motion capabilities. However, these methods are restricted by the paucity of annotated motion data, a focus on single-person motions, and a lack of detailed control. In this paper, we introduce three forms of composition based on diffusion priors: sequential, parallel, and model composition. Using sequential composition, we tackle the challenge of long sequence generation. We introduce DoubleTake, an inference-time method with which we generate long animations consisting of sequences of prompted intervals and their transitions, using a prior trained only for short clips. Using parallel composition, we show promising steps toward two-person generation. Beginning with two fixed priors as well as a few two-person training examples, we learn a slim communication block, ComMDM, to coordinate interaction between the two resulting motions. Lastly, using model composition, we first train individual priors to complete motions that realize a prescribed motion for a given joint. We then introduce DiffusionBlending, an interpolation mechanism to effectively blend several such models to enable flexible and efficient fine-grained joint and trajectory-level control and editing. We evaluate the composition methods using an off-the-shelf motion diffusion model, and further compare the results to dedicated models trained for these specific tasks.
<div id='section'>Paperid: <span id='pid'>1015, <a href='https://arxiv.org/pdf/2303.00920.pdf' target='_blank'>https://arxiv.org/pdf/2303.00920.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tamzidul Mina, Wonse Jo, Shyam S. Kannan, Byung-Cheol Min
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.00920">Beacon-based Distributed Structure Formation in Multi-agent Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous shape and structure formation is an important problem in the domain of large-scale multi-agent systems. In this paper, we propose a 3D structure representation method and a distributed structure formation strategy where settled agents guide free moving agents to a prescribed location to settle in the structure. Agents at the structure formation frontier looking for neighbors to settle act as beacons, generating a surface gradient throughout the formed structure propagated by settled agents. Free-moving agents follow the surface gradient along the formed structure surface to the formation frontier, where they eventually reach the closest beacon and settle to continue the structure formation following a local bidding process. Agent behavior is governed by a finite state machine implementation, along with potential field-based motion control laws. We also discuss appropriate rules for recovering from stagnation points. Simulation experiments are presented to show planar and 3D structure formations with continuous and discontinuous boundary/surfaces, which validate the proposed strategy, followed by a scalability analysis.
<div id='section'>Paperid: <span id='pid'>1016, <a href='https://arxiv.org/pdf/2211.09707.pdf' target='_blank'>https://arxiv.org/pdf/2211.09707.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Simon Alexanderson, Rajmund Nagy, Jonas Beskow, Gustav Eje Henter
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.09707">Listen, Denoise, Action! Audio-Driven Motion Synthesis with Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion models have experienced a surge of interest as highly expressive yet efficiently trainable probabilistic models. We show that these models are an excellent fit for synthesising human motion that co-occurs with audio, e.g., dancing and co-speech gesticulation, since motion is complex and highly ambiguous given audio, calling for a probabilistic description. Specifically, we adapt the DiffWave architecture to model 3D pose sequences, putting Conformers in place of dilated convolutions for improved modelling power. We also demonstrate control over motion style, using classifier-free guidance to adjust the strength of the stylistic expression. Experiments on gesture and dance generation confirm that the proposed method achieves top-of-the-line motion quality, with distinctive styles whose expression can be made more or less pronounced. We also synthesise path-driven locomotion using the same model architecture. Finally, we generalise the guidance procedure to obtain product-of-expert ensembles of diffusion models and demonstrate how these may be used for, e.g., style interpolation, a contribution we believe is of independent interest. See https://www.speech.kth.se/research/listen-denoise-action/ for video examples, data, and code.
<div id='section'>Paperid: <span id='pid'>1017, <a href='https://arxiv.org/pdf/2207.09644.pdf' target='_blank'>https://arxiv.org/pdf/2207.09644.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxiao Chen, Long Zhao, Jianbo Yuan, Yu Tian, Zhaoyang Xia, Shijie Geng, Ligong Han, Dimitris N. Metaxas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2207.09644">Hierarchically Self-Supervised Transformer for Human Skeleton Representation Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite the success of fully-supervised human skeleton sequence modeling, utilizing self-supervised pre-training for skeleton sequence representation learning has been an active field because acquiring task-specific skeleton annotations at large scales is difficult. Recent studies focus on learning video-level temporal and discriminative information using contrastive learning, but overlook the hierarchical spatial-temporal nature of human skeletons. Different from such superficial supervision at the video level, we propose a self-supervised hierarchical pre-training scheme incorporated into a hierarchical Transformer-based skeleton sequence encoder (Hi-TRS), to explicitly capture spatial, short-term, and long-term temporal dependencies at frame, clip, and video levels, respectively. To evaluate the proposed self-supervised pre-training scheme with Hi-TRS, we conduct extensive experiments covering three skeleton-based downstream tasks including action recognition, action detection, and motion prediction. Under both supervised and semi-supervised evaluation protocols, our method achieves the state-of-the-art performance. Additionally, we demonstrate that the prior knowledge learned by our model in the pre-training stage has strong transfer capability for different downstream tasks.
<div id='section'>Paperid: <span id='pid'>1018, <a href='https://arxiv.org/pdf/2105.13061.pdf' target='_blank'>https://arxiv.org/pdf/2105.13061.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junxiao Shen, John Dudley, Per Ola Kristensson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2105.13061">The Imaginative Generative Adversarial Network: Automatic Data Augmentation for Dynamic Skeleton-Based Hand Gesture and Human Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning approaches deliver state-of-the-art performance in recognition of spatiotemporal human motion data. However, one of the main challenges in these recognition tasks is limited available training data. Insufficient training data results in over-fitting and data augmentation is one approach to address this challenge. Existing data augmentation strategies based on scaling, shifting and interpolating offer limited generalizability and typically require detailed inspection of the dataset as well as hundreds of GPU hours for hyperparameter optimization. In this paper, we present a novel automatic data augmentation model, the Imaginative Generative Adversarial Network (GAN), that approximates the distribution of the input data and samples new data from this distribution. It is automatic in that it requires no data inspection and little hyperparameter tuning and therefore it is a low-cost and low-effort approach to generate synthetic data. We demonstrate our approach on small-scale skeleton-based datasets with a comprehensive experimental analysis. Our results show that the augmentation strategy is fast to train and can improve classification accuracy for both conventional neural networks and state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>1019, <a href='https://arxiv.org/pdf/2003.13510.pdf' target='_blank'>https://arxiv.org/pdf/2003.13510.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yang-Tian Sun, Qian-Cheng Fu, Yue-Ren Jiang, Zitao Liu, Yu-Kun Lai, Hongbo Fu, Lin Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2003.13510">Human Motion Transfer with 3D Constraints and Detail Enhancement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a new method for realistic human motion transfer using a generative adversarial network (GAN), which generates a motion video of a target character imitating actions of a source character, while maintaining high authenticity of the generated results. We tackle the problem by decoupling and recombining the posture information and appearance information of both the source and target characters. The innovation of our approach lies in the use of the projection of a reconstructed 3D human model as the condition of GAN to better maintain the structural integrity of transfer results in different poses. We further introduce a detail enhancement net to enhance the details of transfer results by exploiting the details in real source frames. Extensive experiments show that our approach yields better results both qualitatively and quantitatively than the state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>1020, <a href='https://arxiv.org/pdf/2510.04076.pdf' target='_blank'>https://arxiv.org/pdf/2510.04076.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Amin Vahidi-Moghaddam, Sayed Pedram Haeri Boroujeni, Iman Jebellat, Ehsan Jebellat, Niloufar Mehrabi, Zhaojian Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04076">From Shadow to Light: Toward Safe and Efficient Policy Learning Across MPC, DeePC, RL, and LLM Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>One of the main challenges in modern control applications, particularly in robot and vehicle motion control, is achieving accurate, fast, and safe movement. To address this, optimal control policies have been developed to enforce safety while ensuring high performance. Since basic first-principles models of real systems are often available, model-based controllers are widely used. Model predictive control (MPC) is a leading approach that optimizes performance while explicitly handling safety constraints. However, obtaining accurate models for complex systems is difficult, which motivates data-driven alternatives. ML-based MPC leverages learned models to reduce reliance on hand-crafted dynamics, while reinforcement learning (RL) can learn near-optimal policies directly from interaction data. Data-enabled predictive control (DeePC) goes further by bypassing modeling altogether, directly learning safe policies from raw input-output data. Recently, large language model (LLM) agents have also emerged, translating natural language instructions into structured formulations of optimal control problems. Despite these advances, data-driven policies face significant limitations. They often suffer from slow response times, high computational demands, and large memory needs, making them less practical for real-world systems with fast dynamics, limited onboard computing, or strict memory constraints. To address this, various technique, such as reduced-order modeling, function-approximated policy learning, and convex relaxations, have been proposed to reduce computational complexity. In this paper, we present eight such approaches and demonstrate their effectiveness across real-world applications, including robotic arms, soft robots, and vehicle motion control.
<div id='section'>Paperid: <span id='pid'>1021, <a href='https://arxiv.org/pdf/2509.17476.pdf' target='_blank'>https://arxiv.org/pdf/2509.17476.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mallikarjun B. R., Fei Yin, Vikram Voleti, Nikita Drobyshev, Maksim Lapin, Aaryaman Vasishta, Varun Jampani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17476">Stable Video-Driven Portraits</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Portrait animation aims to generate photo-realistic videos from a single source image by reenacting the expression and pose from a driving video. While early methods relied on 3D morphable models or feature warping techniques, they often suffered from limited expressivity, temporal inconsistency, and poor generalization to unseen identities or large pose variations. Recent advances using diffusion models have demonstrated improved quality but remain constrained by weak control signals and architectural limitations. In this work, we propose a novel diffusion based framework that leverages masked facial regions specifically the eyes, nose, and mouth from the driving video as strong motion control cues. To enable robust training without appearance leakage, we adopt cross identity supervision. To leverage the strong prior from the pretrained diffusion model, our novel architecture introduces minimal new parameters that converge faster and help in better generalization. We introduce spatial temporal attention mechanisms that allow inter frame and intra frame interactions, effectively capturing subtle motions and reducing temporal artifacts. Our model uses history frames to ensure continuity across segments. At inference, we propose a novel signal fusion strategy that balances motion fidelity with identity preservation. Our approach achieves superior temporal consistency and accurate expression control, enabling high-quality, controllable portrait animation suitable for real-world applications.
<div id='section'>Paperid: <span id='pid'>1022, <a href='https://arxiv.org/pdf/2509.16919.pdf' target='_blank'>https://arxiv.org/pdf/2509.16919.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Huong Hoang, Keito Suzuki, Truong Nguyen, Pamela Cosman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.16919">Bi-modal Prediction and Transformation Coding for Compressing Complex Human Dynamics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>For dynamic human motion sequences, the original KeyNode-Driven codec often struggles to retain compression efficiency when confronted with rapid movements or strong non-rigid deformations. This paper proposes a novel Bi-modal coding framework that enhances the flexibility of motion representation by integrating semantic segmentation and region-specific transformation modeling. The rigid transformation model (rotation & translation) is extended with a hybrid scheme that selectively applies affine transformations-rotation, translation, scaling, and shearing-only to deformation-rich regions (e.g., the torso, where loose clothing induces high variability), while retaining rigid models elsewhere. The affine model is decomposed into minimal parameter sets for efficient coding and combined through a component selection strategy guided by a Lagrangian Rate-Distortion optimization. The results show that the Bi-modal method achieves more accurate mesh deformation, especially in sequences involving complex non-rigid motion, without compromising compression efficiency in simpler regions, with an average bit-rate saving of 33.81% compared to the baseline.
<div id='section'>Paperid: <span id='pid'>1023, <a href='https://arxiv.org/pdf/2509.09496.pdf' target='_blank'>https://arxiv.org/pdf/2509.09496.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ha Linh Nguyen, Tze Ho Elden Tse, Angela Yao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09496">Improving Human Motion Plausibility with Body Momentum</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Many studies decompose human motion into local motion in a frame attached to the root joint and global motion of the root joint in the world frame, treating them separately. However, these two components are not independent. Global movement arises from interactions with the environment, which are, in turn, driven by changes in the body configuration. Motion models often fail to precisely capture this physical coupling between local and global dynamics, while deriving global trajectories from joint torques and external forces is computationally expensive and complex. To address these challenges, we propose using whole-body linear and angular momentum as a constraint to link local motion with global movement. Since momentum reflects the aggregate effect of joint-level dynamics on the body's movement through space, it provides a physically grounded way to relate local joint behavior to global displacement. Building on this insight, we introduce a new loss term that enforces consistency between the generated momentum profiles and those observed in ground-truth data. Incorporating our loss reduces foot sliding and jitter, improves balance, and preserves the accuracy of the recovered motion. Code and data are available at the project page https://hlinhn.github.io/momentum_bmvc.
<div id='section'>Paperid: <span id='pid'>1024, <a href='https://arxiv.org/pdf/2508.03695.pdf' target='_blank'>https://arxiv.org/pdf/2508.03695.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pulkit Kumar, Shuaiyi Huang, Matthew Walmer, Sai Saketh Rambhatla, Abhinav Shrivastava
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.03695">Trokens: Semantic-Aware Relational Trajectory Tokens for Few-Shot Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video understanding requires effective modeling of both motion and appearance information, particularly for few-shot action recognition. While recent advances in point tracking have been shown to improve few-shot action recognition, two fundamental challenges persist: selecting informative points to track and effectively modeling their motion patterns. We present Trokens, a novel approach that transforms trajectory points into semantic-aware relational tokens for action recognition. First, we introduce a semantic-aware sampling strategy to adaptively distribute tracking points based on object scale and semantic relevance. Second, we develop a motion modeling framework that captures both intra-trajectory dynamics through the Histogram of Oriented Displacements (HoD) and inter-trajectory relationships to model complex action patterns. Our approach effectively combines these trajectory tokens with semantic features to enhance appearance features with motion information, achieving state-of-the-art performance across six diverse few-shot action recognition benchmarks: Something-Something-V2 (both full and small splits), Kinetics, UCF101, HMDB51, and FineGym. For project page see https://trokens-iccv25.github.io
<div id='section'>Paperid: <span id='pid'>1025, <a href='https://arxiv.org/pdf/2508.02632.pdf' target='_blank'>https://arxiv.org/pdf/2508.02632.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Italo Napolitano, Stefano Covone, Andrea Lama, Francesco De Lellis, Mario di Bernardo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02632">Hierarchical Learning-Based Control for Multi-Agent Shepherding of Stochastic Autonomous Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent shepherding represents a challenging distributed control problem where herder agents must coordinate to guide independently moving targets to desired spatial configurations. Most existing control strategies assume cohesive target behavior, which frequently fails in practical applications where targets exhibit stochastic autonomous behavior. This paper presents a hierarchical learning-based control architecture that decomposes the shepherding problem into a high-level decision-making module and a low-level motion control component. The proposed distributed control system synthesizes effective control policies directly from closed-loop experience without requiring explicit inter-agent communication or prior knowledge of target dynamics. The decentralized architecture achieves cooperative control behavior through emergent coordination without centralized supervision. Experimental validation demonstrates superior closed-loop performance compared to state-of-the-art heuristic control methods, achieving 100\% success rates with improved settling times and control efficiency. The control architecture scales beyond its design conditions, adapts to time-varying goal regions, and demonstrates practical implementation feasibility through real-time experiments on the Robotarium platform.
<div id='section'>Paperid: <span id='pid'>1026, <a href='https://arxiv.org/pdf/2508.01590.pdf' target='_blank'>https://arxiv.org/pdf/2508.01590.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hua Yu, Jiao Liu, Xu Gui, Melvin Wong, Yaqing Hou, Yew-Soon Ong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01590">A Plug-and-Play Multi-Criteria Guidance for Diverse In-Betweening Human Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In-betweening human motion generation aims to synthesize intermediate motions that transition between user-specified keyframes. In addition to maintaining smooth transitions, a crucial requirement of this task is to generate diverse motion sequences. It is still challenging to maintain diversity, particularly when it is necessary for the motions within a generated batch sampling to differ meaningfully from one another due to complex motion dynamics. In this paper, we propose a novel method, termed the Multi-Criteria Guidance with In-Betweening Motion Model (MCG-IMM), for in-betweening human motion generation. A key strength of MCG-IMM lies in its plug-and-play nature: it enhances the diversity of motions generated by pretrained models without introducing additional parameters This is achieved by providing a sampling process of pretrained generative models with multi-criteria guidance. Specifically, MCG-IMM reformulates the sampling process of pretrained generative model as a multi-criteria optimization problem, and introduces an optimization process to explore motion sequences that satisfy multiple criteria, e.g., diversity and smoothness. Moreover, our proposed plug-and-play multi-criteria guidance is compatible with different families of generative models, including denoised diffusion probabilistic models, variational autoencoders, and generative adversarial networks. Experiments on four popular human motion datasets demonstrate that MCG-IMM consistently state-of-the-art methods in in-betweening motion generation task.
<div id='section'>Paperid: <span id='pid'>1027, <a href='https://arxiv.org/pdf/2507.21018.pdf' target='_blank'>https://arxiv.org/pdf/2507.21018.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ali Ismail-Fawaz, Maxime Devanne, Stefano Berretti, Jonathan Weber, Germain Forestier
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.21018">Deep Learning for Skeleton Based Human Motion Rehabilitation Assessment: A Benchmark</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automated assessment of human motion plays a vital role in rehabilitation, enabling objective evaluation of patient performance and progress. Unlike general human activity recognition, rehabilitation motion assessment focuses on analyzing the quality of movement within the same action class, requiring the detection of subtle deviations from ideal motion. Recent advances in deep learning and video-based skeleton extraction have opened new possibilities for accessible, scalable motion assessment using affordable devices such as smartphones or webcams. However, the field lacks standardized benchmarks, consistent evaluation protocols, and reproducible methodologies, limiting progress and comparability across studies. In this work, we address these gaps by (i) aggregating existing rehabilitation datasets into a unified archive called Rehab-Pile, (ii) proposing a general benchmarking framework for evaluating deep learning methods in this domain, and (iii) conducting extensive benchmarking of multiple architectures across classification and regression tasks. All datasets and implementations are released to the community to support transparency and reproducibility. This paper aims to establish a solid foundation for future research in automated rehabilitation assessment and foster the development of reliable, accessible, and personalized rehabilitation solutions. The datasets, source-code and results of this article are all publicly available.
<div id='section'>Paperid: <span id='pid'>1028, <a href='https://arxiv.org/pdf/2507.11892.pdf' target='_blank'>https://arxiv.org/pdf/2507.11892.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Liu, Leyuan Qu, Hanlei Shi, Di Gao, Yuhua Zheng, Taihao Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.11892">From Coarse to Nuanced: Cross-Modal Alignment of Fine-Grained Linguistic Cues and Visual Salient Regions for Dynamic Emotion Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dynamic Facial Expression Recognition (DFER) aims to identify human emotions from temporally evolving facial movements and plays a critical role in affective computing. While recent vision-language approaches have introduced semantic textual descriptions to guide expression recognition, existing methods still face two key limitations: they often underutilize the subtle emotional cues embedded in generated text, and they have yet to incorporate sufficiently effective mechanisms for filtering out facial dynamics that are irrelevant to emotional expression. To address these gaps, We propose GRACE, Granular Representation Alignment for Cross-modal Emotion recognition that integrates dynamic motion modeling, semantic text refinement, and token-level cross-modal alignment to facilitate the precise localization of emotionally salient spatiotemporal features. Our method constructs emotion-aware textual descriptions via a Coarse-to-fine Affective Text Enhancement (CATE) module and highlights expression-relevant facial motion through a motion-difference weighting mechanism. These refined semantic and visual signals are aligned at the token level using entropy-regularized optimal transport. Experiments on three benchmark datasets demonstrate that our method significantly improves recognition performance, particularly in challenging settings with ambiguous or imbalanced emotion classes, establishing new state-of-the-art (SOTA) results in terms of both UAR and WAR.
<div id='section'>Paperid: <span id='pid'>1029, <a href='https://arxiv.org/pdf/2507.09122.pdf' target='_blank'>https://arxiv.org/pdf/2507.09122.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chuan Guo, Inwoo Hwang, Jian Wang, Bing Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.09122">SnapMoGen: Human Motion Generation from Expressive Texts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-to-motion generation has experienced remarkable progress in recent years. However, current approaches remain limited to synthesizing motion from short or general text prompts, primarily due to dataset constraints. This limitation undermines fine-grained controllability and generalization to unseen prompts. In this paper, we introduce SnapMoGen, a new text-motion dataset featuring high-quality motion capture data paired with accurate, expressive textual annotations. The dataset comprises 20K motion clips totaling 44 hours, accompanied by 122K detailed textual descriptions averaging 48 words per description (vs. 12 words of HumanML3D). Importantly, these motion clips preserve original temporal continuity as they were in long sequences, facilitating research in long-term motion generation and blending. We also improve upon previous generative masked modeling approaches. Our model, MoMask++, transforms motion into multi-scale token sequences that better exploit the token capacity, and learns to generate all tokens using a single generative masked transformer. MoMask++ achieves state-of-the-art performance on both HumanML3D and SnapMoGen benchmarks. Additionally, we demonstrate the ability to process casual user prompts by employing an LLM to reformat inputs to align with the expressivity and narration style of SnapMoGen. Project webpage: https://snap-research.github.io/SnapMoGen/
<div id='section'>Paperid: <span id='pid'>1030, <a href='https://arxiv.org/pdf/2506.13040.pdf' target='_blank'>https://arxiv.org/pdf/2506.13040.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hanz Cuevas-Velasquez, Anastasios Yiannakidis, Soyong Shin, Giorgio Becherini, Markus HÃ¶schle, Joachim Tesch, Taylor Obersat, Tsvetelina Alexiadis, Michael J. Black
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.13040">MAMMA: Markerless & Automatic Multi-Person Motion Action Capture</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present MAMMA, a markerless motion-capture pipeline that accurately recovers SMPL-X parameters from multi-view video of two-person interaction sequences. Traditional motion-capture systems rely on physical markers. Although they offer high accuracy, their requirements of specialized hardware, manual marker placement, and extensive post-processing make them costly and time-consuming. Recent learning-based methods attempt to overcome these limitations, but most are designed for single-person capture, rely on sparse keypoints, or struggle with occlusions and physical interactions. In this work, we introduce a method that predicts dense 2D surface landmarks conditioned on segmentation masks, enabling person-specific correspondence estimation even under heavy occlusion. We employ a novel architecture that exploits learnable queries for each landmark. We demonstrate that our approach can handle complex person--person interaction and offers greater accuracy than existing methods. To train our network, we construct a large, synthetic multi-view dataset combining human motions from diverse sources, including extreme poses, hand motions, and close interactions. Our dataset yields high-variability synthetic sequences with rich body contact and occlusion, and includes SMPL-X ground-truth annotations with dense 2D landmarks. The result is a system capable of capturing human motion without the need for markers. Our approach offers competitive reconstruction quality compared to commercial marker-based motion-capture solutions, without the extensive manual cleanup. Finally, we address the absence of common benchmarks for dense-landmark prediction and markerless motion capture by introducing two evaluation settings built from real multi-view sequences. We will release our dataset, benchmark, method, training code, and pre-trained model weights for research purposes.
<div id='section'>Paperid: <span id='pid'>1031, <a href='https://arxiv.org/pdf/2506.00411.pdf' target='_blank'>https://arxiv.org/pdf/2506.00411.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi Yang, Jiaxuan Sun, Siqi Kou, Yihan Wang, Zhijie Deng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.00411">LoHoVLA: A Unified Vision-Language-Action Model for Long-Horizon Embodied Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Real-world embodied agents face long-horizon tasks, characterized by high-level goals demanding multi-step solutions beyond single actions. Successfully navigating these requires both high-level task planning (i.e., decomposing goals into sub-tasks) and low-level motion control (i.e., generating precise robot actions). While existing vision language action (VLA) models and hierarchical architectures offer potential in embodied tasks, the former often falter in planning, and the latter can suffer from coordination issues, both hampering performance. We introduce a new unified VLA framework for long-horizon tasks, dubbed LoHoVLA, to overcome these limitations. LoHoVLA leverages a large pretrained vision language model (VLM) as the backbone to jointly generate language and action tokens for sub-task generation and robot action prediction, respectively. This shared representation promotes better generalization across tasks. Additionally, LoHoVLA embraces a hierarchical closed-loop control mechanism to mitigate errors originating from both high-level planning and low-level control. To train LoHoVLA, we introduce LoHoSet, a dataset built on the Ravens simulator, containing 20 long-horizon tasks, each with 1,000 expert demonstrations composed of visual observations, linguistic goals, sub-tasks, and robot actions. Experimental results show that LoHoVLA significantly surpasses both hierarchical and standard VLA approaches on long-horizon embodied tasks in the Ravens simulator. These findings underscore the promise of unified architectures for advancing generalizable embodied intelligence.
<div id='section'>Paperid: <span id='pid'>1032, <a href='https://arxiv.org/pdf/2505.24266.pdf' target='_blank'>https://arxiv.org/pdf/2505.24266.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guanren Qiao, Sixu Lin, Ronglai Zuo, Zhizheng Wu, Kui Jia, Guiliang Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.24266">SignBot: Learning Human-to-Humanoid Sign Language Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Sign language is a natural and visual form of language that uses movements and expressions to convey meaning, serving as a crucial means of communication for individuals who are deaf or hard-of-hearing (DHH). However, the number of people proficient in sign language remains limited, highlighting the need for technological advancements to bridge communication gaps and foster interactions with minorities. Based on recent advancements in embodied humanoid robots, we propose SignBot, a novel framework for human-robot sign language interaction. SignBot integrates a cerebellum-inspired motion control component and a cerebral-oriented module for comprehension and interaction. Specifically, SignBot consists of: 1) Motion Retargeting, which converts human sign language datasets into robot-compatible kinematics; 2) Motion Control, which leverages a learning-based paradigm to develop a robust humanoid control policy for tracking sign language gestures; and 3) Generative Interaction, which incorporates translator, responser, and generator of sign language, thereby enabling natural and effective communication between robots and humans. Simulation and real-world experimental results demonstrate that SignBot can effectively facilitate human-robot interaction and perform sign language motions with diverse robots and datasets. SignBot represents a significant advancement in automatic sign language interaction on embodied humanoid robot platforms, providing a promising solution to improve communication accessibility for the DHH community.
<div id='section'>Paperid: <span id='pid'>1033, <a href='https://arxiv.org/pdf/2505.21531.pdf' target='_blank'>https://arxiv.org/pdf/2505.21531.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kunhang Li, Jason Naradowsky, Yansong Feng, Yusuke Miyao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.21531">How Much Do Large Language Models Know about Human Motion? A Case Study in 3D Avatar Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We explore the human motion knowledge of Large Language Models (LLMs) through 3D avatar control. Given a motion instruction, we prompt LLMs to first generate a high-level movement plan with consecutive steps (High-level Planning), then specify body part positions in each step (Low-level Planning), which we linearly interpolate into avatar animations. Using 20 representative motion instructions that cover fundamental movements and balance body part usage, we conduct comprehensive evaluations, including human and automatic scoring of both high-level movement plans and generated animations, as well as automatic comparison with oracle positions in low-level planning. Our findings show that LLMs are strong at interpreting high-level body movements but struggle with precise body part positioning. While decomposing motion queries into atomic components improves planning, LLMs face challenges in multi-step movements involving high-degree-of-freedom body parts. Furthermore, LLMs provide reasonable approximations for general spatial descriptions, but fall short in handling precise spatial specifications. Notably, LLMs demonstrate promise in conceptualizing creative motions and distinguishing culturally specific motion patterns.
<div id='section'>Paperid: <span id='pid'>1034, <a href='https://arxiv.org/pdf/2505.20582.pdf' target='_blank'>https://arxiv.org/pdf/2505.20582.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yizhou Zhao, Chunjiang Liu, Haoyu Chen, Bhiksha Raj, Min Xu, Tadas Baltrusaitis, Mitch Rundle, HsiangTao Wu, Kamran Ghasedi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.20582">Total-Editing: Head Avatar with Editable Appearance, Motion, and Lighting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Face reenactment and portrait relighting are essential tasks in portrait editing, yet they are typically addressed independently, without much synergy. Most face reenactment methods prioritize motion control and multiview consistency, while portrait relighting focuses on adjusting shading effects. To take advantage of both geometric consistency and illumination awareness, we introduce Total-Editing, a unified portrait editing framework that enables precise control over appearance, motion, and lighting. Specifically, we design a neural radiance field decoder with intrinsic decomposition capabilities. This allows seamless integration of lighting information from portrait images or HDR environment maps into synthesized portraits. We also incorporate a moving least squares based deformation field to enhance the spatiotemporal coherence of avatar motion and shading effects. With these innovations, our unified framework significantly improves the quality and realism of portrait editing results. Further, the multi-source nature of Total-Editing supports more flexible applications, such as illumination transfer from one portrait to another, or portrait animation with customized backgrounds.
<div id='section'>Paperid: <span id='pid'>1035, <a href='https://arxiv.org/pdf/2505.19580.pdf' target='_blank'>https://arxiv.org/pdf/2505.19580.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Masaki Murooka, Kensuke Fukumitsu, Marwan Hamze, Mitsuharu Morisawa, Hiroshi Kaminaga, Fumio Kanehiro, Eiichi Yoshida
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.19580">Whole-body Multi-contact Motion Control for Humanoid Robots Based on Distributed Tactile Sensors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To enable humanoid robots to work robustly in confined environments, multi-contact motion that makes contacts not only at extremities, such as hands and feet, but also at intermediate areas of the limbs, such as knees and elbows, is essential. We develop a method to realize such whole-body multi-contact motion involving contacts at intermediate areas by a humanoid robot. Deformable sheet-shaped distributed tactile sensors are mounted on the surface of the robot's limbs to measure the contact force without significantly changing the robot body shape. The multi-contact motion controller developed earlier, which is dedicated to contact at extremities, is extended to handle contact at intermediate areas, and the robot motion is stabilized by feedback control using not only force/torque sensors but also distributed tactile sensors. Through verification on dynamics simulations, we show that the developed tactile feedback improves the stability of whole-body multi-contact motion against disturbances and environmental errors. Furthermore, the life-sized humanoid RHP Kaleido demonstrates whole-body multi-contact motions, such as stepping forward while supporting the body with forearm contact and balancing in a sitting posture with thigh contacts.
<div id='section'>Paperid: <span id='pid'>1036, <a href='https://arxiv.org/pdf/2505.19463.pdf' target='_blank'>https://arxiv.org/pdf/2505.19463.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoyu Zhao, Sixu Lin, Qingwei Ben, Minyue Dai, Hao Fei, Jingbo Wang, Hua Zou, Junting Dong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.19463">SMAP: Self-supervised Motion Adaptation for Physically Plausible Humanoid Whole-body Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a novel framework that enables real-world humanoid robots to maintain stability while performing human-like motion. Current methods train a policy which allows humanoid robots to follow human body using the massive retargeted human data via reinforcement learning. However, due to the heterogeneity between human and humanoid robot motion, directly using retargeted human motion reduces training efficiency and stability. To this end, we introduce SMAP, a novel whole-body tracking framework that bridges the gap between human and humanoid action spaces, enabling accurate motion mimicry by humanoid robots. The core idea is to use a vector-quantized periodic autoencoder to capture generic atomic behaviors and adapt human motion into physically plausible humanoid motion. This adaptation accelerates training convergence and improves stability when handling novel or challenging motions. We then employ a privileged teacher to distill precise mimicry skills into the student policy with a proposed decoupled reward. We conduct experiments in simulation and real world to demonstrate the superiority stability and performance of SMAP over SOTA methods, offering practical guidelines for advancing whole-body control in humanoid robots.
<div id='section'>Paperid: <span id='pid'>1037, <a href='https://arxiv.org/pdf/2505.07096.pdf' target='_blank'>https://arxiv.org/pdf/2505.07096.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Prithwish Dan, Kushal Kedia, Angela Chao, Edward Weiyi Duan, Maximus Adrian Pace, Wei-Chiu Ma, Sanjiban Choudhury
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.07096">X-Sim: Cross-Embodiment Learning via Real-to-Sim-to-Real</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human videos offer a scalable way to train robot manipulation policies, but lack the action labels needed by standard imitation learning algorithms. Existing cross-embodiment approaches try to map human motion to robot actions, but often fail when the embodiments differ significantly. We propose X-Sim, a real-to-sim-to-real framework that uses object motion as a dense and transferable signal for learning robot policies. X-Sim starts by reconstructing a photorealistic simulation from an RGBD human video and tracking object trajectories to define object-centric rewards. These rewards are used to train a reinforcement learning (RL) policy in simulation. The learned policy is then distilled into an image-conditioned diffusion policy using synthetic rollouts rendered with varied viewpoints and lighting. To transfer to the real world, X-Sim introduces an online domain adaptation technique that aligns real and simulated observations during deployment. Importantly, X-Sim does not require any robot teleoperation data. We evaluate it across 5 manipulation tasks in 2 environments and show that it: (1) improves task progress by 30% on average over hand-tracking and sim-to-real baselines, (2) matches behavior cloning with 10x less data collection time, and (3) generalizes to new camera viewpoints and test-time changes. Code and videos are available at https://portal-cornell.github.io/X-Sim/.
<div id='section'>Paperid: <span id='pid'>1038, <a href='https://arxiv.org/pdf/2505.00237.pdf' target='_blank'>https://arxiv.org/pdf/2505.00237.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ze Zhang, Georg Hess, Junjie Hu, Emmanuel Dean, Lennart Svensson, Knut Ãkesson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.00237">Future-Oriented Navigation: Dynamic Obstacle Avoidance with One-Shot Energy-Based Multimodal Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper proposes an integrated approach for the safe and efficient control of mobile robots in dynamic and uncertain environments. The approach consists of two key steps: one-shot multimodal motion prediction to anticipate motions of dynamic obstacles and model predictive control to incorporate these predictions into the motion planning process. Motion prediction is driven by an energy-based neural network that generates high-resolution, multi-step predictions in a single operation. The prediction outcomes are further utilized to create geometric shapes formulated as mathematical constraints. Instead of treating each dynamic obstacle individually, predicted obstacles are grouped by proximity in an unsupervised way to improve performance and efficiency. The overall collision-free navigation is handled by model predictive control with a specific design for proactive dynamic obstacle avoidance. The proposed approach allows mobile robots to navigate effectively in dynamic environments. Its performance is accessed across various scenarios that represent typical warehouse settings. The results demonstrate that the proposed approach outperforms other existing dynamic obstacle avoidance methods.
<div id='section'>Paperid: <span id='pid'>1039, <a href='https://arxiv.org/pdf/2504.15122.pdf' target='_blank'>https://arxiv.org/pdf/2504.15122.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Minh-Quan Viet Bui, Jongmin Park, Juan Luis Gonzalez Bello, Jaeho Moon, Jihyong Oh, Munchurl Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.15122">MoBGS: Motion Deblurring Dynamic 3D Gaussian Splatting for Blurry Monocular Video</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present MoBGS, a novel deblurring dynamic 3D Gaussian Splatting (3DGS) framework capable of reconstructing sharp and high-quality novel spatio-temporal views from blurry monocular videos in an end-to-end manner. Existing dynamic novel view synthesis (NVS) methods are highly sensitive to motion blur in casually captured videos, resulting in significant degradation of rendering quality. While recent approaches address motion-blurred inputs for NVS, they primarily focus on static scene reconstruction and lack dedicated motion modeling for dynamic objects. To overcome these limitations, our MoBGS introduces a novel Blur-adaptive Latent Camera Estimation (BLCE) method for effective latent camera trajectory estimation, improving global camera motion deblurring. In addition, we propose a physically-inspired Latent Camera-induced Exposure Estimation (LCEE) method to ensure consistent deblurring of both global camera and local object motion. Our MoBGS framework ensures the temporal consistency of unseen latent timestamps and robust motion decomposition of static and dynamic regions. Extensive experiments on the Stereo Blur dataset and real-world blurry videos show that our MoBGS significantly outperforms the very recent advanced methods (DyBluRF and Deblur4DGS), achieving state-of-the-art performance for dynamic NVS under motion blur.
<div id='section'>Paperid: <span id='pid'>1040, <a href='https://arxiv.org/pdf/2504.11733.pdf' target='_blank'>https://arxiv.org/pdf/2504.11733.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Li Yu, Situo Wang, Wei Zhou, Moncef Gabbouj
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.11733">DVLTA-VQA: Decoupled Vision-Language Modeling with Text-Guided Adaptation for Blind Video Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Inspired by the dual-stream theory of the human visual system (HVS) - where the ventral stream is responsible for object recognition and detail analysis, while the dorsal stream focuses on spatial relationships and motion perception - an increasing number of video quality assessment (VQA) works built upon this framework are proposed. Recent advancements in large multi-modal models, notably Contrastive Language-Image Pretraining (CLIP), have motivated researchers to incorporate CLIP into dual-stream-based VQA methods. This integration aims to harness the model's superior semantic understanding capabilities to replicate the object recognition and detail analysis in ventral stream, as well as spatial relationship analysis in dorsal stream. However, CLIP is originally designed for images and lacks the ability to capture temporal and motion information inherent in videos. To address the limitation, this paper propose a Decoupled Vision-Language Modeling with Text-Guided Adaptation for Blind Video Quality Assessment (DVLTA-VQA), which decouples CLIP's visual and textual components, and integrates them into different stages of the NR-VQA pipeline. Specifically, a Video-Based Temporal CLIP module is proposed to explicitly model temporal dynamics and enhance motion perception, aligning with the dorsal stream. Additionally, a Temporal Context Module is developed to refine inter-frame dependencies, further improving motion modeling. On the ventral stream side, a Basic Visual Feature Extraction Module is employed to strengthen detail analysis. Finally, a text-guided adaptive fusion strategy is proposed to enable dynamic weighting of features, facilitating more effective integration of spatial and temporal information.
<div id='section'>Paperid: <span id='pid'>1041, <a href='https://arxiv.org/pdf/2504.04956.pdf' target='_blank'>https://arxiv.org/pdf/2504.04956.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jihyun Lee, Weipeng Xu, Alexander Richard, Shih-En Wei, Shunsuke Saito, Shaojie Bai, Te-Li Wang, Minhyuk Sung, Tae-Kyun Kim, Jason Saragih
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.04956">REWIND: Real-Time Egocentric Whole-Body Motion Diffusion with Exemplar-Based Identity Conditioning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present REWIND (Real-Time Egocentric Whole-Body Motion Diffusion), a one-step diffusion model for real-time, high-fidelity human motion estimation from egocentric image inputs. While an existing method for egocentric whole-body (i.e., body and hands) motion estimation is non-real-time and acausal due to diffusion-based iterative motion refinement to capture correlations between body and hand poses, REWIND operates in a fully causal and real-time manner. To enable real-time inference, we introduce (1) cascaded body-hand denoising diffusion, which effectively models the correlation between egocentric body and hand motions in a fast, feed-forward manner, and (2) diffusion distillation, which enables high-quality motion estimation with a single denoising step. Our denoising diffusion model is based on a modified Transformer architecture, designed to causally model output motions while enhancing generalizability to unseen motion lengths. Additionally, REWIND optionally supports identity-conditioned motion estimation when identity prior is available. To this end, we propose a novel identity conditioning method based on a small set of pose exemplars of the target identity, which further enhances motion estimation quality. Through extensive experiments, we demonstrate that REWIND significantly outperforms the existing baselines both with and without exemplar-based identity conditioning.
<div id='section'>Paperid: <span id='pid'>1042, <a href='https://arxiv.org/pdf/2504.04842.pdf' target='_blank'>https://arxiv.org/pdf/2504.04842.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mengchao Wang, Qiang Wang, Fan Jiang, Yaqi Fan, Yunpeng Zhang, Yonggang Qi, Kun Zhao, Mu Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.04842">FantasyTalking: Realistic Talking Portrait Generation via Coherent Motion Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Creating a realistic animatable avatar from a single static portrait remains challenging. Existing approaches often struggle to capture subtle facial expressions, the associated global body movements, and the dynamic background. To address these limitations, we propose a novel framework that leverages a pretrained video diffusion transformer model to generate high-fidelity, coherent talking portraits with controllable motion dynamics. At the core of our work is a dual-stage audio-visual alignment strategy. In the first stage, we employ a clip-level training scheme to establish coherent global motion by aligning audio-driven dynamics across the entire scene, including the reference portrait, contextual objects, and background. In the second stage, we refine lip movements at the frame level using a lip-tracing mask, ensuring precise synchronization with audio signals. To preserve identity without compromising motion flexibility, we replace the commonly used reference network with a facial-focused cross-attention module that effectively maintains facial consistency throughout the video. Furthermore, we integrate a motion intensity modulation module that explicitly controls expression and body motion intensity, enabling controllable manipulation of portrait movements beyond mere lip motion. Extensive experimental results show that our proposed approach achieves higher quality with better realism, coherence, motion intensity, and identity preservation. Ours project page: https://fantasy-amap.github.io/fantasy-talking/.
<div id='section'>Paperid: <span id='pid'>1043, <a href='https://arxiv.org/pdf/2503.22605.pdf' target='_blank'>https://arxiv.org/pdf/2503.22605.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuai Shen, Wanhua Li, Yunpeng Zhang, Yap-Peng Tan, Jiwen Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.22605">Audio-Plane: Audio Factorization Plane Gaussian Splatting for Real-Time Talking Head Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Talking head synthesis has emerged as a prominent research topic in computer graphics and multimedia, yet most existing methods often struggle to strike a balance between generation quality and computational efficiency, particularly under real-time constraints. In this paper, we propose a novel framework that integrates Gaussian Splatting with a structured Audio Factorization Plane (Audio-Plane) to enable high-quality, audio-synchronized, and real-time talking head generation. For modeling a dynamic talking head, a 4D volume representation, which consists of three axes in 3D space and one temporal axis aligned with audio progression, is typically required. However, directly storing and processing a dense 4D grid is impractical due to the high memory and computation cost, and lack of scalability for longer durations. We address this challenge by decomposing the 4D volume representation into a set of audio-independent spatial planes and audio-dependent planes, forming a compact and interpretable representation for talking head modeling that we refer to as the Audio-Plane. This factorized design allows for efficient and fine-grained audio-aware spatial encoding, and significantly enhances the model's ability to capture complex lip dynamics driven by speech signals. To further improve region-specific motion modeling, we introduce an audio-guided saliency splatting mechanism based on region-aware modulation, which adaptively emphasizes highly dynamic regions such as the mouth area. This allows the model to focus its learning capacity on where it matters most for accurate speech-driven animation. Extensive experiments on both the self-driven and the cross-driven settings demonstrate that our method achieves state-of-the-art visual quality, precise audio-lip synchronization, and real-time performance, outperforming prior approaches across both 2D- and 3D-based paradigms.
<div id='section'>Paperid: <span id='pid'>1044, <a href='https://arxiv.org/pdf/2503.16455.pdf' target='_blank'>https://arxiv.org/pdf/2503.16455.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiwen Dong, Jessica Rose, Hae Young Noh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.16455">Bridging Structural Dynamics and Biomechanics: Human Motion Estimation through Footstep-Induced Floor Vibrations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Quantitative estimation of human joint motion in daily living spaces is essential for early detection and rehabilitation tracking of neuromusculoskeletal disorders (e.g., Parkinson's) and mitigating trip and fall risks for older adults. Existing approaches involve monitoring devices such as cameras, wearables, and pressure mats, but have operational constraints such as direct line-of-sight, carrying devices, and dense deployment. To overcome these limitations, we leverage gait-induced floor vibration to estimate lower-limb joint motion (e.g., ankle, knee, and hip flexion angles), allowing non-intrusive and contactless gait health monitoring in people's living spaces. To overcome the high uncertainty in lower-limb movement given the limited information provided by the gait-induced floor vibrations, we formulate a physics-informed graph to integrate domain knowledge of gait biomechanics and structural dynamics into the model. Specifically, different types of nodes represent heterogeneous information from joint motions and floor vibrations; Their connecting edges represent the physiological relationships between joints and forces governed by gait biomechanics, as well as the relationships between forces and floor responses governed by the structural dynamics. As a result, our model poses physical constraints to reduce uncertainty while allowing information sharing between the body and the floor to make more accurate predictions. We evaluate our approach with 20 participants through a real-world walking experiment. We achieved an average of 3.7 degrees of mean absolute error in estimating 12 joint flexion angles (38% error reduction from baseline), which is comparable to the performance of cameras and wearables in current medical practices.
<div id='section'>Paperid: <span id='pid'>1045, <a href='https://arxiv.org/pdf/2503.15557.pdf' target='_blank'>https://arxiv.org/pdf/2503.15557.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Inwoo Hwang, Jinseok Bae, Donggeun Lim, Young Min Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.15557">Motion Synthesis with Sparse and Flexible Keyjoint Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Creating expressive character animations is labor-intensive, requiring intricate manual adjustment of animators across space and time. Previous works on controllable motion generation often rely on a predefined set of dense spatio-temporal specifications (e.g., dense pelvis trajectories with exact per-frame timing), limiting practicality for animators. To process high-level intent and intuitive control in diverse scenarios, we propose a practical controllable motions synthesis framework that respects sparse and flexible keyjoint signals. Our approach employs a decomposed diffusion-based motion synthesis framework that first synthesizes keyjoint movements from sparse input control signals and then synthesizes full-body motion based on the completed keyjoint trajectories. The low-dimensional keyjoint movements can easily adapt to various control signal types, such as end-effector position for diverse goal-driven motion synthesis, or incorporate functional constraints on a subset of keyjoints. Additionally, we introduce a time-agnostic control formulation, eliminating the need for frame-specific timing annotations and enhancing control flexibility. Then, the shared second stage can synthesize a natural whole-body motion that precisely satisfies the task requirement from dense keyjoint movements. We demonstrate the effectiveness of sparse and flexible keyjoint control through comprehensive experiments on diverse datasets and scenarios.
<div id='section'>Paperid: <span id='pid'>1046, <a href='https://arxiv.org/pdf/2503.13859.pdf' target='_blank'>https://arxiv.org/pdf/2503.13859.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinseok Bae, Inwoo Hwang, Young Yoon Lee, Ziyu Guo, Joseph Liu, Yizhak Ben-Shabat, Young Min Kim, Mubbasir Kapadia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.13859">Less is More: Improving Motion Diffusion Models with Sparse Keyframes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in motion diffusion models have led to remarkable progress in diverse motion generation tasks, including text-to-motion synthesis. However, existing approaches represent motions as dense frame sequences, requiring the model to process redundant or less informative frames. The processing of dense animation frames imposes significant training complexity, especially when learning intricate distributions of large motion datasets even with modern neural architectures. This severely limits the performance of generative motion models for downstream tasks. Inspired by professional animators who mainly focus on sparse keyframes, we propose a novel diffusion framework explicitly designed around sparse and geometrically meaningful keyframes. Our method reduces computation by masking non-keyframes and efficiently interpolating missing frames. We dynamically refine the keyframe mask during inference to prioritize informative frames in later diffusion steps. Extensive experiments show that our approach consistently outperforms state-of-the-art methods in text alignment and motion realism, while also effectively maintaining high performance at significantly fewer diffusion steps. We further validate the robustness of our framework by using it as a generative prior and adapting it to different downstream tasks.
<div id='section'>Paperid: <span id='pid'>1047, <a href='https://arxiv.org/pdf/2503.07390.pdf' target='_blank'>https://arxiv.org/pdf/2503.07390.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Boeun Kim, Hea In Jeong, JungHoon Sung, Yihua Cheng, Jeongmin Lee, Ju Yong Chang, Sang-Il Choi, Younggeun Choi, Saim Shin, Jungho Kim, Hyung Jin Chang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.07390">PersonaBooth: Personalized Text-to-Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces Motion Personalization, a new task that generates personalized motions aligned with text descriptions using several basic motions containing Persona. To support this novel task, we introduce a new large-scale motion dataset called PerMo (PersonaMotion), which captures the unique personas of multiple actors. We also propose a multi-modal finetuning method of a pretrained motion diffusion model called PersonaBooth. PersonaBooth addresses two main challenges: i) A significant distribution gap between the persona-focused PerMo dataset and the pretraining datasets, which lack persona-specific data, and ii) the difficulty of capturing a consistent persona from the motions vary in content (action type). To tackle the dataset distribution gap, we introduce a persona token to accept new persona features and perform multi-modal adaptation for both text and visuals during finetuning. To capture a consistent persona, we incorporate a contrastive learning technique to enhance intra-cohesion among samples with the same persona. Furthermore, we introduce a context-aware fusion mechanism to maximize the integration of persona cues from multiple input motions. PersonaBooth outperforms state-of-the-art motion style transfer methods, establishing a new benchmark for motion personalization.
<div id='section'>Paperid: <span id='pid'>1048, <a href='https://arxiv.org/pdf/2502.16913.pdf' target='_blank'>https://arxiv.org/pdf/2502.16913.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kedi Lyu, Haipeng Chen, Zhenguang Liu, Yifang Yin, Yukang Lin, Yingying Jiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.16913">HVIS: A Human-like Vision and Inference System for Human Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Grasping the intricacies of human motion, which involve perceiving spatio-temporal dependence and multi-scale effects, is essential for predicting human motion. While humans inherently possess the requisite skills to navigate this issue, it proves to be markedly more challenging for machines to emulate. To bridge the gap, we propose the Human-like Vision and Inference System (HVIS) for human motion prediction, which is designed to emulate human observation and forecast future movements. HVIS comprises two components: the human-like vision encode (HVE) module and the human-like motion inference (HMI) module. The HVE module mimics and refines the human visual process, incorporating a retina-analog component that captures spatiotemporal information separately to avoid unnecessary crosstalk. Additionally, a visual cortex-analogy component is designed to hierarchically extract and treat complex motion features, focusing on both global and local features of human poses. The HMI is employed to simulate the multi-stage learning model of the human brain. The spontaneous learning network simulates the neuronal fracture generation process for the adversarial generation of future motions. Subsequently, the deliberate learning network is optimized for hard-to-train joints to prevent misleading learning. Experimental results demonstrate that our method achieves new state-of-the-art performance, significantly outperforming existing methods by 19.8% on Human3.6M, 15.7% on CMU Mocap, and 11.1% on G3D.
<div id='section'>Paperid: <span id='pid'>1049, <a href='https://arxiv.org/pdf/2502.10724.pdf' target='_blank'>https://arxiv.org/pdf/2502.10724.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiuxia Lin, Rongyu Chen, Kerui Gu, Angela Yao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.10724">Semantics-aware Test-time Adaptation for 3D Human Pose Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work highlights a semantics misalignment in 3D human pose estimation. For the task of test-time adaptation, the misalignment manifests as overly smoothed and unguided predictions. The smoothing settles predictions towards some average pose. Furthermore, when there are occlusions or truncations, the adaptation becomes fully unguided. To this end, we pioneer the integration of a semantics-aware motion prior for the test-time adaptation of 3D pose estimation. We leverage video understanding and a well-structured motion-text space to adapt the model motion prediction to adhere to video semantics during test time. Additionally, we incorporate a missing 2D pose completion based on the motion-text similarity. The pose completion strengthens the motion prior's guidance for occlusions and truncations. Our method significantly improves state-of-the-art 3D human pose estimation TTA techniques, with more than 12% decrease in PA-MPJPE on 3DPW and 3DHP.
<div id='section'>Paperid: <span id='pid'>1050, <a href='https://arxiv.org/pdf/2501.07149.pdf' target='_blank'>https://arxiv.org/pdf/2501.07149.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Simon Hanisch, Julian Todt, Thorsten Strufe
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.07149">Pantomime: Motion Data Anonymization using Foundation Motion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion is a behavioral biometric trait that can be used to identify individuals and infer private attributes such as medical conditions. This poses a serious threat to privacy as motion extraction from video and motion capture are increasingly used for a variety of applications, including mixed reality, robotics, medicine, and the quantified self. In order to protect the privacy of the tracked individuals, anonymization techniques that preserve the utility of the data are required. However, anonymizing motion data is a challenging task because there are many dependencies in motion sequences (such as physiological constraints) that, if ignored, make the anonymized motion sequence appear unnatural. In this paper, we propose Pantomime, a full-body anonymization technique for motion data, which uses foundation motion models to generate motion sequences that adhere to the dependencies in the data, thus keeping the utility of the anonymized data high. Our results show that Pantomime can maintain the naturalness of the motion sequences while reducing the identification accuracy to 10%.
<div id='section'>Paperid: <span id='pid'>1051, <a href='https://arxiv.org/pdf/2501.05020.pdf' target='_blank'>https://arxiv.org/pdf/2501.05020.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yingjie Chen, Yifang Men, Yuan Yao, Miaomiao Cui, Liefeng Bo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.05020">Perception-as-Control: Fine-grained Controllable Image Animation with 3D-aware Motion Representation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion-controllable image animation is a fundamental task with a wide range of potential applications. Recent works have made progress in controlling camera or object motion via various motion representations, while they still struggle to support collaborative camera and object motion control with adaptive control granularity. To this end, we introduce 3D-aware motion representation and propose an image animation framework, called Perception-as-Control, to achieve fine-grained collaborative motion control. Specifically, we construct 3D-aware motion representation from a reference image, manipulate it based on interpreted user instructions, and perceive it from different viewpoints. In this way, camera and object motions are transformed into intuitive and consistent visual changes. Then, our framework leverages the perception results as motion control signals, enabling it to support various motion-related video synthesis tasks in a unified and flexible way. Experiments demonstrate the superiority of the proposed approach. For more details and qualitative results, please refer to our anonymous project webpage: https://chen-yingjie.github.io/projects/Perception-as-Control.
<div id='section'>Paperid: <span id='pid'>1052, <a href='https://arxiv.org/pdf/2412.15664.pdf' target='_blank'>https://arxiv.org/pdf/2412.15664.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaohan Zhang, Sebastian Starke, Vladimir Guzov, Zhensong Zhang, Eduardo PÃ©rez Pellitero, Gerard Pons-Moll
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.15664">SCENIC: Scene-aware Semantic Navigation with Instruction-guided Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Synthesizing natural human motion that adapts to complex environments while allowing creative control remains a fundamental challenge in motion synthesis. Existing models often fall short, either by assuming flat terrain or lacking the ability to control motion semantics through text. To address these limitations, we introduce SCENIC, a diffusion model designed to generate human motion that adapts to dynamic terrains within virtual scenes while enabling semantic control through natural language. The key technical challenge lies in simultaneously reasoning about complex scene geometry while maintaining text control. This requires understanding both high-level navigation goals and fine-grained environmental constraints. The model must ensure physical plausibility and precise navigation across varied terrain, while also preserving user-specified text control, such as ``carefully stepping over obstacles" or ``walking upstairs like a zombie." Our solution introduces a hierarchical scene reasoning approach. At its core is a novel scene-dependent, goal-centric canonicalization that handles high-level goal constraint, and is complemented by an ego-centric distance field that captures local geometric details. This dual representation enables our model to generate physically plausible motion across diverse 3D scenes. By implementing frame-wise text alignment, our system achieves seamless transitions between different motion styles while maintaining scene constraints. Experiments demonstrate our novel diffusion model generates arbitrarily long human motions that both adapt to complex scenes with varying terrain surfaces and respond to textual prompts. Additionally, we show SCENIC can generalize to four real-scene datasets. Our code, dataset, and models will be released at \url{https://virtualhumans.mpi-inf.mpg.de/scenic/}.
<div id='section'>Paperid: <span id='pid'>1053, <a href='https://arxiv.org/pdf/2412.11552.pdf' target='_blank'>https://arxiv.org/pdf/2412.11552.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mario Rosenfelder, Hendrik Carius, Markus Herrmann-Wicklmayr, Peter Eberhard, Kathrin Flaßkamp, Henrik Ebel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.11552">Efficient Avoidance of Ellipsoidal Obstacles with Model Predictive Control for Mobile Robots and Vehicles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In real-world applications of mobile robots, collision avoidance is of critical importance. Typically, global motion planning in constrained environments is addressed through high-level control schemes. However, additionally integrating local collision avoidance into robot motion control offers significant advantages. For instance, it reduces the reliance on heuristics and conservatism that can arise from a two-stage approach separating local collision avoidance and control. Moreover, using model predictive control (MPC), a robot's full potential can be harnessed by considering jointly local collision avoidance, the robot's dynamics, and actuation constraints. In this context, the present paper focuses on obstacle avoidance for wheeled mobile robots, where both the robot's and obstacles' occupied volumes are modeled as ellipsoids. To this end, a computationally efficient overlap test, that works for arbitrary ellipsoids, is conducted and novelly integrated into the MPC framework. We propose a particularly efficient implementation tailored to robots moving in the plane. The functionality of the proposed obstacle-avoiding MPC is demonstrated for two exemplary types of kinematics by means of simulations. A hardware experiment using a real-world wheeled mobile robot shows transferability to reality and real-time applicability. The general computational approach to ellipsoidal obstacle avoidance can also be applied to other robotic systems and vehicles as well as three-dimensional scenarios.
<div id='section'>Paperid: <span id='pid'>1054, <a href='https://arxiv.org/pdf/2412.05515.pdf' target='_blank'>https://arxiv.org/pdf/2412.05515.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Runhao Zeng, Dingjie Zhou, Qiwei Liang, Junlin Liu, Hui Li, Changxin Huang, Jianqiang Li, Xiping Hu, Fuchun Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.05515">Video2Reward: Generating Reward Function from Videos for Legged Robot Behavior Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning behavior in legged robots presents a significant challenge due to its inherent instability and complex constraints. Recent research has proposed the use of a large language model (LLM) to generate reward functions in reinforcement learning, thereby replacing the need for manually designed rewards by experts. However, this approach, which relies on textual descriptions to define learning objectives, fails to achieve controllable and precise behavior learning with clear directionality. In this paper, we introduce a new video2reward method, which directly generates reward functions from videos depicting the behaviors to be mimicked and learned. Specifically, we first process videos containing the target behaviors, converting the motion information of individuals in the videos into keypoint trajectories represented as coordinates through a video2text transforming module. These trajectories are then fed into an LLM to generate the reward function, which in turn is used to train the policy. To enhance the quality of the reward function, we develop a video-assisted iterative reward refinement scheme that visually assesses the learned behaviors and provides textual feedback to the LLM. This feedback guides the LLM to continually refine the reward function, ultimately facilitating more efficient behavior learning. Experimental results on tasks involving bipedal and quadrupedal robot motion control demonstrate that our method surpasses the performance of state-of-the-art LLM-based reward generation methods by over 37.6% in terms of human normalized score. More importantly, by switching video inputs, we find our method can rapidly learn diverse motion behaviors such as walking and running.
<div id='section'>Paperid: <span id='pid'>1055, <a href='https://arxiv.org/pdf/2411.18303.pdf' target='_blank'>https://arxiv.org/pdf/2411.18303.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenjie Zhuo, Fan Ma, Hehe Fan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.18303">InfiniDreamer: Arbitrarily Long Human Motion Generation via Segment Score Distillation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present InfiniDreamer, a novel framework for arbitrarily long human motion generation. InfiniDreamer addresses the limitations of current motion generation methods, which are typically restricted to short sequences due to the lack of long motion training data. To achieve this, we first generate sub-motions corresponding to each textual description and then assemble them into a coarse, extended sequence using randomly initialized transition segments. We then introduce an optimization-based method called Segment Score Distillation (SSD) to refine the entire long motion sequence. SSD is designed to utilize an existing motion prior, which is trained only on short clips, in a training-free manner. Specifically, SSD iteratively refines overlapping short segments sampled from the coarsely extended long motion sequence, progressively aligning them with the pre-trained motion diffusion prior. This process ensures local coherence within each segment, while the refined transitions between segments maintain global consistency across the entire sequence. Extensive qualitative and quantitative experiments validate the superiority of our framework, showcasing its ability to generate coherent, contextually aware motion sequences of arbitrary length.
<div id='section'>Paperid: <span id='pid'>1056, <a href='https://arxiv.org/pdf/2411.17532.pdf' target='_blank'>https://arxiv.org/pdf/2411.17532.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chengjian Li, Xiangbo Shu, Qiongjie Cui, Yazhou Yao, Jinhui Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.17532">FTMoMamba: Motion Generation with Frequency and Text State Space Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion models achieve impressive performance in human motion generation. However, current approaches typically ignore the significance of frequency-domain information in capturing fine-grained motions within the latent space (e.g., low frequencies correlate with static poses, and high frequencies align with fine-grained motions). Additionally, there is a semantic discrepancy between text and motion, leading to inconsistency between the generated motions and the text descriptions. In this work, we propose a novel diffusion-based FTMoMamba framework equipped with a Frequency State Space Model (FreqSSM) and a Text State Space Model (TextSSM). Specifically, to learn fine-grained representation, FreqSSM decomposes sequences into low-frequency and high-frequency components, guiding the generation of static pose (e.g., sits, lay) and fine-grained motions (e.g., transition, stumble), respectively. To ensure the consistency between text and motion, TextSSM encodes text features at the sentence level, aligning textual semantics with sequential features. Extensive experiments show that FTMoMamba achieves superior performance on the text-to-motion generation task, especially gaining the lowest FID of 0.181 (rather lower than 0.421 of MLD) on the HumanML3D dataset.
<div id='section'>Paperid: <span id='pid'>1057, <a href='https://arxiv.org/pdf/2411.08328.pdf' target='_blank'>https://arxiv.org/pdf/2411.08328.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiang Zhou, Shaofeng Zhang, Nianzu Yang, Ye Qian, Hao Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.08328">Motion Control for Enhanced Complex Action Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing text-to-video (T2V) models often struggle with generating videos with sufficiently pronounced or complex actions. A key limitation lies in the text prompt's inability to precisely convey intricate motion details. To address this, we propose a novel framework, MVideo, designed to produce long-duration videos with precise, fluid actions. MVideo overcomes the limitations of text prompts by incorporating mask sequences as an additional motion condition input, providing a clearer, more accurate representation of intended actions. Leveraging foundational vision models such as GroundingDINO and SAM2, MVideo automatically generates mask sequences, enhancing both efficiency and robustness. Our results demonstrate that, after training, MVideo effectively aligns text prompts with motion conditions to produce videos that simultaneously meet both criteria. This dual control mechanism allows for more dynamic video generation by enabling alterations to either the text prompt or motion condition independently, or both in tandem. Furthermore, MVideo supports motion condition editing and composition, facilitating the generation of videos with more complex actions. MVideo thus advances T2V motion generation, setting a strong benchmark for improved action depiction in current video diffusion models. Our project page is available at https://mvideo-v1.github.io/.
<div id='section'>Paperid: <span id='pid'>1058, <a href='https://arxiv.org/pdf/2410.10646.pdf' target='_blank'>https://arxiv.org/pdf/2410.10646.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>James R. Han, Hugues Thomas, Jian Zhang, Nicholas Rhinehart, Timothy D. Barfoot
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.10646">DR-MPC: Deep Residual Model Predictive Control for Real-world Social Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>How can a robot safely navigate around people with complex motion patterns? Deep Reinforcement Learning (DRL) in simulation holds some promise, but much prior work relies on simulators that fail to capture the nuances of real human motion. Thus, we propose Deep Residual Model Predictive Control (DR-MPC) to enable robots to quickly and safely perform DRL from real-world crowd navigation data. By blending MPC with model-free DRL, DR-MPC overcomes the DRL challenges of large data requirements and unsafe initial behavior. DR-MPC is initialized with MPC-based path tracking, and gradually learns to interact more effectively with humans. To further accelerate learning, a safety component estimates out-of-distribution states to guide the robot away from likely collisions. In simulation, we show that DR-MPC substantially outperforms prior work, including traditional DRL and residual DRL models. Hardware experiments show our approach successfully enables a robot to navigate a variety of crowded situations with few errors using less than 4 hours of training data.
<div id='section'>Paperid: <span id='pid'>1059, <a href='https://arxiv.org/pdf/2410.07554.pdf' target='_blank'>https://arxiv.org/pdf/2410.07554.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenhai Liu, Junbo Wang, Yiming Wang, Weiming Wang, Cewu Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.07554">ForceMimic: Force-Centric Imitation Learning with Force-Motion Capture System for Contact-Rich Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In most contact-rich manipulation tasks, humans apply time-varying forces to the target object, compensating for inaccuracies in the vision-guided hand trajectory. However, current robot learning algorithms primarily focus on trajectory-based policy, with limited attention given to learning force-related skills. To address this limitation, we introduce ForceMimic, a force-centric robot learning system, providing a natural, force-aware and robot-free robotic demonstration collection system, along with a hybrid force-motion imitation learning algorithm for robust contact-rich manipulation. Using the proposed ForceCapture system, an operator can peel a zucchini in 5 minutes, while force-feedback teleoperation takes over 13 minutes and struggles with task completion. With the collected data, we propose HybridIL to train a force-centric imitation learning model, equipped with hybrid force-position control primitive to fit the predicted wrench-position parameters during robot execution. Experiments demonstrate that our approach enables the model to learn a more robust policy under the contact-rich task of vegetable peeling, increasing the success rates by 54.5% relatively compared to state-ofthe-art pure-vision-based imitation learning. Hardware, code, data and more results can be found on the project website at https://forcemimic.github.io.
<div id='section'>Paperid: <span id='pid'>1060, <a href='https://arxiv.org/pdf/2409.20527.pdf' target='_blank'>https://arxiv.org/pdf/2409.20527.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoyang Wang, Haoran Guo, He Ba, Zhengxiong Li, Lingfeng Tao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.20527">Bi-directional Momentum-based Haptic Feedback and Control System for In-Hand Dexterous Telemanipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In-hand dexterous telemanipulation requires not only precise remote motion control of the robot but also effective haptic feedback to the human operator to ensure stable and intuitive interactions between them. Most existing haptic devices for dexterous telemanipulation focus on force feedback and lack effective torque rendering, which is essential for tasks involving object rotation. While some torque feedback solutions in virtual reality applications-such as those based on geared motors or mechanically coupled actuators-have been explored, they often rely on bulky mechanical designs, limiting their use in portable or in-hand applications. In this paper, we propose a Bi-directional Momentum-based Haptic Feedback and Control (Bi-Hap) system that utilizes a palm-sized momentum-actuated mechanism to enable real-time haptic and torque feedback. The Bi-Hap system also integrates an Inertial Measurement Unit (IMU) to extract the human's manipulation command to establish a closed-loop learning-based telemanipulation framework. Furthermore, an error-adaptive feedback strategy is introduced to enhance operator perception and task performance in different error categories. Experimental evaluations demonstrate that Bi-Hap achieved feedback capability with low command following latency (Delay < 0.025 s) and highly accurate torque feedback (RMSE < 0.010 Nm).
<div id='section'>Paperid: <span id='pid'>1061, <a href='https://arxiv.org/pdf/2409.14103.pdf' target='_blank'>https://arxiv.org/pdf/2409.14103.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kanghao Chen, Zeyu Wang, Lin Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.14103">ExFMan: Rendering 3D Dynamic Humans with Hybrid Monocular Blurry Frames and Events</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent years have witnessed tremendous progress in the 3D reconstruction of dynamic humans from a monocular video with the advent of neural rendering techniques. This task has a wide range of applications, including the creation of virtual characters for virtual reality (VR) environments. However, it is still challenging to reconstruct clear humans when the monocular video is affected by motion blur, particularly caused by rapid human motion (e.g., running, dancing), as often occurs in the wild. This leads to distinct inconsistency of shape and appearance for the rendered 3D humans, especially in the blurry regions with rapid motion, e.g., hands and legs. In this paper, we propose ExFMan, the first neural rendering framework that unveils the possibility of rendering high-quality humans in rapid motion with a hybrid frame-based RGB and bio-inspired event camera. The ``out-of-the-box'' insight is to leverage the high temporal information of event data in a complementary manner and adaptively reweight the effect of losses for both RGB frames and events in the local regions, according to the velocity of the rendered human. This significantly mitigates the inconsistency associated with motion blur in the RGB frames. Specifically, we first formulate a velocity field of the 3D body in the canonical space and render it to image space to identify the body parts with motion blur. We then propose two novel losses, i.e., velocity-aware photometric loss and velocity-relative event loss, to optimize the neural human for both modalities under the guidance of the estimated velocity. In addition, we incorporate novel pose regularization and alpha losses to facilitate continuous pose and clear boundary. Extensive experiments on synthetic and real-world datasets demonstrate that ExFMan can reconstruct sharper and higher quality humans.
<div id='section'>Paperid: <span id='pid'>1062, <a href='https://arxiv.org/pdf/2409.13426.pdf' target='_blank'>https://arxiv.org/pdf/2409.13426.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vladimir Guzov, Yifeng Jiang, Fangzhou Hong, Gerard Pons-Moll, Richard Newcombe, C. Karen Liu, Yuting Ye, Lingni Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.13426">HMD^2: Environment-aware Motion Generation from Single Egocentric Head-Mounted Device</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper investigates the generation of realistic full-body human motion using a single head-mounted device with an outward-facing color camera and the ability to perform visual SLAM. To address the ambiguity of this setup, we present HMD^2, a novel system that balances motion reconstruction and generation. From a reconstruction standpoint, it aims to maximally utilize the camera streams to produce both analytical and learned features, including head motion, SLAM point cloud, and image embeddings. On the generative front, HMD^2 employs a multi-modal conditional motion diffusion model with a Transformer backbone to maintain temporal coherence of generated motions, and utilizes autoregressive inpainting to facilitate online motion inference with minimal latency (0.17 seconds). We show that our system provides an effective and robust solution that scales to a diverse dataset of over 200 hours of motion in complex indoor and outdoor environments.
<div id='section'>Paperid: <span id='pid'>1063, <a href='https://arxiv.org/pdf/2409.04639.pdf' target='_blank'>https://arxiv.org/pdf/2409.04639.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sylvain Bertrand, Luigi Penco, Dexton Anderson, Duncan Calvert, Valentine Roy, Stephen McCrory, Khizar Mohammed, Sebastian Sanchez, Will Griffith, Steve Morfey, Alexis Maslyczyk, Achintya Mohan, Cody Castello, Bingyin Ma, Kartik Suryavanshi, Patrick Dills, Jerry Pratt, Victor Ragusila, Brandon Shrewsbury, Robert Griffin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.04639">High-Speed and Impact Resilient Teleoperation of Humanoid Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Teleoperation of humanoid robots has long been a challenging domain, necessitating advances in both hardware and software to achieve seamless and intuitive control. This paper presents an integrated solution based on several elements: calibration-free motion capture and retargeting, low-latency fast whole-body kinematics streaming toolbox and high-bandwidth cycloidal actuators. Our motion retargeting approach stands out for its simplicity, requiring only 7 IMUs to generate full-body references for the robot. The kinematics streaming toolbox, ensures real-time, responsive control of the robot's movements, significantly reducing latency and enhancing operational efficiency. Additionally, the use of cycloidal actuators makes it possible to withstand high speeds and impacts with the environment. Together, these approaches contribute to a teleoperation framework that offers unprecedented performance. Experimental results on the humanoid robot Nadia demonstrate the effectiveness of the integrated system.
<div id='section'>Paperid: <span id='pid'>1064, <a href='https://arxiv.org/pdf/2407.08443.pdf' target='_blank'>https://arxiv.org/pdf/2407.08443.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mengtian Li, Chengshuo Zhai, Shengxiang Yao, Zhifeng Xie, Keyu Chen, Yu-Gang Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.08443">Infinite Motion: Extended Motion Generation via Long Text Instructions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the realm of motion generation, the creation of long-duration, high-quality motion sequences remains a significant challenge. This paper presents our groundbreaking work on "Infinite Motion", a novel approach that leverages long text to extended motion generation, effectively bridging the gap between short and long-duration motion synthesis. Our core insight is the strategic extension and reassembly of existing high-quality text-motion datasets, which has led to the creation of a novel benchmark dataset to facilitate the training of models for extended motion sequences. A key innovation of our model is its ability to accept arbitrary lengths of text as input, enabling the generation of motion sequences tailored to specific narratives or scenarios. Furthermore, we incorporate the timestamp design for text which allows precise editing of local segments within the generated sequences, offering unparalleled control and flexibility in motion synthesis. We further demonstrate the versatility and practical utility of "Infinite Motion" through three specific applications: natural language interactive editing, motion sequence editing within long sequences and splicing of independent motion sequences. Each application highlights the adaptability of our approach and broadens the spectrum of possibilities for research and development in motion generation. Through extensive experiments, we demonstrate the superior performance of our model in generating long sequence motions compared to existing methods.Project page: https://shuochengzhai.github.io/Infinite-motion.github.io/
<div id='section'>Paperid: <span id='pid'>1065, <a href='https://arxiv.org/pdf/2407.00574.pdf' target='_blank'>https://arxiv.org/pdf/2407.00574.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fengyuan Yang, Kerui Gu, Ha Linh Nguyen, Tze Ho Elden Tse, Angela Yao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.00574">Humans as Checkerboards: Calibrating Camera Motion Scale for World-Coordinate Human Mesh Recovery</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate camera motion estimation is essential for recovering global human motion in world coordinates from RGB video inputs. SLAM is widely used for estimating camera trajectory and point cloud, but monocular SLAM does so only up to an unknown scale factor. Previous works estimate the scale factor through optimization, but this is unreliable and time-consuming. This paper presents an optimization-free scale calibration framework, Human as Checkerboard (HAC). HAC innovatively leverages the human body predicted by human mesh recovery model as a calibration reference. Specifically, it uses the absolute depth of human-scene contact joints as references to calibrate the corresponding relative scene depth from SLAM. HAC benefits from geometric priors encoded in human mesh recovery models to estimate the SLAM scale and achieves precise global human motion estimation. Simple yet powerful, our method sets a new state-of-the-art performance for global human mesh estimation tasks, reducing motion errors by 50% over prior local-to-global methods while using 100$\times$ less inference time than optimization-based methods. Project page: https://martayang.github.io/HAC.
<div id='section'>Paperid: <span id='pid'>1066, <a href='https://arxiv.org/pdf/2406.04629.pdf' target='_blank'>https://arxiv.org/pdf/2406.04629.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zenghao Chai, Chen Tang, Yongkang Wong, Mohan Kankanhalli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.04629">STAR: Skeleton-aware Text-based 4D Avatar Generation with In-Network Motion Retargeting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The creation of 4D avatars (i.e., animated 3D avatars) from text description typically uses text-to-image (T2I) diffusion models to synthesize 3D avatars in the canonical space and subsequently applies animation with target motions. However, such an optimization-by-animation paradigm has several drawbacks. (1) For pose-agnostic optimization, the rendered images in canonical pose for naive Score Distillation Sampling (SDS) exhibit domain gap and cannot preserve view-consistency using only T2I priors, and (2) For post hoc animation, simply applying the source motions to target 3D avatars yields translation artifacts and misalignment. To address these issues, we propose Skeleton-aware Text-based 4D Avatar generation with in-network motion Retargeting (STAR). STAR considers the geometry and skeleton differences between the template mesh and target avatar, and corrects the mismatched source motion by resorting to the pretrained motion retargeting techniques. With the informatively retargeted and occlusion-aware skeleton, we embrace the skeleton-conditioned T2I and text-to-video (T2V) priors, and propose a hybrid SDS module to coherently provide multi-view and frame-consistent supervision signals. Hence, STAR can progressively optimize the geometry, texture, and motion in an end-to-end manner. The quantitative and qualitative experiments demonstrate our proposed STAR can synthesize high-quality 4D avatars with vivid animations that align well with the text description. Additional ablation studies shows the contributions of each component in STAR. The source code and demos are available at: \href{https://star-avatar.github.io}{https://star-avatar.github.io}.
<div id='section'>Paperid: <span id='pid'>1067, <a href='https://arxiv.org/pdf/2405.17306.pdf' target='_blank'>https://arxiv.org/pdf/2405.17306.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiang Wang, Minghua Liu, Junjun Hu, Fan Jiang, Mu Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.17306">Controllable Longer Image Animation with Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating realistic animated videos from static images is an important area of research in computer vision. Methods based on physical simulation and motion prediction have achieved notable advances, but they are often limited to specific object textures and motion trajectories, failing to exhibit highly complex environments and physical dynamics. In this paper, we introduce an open-domain controllable image animation method using motion priors with video diffusion models. Our method achieves precise control over the direction and speed of motion in the movable region by extracting the motion field information from videos and learning moving trajectories and strengths. Current pretrained video generation models are typically limited to producing very short videos, typically less than 30 frames. In contrast, we propose an efficient long-duration video generation method based on noise reschedule specifically tailored for image animation tasks, facilitating the creation of videos over 100 frames in length while maintaining consistency in content scenery and motion coordination. Specifically, we decompose the denoise process into two distinct phases: the shaping of scene contours and the refining of motion details. Then we reschedule the noise to control the generated frame sequences maintaining long-distance noise correlation. We conducted extensive experiments with 10 baselines, encompassing both commercial tools and academic methodologies, which demonstrate the superiority of our method. Our project page: https://wangqiang9.github.io/Controllable.github.io/
<div id='section'>Paperid: <span id='pid'>1068, <a href='https://arxiv.org/pdf/2405.07680.pdf' target='_blank'>https://arxiv.org/pdf/2405.07680.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ali Ismail-Fawaz, Maxime Devanne, Stefano Berretti, Jonathan Weber, Germain Forestier
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.07680">Establishing a Unified Evaluation Framework for Human Motion Generation: A Comparative Analysis of Metrics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The development of generative artificial intelligence for human motion generation has expanded rapidly, necessitating a unified evaluation framework. This paper presents a detailed review of eight evaluation metrics for human motion generation, highlighting their unique features and shortcomings. We propose standardized practices through a unified evaluation setup to facilitate consistent model comparisons. Additionally, we introduce a novel metric that assesses diversity in temporal distortion by analyzing warping diversity, thereby enhancing the evaluation of temporal data. We also conduct experimental analyses of three generative models using a publicly available dataset, offering insights into the interpretation of each metric in specific case scenarios. Our goal is to offer a clear, user-friendly evaluation framework for newcomers, complemented by publicly accessible code.
<div id='section'>Paperid: <span id='pid'>1069, <a href='https://arxiv.org/pdf/2405.03971.pdf' target='_blank'>https://arxiv.org/pdf/2405.03971.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiwei Li, Bozhen Zhang, Lei Yang, Tianyu Shen, Nuo Xu, Ruosen Hao, Weiting Li, Tao Yan, Huaping Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.03971">Unified End-to-End V2X Cooperative Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>V2X cooperation, through the integration of sensor data from both vehicles and infrastructure, is considered a pivotal approach to advancing autonomous driving technology. Current research primarily focuses on enhancing perception accuracy, often overlooking the systematic improvement of accident prediction accuracy through end-to-end learning, leading to insufficient attention to the safety issues of autonomous driving. To address this challenge, this paper introduces the UniE2EV2X framework, a V2X-integrated end-to-end autonomous driving system that consolidates key driving modules within a unified network. The framework employs a deformable attention-based data fusion strategy, effectively facilitating cooperation between vehicles and infrastructure. The main advantages include: 1) significantly enhancing agents' perception and motion prediction capabilities, thereby improving the accuracy of accident predictions; 2) ensuring high reliability in the data fusion process; 3) superior end-to-end perception compared to modular approaches. Furthermore, We implement the UniE2EV2X framework on the challenging DeepAccident, a simulation dataset designed for V2X cooperative driving.
<div id='section'>Paperid: <span id='pid'>1070, <a href='https://arxiv.org/pdf/2404.10685.pdf' target='_blank'>https://arxiv.org/pdf/2404.10685.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongwei Yi, Justus Thies, Michael J. Black, Xue Bin Peng, Davis Rempe
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.10685">Generating Human Interaction Motions in Scenes with Text Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present TeSMo, a method for text-controlled scene-aware motion generation based on denoising diffusion models. Previous text-to-motion methods focus on characters in isolation without considering scenes due to the limited availability of datasets that include motion, text descriptions, and interactive scenes. Our approach begins with pre-training a scene-agnostic text-to-motion diffusion model, emphasizing goal-reaching constraints on large-scale motion-capture datasets. We then enhance this model with a scene-aware component, fine-tuned using data augmented with detailed scene information, including ground plane and object shapes. To facilitate training, we embed annotated navigation and interaction motions within scenes. The proposed method produces realistic and diverse human-object interactions, such as navigation and sitting, in different scenes with various object shapes, orientations, initial body positions, and poses. Extensive experiments demonstrate that our approach surpasses prior techniques in terms of the plausibility of human-scene interactions, as well as the realism and variety of the generated motions. Code will be released upon publication of this work at https://research.nvidia.com/labs/toronto-ai/tesmo.
<div id='section'>Paperid: <span id='pid'>1071, <a href='https://arxiv.org/pdf/2404.09499.pdf' target='_blank'>https://arxiv.org/pdf/2404.09499.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuaiying Hou, Hongyu Tao, Junheng Fang, Changqing Zou, Hujun Bao, Weiwei Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.09499">Learning Human Motion from Monocular Videos via Cross-Modal Manifold Alignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning 3D human motion from 2D inputs is a fundamental task in the realms of computer vision and computer graphics. Many previous methods grapple with this inherently ambiguous task by introducing motion priors into the learning process. However, these approaches face difficulties in defining the complete configurations of such priors or training a robust model. In this paper, we present the Video-to-Motion Generator (VTM), which leverages motion priors through cross-modal latent feature space alignment between 3D human motion and 2D inputs, namely videos and 2D keypoints. To reduce the complexity of modeling motion priors, we model the motion data separately for the upper and lower body parts. Additionally, we align the motion data with a scale-invariant virtual skeleton to mitigate the interference of human skeleton variations to the motion priors. Evaluated on AIST++, the VTM showcases state-of-the-art performance in reconstructing 3D human motion from monocular videos. Notably, our VTM exhibits the capabilities for generalization to unseen view angles and in-the-wild videos.
<div id='section'>Paperid: <span id='pid'>1072, <a href='https://arxiv.org/pdf/2404.05490.pdf' target='_blank'>https://arxiv.org/pdf/2404.05490.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Baiyi Li, Edmond S. L. Ho, Hubert P. H. Shum, He Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.05490">Two-Person Interaction Augmentation with Skeleton Priors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Close and continuous interaction with rich contacts is a crucial aspect of human activities (e.g. hugging, dancing) and of interest in many domains like activity recognition, motion prediction, character animation, etc. However, acquiring such skeletal motion is challenging. While direct motion capture is expensive and slow, motion editing/generation is also non-trivial, as complex contact patterns with topological and geometric constraints have to be retained. To this end, we propose a new deep learning method for two-body skeletal interaction motion augmentation, which can generate variations of contact-rich interactions with varying body sizes and proportions while retaining the key geometric/topological relations between two bodies. Our system can learn effectively from a relatively small amount of data and generalize to drastically different skeleton sizes. Through exhaustive evaluation and comparison, we show it can generate high-quality motions, has strong generalizability and outperforms traditional optimization-based methods and alternative deep learning solutions.
<div id='section'>Paperid: <span id='pid'>1073, <a href='https://arxiv.org/pdf/2403.05081.pdf' target='_blank'>https://arxiv.org/pdf/2403.05081.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kanghyun Ryu, Negar Mehr
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.05081">Integrating Predictive Motion Uncertainties with Distributionally Robust Risk-Aware Control for Safe Robot Navigation in Crowds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Ensuring safe navigation in human-populated environments is crucial for autonomous mobile robots. Although recent advances in machine learning offer promising methods to predict human trajectories in crowded areas, it remains unclear how one can safely incorporate these learned models into a control loop due to the uncertain nature of human motion, which can make predictions of these models imprecise. In this work, we address this challenge and introduce a distributionally robust chance-constrained model predictive control (DRCC-MPC) which: (i) adopts a probability of collision as a pre-specified, interpretable risk metric, and (ii) offers robustness against discrepancies between actual human trajectories and their predictions. We consider the risk of collision in the form of a chance constraint, providing an interpretable measure of robot safety. To enable real-time evaluation of chance constraints, we consider conservative approximations of chance constraints in the form of distributionally robust Conditional Value at Risk constraints. The resulting formulation offers computational efficiency as well as robustness with respect to out-of-distribution human motion. With the parallelization of a sampling-based optimization technique, our method operates in real-time, demonstrating successful and safe navigation in a number of case studies with real-world pedestrian data.
<div id='section'>Paperid: <span id='pid'>1074, <a href='https://arxiv.org/pdf/2402.18796.pdf' target='_blank'>https://arxiv.org/pdf/2402.18796.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Huaxiaoyue Wang, Kushal Kedia, Juntao Ren, Rahma Abdullah, Atiksh Bhardwaj, Angela Chao, Kelly Y Chen, Nathaniel Chin, Prithwish Dan, Xinyi Fan, Gonzalo Gonzalez-Pumariega, Aditya Kompella, Maximus Adrian Pace, Yash Sharma, Xiangwan Sun, Neha Sunkara, Sanjiban Choudhury
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.18796">MOSAIC: A Modular System for Assistive and Interactive Cooking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present MOSAIC, a modular architecture for home robots to perform complex collaborative tasks, such as cooking with everyday users. MOSAIC tightly collaborates with humans, interacts with users using natural language, coordinates multiple robots, and manages an open vocabulary of everyday objects. At its core, MOSAIC employs modularity: it leverages multiple large-scale pre-trained models for general tasks like language and image recognition, while using streamlined modules designed for task-specific control. We extensively evaluate MOSAIC on 60 end-to-end trials where two robots collaborate with a human user to cook a combination of 6 recipes. We also extensively test individual modules with 180 episodes of visuomotor picking, 60 episodes of human motion forecasting, and 46 online user evaluations of the task planner. We show that MOSAIC is able to efficiently collaborate with humans by running the overall system end-to-end with a real human user, completing 68.3% (41/60) collaborative cooking trials of 6 different recipes with a subtask completion rate of 91.6%. Finally, we discuss the limitations of the current system and exciting open challenges in this domain. The project's website is at https://portal-cornell.github.io/MOSAIC/
<div id='section'>Paperid: <span id='pid'>1075, <a href='https://arxiv.org/pdf/2401.05365.pdf' target='_blank'>https://arxiv.org/pdf/2401.05365.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cheng Guo, Lorenzo Rapetti, Kourosh Darvish, Riccardo Grieco, Francesco Draicchio, Daniele Pucci
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.05365">Online Action Recognition for Human Risk Prediction with Anticipated Haptic Alert via Wearables</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper proposes a framework that combines online human state estimation, action recognition and motion prediction to enable early assessment and prevention of worker biomechanical risk during lifting tasks. The framework leverages the NIOSH index to perform online risk assessment, thus fitting real-time applications. In particular, the human state is retrieved via inverse kinematics/dynamics algorithms from wearable sensor data. Human action recognition and motion prediction are achieved by implementing an LSTM-based Guided Mixture of Experts architecture, which is trained offline and inferred online. With the recognized actions, a single lifting activity is divided into a series of continuous movements and the Revised NIOSH Lifting Equation can be applied for risk assessment. Moreover, the predicted motions enable anticipation of future risks. A haptic actuator, embedded in the wearable system, can alert the subject of potential risk, acting as an active prevention device. The performance of the proposed framework is validated by executing real lifting tasks, while the subject is equipped with the iFeel wearable system.
<div id='section'>Paperid: <span id='pid'>1076, <a href='https://arxiv.org/pdf/2310.13258.pdf' target='_blank'>https://arxiv.org/pdf/2310.13258.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kushal Kedia, Prithwish Dan, Atiksh Bhardwaj, Sanjiban Choudhury
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.13258">ManiCast: Collaborative Manipulation with Cost-Aware Human Forecasting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Seamless human-robot manipulation in close proximity relies on accurate forecasts of human motion. While there has been significant progress in learning forecast models at scale, when applied to manipulation tasks, these models accrue high errors at critical transition points leading to degradation in downstream planning performance. Our key insight is that instead of predicting the most likely human motion, it is sufficient to produce forecasts that capture how future human motion would affect the cost of a robot's plan. We present ManiCast, a novel framework that learns cost-aware human forecasts and feeds them to a model predictive control planner to execute collaborative manipulation tasks. Our framework enables fluid, real-time interactions between a human and a 7-DoF robot arm across a number of real-world tasks such as reactive stirring, object handovers, and collaborative table setting. We evaluate both the motion forecasts and the end-to-end forecaster-planner system against a range of learned and heuristic baselines while additionally contributing new datasets. We release our code and datasets at https://portal-cornell.github.io/manicast/.
<div id='section'>Paperid: <span id='pid'>1077, <a href='https://arxiv.org/pdf/2310.05938.pdf' target='_blank'>https://arxiv.org/pdf/2310.05938.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jia Fu, Jiarui Tan, Wenjie Yin, Sepideh Pashami, MÃ¥rten BjÃ¶rkman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.05938">Component attention network for multimodal dance improvisation recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dance improvisation is an active research topic in the arts. Motion analysis of improvised dance can be challenging due to its unique dynamics. Data-driven dance motion analysis, including recognition and generation, is often limited to skeletal data. However, data of other modalities, such as audio, can be recorded and benefit downstream tasks. This paper explores the application and performance of multimodal fusion methods for human motion recognition in the context of dance improvisation. We propose an attention-based model, component attention network (CANet), for multimodal fusion on three levels: 1) feature fusion with CANet, 2) model fusion with CANet and graph convolutional network (GCN), and 3) late fusion with a voting strategy. We conduct thorough experiments to analyze the impact of each modality in different fusion methods and distinguish critical temporal or component features. We show that our proposed model outperforms the two baseline methods, demonstrating its potential for analyzing improvisation in dance.
<div id='section'>Paperid: <span id='pid'>1078, <a href='https://arxiv.org/pdf/2309.08989.pdf' target='_blank'>https://arxiv.org/pdf/2309.08989.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi Yang, Qingwen Zhang, Thomas Gilles, Nazre Batool, John Folkesson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.08989">RMP: A Random Mask Pretrain Framework for Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As the pretraining technique is growing in popularity, little work has been done on pretrained learning-based motion prediction methods in autonomous driving. In this paper, we propose a framework to formalize the pretraining task for trajectory prediction of traffic participants. Within our framework, inspired by the random masked model in natural language processing (NLP) and computer vision (CV), objects' positions at random timesteps are masked and then filled in by the learned neural network (NN). By changing the mask profile, our framework can easily switch among a range of motion-related tasks. We show that our proposed pretraining framework is able to deal with noisy inputs and improves the motion prediction accuracy and miss rate, especially for objects occluded over time by evaluating it on Argoverse and NuScenes datasets.
<div id='section'>Paperid: <span id='pid'>1079, <a href='https://arxiv.org/pdf/2308.06076.pdf' target='_blank'>https://arxiv.org/pdf/2308.06076.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoyu Wang, Haozhe Wu, Junliang Xing, Jia Jia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.06076">Versatile Face Animator: Driving Arbitrary 3D Facial Avatar in RGBD Space</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Creating realistic 3D facial animation is crucial for various applications in the movie production and gaming industry, especially with the burgeoning demand in the metaverse. However, prevalent methods such as blendshape-based approaches and facial rigging techniques are time-consuming, labor-intensive, and lack standardized configurations, making facial animation production challenging and costly. In this paper, we propose a novel self-supervised framework, Versatile Face Animator, which combines facial motion capture with motion retargeting in an end-to-end manner, eliminating the need for blendshapes or rigs. Our method has the following two main characteristics: 1) we propose an RGBD animation module to learn facial motion from raw RGBD videos by hierarchical motion dictionaries and animate RGBD images rendered from 3D facial mesh coarse-to-fine, enabling facial animation on arbitrary 3D characters regardless of their topology, textures, blendshapes, and rigs; and 2) we introduce a mesh retarget module to utilize RGBD animation to create 3D facial animation by manipulating facial mesh with controller transformations, which are estimated from dense optical flow fields and blended together with geodesic-distance-based weights. Comprehensive experiments demonstrate the effectiveness of our proposed framework in generating impressive 3D facial animation results, highlighting its potential as a promising solution for the cost-effective and efficient production of facial animation in the metaverse.
<div id='section'>Paperid: <span id='pid'>1080, <a href='https://arxiv.org/pdf/2308.05920.pdf' target='_blank'>https://arxiv.org/pdf/2308.05920.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zijie Ye, Jia Jia, Junliang Xing
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.05920">Semantics2Hands: Transferring Hand Motion Semantics between Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human hands, the primary means of non-verbal communication, convey intricate semantics in various scenarios. Due to the high sensitivity of individuals to hand motions, even minor errors in hand motions can significantly impact the user experience. Real applications often involve multiple avatars with varying hand shapes, highlighting the importance of maintaining the intricate semantics of hand motions across the avatars. Therefore, this paper aims to transfer the hand motion semantics between diverse avatars based on their respective hand models. To address this problem, we introduce a novel anatomy-based semantic matrix (ASM) that encodes the semantics of hand motions. The ASM quantifies the positions of the palm and other joints relative to the local frame of the corresponding joint, enabling precise retargeting of hand motions. Subsequently, we obtain a mapping function from the source ASM to the target hand joint rotations by employing an anatomy-based semantics reconstruction network (ASRN). We train the ASRN using a semi-supervised learning strategy on the Mixamo and InterHand2.6M datasets. We evaluate our method in intra-domain and cross-domain hand motion retargeting tasks. The qualitative and quantitative results demonstrate the significant superiority of our ASRN over the state-of-the-arts.
<div id='section'>Paperid: <span id='pid'>1081, <a href='https://arxiv.org/pdf/2307.03829.pdf' target='_blank'>https://arxiv.org/pdf/2307.03829.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rojin Zandi, Hojjat Salehinejad, Kian Behzad, Elaheh Motamedi, Milad Siami
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.03829">Robot Motion Prediction by Channel State Information</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous robotic systems have gained a lot of attention, in recent years. However, accurate prediction of robot motion in indoor environments with limited visibility is challenging. While vision-based and light detection and ranging (LiDAR) sensors are commonly used for motion detection and localization of robotic arms, they are privacy-invasive and depend on a clear line-of-sight (LOS) for precise measurements. In cases where additional sensors are not available or LOS is not possible, these technologies may not be the best option. This paper proposes a novel method that employs channel state information (CSI) from WiFi signals affected by robotic arm motion. We developed a convolutional neural network (CNN) model to classify four different activities of a Franka Emika robotic arm. The implemented method seeks to accurately predict robot motion even in scenarios in which the robot is obscured by obstacles, without relying on any attached or internal sensors.
<div id='section'>Paperid: <span id='pid'>1082, <a href='https://arxiv.org/pdf/2305.18310.pdf' target='_blank'>https://arxiv.org/pdf/2305.18310.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaofeng Liu, Jiaxin Gao, Yaohua Liu, Risheng Liu, Nenggan Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.18310">Motion-Scenario Decoupling for Rat-Aware Video Position Prediction: Strategy and Benchmark</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently significant progress has been made in human action recognition and behavior prediction using deep learning techniques, leading to improved vision-based semantic understanding. However, there is still a lack of high-quality motion datasets for small bio-robotics, which presents more challenging scenarios for long-term movement prediction and behavior control based on third-person observation. In this study, we introduce RatPose, a bio-robot motion prediction dataset constructed by considering the influence factors of individuals and environments based on predefined annotation rules. To enhance the robustness of motion prediction against these factors, we propose a Dual-stream Motion-Scenario Decoupling (\textit{DMSD}) framework that effectively separates scenario-oriented and motion-oriented features and designs a scenario contrast loss and motion clustering loss for overall training. With such distinctive architecture, the dual-branch feature flow information is interacted and compensated in a decomposition-then-fusion manner. Moreover, we demonstrate significant performance improvements of the proposed \textit{DMSD} framework on different difficulty-level tasks. We also implement long-term discretized trajectory prediction tasks to verify the generalization ability of the proposed dataset.
<div id='section'>Paperid: <span id='pid'>1083, <a href='https://arxiv.org/pdf/2305.12411.pdf' target='_blank'>https://arxiv.org/pdf/2305.12411.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaifeng Zhao, Yan Zhang, Shaofei Wang, Thabo Beeler, Siyu Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.12411">Synthesizing Diverse Human Motions in 3D Indoor Scenes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a novel method for populating 3D indoor scenes with virtual humans that can navigate in the environment and interact with objects in a realistic manner. Existing approaches rely on training sequences that contain captured human motions and the 3D scenes they interact with. However, such interaction data are costly, difficult to capture, and can hardly cover all plausible human-scene interactions in complex environments. To address these challenges, we propose a reinforcement learning-based approach that enables virtual humans to navigate in 3D scenes and interact with objects realistically and autonomously, driven by learned motion control policies. The motion control policies employ latent motion action spaces, which correspond to realistic motion primitives and are learned from large-scale motion capture data using a powerful generative motion model. For navigation in a 3D environment, we propose a scene-aware policy with novel state and reward designs for collision avoidance. Combined with navigation mesh-based path-finding algorithms to generate intermediate waypoints, our approach enables the synthesis of diverse human motions navigating in 3D indoor scenes and avoiding obstacles. To generate fine-grained human-object interactions, we carefully curate interaction goal guidance using a marker-based body representation and leverage features based on the signed distance field (SDF) to encode human-scene proximity relations. Our method can synthesize realistic and diverse human-object interactions (e.g.,~sitting on a chair and then getting up) even for out-of-distribution test scenarios with different object shapes, orientations, starting body positions, and poses. Experimental results demonstrate that our approach outperforms state-of-the-art methods in terms of both motion naturalness and diversity. Code and video results are available at: https://zkf1997.github.io/DIMOS.
<div id='section'>Paperid: <span id='pid'>1084, <a href='https://arxiv.org/pdf/2305.08499.pdf' target='_blank'>https://arxiv.org/pdf/2305.08499.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lorenzo Rapetti, Carlotta Sartore, Mohamed Elobaid, Yeshasvi Tirupachuri, Francesco Draicchio, Tomohiro Kawakami, Takahide Yoshiike, Daniele Pucci
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.08499">A Control Approach for Human-Robot Ergonomic Payload Lifting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Collaborative robots can relief human operators from excessive efforts during payload lifting activities. Modelling the human partner allows the design of safe and efficient collaborative strategies. In this paper, we present a control approach for human-robot collaboration based on human monitoring through whole-body wearable sensors, and interaction modelling through coupled rigid-body dynamics. Moreover, a trajectory advancement strategy is proposed, allowing for online adaptation of the robot trajectory depending on the human motion. The resulting framework allows us to perform payload lifting tasks, taking into account the ergonomic requirements of the agents. Validation has been performed in an experimental scenario using the iCub3 humanoid robot and a human subject sensorized with the iFeel wearable system.
<div id='section'>Paperid: <span id='pid'>1085, <a href='https://arxiv.org/pdf/2304.12571.pdf' target='_blank'>https://arxiv.org/pdf/2304.12571.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuaiying Hou, Hongyu Tao, Hujun Bao, Weiwei Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.12571">A Two-part Transformer Network for Controllable Motion Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Although part-based motion synthesis networks have been investigated to reduce the complexity of modeling heterogeneous human motions, their computational cost remains prohibitive in interactive applications. To this end, we propose a novel two-part transformer network that aims to achieve high-quality, controllable motion synthesis results in real-time. Our network separates the skeleton into the upper and lower body parts, reducing the expensive cross-part fusion operations, and models the motions of each part separately through two streams of auto-regressive modules formed by multi-head attention layers. However, such a design might not sufficiently capture the correlations between the parts. We thus intentionally let the two parts share the features of the root joint and design a consistency loss to penalize the difference in the estimated root features and motions by these two auto-regressive modules, significantly improving the quality of synthesized motions. After training on our motion dataset, our network can synthesize a wide range of heterogeneous motions, like cartwheels and twists. Experimental and user study results demonstrate that our network is superior to state-of-the-art human motion synthesis networks in the quality of generated motions.
<div id='section'>Paperid: <span id='pid'>1086, <a href='https://arxiv.org/pdf/2304.02444.pdf' target='_blank'>https://arxiv.org/pdf/2304.02444.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>PÃ©ter Antal, TamÃ¡s PÃ©ni, Roland TÃ³th
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.02444">Autonomous Hook-Based Grasping and Transportation with Quadcopters</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Payload grasping and transportation with quadcopters is an active research area that has rapidly developed over the last decade. To grasp a payload without human interaction, most state-of-the-art approaches apply robotic arms that are attached to the quadcopter body. However, due to the large weight and power consumption of these aerial manipulators, their agility and flight time are limited. This paper proposes a motion control and planning method for transportation with a lightweight, passive manipulator structure that consists of a hook attached to a quadrotor using a 1 DoF revolute joint. To perform payload grasping, transportation, and release, first, time-optimal reference trajectories are designed through specific waypoints to ensure the fast and reliable execution of the tasks. Then, a two-stage motion control approach is developed based on a robust geometric controller for precise and reliable reference tracking and a linear--quadratic payload regulator for rapid setpoint stabilization of the payload swing. Furthermore, stability of the closed-loop system is mathematically proven to give safety guarantee for its operation. The proposed control architecture and design are evaluated in a high-fidelity physical simulator, and also in real flight experiments, using a custom-made quadrotor--hook manipulator platform.
<div id='section'>Paperid: <span id='pid'>1087, <a href='https://arxiv.org/pdf/2303.07605.pdf' target='_blank'>https://arxiv.org/pdf/2303.07605.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhipeng Luo, Gongjie Zhang, Changqing Zhou, Zhonghua Wu, Qingyi Tao, Lewei Lu, Shijian Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.07605">Modeling Continuous Motion for 3D Point Cloud Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The task of 3D single object tracking (SOT) with LiDAR point clouds is crucial for various applications, such as autonomous driving and robotics. However, existing approaches have primarily relied on appearance matching or motion modeling within only two successive frames, thereby overlooking the long-range continuous motion property of objects in 3D space. To address this issue, this paper presents a novel approach that views each tracklet as a continuous stream: at each timestamp, only the current frame is fed into the network to interact with multi-frame historical features stored in a memory bank, enabling efficient exploitation of sequential information. To achieve effective cross-frame message passing, a hybrid attention mechanism is designed to account for both long-range relation modeling and local geometric feature extraction. Furthermore, to enhance the utilization of multi-frame features for robust tracking, a contrastive sequence enhancement strategy is proposed, which uses ground truth tracklets to augment training sequences and promote discrimination against false positives in a contrastive manner. Extensive experiments demonstrate that the proposed method outperforms the state-of-the-art method by significant margins on multiple benchmarks.
<div id='section'>Paperid: <span id='pid'>1088, <a href='https://arxiv.org/pdf/2302.06195.pdf' target='_blank'>https://arxiv.org/pdf/2302.06195.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Julian Schmidt, Julian Jordan, Franz Gritschneder, Thomas Monninger, Klaus Dietmayer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.06195">Exploring Navigation Maps for Learning-Based Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The prediction of surrounding agents' motion is a key for safe autonomous driving. In this paper, we explore navigation maps as an alternative to the predominant High Definition (HD) maps for learning-based motion prediction. Navigation maps provide topological and geometrical information on road-level, HD maps additionally have centimeter-accurate lane-level information. As a result, HD maps are costly and time-consuming to obtain, while navigation maps with near-global coverage are freely available. We describe an approach to integrate navigation maps into learning-based motion prediction models. To exploit locally available HD maps during training, we additionally propose a model-agnostic method for knowledge distillation. In experiments on the publicly available Argoverse dataset with navigation maps obtained from OpenStreetMap, our approach shows a significant improvement over not using a map at all. Combined with our method for knowledge distillation, we achieve results that are close to the original HD map-reliant models. Our publicly available navigation map API for Argoverse enables researchers to develop and evaluate their own approaches using navigation maps.
<div id='section'>Paperid: <span id='pid'>1089, <a href='https://arxiv.org/pdf/2211.01696.pdf' target='_blank'>https://arxiv.org/pdf/2211.01696.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yue Yao, Daniel Goehring, Joerg Reichardt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.01696">An Empirical Bayes Analysis of Object Trajectory Representation Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Linear trajectory models provide mathematical advantages to autonomous driving applications such as motion prediction. However, linear models' expressive power and bias for real-world trajectories have not been thoroughly analyzed. We present an in-depth empirical analysis of the trade-off between model complexity and fit error in modelling object trajectories. We analyze vehicle, cyclist, and pedestrian trajectories. Our methodology estimates observation noise and prior distributions over model parameters from several large-scale datasets. Incorporating these priors can then regularize prediction models. Our results show that linear models do represent real-world trajectories with high fidelity at very moderate model complexity. This suggests the feasibility of using linear trajectory models in future motion prediction systems with inherent mathematical advantages.
<div id='section'>Paperid: <span id='pid'>1090, <a href='https://arxiv.org/pdf/2101.12276.pdf' target='_blank'>https://arxiv.org/pdf/2101.12276.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuaiying Hou, Congyi Wang, Wenlin Zhuang, Yu Chen, Yangang Wang, Hujun Bao, Jinxiang Chai, Weiwei Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2101.12276">A causal convolutional neural network for multi-subject motion modeling and generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Inspired by the success of WaveNet in multi-subject speech synthesis, we propose a novel neural network based on causal convolutions for multi-subject motion modeling and generation. The network can capture the intrinsic characteristics of the motion of different subjects, such as the influence of skeleton scale variation on motion style. Moreover, after fine-tuning the network using a small motion dataset for a novel skeleton that is not included in the training dataset, it is able to synthesize high-quality motions with a personalized style for the novel skeleton. The experimental results demonstrate that our network can model the intrinsic characteristics of motions well and can be applied to various motion modeling and synthesis tasks.
<div id='section'>Paperid: <span id='pid'>1091, <a href='https://arxiv.org/pdf/2007.06343.pdf' target='_blank'>https://arxiv.org/pdf/2007.06343.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rahul Tallamraju, Nitin Saini, Elia Bonetto, Michael Pabst, Yu Tang Liu, Michael J. Black, Aamir Ahmad
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2007.06343">AirCapRL: Autonomous Aerial Human Motion Capture using Deep Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this letter, we introduce a deep reinforcement learning (RL) based multi-robot formation controller for the task of autonomous aerial human motion capture (MoCap). We focus on vision-based MoCap, where the objective is to estimate the trajectory of body pose and shape of a single moving person using multiple micro aerial vehicles. State-of-the-art solutions to this problem are based on classical control methods, which depend on hand-crafted system and observation models. Such models are difficult to derive and generalize across different systems. Moreover, the non-linearity and non-convexities of these models lead to sub-optimal controls. In our work, we formulate this problem as a sequential decision making task to achieve the vision-based motion capture objectives, and solve it using a deep neural network-based RL method. We leverage proximal policy optimization (PPO) to train a stochastic decentralized control policy for formation control. The neural network is trained in a parallelized setup in synthetic environments. We performed extensive simulation experiments to validate our approach. Finally, real-robot experiments demonstrate that our policies generalize to real world conditions. Video Link: https://bit.ly/38SJfjo Supplementary: https://bit.ly/3evfo1O
<div id='section'>Paperid: <span id='pid'>1092, <a href='https://arxiv.org/pdf/2007.02219.pdf' target='_blank'>https://arxiv.org/pdf/2007.02219.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yongqian Xiao, Xinglong Zhang, Xin Xu, Xueqing Liu, Jiahang Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2007.02219">Deep Neural Networks with Koopman Operators for Modeling and Control of Autonomous Vehicles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous driving technologies have received notable attention in the past decades. In autonomous driving systems, identifying a precise dynamical model for motion control is nontrivial due to the strong nonlinearity and uncertainty in vehicle dynamics. Recent efforts have resorted to machine learning techniques for building vehicle dynamical models, but the generalization ability and interpretability of existing methods still need to be improved. In this paper, we propose a data-driven vehicle modeling approach based on deep neural networks with an interpretable Koopman operator. The main advantage of using the Koopman operator is to represent the nonlinear dynamics in a linear lifted feature space. In the proposed approach, a deep learning-based extended dynamic mode decomposition algorithm is presented to learn a finite-dimensional approximation of the Koopman operator. Furthermore, a data-driven model predictive controller with the learned Koopman model is designed for path tracking control of autonomous vehicles. Simulation results in a high-fidelity CarSim environment show that our approach exhibit a high modeling precision at a wide operating range and outperforms previously developed methods in terms of modeling performance. Path tracking tests of the autonomous vehicle are also performed in the CarSim environment and the results show the effectiveness of the proposed approach.
<div id='section'>Paperid: <span id='pid'>1093, <a href='https://arxiv.org/pdf/2509.20263.pdf' target='_blank'>https://arxiv.org/pdf/2509.20263.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bingjie Chen, Zihan Wang, Zhe Han, Guoping Pan, Yi Cheng, Houde Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20263">HL-IK: A Lightweight Implementation of Human-Like Inverse Kinematics in Humanoid Arms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Traditional IK methods for redundant humanoid manipulators emphasize end-effector (EE) tracking, frequently producing configurations that are valid mechanically but not human-like. We present Human-Like Inverse Kinematics (HL-IK), a lightweight IK framework that preserves EE tracking while shaping whole-arm configurations to appear human-like, without full-body sensing at runtime. The key idea is a learned elbow prior: using large-scale human motion data retargeted to the robot, we train a FiLM-modulated spatio-temporal attention network (FiSTA) to predict the next-step elbow pose from the EE target and a short history of EE-elbow states.This prediction is incorporated as a small residual alongside EE and smoothness terms in a standard Levenberg-Marquardt optimizer, making HL-IK a drop-in addition to numerical IK stacks. Over 183k simulation steps, HL-IK reduces arm-similarity position and direction error by 30.6% and 35.4% on average, and by 42.2% and 47.4% on the most challenging trajectories. Hardware teleoperation on a robot distinct from simulation further confirms the gains in anthropomorphism. HL-IK is simple to integrate, adaptable across platforms via our pipeline, and adds minimal computation, enabling human-like motions for humanoid robots. Project page: https://hl-ik.github.io/
<div id='section'>Paperid: <span id='pid'>1094, <a href='https://arxiv.org/pdf/2509.18427.pdf' target='_blank'>https://arxiv.org/pdf/2509.18427.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinyang Wu, Muheng Li, Xia Li, Orso Pusterla, Sairos Safai, Philippe C. Cattin, Antony J. Lomax, Ye Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.18427">CPT-4DMR: Continuous sPatial-Temporal Representation for 4D-MRI Reconstruction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Four-dimensional MRI (4D-MRI) is an promising technique for capturing respiratory-induced motion in radiation therapy planning and delivery. Conventional 4D reconstruction methods, which typically rely on phase binning or separate template scans, struggle to capture temporal variability, complicate workflows, and impose heavy computational loads. We introduce a neural representation framework that considers respiratory motion as a smooth, continuous deformation steered by a 1D surrogate signal, completely replacing the conventional discrete sorting approach. The new method fuses motion modeling with image reconstruction through two synergistic networks: the Spatial Anatomy Network (SAN) encodes a continuous 3D anatomical representation, while a Temporal Motion Network (TMN), guided by Transformer-derived respiratory signals, produces temporally consistent deformation fields. Evaluation using a free-breathing dataset of 19 volunteers demonstrates that our template- and phase-free method accurately captures both regular and irregular respiratory patterns, while preserving vessel and bronchial continuity with high anatomical fidelity. The proposed method significantly improves efficiency, reducing the total processing time from approximately five hours required by conventional discrete sorting methods to just 15 minutes of training. Furthermore, it enables inference of each 3D volume in under one second. The framework accurately reconstructs 3D images at any respiratory state, achieves superior performance compared to conventional methods, and demonstrates strong potential for application in 4D radiation therapy planning and real-time adaptive treatment.
<div id='section'>Paperid: <span id='pid'>1095, <a href='https://arxiv.org/pdf/2508.21095.pdf' target='_blank'>https://arxiv.org/pdf/2508.21095.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Thomas Besnier, Sylvain ArguillÃ¨re, Mohamed Daoudi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.21095">ScanMove: Motion Prediction and Transfer for Unregistered Body Meshes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unregistered surface meshes, especially raw 3D scans, present significant challenges for automatic computation of plausible deformations due to the lack of established point-wise correspondences and the presence of noise in the data. In this paper, we propose a new, rig-free, data-driven framework for motion prediction and transfer on such body meshes. Our method couples a robust motion embedding network with a learned per-vertex feature field to generate a spatio-temporal deformation field, which drives the mesh deformation. Extensive evaluations, including quantitative benchmarks and qualitative visuals on tasks such as walking and running, demonstrate the effectiveness and versatility of our approach on challenging unregistered meshes.
<div id='section'>Paperid: <span id='pid'>1096, <a href='https://arxiv.org/pdf/2508.21095.pdf' target='_blank'>https://arxiv.org/pdf/2508.21095.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Thomas Besnier, Sylvain Arguillère, Mohamed Daoudi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.21095">ScanMove: Motion Prediction and Transfer for Unregistered Body Meshes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unregistered surface meshes, especially raw 3D scans, present significant challenges for automatic computation of plausible deformations due to the lack of established point-wise correspondences and the presence of noise in the data. In this paper, we propose a new, rig-free, data-driven framework for motion prediction and transfer on such body meshes. Our method couples a robust motion embedding network with a learned per-vertex feature field to generate a spatio-temporal deformation field, which drives the mesh deformation. Extensive evaluations, including quantitative benchmarks and qualitative visuals on tasks such as walking and running, demonstrate the effectiveness and versatility of our approach on challenging unregistered meshes.
<div id='section'>Paperid: <span id='pid'>1097, <a href='https://arxiv.org/pdf/2508.14033.pdf' target='_blank'>https://arxiv.org/pdf/2508.14033.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shaoshu Yang, Zhe Kong, Feng Gao, Meng Cheng, Xiangyu Liu, Yong Zhang, Zhuoliang Kang, Wenhan Luo, Xunliang Cai, Ran He, Xiaoming Wei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.14033">InfiniteTalk: Audio-driven Video Generation for Sparse-Frame Video Dubbing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent breakthroughs in video AIGC have ushered in a transformative era for audio-driven human animation. However, conventional video dubbing techniques remain constrained to mouth region editing, resulting in discordant facial expressions and body gestures that compromise viewer immersion. To overcome this limitation, we introduce sparse-frame video dubbing, a novel paradigm that strategically preserves reference keyframes to maintain identity, iconic gestures, and camera trajectories while enabling holistic, audio-synchronized full-body motion editing. Through critical analysis, we identify why naive image-to-video models fail in this task, particularly their inability to achieve adaptive conditioning. Addressing this, we propose InfiniteTalk, a streaming audio-driven generator designed for infinite-length long sequence dubbing. This architecture leverages temporal context frames for seamless inter-chunk transitions and incorporates a simple yet effective sampling strategy that optimizes control strength via fine-grained reference frame positioning. Comprehensive evaluations on HDTF, CelebV-HQ, and EMTD datasets demonstrate state-of-the-art performance. Quantitative metrics confirm superior visual realism, emotional coherence, and full-body motion synchronization.
<div id='section'>Paperid: <span id='pid'>1098, <a href='https://arxiv.org/pdf/2508.01585.pdf' target='_blank'>https://arxiv.org/pdf/2508.01585.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hua Yu, Yaqing Hou, Xu Gui, Shanshan Feng, Dongsheng Zhou, Qiang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01585">A Spatio-temporal Continuous Network for Stochastic 3D Human Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Stochastic Human Motion Prediction (HMP) has received increasing attention due to its wide applications. Despite the rapid progress in generative fields, existing methods often face challenges in learning continuous temporal dynamics and predicting stochastic motion sequences. They tend to overlook the flexibility inherent in complex human motions and are prone to mode collapse. To alleviate these issues, we propose a novel method called STCN, for stochastic and continuous human motion prediction, which consists of two stages. Specifically, in the first stage, we propose a spatio-temporal continuous network to generate smoother human motion sequences. In addition, the anchor set is innovatively introduced into the stochastic HMP task to prevent mode collapse, which refers to the potential human motion patterns. In the second stage, STCN endeavors to acquire the Gaussian mixture distribution (GMM) of observed motion sequences with the aid of the anchor set. It also focuses on the probability associated with each anchor, and employs the strategy of sampling multiple sequences from each anchor to alleviate intra-class differences in human motions. Experimental results on two widely-used datasets (Human3.6M and HumanEva-I) demonstrate that our model obtains competitive performance on both diversity and accuracy.
<div id='section'>Paperid: <span id='pid'>1099, <a href='https://arxiv.org/pdf/2508.01126.pdf' target='_blank'>https://arxiv.org/pdf/2508.01126.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chaitanya Patel, Hiroki Nakamura, Yuta Kyuragi, Kazuki Kozuka, Juan Carlos Niebles, Ehsan Adeli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01126">UniEgoMotion: A Unified Model for Egocentric Motion Reconstruction, Forecasting, and Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Egocentric human motion generation and forecasting with scene-context is crucial for enhancing AR/VR experiences, improving human-robot interaction, advancing assistive technologies, and enabling adaptive healthcare solutions by accurately predicting and simulating movement from a first-person perspective. However, existing methods primarily focus on third-person motion synthesis with structured 3D scene contexts, limiting their effectiveness in real-world egocentric settings where limited field of view, frequent occlusions, and dynamic cameras hinder scene perception. To bridge this gap, we introduce Egocentric Motion Generation and Egocentric Motion Forecasting, two novel tasks that utilize first-person images for scene-aware motion synthesis without relying on explicit 3D scene. We propose UniEgoMotion, a unified conditional motion diffusion model with a novel head-centric motion representation tailored for egocentric devices. UniEgoMotion's simple yet effective design supports egocentric motion reconstruction, forecasting, and generation from first-person visual inputs in a unified framework. Unlike previous works that overlook scene semantics, our model effectively extracts image-based scene context to infer plausible 3D motion. To facilitate training, we introduce EE4D-Motion, a large-scale dataset derived from EgoExo4D, augmented with pseudo-ground-truth 3D motion annotations. UniEgoMotion achieves state-of-the-art performance in egocentric motion reconstruction and is the first to generate motion from a single egocentric image. Extensive evaluations demonstrate the effectiveness of our unified framework, setting a new benchmark for egocentric motion modeling and unlocking new possibilities for egocentric applications.
<div id='section'>Paperid: <span id='pid'>1100, <a href='https://arxiv.org/pdf/2507.22792.pdf' target='_blank'>https://arxiv.org/pdf/2507.22792.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guoping Xu, Jayaram K. Udupa, Yajun Yu, Hua-Chieh Shao, Songlin Zhao, Wei Liu, You Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.22792">Segment Anything for Video: A Comprehensive Review of Video Object Segmentation and Tracking from Past to Future</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video Object Segmentation and Tracking (VOST) presents a complex yet critical challenge in computer vision, requiring robust integration of segmentation and tracking across temporally dynamic frames. Traditional methods have struggled with domain generalization, temporal consistency, and computational efficiency. The emergence of foundation models like the Segment Anything Model (SAM) and its successor, SAM2, has introduced a paradigm shift, enabling prompt-driven segmentation with strong generalization capabilities. Building upon these advances, this survey provides a comprehensive review of SAM/SAM2-based methods for VOST, structured along three temporal dimensions: past, present, and future. We examine strategies for retaining and updating historical information (past), approaches for extracting and optimizing discriminative features from the current frame (present), and motion prediction and trajectory estimation mechanisms for anticipating object dynamics in subsequent frames (future). In doing so, we highlight the evolution from early memory-based architectures to the streaming memory and real-time segmentation capabilities of SAM2. We also discuss recent innovations such as motion-aware memory selection and trajectory-guided prompting, which aim to enhance both accuracy and efficiency. Finally, we identify remaining challenges including memory redundancy, error accumulation, and prompt inefficiency, and suggest promising directions for future research. This survey offers a timely and structured overview of the field, aiming to guide researchers and practitioners in advancing the state of VOST through the lens of foundation models.
<div id='section'>Paperid: <span id='pid'>1101, <a href='https://arxiv.org/pdf/2507.07394.pdf' target='_blank'>https://arxiv.org/pdf/2507.07394.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhimin Zhang, Bi'an Du, Caoyuan Ma, Zheng Wang, Wei Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07394">Behave Your Motion: Habit-preserved Cross-category Animal Motion Transfer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Animal motion embodies species-specific behavioral habits, making the transfer of motion across categories a critical yet complex task for applications in animation and virtual reality. Existing motion transfer methods, primarily focused on human motion, emphasize skeletal alignment (motion retargeting) or stylistic consistency (motion style transfer), often neglecting the preservation of distinct habitual behaviors in animals. To bridge this gap, we propose a novel habit-preserved motion transfer framework for cross-category animal motion. Built upon a generative framework, our model introduces a habit-preservation module with category-specific habit encoder, allowing it to learn motion priors that capture distinctive habitual characteristics. Furthermore, we integrate a large language model (LLM) to facilitate the motion transfer to previously unobserved species. To evaluate the effectiveness of our approach, we introduce the DeformingThings4D-skl dataset, a quadruped dataset with skeletal bindings, and conduct extensive experiments and quantitative analyses, which validate the superiority of our proposed model.
<div id='section'>Paperid: <span id='pid'>1102, <a href='https://arxiv.org/pdf/2506.07456.pdf' target='_blank'>https://arxiv.org/pdf/2506.07456.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Yao, Yunlian Sun, Chang Liu, Hongwen Zhang, Jinhui Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.07456">PhysiInter: Integrating Physical Mapping for High-Fidelity Human Interaction Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Driven by advancements in motion capture and generative artificial intelligence, leveraging large-scale MoCap datasets to train generative models for synthesizing diverse, realistic human motions has become a promising research direction. However, existing motion-capture techniques and generative models often neglect physical constraints, leading to artifacts such as interpenetration, sliding, and floating. These issues are exacerbated in multi-person motion generation, where complex interactions are involved. To address these limitations, we introduce physical mapping, integrated throughout the human interaction generation pipeline. Specifically, motion imitation within a physics-based simulation environment is used to project target motions into a physically valid space. The resulting motions are adjusted to adhere to real-world physics constraints while retaining their original semantic meaning. This mapping not only improves MoCap data quality but also directly informs post-processing of generated motions. Given the unique interactivity of multi-person scenarios, we propose a tailored motion representation framework. Motion Consistency (MC) and Marker-based Interaction (MI) loss functions are introduced to improve model performance. Experiments show our method achieves impressive results in generated human motion quality, with a 3%-89% improvement in physical fidelity. Project page http://yw0208.github.io/physiinter
<div id='section'>Paperid: <span id='pid'>1103, <a href='https://arxiv.org/pdf/2505.20744.pdf' target='_blank'>https://arxiv.org/pdf/2505.20744.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Zhang, Zhan Zhuang, Xuehao Wang, Xiaodong Yang, Yu Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.20744">MoPFormer: Motion-Primitive Transformer for Wearable-Sensor Activity Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human Activity Recognition (HAR) with wearable sensors is challenged by limited interpretability, which significantly impacts cross-dataset generalization. To address this challenge, we propose Motion-Primitive Transformer (MoPFormer), a novel self-supervised framework that enhances interpretability by tokenizing inertial measurement unit signals into semantically meaningful motion primitives and leverages a Transformer architecture to learn rich temporal representations. MoPFormer comprises two-stages. first stage is to partition multi-channel sensor streams into short segments and quantizing them into discrete "motion primitive" codewords, while the second stage enriches those tokenized sequences through a context-aware embedding module and then processes them with a Transformer encoder. The proposed MoPFormer can be pre-trained using a masked motion-modeling objective that reconstructs missing primitives, enabling it to develop robust representations across diverse sensor configurations. Experiments on six HAR benchmarks demonstrate that MoPFormer not only outperforms state-of-the-art methods but also successfully generalizes across multiple datasets. Most importantly, the learned motion primitives significantly enhance both interpretability and cross-dataset performance by capturing fundamental movement patterns that remain consistent across similar activities regardless of dataset origin.
<div id='section'>Paperid: <span id='pid'>1104, <a href='https://arxiv.org/pdf/2505.19086.pdf' target='_blank'>https://arxiv.org/pdf/2505.19086.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chen Tessler, Yifeng Jiang, Erwin Coumans, Zhengyi Luo, Gal Chechik, Xue Bin Peng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.19086">MaskedManipulator: Versatile Whole-Body Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We tackle the challenges of synthesizing versatile, physically simulated human motions for full-body object manipulation. Unlike prior methods that are focused on detailed motion tracking, trajectory following, or teleoperation, our framework enables users to specify versatile high-level objectives such as target object poses or body poses. To achieve this, we introduce MaskedManipulator, a generative control policy distilled from a tracking controller trained on large-scale human motion capture data. This two-stage learning process allows the system to perform complex interaction behaviors, while providing intuitive user control over both character and object motions. MaskedManipulator produces goal-directed manipulation behaviors that expand the scope of interactive animation systems beyond task-specific solutions.
<div id='section'>Paperid: <span id='pid'>1105, <a href='https://arxiv.org/pdf/2505.11832.pdf' target='_blank'>https://arxiv.org/pdf/2505.11832.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxiang Lai, Jike Zhong, Vanessa Su, Xiaofeng Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.11832">Patient-Specific Autoregressive Models for Organ Motion Prediction in Radiotherapy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Radiotherapy often involves a prolonged treatment period. During this time, patients may experience organ motion due to breathing and other physiological factors. Predicting and modeling this motion before treatment is crucial for ensuring precise radiation delivery. However, existing pre-treatment organ motion prediction methods primarily rely on deformation analysis using principal component analysis (PCA), which is highly dependent on registration quality and struggles to capture periodic temporal dynamics for motion modeling.In this paper, we observe that organ motion prediction closely resembles an autoregressive process, a technique widely used in natural language processing (NLP). Autoregressive models predict the next token based on previous inputs, naturally aligning with our objective of predicting future organ motion phases. Building on this insight, we reformulate organ motion prediction as an autoregressive process to better capture patient-specific motion patterns. Specifically, we acquire 4D CT scans for each patient before treatment, with each sequence comprising multiple 3D CT phases. These phases are fed into the autoregressive model to predict future phases based on prior phase motion patterns. We evaluate our method on a real-world test set of 4D CT scans from 50 patients who underwent radiotherapy at our institution and a public dataset containing 4D CT scans from 20 patients (some with multiple scans), totaling over 1,300 3D CT phases. The performance in predicting the motion of the lung and heart surpasses existing benchmarks, demonstrating its effectiveness in capturing motion dynamics from CT images. These results highlight the potential of our method to improve pre-treatment planning in radiotherapy, enabling more precise and adaptive radiation delivery.
<div id='section'>Paperid: <span id='pid'>1106, <a href='https://arxiv.org/pdf/2505.09393.pdf' target='_blank'>https://arxiv.org/pdf/2505.09393.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Huakun Liu, Hiroki Ota, Xin Wei, Yutaro Hirao, Monica Perusquia-Hernandez, Hideaki Uchiyama, Kiyoshi Kiyokawa
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.09393">UMotion: Uncertainty-driven Human Motion Estimation from Inertial and Ultra-wideband Units</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Sparse wearable inertial measurement units (IMUs) have gained popularity for estimating 3D human motion. However, challenges such as pose ambiguity, data drift, and limited adaptability to diverse bodies persist. To address these issues, we propose UMotion, an uncertainty-driven, online fusing-all state estimation framework for 3D human shape and pose estimation, supported by six integrated, body-worn ultra-wideband (UWB) distance sensors with IMUs. UWB sensors measure inter-node distances to infer spatial relationships, aiding in resolving pose ambiguities and body shape variations when combined with anthropometric data. Unfortunately, IMUs are prone to drift, and UWB sensors are affected by body occlusions. Consequently, we develop a tightly coupled Unscented Kalman Filter (UKF) framework that fuses uncertainties from sensor data and estimated human motion based on individual body shape. The UKF iteratively refines IMU and UWB measurements by aligning them with uncertain human motion constraints in real-time, producing optimal estimates for each. Experiments on both synthetic and real-world datasets demonstrate the effectiveness of UMotion in stabilizing sensor data and the improvement over state of the art in pose accuracy.
<div id='section'>Paperid: <span id='pid'>1107, <a href='https://arxiv.org/pdf/2505.08238.pdf' target='_blank'>https://arxiv.org/pdf/2505.08238.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunyue Wei, Shanning Zhuang, Vincent Zhuang, Yanan Sui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.08238">Motion Control of High-Dimensional Musculoskeletal Systems with Hierarchical Model-Based Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Controlling high-dimensional nonlinear systems, such as those found in biological and robotic applications, is challenging due to large state and action spaces. While deep reinforcement learning has achieved a number of successes in these domains, it is computationally intensive and time consuming, and therefore not suitable for solving large collections of tasks that require significant manual tuning. In this work, we introduce Model Predictive Control with Morphology-aware Proportional Control (MPC^2), a hierarchical model-based learning algorithm for zero-shot and near-real-time control of high-dimensional complex dynamical systems. MPC^2 uses a sampling-based model predictive controller for target posture planning, and enables robust control for high-dimensional tasks by incorporating a morphology-aware proportional controller for actuator coordination. The algorithm enables motion control of a high-dimensional human musculoskeletal model in a variety of motion tasks, such as standing, walking on different terrains, and imitating sports activities. The reward function of MPC^2 can be tuned via black-box optimization, drastically reducing the need for human-intensive reward engineering.
<div id='section'>Paperid: <span id='pid'>1108, <a href='https://arxiv.org/pdf/2505.00998.pdf' target='_blank'>https://arxiv.org/pdf/2505.00998.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Hua, Weiming Liu, Gui Xu, Yaqing Hou, Yew-Soon Ong, Qiang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.00998">Deterministic-to-Stochastic Diverse Latent Feature Mapping for Human Motion Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion synthesis aims to generate plausible human motion sequences, which has raised widespread attention in computer animation. Recent score-based generative models (SGMs) have demonstrated impressive results on this task. However, their training process involves complex curvature trajectories, leading to unstable training process. In this paper, we propose a Deterministic-to-Stochastic Diverse Latent Feature Mapping (DSDFM) method for human motion synthesis. DSDFM consists of two stages. The first human motion reconstruction stage aims to learn the latent space distribution of human motions. The second diverse motion generation stage aims to build connections between the Gaussian distribution and the latent space distribution of human motions, thereby enhancing the diversity and accuracy of the generated human motions. This stage is achieved by the designed deterministic feature mapping procedure with DerODE and stochastic diverse output generation procedure with DivSDE.DSDFM is easy to train compared to previous SGMs-based methods and can enhance diversity without introducing additional training parameters.Through qualitative and quantitative experiments, DSDFM achieves state-of-the-art results surpassing the latest methods, validating its superiority in human motion synthesis.
<div id='section'>Paperid: <span id='pid'>1109, <a href='https://arxiv.org/pdf/2504.08959.pdf' target='_blank'>https://arxiv.org/pdf/2504.08959.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yilin Wang, Chuan Guo, Yuxuan Mu, Muhammad Gohar Javed, Xinxin Zuo, Juwei Lu, Hai Jiang, Li Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.08959">MotionDreamer: One-to-Many Motion Synthesis with Localized Generative Masked Transformer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generative masked transformers have demonstrated remarkable success across various content generation tasks, primarily due to their ability to effectively model large-scale dataset distributions with high consistency. However, in the animation domain, large datasets are not always available. Applying generative masked modeling to generate diverse instances from a single MoCap reference may lead to overfitting, a challenge that remains unexplored. In this work, we present MotionDreamer, a localized masked modeling paradigm designed to learn internal motion patterns from a given motion with arbitrary topology and duration. By embedding the given motion into quantized tokens with a novel distribution regularization method, MotionDreamer constructs a robust and informative codebook for local motion patterns. Moreover, a sliding window local attention is introduced in our masked transformer, enabling the generation of natural yet diverse animations that closely resemble the reference motion patterns. As demonstrated through comprehensive experiments, MotionDreamer outperforms the state-of-the-art methods that are typically GAN or Diffusion-based in both faithfulness and diversity. Thanks to the consistency and robustness of the quantization-based approach, MotionDreamer can also effectively perform downstream tasks such as temporal motion editing, \textcolor{update}{crowd animation}, and beat-aligned dance generation, all using a single reference motion. Visit our project page: https://motiondreamer.github.io/
<div id='section'>Paperid: <span id='pid'>1110, <a href='https://arxiv.org/pdf/2503.19738.pdf' target='_blank'>https://arxiv.org/pdf/2503.19738.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yingqing Chen, Christos G. Cassandras
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.19738">Optimal Safe Sequencing and Motion Control for Mixed Traffic Roundabouts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper develops an Optimal Safe Sequencing (OSS) control framework for Connected and Automated Vehicles (CAVs) navigating a single-lane roundabout in mixed traffic, where both CAVs and Human-Driven Vehicles (HDVs) coexist. The framework jointly optimizes vehicle sequencing and motion control to minimize travel time, energy consumption, and discomfort while ensuring speed-dependent safety guarantees and adhering to velocity and acceleration constraints. This is achieved by integrating (a) a Safe Sequencing (SS) policy that ensures merging safety without requiring any knowledge of HDV behavior, and (b) a Model Predictive Control with Control Lyapunov Barrier Functions (MPC-CLBF) framework, which optimizes CAV motion control while mitigating infeasibility and myopic control issues common in the use of Control Barrier Functions (CBFs) to provide safety guarantees. Simulation results across various traffic demands, CAV penetration rates, and control parameters demonstrate the framework's effectiveness and stability.
<div id='section'>Paperid: <span id='pid'>1111, <a href='https://arxiv.org/pdf/2503.15819.pdf' target='_blank'>https://arxiv.org/pdf/2503.15819.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junyi Shen, Tetsuro Miyazaki, Kenji Kawashima
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.15819">Control Pneumatic Soft Bending Actuator with Online Learning Pneumatic Physical Reservoir Computing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The intrinsic nonlinearities of soft robots present significant control but simultaneously provide them with rich computational potential. Reservoir computing (RC) has shown effectiveness in online learning systems for controlling nonlinear systems such as soft actuators. Conventional RC can be extended into physical reservoir computing (PRC) by leveraging the nonlinear dynamics of soft actuators for computation. This paper introduces a PRC-based online learning framework to control the motion of a pneumatic soft bending actuator, utilizing another pneumatic soft actuator as the PRC model. Unlike conventional designs requiring two RC models, the proposed control system employs a more compact architecture with a single RC model. Additionally, the framework enables zero-shot online learning, addressing limitations of previous PRC-based control systems reliant on offline training. Simulations and experiments validated the performance of the proposed system. Experimental results indicate that the PRC model achieved superior control performance compared to a linear model, reducing the root-mean-square error (RMSE) by an average of over 37% in bending motion control tasks. The proposed PRC-based online learning control framework provides a novel approach for harnessing physical systems' inherent nonlinearities to enhance the control of soft actuators.
<div id='section'>Paperid: <span id='pid'>1112, <a href='https://arxiv.org/pdf/2503.13836.pdf' target='_blank'>https://arxiv.org/pdf/2503.13836.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Seokhyeon Hong, Chaelin Kim, Serin Yoon, Junghyun Nam, Sihun Cha, Junyong Noh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.13836">SALAD: Skeleton-aware Latent Diffusion for Text-driven Motion Generation and Editing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-driven motion generation has advanced significantly with the rise of denoising diffusion models. However, previous methods often oversimplify representations for the skeletal joints, temporal frames, and textual words, limiting their ability to fully capture the information within each modality and their interactions. Moreover, when using pre-trained models for downstream tasks, such as editing, they typically require additional efforts, including manual interventions, optimization, or fine-tuning. In this paper, we introduce a skeleton-aware latent diffusion (SALAD), a model that explicitly captures the intricate inter-relationships between joints, frames, and words. Furthermore, by leveraging cross-attention maps produced during the generation process, we enable attention-based zero-shot text-driven motion editing using a pre-trained SALAD model, requiring no additional user input beyond text prompts. Our approach significantly outperforms previous methods in terms of text-motion alignment without compromising generation quality, and demonstrates practical versatility by providing diverse editing capabilities beyond generation. Code is available at project page.
<div id='section'>Paperid: <span id='pid'>1113, <a href='https://arxiv.org/pdf/2502.14917.pdf' target='_blank'>https://arxiv.org/pdf/2502.14917.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rui Zhao, Qirui Yuan, Jinyu Li, Haofeng Hu, Yun Li, Chengyuan Zheng, Fei Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.14917">Sce2DriveX: A Generalized MLLM Framework for Scene-to-Drive Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>End-to-end autonomous driving, which directly maps raw sensor inputs to low-level vehicle controls, is an important part of Embodied AI. Despite successes in applying Multimodal Large Language Models (MLLMs) for high-level traffic scene semantic understanding, it remains challenging to effectively translate these conceptual semantics understandings into low-level motion control commands and achieve generalization and consensus in cross-scene driving. We introduce Sce2DriveX, a human-like driving chain-of-thought (CoT) reasoning MLLM framework. Sce2DriveX utilizes multimodal joint learning from local scene videos and global BEV maps to deeply understand long-range spatiotemporal relationships and road topology, enhancing its comprehensive perception and reasoning capabilities in 3D dynamic/static scenes and achieving driving generalization across scenes. Building on this, it reconstructs the implicit cognitive chain inherent in human driving, covering scene understanding, meta-action reasoning, behavior interpretation analysis, motion planning and control, thereby further bridging the gap between autonomous driving and human thought processes. To elevate model performance, we have developed the first extensive Visual Question Answering (VQA) driving instruction dataset tailored for 3D spatial understanding and long-axis task reasoning. Extensive experiments demonstrate that Sce2DriveX achieves state-of-the-art performance from scene understanding to end-to-end driving, as well as robust generalization on the CARLA Bench2Drive benchmark.
<div id='section'>Paperid: <span id='pid'>1114, <a href='https://arxiv.org/pdf/2502.11515.pdf' target='_blank'>https://arxiv.org/pdf/2502.11515.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junxian Ma, Shiwen Wang, Jian Yang, Junyi Hu, Jian Liang, Guosheng Lin, Jingbo chen, Kai Li, Yu Meng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.11515">SayAnything: Audio-Driven Lip Synchronization with Conditional Video Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in diffusion models have led to significant progress in audio-driven lip synchronization. However, existing methods typically rely on constrained audio-visual alignment priors or multi-stage learning of intermediate representations to force lip motion synthesis. This leads to complex training pipelines and limited motion naturalness. In this paper, we present SayAnything, a conditional video diffusion framework that directly synthesizes lip movements from audio input while preserving speaker identity. Specifically, we propose three specialized modules including identity preservation module, audio guidance module, and editing control module. Our novel design effectively balances different condition signals in the latent space, enabling precise control over appearance, motion, and region-specific generation without requiring additional supervision signals or intermediate representations. Extensive experiments demonstrate that SayAnything generates highly realistic videos with improved lip-teeth coherence, enabling unseen characters to say anything, while effectively generalizing to animated characters.
<div id='section'>Paperid: <span id='pid'>1115, <a href='https://arxiv.org/pdf/2502.08377.pdf' target='_blank'>https://arxiv.org/pdf/2502.08377.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Liying Yang, Chen Liu, Zhenwei Zhu, Ajian Liu, Hui Ma, Jian Nong, Yanyan Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.08377">Not All Frame Features Are Equal: Video-to-4D Generation via Decoupling Dynamic-Static Features</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, the generation of dynamic 3D objects from a video has shown impressive results. Existing methods directly optimize Gaussians using whole information in frames. However, when dynamic regions are interwoven with static regions within frames, particularly if the static regions account for a large proportion, existing methods often overlook information in dynamic regions and are prone to overfitting on static regions. This leads to producing results with blurry textures. We consider that decoupling dynamic-static features to enhance dynamic representations can alleviate this issue. Thus, we propose a dynamic-static feature decoupling module (DSFD). Along temporal axes, it regards the regions of current frame features that possess significant differences relative to reference frame features as dynamic features. Conversely, the remaining parts are the static features. Then, we acquire decoupled features driven by dynamic features and current frame features. Moreover, to further enhance the dynamic representation of decoupled features from different viewpoints and ensure accurate motion prediction, we design a temporal-spatial similarity fusion module (TSSF). Along spatial axes, it adaptively selects similar information of dynamic regions. Hinging on the above, we construct a novel approach, DS4D. Experimental results verify our method achieves state-of-the-art (SOTA) results in video-to-4D. In addition, the experiments on a real-world scenario dataset demonstrate its effectiveness on the 4D scene. Our code will be publicly available.
<div id='section'>Paperid: <span id='pid'>1116, <a href='https://arxiv.org/pdf/2501.02158.pdf' target='_blank'>https://arxiv.org/pdf/2501.02158.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhizheng Liu, Joe Lin, Wayne Wu, Bolei Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.02158">Joint Optimization for 4D Human-Scene Reconstruction in the Wild</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reconstructing human motion and its surrounding environment is crucial for understanding human-scene interaction and predicting human movements in the scene. While much progress has been made in capturing human-scene interaction in constrained environments, those prior methods can hardly reconstruct the natural and diverse human motion and scene context from web videos. In this work, we propose JOSH, a novel optimization-based method for 4D human-scene reconstruction in the wild from monocular videos. JOSH uses techniques in both dense scene reconstruction and human mesh recovery as initialization, and then it leverages the human-scene contact constraints to jointly optimize the scene, the camera poses, and the human motion. Experiment results show JOSH achieves better results on both global human motion estimation and dense scene reconstruction by joint optimization of scene geometry and human motion. We further design a more efficient model, JOSH3R, and directly train it with pseudo-labels from web videos. JOSH3R outperforms other optimization-free methods by only training with labels predicted from JOSH, further demonstrating its accuracy and generalization ability.
<div id='section'>Paperid: <span id='pid'>1117, <a href='https://arxiv.org/pdf/2501.00317.pdf' target='_blank'>https://arxiv.org/pdf/2501.00317.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiexin Wang, Yiju Guo, Bing Su
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.00317">Spatio-Temporal Multi-Subgraph GCN for 3D Human Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion prediction (HMP) involves forecasting future human motion based on historical data. Graph Convolutional Networks (GCNs) have garnered widespread attention in this field for their proficiency in capturing relationships among joints in human motion. However, existing GCN-based methods tend to focus on either temporal-domain or spatial-domain features, or they combine spatio-temporal features without fully leveraging the complementarity and cross-dependency of these two features. In this paper, we propose the Spatial-Temporal Multi-Subgraph Graph Convolutional Network (STMS-GCN) to capture complex spatio-temporal dependencies in human motion. Specifically, we decouple the modeling of temporal and spatial dependencies, enabling cross-domain knowledge transfer at multiple scales through a spatio-temporal information consistency constraint mechanism. Besides, we utilize multiple subgraphs to extract richer motion information and enhance the learning associations of diverse subgraphs through a homogeneous information constraint mechanism. Extensive experiments on the standard HMP benchmarks demonstrate the superiority of our method.
<div id='section'>Paperid: <span id='pid'>1118, <a href='https://arxiv.org/pdf/2501.00315.pdf' target='_blank'>https://arxiv.org/pdf/2501.00315.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiexin Wang, Yiju Guo, Bing Su
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.00315">Temporal Dynamics Decoupling with Inverse Processing for Enhancing Human Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Exploring the bridge between historical and future motion behaviors remains a central challenge in human motion prediction. While most existing methods incorporate a reconstruction task as an auxiliary task into the decoder, thereby improving the modeling of spatio-temporal dependencies, they overlook the potential conflicts between reconstruction and prediction tasks. In this paper, we propose a novel approach: Temporal Decoupling Decoding with Inverse Processing (\textbf{$TD^2IP$}). Our method strategically separates reconstruction and prediction decoding processes, employing distinct decoders to decode the shared motion features into historical or future sequences. Additionally, inverse processing reverses motion information in the temporal dimension and reintroduces it into the model, leveraging the bidirectional temporal correlation of human motion behaviors. By alleviating the conflicts between reconstruction and prediction tasks and enhancing the association of historical and future information, \textbf{$TD^2IP$} fosters a deeper understanding of motion patterns. Extensive experiments demonstrate the adaptability of our method within existing methods.
<div id='section'>Paperid: <span id='pid'>1119, <a href='https://arxiv.org/pdf/2412.20350.pdf' target='_blank'>https://arxiv.org/pdf/2412.20350.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunyue Wei, Zeji Yi, Hongda Li, Saraswati Soedarmadji, Yanan Sui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.20350">Safe Bayesian Optimization for the Control of High-Dimensional Embodied Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning to move is a primary goal for animals and robots, where ensuring safety is often important when optimizing control policies on the embodied systems. For complex tasks such as the control of human or humanoid control, the high-dimensional parameter space adds complexity to the safe optimization effort. Current safe exploration algorithms exhibit inefficiency and may even become infeasible with large high-dimensional input spaces. Furthermore, existing high-dimensional constrained optimization methods neglect safety in the search process. In this paper, we propose High-dimensional Safe Bayesian Optimization with local optimistic exploration (HdSafeBO), a novel approach designed to handle high-dimensional sampling problems under probabilistic safety constraints. We introduce a local optimistic strategy to efficiently and safely optimize the objective function, providing a probabilistic safety guarantee and a cumulative safety violation bound. Through the use of isometric embedding, HdSafeBO addresses problems ranging from a few hundred to several thousand dimensions while maintaining safety guarantees. To our knowledge, HdSafeBO is the first algorithm capable of optimizing the control of high-dimensional musculoskeletal systems with high safety probability. We also demonstrate the real-world applicability of HdSafeBO through its use in the safe online optimization of neural stimulation induced human motion control.
<div id='section'>Paperid: <span id='pid'>1120, <a href='https://arxiv.org/pdf/2412.10350.pdf' target='_blank'>https://arxiv.org/pdf/2412.10350.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aykut Ä°Åleyen, Abhidnya Kadu, RenÃ© van de Molengraft, ÃmÃ¼r Arslan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.10350">Adaptive Dual-Headway Unicycle Pose Control and Motion Prediction for Optimal Sampling-Based Feedback Motion Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Safe, smooth, and optimal motion planning for nonholonomically constrained mobile robots and autonomous vehicles is essential for achieving reliable, seamless, and efficient autonomy in logistics, mobility, and service industries. In many such application settings, nonholonomic robots, like unicycles with restricted motion, require precise planning and control of both translational and orientational motion to approach specific locations in a designated orientation, such as for approaching changing, parking, and loading areas. In this paper, we introduce a new dual-headway unicycle pose control method by leveraging an adaptively placed headway point in front of the unicycle pose and a tailway point behind the goal pose. In summary, the unicycle robot continuously follows its headway point, which chases the tailway point of the goal pose and the asymptotic motion of the tailway point towards the goal position guides the unicycle robot to approach the goal location with the correct orientation. The simple and intuitive geometric construction of dual-headway unicycle pose control enables an explicit convex feedback motion prediction bound on the closed-loop unicycle motion trajectory for fast and accurate safety verification. We present an application of dual-headway unicycle control for optimal sampling-based motion planning around obstacles. In numerical simulations, we show that optimal unicycle motion planning using dual-headway translation and orientation distances significantly outperforms Euclidean translation and cosine orientation distances in generating smooth motion with minimal travel and turning effort.
<div id='section'>Paperid: <span id='pid'>1121, <a href='https://arxiv.org/pdf/2412.00115.pdf' target='_blank'>https://arxiv.org/pdf/2412.00115.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hui Li, Mingwang Xu, Yun Zhan, Shan Mu, Jiaye Li, Kaihui Cheng, Yuxuan Chen, Tan Chen, Mao Ye, Jingdong Wang, Siyu Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.00115">OpenHumanVid: A Large-Scale High-Quality Dataset for Enhancing Human-Centric Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in visual generation technologies have markedly increased the scale and availability of video datasets, which are crucial for training effective video generation models. However, a significant lack of high-quality, human-centric video datasets presents a challenge to progress in this field. To bridge this gap, we introduce OpenHumanVid, a large-scale and high-quality human-centric video dataset characterized by precise and detailed captions that encompass both human appearance and motion states, along with supplementary human motion conditions, including skeleton sequences and speech audio. To validate the efficacy of this dataset and the associated training strategies, we propose an extension of existing classical diffusion transformer architectures and conduct further pretraining of our models on the proposed dataset. Our findings yield two critical insights: First, the incorporation of a large-scale, high-quality dataset substantially enhances evaluation metrics for generated human videos while preserving performance in general video generation tasks. Second, the effective alignment of text with human appearance, human motion, and facial motion is essential for producing high-quality video outputs. Based on these insights and corresponding methodologies, the straightforward extended network trained on the proposed dataset demonstrates an obvious improvement in the generation of human-centric videos. Project page https://fudan-generative-vision.github.io/OpenHumanVid
<div id='section'>Paperid: <span id='pid'>1122, <a href='https://arxiv.org/pdf/2411.19786.pdf' target='_blank'>https://arxiv.org/pdf/2411.19786.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiming Wu, Wei Ji, Kecheng Zheng, Zicheng Wang, Dong Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.19786">MoTe: Learning Motion-Text Diffusion Model for Multiple Generation Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, human motion analysis has experienced great improvement due to inspiring generative models such as the denoising diffusion model and large language model. While the existing approaches mainly focus on generating motions with textual descriptions and overlook the reciprocal task. In this paper, we present~\textbf{MoTe}, a unified multi-modal model that could handle diverse tasks by learning the marginal, conditional, and joint distributions of motion and text simultaneously. MoTe enables us to handle the paired text-motion generation, motion captioning, and text-driven motion generation by simply modifying the input context. Specifically, MoTe is composed of three components: Motion Encoder-Decoder (MED), Text Encoder-Decoder (TED), and Moti-on-Text Diffusion Model (MTDM). In particular, MED and TED are trained for extracting latent embeddings, and subsequently reconstructing the motion sequences and textual descriptions from the extracted embeddings, respectively. MTDM, on the other hand, performs an iterative denoising process on the input context to handle diverse tasks. Experimental results on the benchmark datasets demonstrate the superior performance of our proposed method on text-to-motion generation and competitive performance on motion captioning.
<div id='section'>Paperid: <span id='pid'>1123, <a href='https://arxiv.org/pdf/2411.11911.pdf' target='_blank'>https://arxiv.org/pdf/2411.11911.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zikang Zhou, Hengjian Zhou, Haibo Hu, Zihao Wen, Jianping Wang, Yung-Hui Li, Yu-Kai Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.11911">ModeSeq: Taming Sparse Multimodal Motion Prediction with Sequential Mode Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Anticipating the multimodality of future events lays the foundation for safe autonomous driving. However, multimodal motion prediction for traffic agents has been clouded by the lack of multimodal ground truth. Existing works predominantly adopt the winner-take-all training strategy to tackle this challenge, yet still suffer from limited trajectory diversity and uncalibrated mode confidence. While some approaches address these limitations by generating excessive trajectory candidates, they necessitate a post-processing stage to identify the most representative modes, a process lacking universal principles and compromising trajectory accuracy. We are thus motivated to introduce ModeSeq, a new multimodal prediction paradigm that models modes as sequences. Unlike the common practice of decoding multiple plausible trajectories in one shot, ModeSeq requires motion decoders to infer the next mode step by step, thereby more explicitly capturing the correlation between modes and significantly enhancing the ability to reason about multimodality. Leveraging the inductive bias of sequential mode prediction, we also propose the Early-Match-Take-All (EMTA) training strategy to diversify the trajectories further. Without relying on dense mode prediction or heuristic post-processing, ModeSeq considerably improves the diversity of multimodal output while attaining satisfactory trajectory accuracy, resulting in balanced performance on motion prediction benchmarks. Moreover, ModeSeq naturally emerges with the capability of mode extrapolation, which supports forecasting more behavior modes when the future is highly uncertain.
<div id='section'>Paperid: <span id='pid'>1124, <a href='https://arxiv.org/pdf/2410.03860.pdf' target='_blank'>https://arxiv.org/pdf/2410.03860.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Leo Bringer, Joey Wilson, Kira Barton, Maani Ghaffari
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.03860">MDMP: Multi-modal Diffusion for supervised Motion Predictions with uncertainty</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces a Multi-modal Diffusion model for Motion Prediction (MDMP) that integrates and synchronizes skeletal data and textual descriptions of actions to generate refined long-term motion predictions with quantifiable uncertainty. Existing methods for motion forecasting or motion generation rely solely on either prior motions or text prompts, facing limitations with precision or control, particularly over extended durations. The multi-modal nature of our approach enhances the contextual understanding of human motion, while our graph-based transformer framework effectively capture both spatial and temporal motion dynamics. As a result, our model consistently outperforms existing generative techniques in accurately predicting long-term motions. Additionally, by leveraging diffusion models' ability to capture different modes of prediction, we estimate uncertainty, significantly improving spatial awareness in human-robot interactions by incorporating zones of presence with varying confidence levels for each body joint.
<div id='section'>Paperid: <span id='pid'>1125, <a href='https://arxiv.org/pdf/2409.00014.pdf' target='_blank'>https://arxiv.org/pdf/2409.00014.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hua Yu, Yaqing Hou, Wenbin Pei, Qiang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.00014">DivDiff: A Conditional Diffusion Model for Diverse Human Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diverse human motion prediction (HMP) aims to predict multiple plausible future motions given an observed human motion sequence. It is a challenging task due to the diversity of potential human motions while ensuring an accurate description of future human motions. Current solutions are either low-diversity or limited in expressiveness. Recent denoising diffusion models (DDPM) hold potential generative capabilities in generative tasks. However, introducing DDPM directly into diverse HMP incurs some issues. Although DDPM can increase the diversity of the potential patterns of human motions, the predicted human motions become implausible over time because of the significant noise disturbances in the forward process of DDPM. This phenomenon leads to the predicted human motions being hard to control, seriously impacting the quality of predicted motions and restricting their practical applicability in real-world scenarios. To alleviate this, we propose a novel conditional diffusion-based generative model, called DivDiff, to predict more diverse and realistic human motions. Specifically, the DivDiff employs DDPM as our backbone and incorporates Discrete Cosine Transform (DCT) and transformer mechanisms to encode the observed human motion sequence as a condition to instruct the reverse process of DDPM. More importantly, we design a diversified reinforcement sampling function (DRSF) to enforce human skeletal constraints on the predicted human motions. DRSF utilizes the acquired information from human skeletal as prior knowledge, thereby reducing significant disturbances introduced during the forward process. Extensive results received in the experiments on two widely-used datasets (Human3.6M and HumanEva-I) demonstrate that our model obtains competitive performance on both diversity and accuracy.
<div id='section'>Paperid: <span id='pid'>1126, <a href='https://arxiv.org/pdf/2407.14502.pdf' target='_blank'>https://arxiv.org/pdf/2407.14502.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Seunggeun Chi, Hyung-gun Chi, Hengbo Ma, Nakul Agarwal, Faizan Siddiqui, Karthik Ramani, Kwonjoon Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.14502">M2D2M: Multi-Motion Generation from Text with Discrete Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce the Multi-Motion Discrete Diffusion Models (M2D2M), a novel approach for human motion generation from textual descriptions of multiple actions, utilizing the strengths of discrete diffusion models. This approach adeptly addresses the challenge of generating multi-motion sequences, ensuring seamless transitions of motions and coherence across a series of actions. The strength of M2D2M lies in its dynamic transition probability within the discrete diffusion model, which adapts transition probabilities based on the proximity between motion tokens, encouraging mixing between different modes. Complemented by a two-phase sampling strategy that includes independent and joint denoising steps, M2D2M effectively generates long-term, smooth, and contextually coherent human motion sequences, utilizing a model trained for single-motion generation. Extensive experiments demonstrate that M2D2M surpasses current state-of-the-art benchmarks for motion generation from text descriptions, showcasing its efficacy in interpreting language semantics and generating dynamic, realistic motions.
<div id='section'>Paperid: <span id='pid'>1127, <a href='https://arxiv.org/pdf/2407.10127.pdf' target='_blank'>https://arxiv.org/pdf/2407.10127.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziqi Zhao, Peijia Xie, Max Q. -H. Meng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.10127">ODD: Omni Differential Drive for Simultaneous Reconfiguration and Omnidirectional Mobility of Wheeled Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Wheeled robots are highly efficient in human living environments. However, conventional wheeled designs, with their limited degrees of freedom and constraints in robot configuration, struggle to simultaneously achieve stability, passability, and agility due to varying footprint needs. This paper proposes a novel robot drive model inspired by human movements, termed as the Omni Differential Drive (ODD). The ODD model innovatively utilizes a lateral differential drive to adjust wheel spacing without adding additional actuators to the existing omnidirectional drive. This approach enables wheeled robots to achieve both simultaneous reconfiguration and omnidirectional mobility. To validate the feasibility of the ODD model, a functional prototype was developed, followed by comprehensive kinematic analyses. Control systems for self-balancing and motion control were designed and implemented. Experimental validations confirmed the feasibility of the ODD mechanism and the effectiveness of the control strategies. The results underline the potential of this innovative drive system to enhance the mobility and adaptability of robotic platforms.
<div id='section'>Paperid: <span id='pid'>1128, <a href='https://arxiv.org/pdf/2406.14567.pdf' target='_blank'>https://arxiv.org/pdf/2406.14567.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jose Luis Ponton, Eduard Pujol, Andreas Aristidou, Carlos Andujar, Nuria Pelechano
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.14567">DragPoser: Motion Reconstruction from Variable Sparse Tracking Signals via Latent Space Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>High-quality motion reconstruction that follows the user's movements can be achieved by high-end mocap systems with many sensors. However, obtaining such animation quality with fewer input devices is gaining popularity as it brings mocap closer to the general public. The main challenges include the loss of end-effector accuracy in learning-based approaches, or the lack of naturalness and smoothness in IK-based solutions. In addition, such systems are often finely tuned to a specific number of trackers and are highly sensitive to missing data e.g., in scenarios where a sensor is occluded or malfunctions. In response to these challenges, we introduce DragPoser, a novel deep-learning-based motion reconstruction system that accurately represents hard and dynamic on-the-fly constraints, attaining real-time high end-effectors position accuracy. This is achieved through a pose optimization process within a structured latent space. Our system requires only one-time training on a large human motion dataset, and then constraints can be dynamically defined as losses, while the pose is iteratively refined by computing the gradients of these losses within the latent space. To further enhance our approach, we incorporate a Temporal Predictor network, which employs a Transformer architecture to directly encode temporality within the latent space. This network ensures the pose optimization is confined to the manifold of valid poses and also leverages past pose data to predict temporally coherent poses. Results demonstrate that DragPoser surpasses both IK-based and the latest data-driven methods in achieving precise end-effector positioning, while it produces natural poses and temporally coherent motion. In addition, our system showcases robustness against on-the-fly constraint modifications, and exhibits exceptional adaptability to various input configurations and changes.
<div id='section'>Paperid: <span id='pid'>1129, <a href='https://arxiv.org/pdf/2406.04858.pdf' target='_blank'>https://arxiv.org/pdf/2406.04858.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bingheng Wang, Rui Huang, Lin Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.04858">Auto-Multilift: Distributed Learning and Control for Cooperative Load Transportation With Quadrotors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Designing motion control and planning algorithms for multilift systems remains challenging due to the complexities of dynamics, collision avoidance, actuator limits, and scalability. Existing methods that use optimization and distributed techniques effectively address these constraints and scalability issues. However, they often require substantial manual tuning, leading to suboptimal performance. This paper proposes Auto-Multilift, a novel framework that automates the tuning of model predictive controllers (MPCs) for multilift systems. We model the MPC cost functions with deep neural networks (DNNs), enabling fast online adaptation to various scenarios. We develop a distributed policy gradient algorithm to train these DNNs efficiently in a closed-loop manner. Central to our algorithm is distributed sensitivity propagation, which is built on fully exploiting the unique dynamic couplings within the multilift system. It parallelizes gradient computation across quadrotors and focuses on actual system state sensitivities relative to key MPC parameters. Extensive simulations demonstrate favorable scalability to a large number of quadrotors. Our method outperforms a state-of-the-art open-loop MPC tuning approach by effectively learning adaptive MPCs from trajectory tracking errors. It also excels in learning an adaptive reference for reconfiguring the system when traversing multiple narrow slots.
<div id='section'>Paperid: <span id='pid'>1130, <a href='https://arxiv.org/pdf/2405.14626.pdf' target='_blank'>https://arxiv.org/pdf/2405.14626.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Laura Duarte, Pedro Neto
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.14626">Event-based dataset for the detection and classification of manufacturing assembly tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The featured dataset, the Event-based Dataset of Assembly Tasks (EDAT24), showcases a selection of manufacturing primitive tasks (idle, pick, place, and screw), which are basic actions performed by human operators in any manufacturing assembly. The data were captured using a DAVIS240C event camera, an asynchronous vision sensor that registers events when changes in light intensity value occur. Events are a lightweight data format for conveying visual information and are well-suited for real-time detection and analysis of human motion. Each manufacturing primitive has 100 recorded samples of DAVIS240C data, including events and greyscale frames, for a total of 400 samples. In the dataset, the user interacts with objects from the open-source CT-Benchmark in front of the static DAVIS event camera. All data are made available in raw form (.aedat) and in pre-processed form (.npy). Custom-built Python code is made available together with the dataset to aid researchers to add new manufacturing primitives or extend the dataset with more samples.
<div id='section'>Paperid: <span id='pid'>1131, <a href='https://arxiv.org/pdf/2405.09814.pdf' target='_blank'>https://arxiv.org/pdf/2405.09814.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zeyi Zhang, Tenglong Ao, Yuyao Zhang, Qingzhe Gao, Chuan Lin, Baoquan Chen, Libin Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.09814">Semantic Gesticulator: Semantics-Aware Co-Speech Gesture Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we present Semantic Gesticulator, a novel framework designed to synthesize realistic gestures accompanying speech with strong semantic correspondence. Semantically meaningful gestures are crucial for effective non-verbal communication, but such gestures often fall within the long tail of the distribution of natural human motion. The sparsity of these movements makes it challenging for deep learning-based systems, trained on moderately sized datasets, to capture the relationship between the movements and the corresponding speech semantics. To address this challenge, we develop a generative retrieval framework based on a large language model. This framework efficiently retrieves suitable semantic gesture candidates from a motion library in response to the input speech. To construct this motion library, we summarize a comprehensive list of commonly used semantic gestures based on findings in linguistics, and we collect a high-quality motion dataset encompassing both body and hand movements. We also design a novel GPT-based model with strong generalization capabilities to audio, capable of generating high-quality gestures that match the rhythm of speech. Furthermore, we propose a semantic alignment mechanism to efficiently align the retrieved semantic gestures with the GPT's output, ensuring the naturalness of the final animation. Our system demonstrates robustness in generating gestures that are rhythmically coherent and semantically explicit, as evidenced by a comprehensive collection of examples. User studies confirm the quality and human-likeness of our results, and show that our system outperforms state-of-the-art systems in terms of semantic appropriateness by a clear margin.
<div id='section'>Paperid: <span id='pid'>1132, <a href='https://arxiv.org/pdf/2404.16705.pdf' target='_blank'>https://arxiv.org/pdf/2404.16705.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Diego Martinez-Baselga, Oscar de Groot, Luzia Knoedler, Luis Riazuelo, Javier Alonso-Mora, Luis Montano
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.16705">SHINE: Social Homology Identification for Navigation in Crowded Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Navigating mobile robots in social environments remains a challenging task due to the intricacies of human-robot interactions. Most of the motion planners designed for crowded and dynamic environments focus on choosing the best velocity to reach the goal while avoiding collisions, but do not explicitly consider the high-level navigation behavior (avoiding through the left or right side, letting others pass or passing before others, etc.). In this work, we present a novel motion planner that incorporates topology distinct paths representing diverse navigation strategies around humans. The planner selects the topology class that imitates human behavior the best using a deep neural network model trained on real-world human motion data, ensuring socially intelligent and contextually aware navigation. Our system refines the chosen path through an optimization-based local planner in real time, ensuring seamless adherence to desired social behaviors. In this way, we decouple perception and local planning from the decision-making process. We evaluate the prediction accuracy of the network with real-world data. In addition, we assess the navigation capabilities in both simulation and a real-world platform, comparing it with other state-of-the-art planners. We demonstrate that our planner exhibits socially desirable behaviors and shows a smooth and remarkable performance.
<div id='section'>Paperid: <span id='pid'>1133, <a href='https://arxiv.org/pdf/2404.06892.pdf' target='_blank'>https://arxiv.org/pdf/2404.06892.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Diankun Zhang, Guoan Wang, Runwen Zhu, Jianbo Zhao, Xiwu Chen, Siyu Zhang, Jiahao Gong, Qibin Zhou, Wenyuan Zhang, Ningzi Wang, Feiyang Tan, Hangning Zhou, Ziyao Xu, Haotian Yao, Chi Zhang, Xiaojun Liu, Xiaoguang Di, Bin Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.06892">SparseAD: Sparse Query-Centric Paradigm for Efficient End-to-End Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>End-to-End paradigms use a unified framework to implement multi-tasks in an autonomous driving system. Despite simplicity and clarity, the performance of end-to-end autonomous driving methods on sub-tasks is still far behind the single-task methods. Meanwhile, the widely used dense BEV features in previous end-to-end methods make it costly to extend to more modalities or tasks. In this paper, we propose a Sparse query-centric paradigm for end-to-end Autonomous Driving (SparseAD), where the sparse queries completely represent the whole driving scenario across space, time and tasks without any dense BEV representation. Concretely, we design a unified sparse architecture for perception tasks including detection, tracking, and online mapping. Moreover, we revisit motion prediction and planning, and devise a more justifiable motion planner framework. On the challenging nuScenes dataset, SparseAD achieves SOTA full-task performance among end-to-end methods and significantly narrows the performance gap between end-to-end paradigms and single-task methods. Codes will be released soon.
<div id='section'>Paperid: <span id='pid'>1134, <a href='https://arxiv.org/pdf/2403.19026.pdf' target='_blank'>https://arxiv.org/pdf/2403.19026.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weizhuo Wang, C. Karen Liu, Monroe Kennedy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.19026">EgoNav: Egocentric Scene-aware Human Trajectory Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Wearable collaborative robots stand to assist human wearers who need fall prevention assistance or wear exoskeletons. Such a robot needs to be able to constantly adapt to the surrounding scene based on egocentric vision, and predict the ego motion of the wearer. In this work, we leveraged body-mounted cameras and sensors to anticipate the trajectory of human wearers through complex surroundings. To facilitate research in ego-motion prediction, we have collected a comprehensive walking scene navigation dataset centered on the user's perspective. We then present a method to predict human motion conditioning on the surrounding static scene. Our method leverages a diffusion model to produce a distribution of potential future trajectories, taking into account the user's observation of the environment. To that end, we introduce a compact representation to encode the user's visual memory of the surroundings, as well as an efficient sample-generating technique to speed up real-time inference of a diffusion model. We ablate our model and compare it to baselines, and results show that our model outperforms existing methods on key metrics of collision avoidance and trajectory mode coverage.
<div id='section'>Paperid: <span id='pid'>1135, <a href='https://arxiv.org/pdf/2403.15891.pdf' target='_blank'>https://arxiv.org/pdf/2403.15891.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiangbei Yue, Baiyi Li, Julien PettrÃ©, Armin Seyfried, He Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.15891">Human Motion Prediction under Unexpected Perturbation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We investigate a new task in human motion prediction, which is predicting motions under unexpected physical perturbation potentially involving multiple people. Compared with existing research, this task involves predicting less controlled, unpremeditated and pure reactive motions in response to external impact and how such motions can propagate through people. It brings new challenges such as data scarcity and predicting complex interactions. To this end, we propose a new method capitalizing differential physics and deep neural networks, leading to an explicit Latent Differential Physics (LDP) model. Through experiments, we demonstrate that LDP has high data efficiency, outstanding prediction accuracy, strong generalizability and good explainability. Since there is no similar research, a comprehensive comparison with 11 adapted baselines from several relevant domains is conducted, showing LDP outperforming existing research both quantitatively and qualitatively, improving prediction accuracy by as much as 70%, and demonstrating significantly stronger generalization.
<div id='section'>Paperid: <span id='pid'>1136, <a href='https://arxiv.org/pdf/2403.09923.pdf' target='_blank'>https://arxiv.org/pdf/2403.09923.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yingqing Chen, Christos G. Cassandras, Kaiyuan Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.09923">Optimal Sequencing and Motion Control in a Roundabout with Safety Guarantees</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper develops a controller for Connected and Automated Vehicles (CAVs) traversing a single-lane roundabout. The controller simultaneously determines the optimal sequence and associated optimal motion control jointly minimizing travel time and energy consumption while providing speed-dependent safety guarantees, as well as satisfying velocity and acceleration constraints. This is achieved by integrating (a) Model Predictive Control (MPC) to enable receding horizon optimization with (b) Control Lyapunov-Barrier Functions (CLBFs) to guarantee convergence to a safe set in finite time, thus providing an extended stability region compared to the use of classic Control Barrier Functions (CBFs). The proposed MPC-CLBF framework addresses both infeasibility and myopic control issues commonly encountered when controlling CAVs over multiple interconnected control zones in a traffic network, which has been a limitation of prior work on CAVs going through roundabouts, while still providing safety guarantees. Simulations under varying traffic demands demonstrate the controller's effectiveness and stability.
<div id='section'>Paperid: <span id='pid'>1137, <a href='https://arxiv.org/pdf/2403.03460.pdf' target='_blank'>https://arxiv.org/pdf/2403.03460.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xunjie Chen, Aditya Anikode, Jingang Yi, Tao Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.03460">Foot Shape-Dependent Resistive Force Model for Bipedal Walkers on Granular Terrains</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Legged robots have demonstrated high efficiency and effectiveness in unstructured and dynamic environments. However, it is still challenging for legged robots to achieve rapid and efficient locomotion on deformable, yielding substrates, such as granular terrains. We present an enhanced resistive force model for bipedal walkers on soft granular terrains by introducing effective intrusion depth correction. The enhanced force model captures fundamental kinetic results considering the robot foot shape, walking gait speed variation, and energy expense. The model is validated by extensive foot intrusion experiments with a bipedal robot. The results confirm the model accuracy on the given type of granular terrains. The model can be further integrated with the motion control of bipedal robotic walkers.
<div id='section'>Paperid: <span id='pid'>1138, <a href='https://arxiv.org/pdf/2403.03105.pdf' target='_blank'>https://arxiv.org/pdf/2403.03105.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chunchu Zhu, Xunjie Chen, Jingang Yi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.03105">Biomechanical Comparison of Human Walking Locomotion on Solid Ground and Sand</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current studies on human locomotion focus mainly on solid ground walking conditions. In this paper, we present a biomechanic comparison of human walking locomotion on solid ground and sand. A novel dataset containing 3-dimensional motion and biomechanical data from 20 able-bodied adults for locomotion on solid ground and sand is collected. We present the data collection methods and report the sensor data along with the kinematic and kinetic profiles of joint biomechanics. A comprehensive analysis of human gait and joint stiffness profiles is presented. The kinematic and kinetic analysis reveals that human walking locomotion on sand shows different ground reaction forces and joint torque profiles, compared with those patterns from walking on solid ground. These gait differences reflect that humans adopt motion control strategies for yielding terrain conditions such as sand. The dataset also provides a source of locomotion data for researchers to study human activity recognition and assistive devices for walking on different terrains.
<div id='section'>Paperid: <span id='pid'>1139, <a href='https://arxiv.org/pdf/2403.00353.pdf' target='_blank'>https://arxiv.org/pdf/2403.00353.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaqiang Tang, Weigao Sun, Siyuan Hu, Yiyang Sun, Yafeng Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.00353">MS-Net: A Multi-Path Sparse Model for Motion Prediction in Multi-Scenes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The multi-modality and stochastic characteristics of human behavior make motion prediction a highly challenging task, which is critical for autonomous driving. While deep learning approaches have demonstrated their great potential in this area, it still remains unsolved to establish a connection between multiple driving scenes (e.g., merging, roundabout, intersection) and the design of deep learning models. Current learning-based methods typically use one unified model to predict trajectories in different scenarios, which may result in sub-optimal results for one individual scene. To address this issue, we propose Multi-Scenes Network (aka. MS-Net), which is a multi-path sparse model trained by an evolutionary process. MS-Net selectively activates a subset of its parameters during the inference stage to produce prediction results for each scene. In the training stage, the motion prediction task under differentiated scenes is abstracted as a multi-task learning problem, an evolutionary algorithm is designed to encourage the network search of the optimal parameters for each scene while sharing common knowledge between different scenes. Our experiment results show that with substantially reduced parameters, MS-Net outperforms existing state-of-the-art methods on well-established pedestrian motion prediction datasets, e.g., ETH and UCY, and ranks the 2nd place on the INTERACTION challenge.
<div id='section'>Paperid: <span id='pid'>1140, <a href='https://arxiv.org/pdf/2402.07087.pdf' target='_blank'>https://arxiv.org/pdf/2402.07087.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nate Gillman, Michael Freeman, Daksh Aggarwal, Chia-Hong Hsu, Calvin Luo, Yonglong Tian, Chen Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.07087">Self-Correcting Self-Consuming Loops for Generative Model Training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As synthetic data becomes higher quality and proliferates on the internet, machine learning models are increasingly trained on a mix of human- and machine-generated data. Despite the successful stories of using synthetic data for representation learning, using synthetic data for generative model training creates "self-consuming loops" which may lead to training instability or even collapse, unless certain conditions are met. Our paper aims to stabilize self-consuming generative model training. Our theoretical results demonstrate that by introducing an idealized correction function, which maps a data point to be more likely under the true data distribution, self-consuming loops can be made exponentially more stable. We then propose self-correction functions, which rely on expert knowledge (e.g. the laws of physics programmed in a simulator), and aim to approximate the idealized corrector automatically and at scale. We empirically validate the effectiveness of self-correcting self-consuming loops on the challenging human motion synthesis task, and observe that it successfully avoids model collapse, even when the ratio of synthetic data to real data is as high as 100%.
<div id='section'>Paperid: <span id='pid'>1141, <a href='https://arxiv.org/pdf/2402.04061.pdf' target='_blank'>https://arxiv.org/pdf/2402.04061.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jumman Hossain, Abu-Zaher Faridee, Nirmalya Roy, Jade Freeman, Timothy Gregory, Theron T. Trout
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.04061">TopoNav: Topological Navigation for Efficient Exploration in Sparse Reward Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous robots exploring unknown environments face a significant challenge: navigating effectively without prior maps and with limited external feedback. This challenge intensifies in sparse reward environments, where traditional exploration techniques often fail. In this paper, we present TopoNav, a novel topological navigation framework that integrates active mapping, hierarchical reinforcement learning, and intrinsic motivation to enable efficient goal-oriented exploration and navigation in sparse-reward settings. TopoNav dynamically constructs a topological map of the environment, capturing key locations and pathways. A two-level hierarchical policy architecture, comprising a high-level graph traversal policy and low-level motion control policies, enables effective navigation and obstacle avoidance while maintaining focus on the overall goal. Additionally, TopoNav incorporates intrinsic motivation to guide exploration toward relevant regions and frontier nodes in the topological map, addressing the challenges of sparse extrinsic rewards. We evaluate TopoNav both in the simulated and real-world off-road environments using a Clearpath Jackal robot, across three challenging navigation scenarios: goal-reaching, feature-based navigation, and navigation in complex terrains. We observe an increase in exploration coverage by 7- 20%, in success rates by 9-19%, and reductions in navigation times by 15-36% across various scenarios, compared to state-of-the-art methods
<div id='section'>Paperid: <span id='pid'>1142, <a href='https://arxiv.org/pdf/2402.03703.pdf' target='_blank'>https://arxiv.org/pdf/2402.03703.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhirong Luan, Yujun Lai, Rundong Huang, Yan Yan, Jingwei Wang, Jizhou Lu, Badong Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.03703">Hierarchical Large Language Models in Cloud Edge End Architecture for Heterogeneous Robot Cluster Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite their powerful semantic understanding and code generation capabilities, Large Language Models (LLMs) still face challenges when dealing with complex tasks. Multi agent strategy generation and motion control are highly complex domains that inherently require experts from multiple fields to collaborate. To enhance multi agent strategy generation and motion control, we propose an innovative architecture that employs the concept of a cloud edge end hierarchical structure. By leveraging multiple large language models with distinct areas of expertise, we can efficiently generate strategies and perform task decomposition. Introducing the cosine similarity approach,aligning task decomposition instructions with robot task sequences at the vector level, we can identify subtasks with incomplete task decomposition and iterate on them multiple times to ultimately generate executable machine task sequences.The robot is guided through these task sequences to complete tasks of higher complexity. With this architecture, we implement the process of natural language control of robots to perform complex tasks, and successfully address the challenge of multi agent execution of open tasks in open scenarios and the problem of task decomposition.
<div id='section'>Paperid: <span id='pid'>1143, <a href='https://arxiv.org/pdf/2402.02150.pdf' target='_blank'>https://arxiv.org/pdf/2402.02150.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Koyu Mizutani, Haruki Mitarai, Kakeru Miyazaki, Soichiro Kumano, Toshihiko Yamasaki
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.02150">Data-Driven Prediction of Seismic Intensity Distributions Featuring Hybrid Classification-Regression Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Earthquakes are among the most immediate and deadly natural disasters that humans face. Accurately forecasting the extent of earthquake damage and assessing potential risks can be instrumental in saving numerous lives. In this study, we developed linear regression models capable of predicting seismic intensity distributions based on earthquake parameters: location, depth, and magnitude. Because it is completely data-driven, it can predict intensity distributions without geographical information. The dataset comprises seismic intensity data from earthquakes that occurred in the vicinity of Japan between 1997 and 2020, specifically containing 1,857 instances of earthquakes with a magnitude of 5.0 or greater, sourced from the Japan Meteorological Agency. We trained both regression and classification models and combined them to take advantage of both to create a hybrid model. The proposed model outperformed commonly used Ground Motion Prediction Equations (GMPEs) in terms of the correlation coefficient, F1 score, and MCC. Furthermore, the proposed model can predict even abnormal seismic intensity distributions, a task at conventional GMPEs often struggle.
<div id='section'>Paperid: <span id='pid'>1144, <a href='https://arxiv.org/pdf/2401.13505.pdf' target='_blank'>https://arxiv.org/pdf/2401.13505.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chuan Guo, Yuxuan Mu, Xinxin Zuo, Peng Dai, Youliang Yan, Juwei Lu, Li Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.13505">Generative Human Motion Stylization in Latent Space</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion stylization aims to revise the style of an input motion while keeping its content unaltered. Unlike existing works that operate directly in pose space, we leverage the latent space of pretrained autoencoders as a more expressive and robust representation for motion extraction and infusion. Building upon this, we present a novel generative model that produces diverse stylization results of a single motion (latent) code. During training, a motion code is decomposed into two coding components: a deterministic content code, and a probabilistic style code adhering to a prior distribution; then a generator massages the random combination of content and style codes to reconstruct the corresponding motion codes. Our approach is versatile, allowing the learning of probabilistic style space from either style labeled or unlabeled motions, providing notable flexibility in stylization as well. In inference, users can opt to stylize a motion using style cues from a reference motion or a label. Even in the absence of explicit style input, our model facilitates novel re-stylization by sampling from the unconditional style prior distribution. Experimental results show that our proposed stylization models, despite their lightweight design, outperform the state-of-the-art in style reenactment, content preservation, and generalization across various applications and settings. Project Page: https://murrol.github.io/GenMoStyle
<div id='section'>Paperid: <span id='pid'>1145, <a href='https://arxiv.org/pdf/2401.12965.pdf' target='_blank'>https://arxiv.org/pdf/2401.12965.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi-Shiuan Tung, Matthew B. Luebbers, Alessandro Roncone, Bradley Hayes
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.12965">Workspace Optimization Techniques to Improve Prediction of Human Motion During Human-Robot Collaboration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding human intentions is critical for safe and effective human-robot collaboration. While state of the art methods for human goal prediction utilize learned models to account for the uncertainty of human motion data, that data is inherently stochastic and high variance, hindering those models' utility for interactions requiring coordination, including safety-critical or close-proximity tasks. Our key insight is that robot teammates can deliberately configure shared workspaces prior to interaction in order to reduce the variance in human motion, realizing classifier-agnostic improvements in goal prediction. In this work, we present an algorithmic approach for a robot to arrange physical objects and project "virtual obstacles" using augmented reality in shared human-robot workspaces, optimizing for human legibility over a given set of tasks. We compare our approach against other workspace arrangement strategies using two human-subjects studies, one in a virtual 2D navigation domain and the other in a live tabletop manipulation domain involving a robotic manipulator arm. We evaluate the accuracy of human motion prediction models learned from each condition, demonstrating that our workspace optimization technique with virtual obstacles leads to higher robot prediction accuracy using less training data.
<div id='section'>Paperid: <span id='pid'>1146, <a href='https://arxiv.org/pdf/2401.08739.pdf' target='_blank'>https://arxiv.org/pdf/2401.08739.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gen Li, Kaifeng Zhao, Siwei Zhang, Xiaozhong Lyu, Mihai Dusmanu, Yan Zhang, Marc Pollefeys, Siyu Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.08739">EgoGen: An Egocentric Synthetic Data Generator</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding the world in first-person view is fundamental in Augmented Reality (AR). This immersive perspective brings dramatic visual changes and unique challenges compared to third-person views. Synthetic data has empowered third-person-view vision models, but its application to embodied egocentric perception tasks remains largely unexplored. A critical challenge lies in simulating natural human movements and behaviors that effectively steer the embodied cameras to capture a faithful egocentric representation of the 3D world. To address this challenge, we introduce EgoGen, a new synthetic data generator that can produce accurate and rich ground-truth training data for egocentric perception tasks. At the heart of EgoGen is a novel human motion synthesis model that directly leverages egocentric visual inputs of a virtual human to sense the 3D environment. Combined with collision-avoiding motion primitives and a two-stage reinforcement learning approach, our motion synthesis model offers a closed-loop solution where the embodied perception and movement of the virtual human are seamlessly coupled. Compared to previous works, our model eliminates the need for a pre-defined global path, and is directly applicable to dynamic environments. Combined with our easy-to-use and scalable data generation pipeline, we demonstrate EgoGen's efficacy in three tasks: mapping and localization for head-mounted cameras, egocentric camera tracking, and human mesh recovery from egocentric views. EgoGen will be fully open-sourced, offering a practical solution for creating realistic egocentric training data and aiming to serve as a useful tool for egocentric computer vision research. Refer to our project page: https://ego-gen.github.io/.
<div id='section'>Paperid: <span id='pid'>1147, <a href='https://arxiv.org/pdf/2401.01730.pdf' target='_blank'>https://arxiv.org/pdf/2401.01730.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Yao, Hongwen Zhang, Yunlian Sun, Jinhui Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.01730">STAF: 3D Human Mesh Recovery from Video with Spatio-Temporal Alignment Fusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The recovery of 3D human mesh from monocular images has significantly been developed in recent years. However, existing models usually ignore spatial and temporal information, which might lead to mesh and image misalignment and temporal discontinuity. For this reason, we propose a novel Spatio-Temporal Alignment Fusion (STAF) model. As a video-based model, it leverages coherence clues from human motion by an attention-based Temporal Coherence Fusion Module (TCFM). As for spatial mesh-alignment evidence, we extract fine-grained local information through predicted mesh projection on the feature maps. Based on the spatial features, we further introduce a multi-stage adjacent Spatial Alignment Fusion Module (SAFM) to enhance the feature representation of the target frame. In addition to the above, we propose an Average Pooling Module (APM) to allow the model to focus on the entire input sequence rather than just the target frame. This method can remarkably improve the smoothness of recovery results from video. Extensive experiments on 3DPW, MPII3D, and H36M demonstrate the superiority of STAF. We achieve a state-of-the-art trade-off between precision and smoothness. Our code and more video results are on the project page https://yw0208.github.io/staf/
<div id='section'>Paperid: <span id='pid'>1148, <a href='https://arxiv.org/pdf/2312.14828.pdf' target='_blank'>https://arxiv.org/pdf/2312.14828.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinpeng Liu, Wenxun Dai, Chunyu Wang, Yiji Cheng, Yansong Tang, Xin Tong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.14828">Plan, Posture and Go: Towards Open-World Text-to-Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Conventional text-to-motion generation methods are usually trained on limited text-motion pairs, making them hard to generalize to open-world scenarios. Some works use the CLIP model to align the motion space and the text space, aiming to enable motion generation from natural language motion descriptions. However, they are still constrained to generate limited and unrealistic in-place motions. To address these issues, we present a divide-and-conquer framework named PRO-Motion, which consists of three modules as motion planner, posture-diffuser and go-diffuser. The motion planner instructs Large Language Models (LLMs) to generate a sequence of scripts describing the key postures in the target motion. Differing from natural languages, the scripts can describe all possible postures following very simple text templates. This significantly reduces the complexity of posture-diffuser, which transforms a script to a posture, paving the way for open-world generation. Finally, go-diffuser, implemented as another diffusion model, estimates whole-body translations and rotations for all postures, resulting in realistic motions. Experimental results have shown the superiority of our method with other counterparts, and demonstrated its capability of generating diverse and realistic motions from complex open-world prompts such as "Experiencing a profound sense of joy". The project page is available at https://moonsliu.github.io/Pro-Motion.
<div id='section'>Paperid: <span id='pid'>1149, <a href='https://arxiv.org/pdf/2312.04784.pdf' target='_blank'>https://arxiv.org/pdf/2312.04784.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuchen Rao, Eduardo Perez Pellitero, Benjamin Busam, Yiren Zhou, Jifei Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.04784">Reality's Canvas, Language's Brush: Crafting 3D Avatars from Monocular Video</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in 3D avatar generation excel with multi-view supervision for photorealistic models. However, monocular counterparts lag in quality despite broader applicability. We propose ReCaLaB to close this gap. ReCaLaB is a fully-differentiable pipeline that learns high-fidelity 3D human avatars from just a single RGB video. A pose-conditioned deformable NeRF is optimized to volumetrically represent a human subject in canonical T-pose. The canonical representation is then leveraged to efficiently associate neural textures using 2D-3D correspondences. This enables the separation of diffused color generation and lighting correction branches that jointly compose an RGB prediction. The design allows to control intermediate results for human pose, body shape, texture, and lighting with text prompts. An image-conditioned diffusion model thereby helps to animate appearance and pose of the 3D avatar to create video sequences with previously unseen human motion. Extensive experiments show that ReCaLaB outperforms previous monocular approaches in terms of image quality for image synthesis tasks. Moreover, natural language offers an intuitive user interface for creative manipulation of 3D human avatars.
<div id='section'>Paperid: <span id='pid'>1150, <a href='https://arxiv.org/pdf/2312.00870.pdf' target='_blank'>https://arxiv.org/pdf/2312.00870.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Balamurugan Thambiraja, Sadegh Aliakbarian, Darren Cosker, Justus Thies
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.00870">3DiFACE: Diffusion-based Speech-driven 3D Facial Animation and Editing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present 3DiFACE, a novel method for personalized speech-driven 3D facial animation and editing. While existing methods deterministically predict facial animations from speech, they overlook the inherent one-to-many relationship between speech and facial expressions, i.e., there are multiple reasonable facial expression animations matching an audio input. It is especially important in content creation to be able to modify generated motion or to specify keyframes. To enable stochasticity as well as motion editing, we propose a lightweight audio-conditioned diffusion model for 3D facial motion. This diffusion model can be trained on a small 3D motion dataset, maintaining expressive lip motion output. In addition, it can be finetuned for specific subjects, requiring only a short video of the person. Through quantitative and qualitative evaluations, we show that our method outperforms existing state-of-the-art techniques and yields speech-driven animations with greater fidelity and diversity.
<div id='section'>Paperid: <span id='pid'>1151, <a href='https://arxiv.org/pdf/2312.00063.pdf' target='_blank'>https://arxiv.org/pdf/2312.00063.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chuan Guo, Yuxuan Mu, Muhammad Gohar Javed, Sen Wang, Li Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.00063">MoMask: Generative Masked Modeling of 3D Human Motions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce MoMask, a novel masked modeling framework for text-driven 3D human motion generation. In MoMask, a hierarchical quantization scheme is employed to represent human motion as multi-layer discrete motion tokens with high-fidelity details. Starting at the base layer, with a sequence of motion tokens obtained by vector quantization, the residual tokens of increasing orders are derived and stored at the subsequent layers of the hierarchy. This is consequently followed by two distinct bidirectional transformers. For the base-layer motion tokens, a Masked Transformer is designated to predict randomly masked motion tokens conditioned on text input at training stage. During generation (i.e. inference) stage, starting from an empty sequence, our Masked Transformer iteratively fills up the missing tokens; Subsequently, a Residual Transformer learns to progressively predict the next-layer tokens based on the results from current layer. Extensive experiments demonstrate that MoMask outperforms the state-of-art methods on the text-to-motion generation task, with an FID of 0.045 (vs e.g. 0.141 of T2M-GPT) on the HumanML3D dataset, and 0.228 (vs 0.514) on KIT-ML, respectively. MoMask can also be seamlessly applied in related tasks without further model fine-tuning, such as text-guided temporal inpainting.
<div id='section'>Paperid: <span id='pid'>1152, <a href='https://arxiv.org/pdf/2311.17408.pdf' target='_blank'>https://arxiv.org/pdf/2311.17408.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinshun Wang, Wanying Zhang, Can Wang, Yuan Gao, Mengyuan Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.17408">Dynamic Dense Graph Convolutional Network for Skeleton-based Human Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Graph Convolutional Networks (GCN) which typically follows a neural message passing framework to model dependencies among skeletal joints has achieved high success in skeleton-based human motion prediction task. Nevertheless, how to construct a graph from a skeleton sequence and how to perform message passing on the graph are still open problems, which severely affect the performance of GCN. To solve both problems, this paper presents a Dynamic Dense Graph Convolutional Network (DD-GCN), which constructs a dense graph and implements an integrated dynamic message passing. More specifically, we construct a dense graph with 4D adjacency modeling as a comprehensive representation of motion sequence at different levels of abstraction. Based on the dense graph, we propose a dynamic message passing framework that learns dynamically from data to generate distinctive messages reflecting sample-specific relevance among nodes in the graph. Extensive experiments on benchmark Human 3.6M and CMU Mocap datasets verify the effectiveness of our DD-GCN which obviously outperforms state-of-the-art GCN-based methods, especially when using long-term and our proposed extremely long-term protocol.
<div id='section'>Paperid: <span id='pid'>1153, <a href='https://arxiv.org/pdf/2311.13781.pdf' target='_blank'>https://arxiv.org/pdf/2311.13781.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wanying Zhang, Shen Zhao, Fanyang Meng, Songtao Wu, Mengyuan Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.13781">Dynamic Compositional Graph Convolutional Network for Efficient Composite Human Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With potential applications in fields including intelligent surveillance and human-robot interaction, the human motion prediction task has become a hot research topic and also has achieved high success, especially using the recent Graph Convolutional Network (GCN). Current human motion prediction task usually focuses on predicting human motions for atomic actions. Observing that atomic actions can happen at the same time and thus formulating the composite actions, we propose the composite human motion prediction task. To handle this task, we first present a Composite Action Generation (CAG) module to generate synthetic composite actions for training, thus avoiding the laborious work of collecting composite action samples. Moreover, we alleviate the effect of composite actions on demand for a more complicated model by presenting a Dynamic Compositional Graph Convolutional Network (DC-GCN). Extensive experiments on the Human3.6M dataset and our newly collected CHAMP dataset consistently verify the efficiency of our DC-GCN method, which achieves state-of-the-art motion prediction accuracies and meanwhile needs few extra computational costs than traditional GCN-based human motion methods.
<div id='section'>Paperid: <span id='pid'>1154, <a href='https://arxiv.org/pdf/2311.12532.pdf' target='_blank'>https://arxiv.org/pdf/2311.12532.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Abdulla Tarshahani, Aykut Ä°Åleyen, ÃmÃ¼r Arslan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.12532">Total Turning and Motion Range Prediction for Safe Unicycle Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Safe and smooth motion control is essential for mobile robots when performing various automation tasks around obstacles, especially in the presence of people and other mobile robots. The total turning and space used by a mobile robot while moving towards a specified goal position play a crucial role in determining the required control effort and complexity. In this paper, we consider a standard unicycle control approach based on angular feedback linearization and provide an explicit analytical measure for determining the total turning effort during unicycle control in terms of unicycle state and control gains. We show that undesired spiral oscillatory motion around the goal position can be avoided by choosing a higher angular control gain compared to the linear control gain. Accordingly, we establish an accurate, explicit triangular motion range bound on the closed-loop unicycle trajectory using the total turning effort. The improved accuracy in motion range prediction results from a stronger dependency on the unicycle state and control parameters. To compare alternative circular, conic, and triangular motion range prediction approaches, we present an application of the proposed unicycle motion control and motion prediction methods for safe unicycle path following around obstacles in numerical simulations.
<div id='section'>Paperid: <span id='pid'>1155, <a href='https://arxiv.org/pdf/2311.09543.pdf' target='_blank'>https://arxiv.org/pdf/2311.09543.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ming Chen, Yan Zhou, Weihua Jian, Pengfei Wan, Zhongyuan Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.09543">Temporal-Aware Refinement for Video-based Human Pose and Shape Recovery</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Though significant progress in human pose and shape recovery from monocular RGB images has been made in recent years, obtaining 3D human motion with high accuracy and temporal consistency from videos remains challenging. Existing video-based methods tend to reconstruct human motion from global image features, which lack detailed representation capability and limit the reconstruction accuracy. In this paper, we propose a Temporal-Aware Refining Network (TAR), to synchronously explore temporal-aware global and local image features for accurate pose and shape recovery. First, a global transformer encoder is introduced to obtain temporal global features from static feature sequences. Second, a bidirectional ConvGRU network takes the sequence of high-resolution feature maps as input, and outputs temporal local feature maps that maintain high resolution and capture the local motion of the human body. Finally, a recurrent refinement module iteratively updates estimated SMPL parameters by leveraging both global and local temporal information to achieve accurate and smooth results. Extensive experiments demonstrate that our TAR obtains more accurate results than previous state-of-the-art methods on popular benchmarks, i.e., 3DPW, MPI-INF-3DHP, and Human3.6M.
<div id='section'>Paperid: <span id='pid'>1156, <a href='https://arxiv.org/pdf/2311.02191.pdf' target='_blank'>https://arxiv.org/pdf/2311.02191.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jose Luis Ponton, Haoran Yun, Andreas Aristidou, Carlos Andujar, Nuria Pelechano
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.02191">SparsePoser: Real-time Full-body Motion Reconstruction from Sparse Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate and reliable human motion reconstruction is crucial for creating natural interactions of full-body avatars in Virtual Reality (VR) and entertainment applications. As the Metaverse and social applications gain popularity, users are seeking cost-effective solutions to create full-body animations that are comparable in quality to those produced by commercial motion capture systems. In order to provide affordable solutions, though, it is important to minimize the number of sensors attached to the subject's body. Unfortunately, reconstructing the full-body pose from sparse data is a heavily under-determined problem. Some studies that use IMU sensors face challenges in reconstructing the pose due to positional drift and ambiguity of the poses. In recent years, some mainstream VR systems have released 6-degree-of-freedom (6-DoF) tracking devices providing positional and rotational information. Nevertheless, most solutions for reconstructing full-body poses rely on traditional inverse kinematics (IK) solutions, which often produce non-continuous and unnatural poses. In this article, we introduce SparsePoser, a novel deep learning-based solution for reconstructing a full-body pose from a reduced set of six tracking devices. Our system incorporates a convolutional-based autoencoder that synthesizes high-quality continuous human poses by learning the human motion manifold from motion capture data. Then, we employ a learned IK component, made of multiple lightweight feed-forward neural networks, to adjust the hands and feet toward the corresponding trackers. We extensively evaluate our method on publicly available motion capture datasets and with real-time live demos. We show that our method outperforms state-of-the-art techniques using IMU sensors or 6-DoF tracking devices, and can be used for users with different body dimensions and proportions.
<div id='section'>Paperid: <span id='pid'>1157, <a href='https://arxiv.org/pdf/2310.19620.pdf' target='_blank'>https://arxiv.org/pdf/2310.19620.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiao Sun, Shiduo Zhang, Danjiao Ma, Jingzhe Shi, Derun Li, Simian Luo, Yu Wang, Ningyi Xu, Guangzhi Cao, Hang Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.19620">Large Trajectory Models are Scalable Motion Predictors and Planners</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion prediction and planning are vital tasks in autonomous driving, and recent efforts have shifted to machine learning-based approaches. The challenges include understanding diverse road topologies, reasoning traffic dynamics over a long time horizon, interpreting heterogeneous behaviors, and generating policies in a large continuous state space. Inspired by the success of large language models in addressing similar complexities through model scaling, we introduce a scalable trajectory model called State Transformer (STR). STR reformulates the motion prediction and motion planning problems by arranging observations, states, and actions into one unified sequence modeling task. Our approach unites trajectory generation problems with other sequence modeling problems, powering rapid iterations with breakthroughs in neighbor domains such as language modeling. Remarkably, experimental results reveal that large trajectory models (LTMs), such as STR, adhere to the scaling laws by presenting outstanding adaptability and learning efficiency. Qualitative results further demonstrate that LTMs are capable of making plausible predictions in scenarios that diverge significantly from the training data distribution. LTMs also learn to make complex reasonings for long-term planning, without explicit loss designs or costly high-level annotations.
<div id='section'>Paperid: <span id='pid'>1158, <a href='https://arxiv.org/pdf/2310.18698.pdf' target='_blank'>https://arxiv.org/pdf/2310.18698.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuesong Nie, Xi Chen, Haoyuan Jin, Zhihang Zhu, Yunfeng Yan, Donglian Qi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.18698">Triplet Attention Transformer for Spatiotemporal Predictive Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Spatiotemporal predictive learning offers a self-supervised learning paradigm that enables models to learn both spatial and temporal patterns by predicting future sequences based on historical sequences. Mainstream methods are dominated by recurrent units, yet they are limited by their lack of parallelization and often underperform in real-world scenarios. To improve prediction quality while maintaining computational efficiency, we propose an innovative triplet attention transformer designed to capture both inter-frame dynamics and intra-frame static features. Specifically, the model incorporates the Triplet Attention Module (TAM), which replaces traditional recurrent units by exploring self-attention mechanisms in temporal, spatial, and channel dimensions. In this configuration: (i) temporal tokens contain abstract representations of inter-frame, facilitating the capture of inherent temporal dependencies; (ii) spatial and channel attention combine to refine the intra-frame representation by performing fine-grained interactions across spatial and channel dimensions. Alternating temporal, spatial, and channel-level attention allows our approach to learn more complex short- and long-range spatiotemporal dependencies. Extensive experiments demonstrate performance surpassing existing recurrent-based and recurrent-free methods, achieving state-of-the-art under multi-scenario examination including moving object trajectory prediction, traffic flow prediction, driving scene prediction, and human motion capture.
<div id='section'>Paperid: <span id='pid'>1159, <a href='https://arxiv.org/pdf/2310.10198.pdf' target='_blank'>https://arxiv.org/pdf/2310.10198.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Heyuan Yao, Zhenhua Song, Yuyang Zhou, Tenglong Ao, Baoquan Chen, Libin Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.10198">MoConVQ: Unified Physics-Based Motion Control via Scalable Discrete Representations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we present MoConVQ, a novel unified framework for physics-based motion control leveraging scalable discrete representations. Building upon vector quantized variational autoencoders (VQ-VAE) and model-based reinforcement learning, our approach effectively learns motion embeddings from a large, unstructured dataset spanning tens of hours of motion examples. The resultant motion representation not only captures diverse motion skills but also offers a robust and intuitive interface for various applications. We demonstrate the versatility of MoConVQ through several applications: universal tracking control from various motion sources, interactive character control with latent motion representations using supervised learning, physics-based motion generation from natural language descriptions using the GPT framework, and, most interestingly, seamless integration with large language models (LLMs) with in-context learning to tackle complex and abstract tasks.
<div id='section'>Paperid: <span id='pid'>1160, <a href='https://arxiv.org/pdf/2310.06751.pdf' target='_blank'>https://arxiv.org/pdf/2310.06751.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Baichuan Huang, Jingjin Yu, Siddarth Jain
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.06751">EARL: Eye-on-Hand Reinforcement Learner for Dynamic Grasping with Active Pose Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we explore the dynamic grasping of moving objects through active pose tracking and reinforcement learning for hand-eye coordination systems. Most existing vision-based robotic grasping methods implicitly assume target objects are stationary or moving predictably. Performing grasping of unpredictably moving objects presents a unique set of challenges. For example, a pre-computed robust grasp can become unreachable or unstable as the target object moves, and motion planning must also be adaptive. In this work, we present a new approach, Eye-on-hAnd Reinforcement Learner (EARL), for enabling coupled Eye-on-Hand (EoH) robotic manipulation systems to perform real-time active pose tracking and dynamic grasping of novel objects without explicit motion prediction. EARL readily addresses many thorny issues in automated hand-eye coordination, including fast-tracking of 6D object pose from vision, learning control policy for a robotic arm to track a moving object while keeping the object in the camera's field of view, and performing dynamic grasping. We demonstrate the effectiveness of our approach in extensive experiments validated on multiple commercial robotic arms in both simulations and complex real-world tasks.
<div id='section'>Paperid: <span id='pid'>1161, <a href='https://arxiv.org/pdf/2310.01363.pdf' target='_blank'>https://arxiv.org/pdf/2310.01363.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhichao Li, Yinzhuang Yi, Zhuolin Niu, Nikolay Atanasov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.01363">EAST: Environment Aware Safe Tracking using Planning and Control Co-Design</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper considers the problem of autonomous mobile robot navigation in unknown environments with moving obstacles. We propose a new method to achieve environment-aware safe tracking (EAST) of robot motion plans that integrates an obstacle clearance cost for path planning, a convex reachable set for robot motion prediction, and safety constraints for dynamic obstacle avoidance. EAST adapts the motion of the robot according to the locally sensed environment geometry and dynamics, leading to fast motion in wide open areas and cautious behavior in narrow passages or near moving obstacles. Our control design uses a reference governor, a virtual dynamical system that guides the robot's motion and decouples the path tracking and safety objectives. While reference governor methods have been used for safe tracking control in static environments, our key contribution is an extension to dynamic environments using convex optimization with control barrier function (CBF) constraints. Thus, our work establishes a connection between reference governor techniques and CBF techniques for safe control in dynamic environments. We validate our approach in simulated and real-world environments, featuring complex obstacle configurations and natural dynamic obstacle motion.
<div id='section'>Paperid: <span id='pid'>1162, <a href='https://arxiv.org/pdf/2309.17338.pdf' target='_blank'>https://arxiv.org/pdf/2309.17338.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pranav Singh Chib, Pravendra Singh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.17338">Improving Trajectory Prediction in Dynamic Multi-Agent Environment by Dropping Waypoints</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The inherently diverse and uncertain nature of trajectories presents a formidable challenge in accurately modeling them. Motion prediction systems must effectively learn spatial and temporal information from the past to forecast the future trajectories of the agent. Many existing methods learn temporal motion via separate components within stacked models to capture temporal features. Furthermore, prediction methods often operate under the assumption that observed trajectory waypoint sequences are complete, disregarding scenarios where missing values may occur, which can influence their performance. Moreover, these models may be biased toward particular waypoint sequences when making predictions. We propose a novel approach called Temporal Waypoint Dropping (TWD) that explicitly incorporates temporal dependencies during the training of a trajectory prediction model. By stochastically dropping waypoints from past observed trajectories, the model is forced to learn the underlying temporal representation from the remaining waypoints, resulting in an improved model. Incorporating stochastic temporal waypoint dropping into the model learning process significantly enhances its performance in scenarios with missing values. Experimental results demonstrate our approach's substantial improvement in trajectory prediction capabilities. Our approach can complement existing trajectory prediction methods to improve their prediction accuracy. We evaluate our proposed approach on three datasets: NBA Sports VU, ETH-UCY, and TrajNet++.
<div id='section'>Paperid: <span id='pid'>1163, <a href='https://arxiv.org/pdf/2309.14793.pdf' target='_blank'>https://arxiv.org/pdf/2309.14793.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Thomas Monninger, Andreas Weber, Steffen Staab
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.14793">Semantic Map Learning of Traffic Light to Lane Assignment based on Motion Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding which traffic light controls which lane is crucial to navigate intersections safely. Autonomous vehicles commonly rely on High Definition (HD) maps that contain information about the assignment of traffic lights to lanes. The manual provisioning of this information is tedious, expensive, and not scalable. To remedy these issues, our novel approach derives the assignments from traffic light states and the corresponding motion patterns of vehicle traffic. This works in an automated way and independently of the geometric arrangement. We show the effectiveness of basic statistical approaches for this task by implementing and evaluating a pattern-based contribution method. In addition, our novel rejection method includes accompanying safety considerations by leveraging statistical hypothesis testing. Finally, we propose a dataset transformation to re-purpose available motion prediction datasets for semantic map learning. Our publicly available API for the Lyft Level 5 dataset enables researchers to develop and evaluate their own approaches.
<div id='section'>Paperid: <span id='pid'>1164, <a href='https://arxiv.org/pdf/2309.03447.pdf' target='_blank'>https://arxiv.org/pdf/2309.03447.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yaozhong Shi, Grigorios Lavrentiadis, Domniki Asimaki, Zachary E. Ross, Kamyar Azizzadenesheli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.03447">Broadband Ground Motion Synthesis via Generative Adversarial Neural Operators: Development and Validation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a data-driven framework for ground-motion synthesis that generates three-component acceleration time histories conditioned on moment magnitude, rupture distance , time-average shear-wave velocity at the top $30m$ ($V_{S30}$), and style of faulting. We use a Generative Adversarial Neural Operator (GANO), a resolution invariant architecture that guarantees model training independent of the data sampling frequency. We first present the conditional ground-motion synthesis algorithm (cGM-GANO) and discuss its advantages compared to previous work. We next train cGM-GANO on simulated ground motions generated by the Southern California Earthquake Center Broadband Platform (BBP) and on recorded KiK-net data and show that the model can learn the overall magnitude, distance, and $V_{S30}$ scaling of effective amplitude spectra (EAS) ordinates and pseudo-spectral accelerations (PSA). Results specifically show that cGM-GANO produces consistent median scaling with the training data for the corresponding tectonic environments over a wide range of frequencies for scenarios with sufficient data coverage. For the BBP dataset, cGM-GANO cannot learn the ground motion scaling of the stochastic frequency components; for the KiK-net dataset, the largest misfit is observed at short distances and for soft soil conditions due to the scarcity of such data. Except for these conditions, the aleatory variability of EAS and PSA are captured reasonably well. Lastly, cGM-GANO produces similar median scaling to traditional GMMs for frequencies greater than 1Hz for both PSA and EAS but underestimates the aleatory variability of EAS. Discrepancies in the comparisons between the synthetic ground motions and GMMs are attributed to inconsistencies between the training dataset and the datasets used in GMM development. Our pilot study demonstrates GANO's potential for efficient synthesis of broad-band ground motions
<div id='section'>Paperid: <span id='pid'>1165, <a href='https://arxiv.org/pdf/2308.10705.pdf' target='_blank'>https://arxiv.org/pdf/2308.10705.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haorui Ji, Hui Deng, Yuchao Dai, Hongdong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.10705">Unsupervised 3D Pose Estimation with Non-Rigid Structure-from-Motion Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Most of the previous 3D human pose estimation work relied on the powerful memory capability of the network to obtain suitable 2D-3D mappings from the training data. Few works have studied the modeling of human posture deformation in motion. In this paper, we propose a new modeling method for human pose deformations and design an accompanying diffusion-based motion prior. Inspired by the field of non-rigid structure-from-motion, we divide the task of reconstructing 3D human skeletons in motion into the estimation of a 3D reference skeleton, and a frame-by-frame skeleton deformation. A mixed spatial-temporal NRSfMformer is used to simultaneously estimate the 3D reference skeleton and the skeleton deformation of each frame from 2D observations sequence, and then sum them to obtain the pose of each frame. Subsequently, a loss term based on the diffusion model is used to ensure that the pipeline learns the correct prior motion knowledge. Finally, we have evaluated our proposed method on mainstream datasets and obtained superior results outperforming the state-of-the-art.
<div id='section'>Paperid: <span id='pid'>1166, <a href='https://arxiv.org/pdf/2308.00259.pdf' target='_blank'>https://arxiv.org/pdf/2308.00259.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiawei Xu, Diego S D'antonio, Dominic J Ammirato, David SaldaÃ±a
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.00259">SBlimp: Design, Model, and Translational Motion Control for a Swing-Blimp</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present an aerial vehicle composed of a custom quadrotor with tilted rotors and a helium balloon, called SBlimp. We propose a novel control strategy that takes advantage of the natural stable attitude of the blimp to control translational motion. Different from cascade controllers in the literature that controls attitude to achieve desired translational motion, our approach directly controls the linear velocity regardless of the heading orientation of the vehicle. As a result, the vehicle swings during the translational motion. We provide a planar analysis of the dynamic model, demonstrating stability for our controller. Our design is evaluated in numerical simulations with different physical factors and validated with experiments using a real-world prototype, showing that the SBlimp is able to achieve stable translation regardless of its orientation.
<div id='section'>Paperid: <span id='pid'>1167, <a href='https://arxiv.org/pdf/2306.10699.pdf' target='_blank'>https://arxiv.org/pdf/2306.10699.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xirui Li, Feng Wang, Naiyan Wang, Chao Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.10699">Frame Fusion with Vehicle Motion Prediction for 3D Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In LiDAR-based 3D detection, history point clouds contain rich temporal information helpful for future prediction. In the same way, history detections should contribute to future detections. In this paper, we propose a detection enhancement method, namely FrameFusion, which improves 3D object detection results by fusing history frames. In FrameFusion, we ''forward'' history frames to the current frame and apply weighted Non-Maximum-Suppression on dense bounding boxes to obtain a fused frame with merged boxes. To ''forward'' frames, we use vehicle motion models to estimate the future pose of the bounding boxes. However, the commonly used constant velocity model fails naturally on turning vehicles, so we explore two vehicle motion models to address this issue. On Waymo Open Dataset, our FrameFusion method consistently improves the performance of various 3D detectors by about $2$ vehicle level 2 APH with negligible latency and slightly enhances the performance of the temporal fusion method MPPNet. We also conduct extensive experiments on motion model selection.
<div id='section'>Paperid: <span id='pid'>1168, <a href='https://arxiv.org/pdf/2306.09483.pdf' target='_blank'>https://arxiv.org/pdf/2306.09483.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Takeru Oba, Norimichi Ukita
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.09483">R2-Diff: Denoising by diffusion as a refinement of retrieved motion for image-based motion prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image-based motion prediction is one of the essential techniques for robot manipulation. Among the various prediction models, we focus on diffusion models because they have achieved state-of-the-art performance in various applications. In image-based motion prediction, diffusion models stochastically predict contextually appropriate motion by gradually denoising random Gaussian noise based on the image context. While diffusion models are able to predict various motions by changing the random noise, they sometimes fail to predict a contextually appropriate motion based on the image because the random noise is sampled independently of the image context. To solve this problem, we propose R2-Diff. In R2-Diff, a motion retrieved from a dataset based on image similarity is fed into a diffusion model instead of random noise. Then, the retrieved motion is refined through the denoising process of the diffusion model. Since the retrieved motion is almost appropriate to the context, it becomes easier to predict contextually appropriate motion. However, traditional diffusion models are not optimized to refine the retrieved motion. Therefore, we propose the method of tuning the hyperparameters based on the distance of the nearest neighbor motion among the dataset to optimize the diffusion model for refinement. Furthermore, we propose an image-based retrieval method to retrieve the nearest neighbor motion in inference. Our proposed retrieval efficiently computes the similarity based on the image features along the motion trajectory. We demonstrate that R2-Diff accurately predicts appropriate motions and achieves high task success rates compared to recent state-of-the-art models in robot manipulation.
<div id='section'>Paperid: <span id='pid'>1169, <a href='https://arxiv.org/pdf/2306.07399.pdf' target='_blank'>https://arxiv.org/pdf/2306.07399.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Matthieu Armando, Laurence Boissieux, Edmond Boyer, Jean-Sebastien Franco, Martin Humenberger, Christophe Legras, Vincent Leroy, Mathieu Marsot, Julien Pansiot, Sergi Pujades, Rim Rekik, Gregory Rogez, Anilkumar Swamy, Stefanie Wuhrer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.07399">4DHumanOutfit: a multi-subject 4D dataset of human motion sequences in varying outfits exhibiting large displacements</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work presents 4DHumanOutfit, a new dataset of densely sampled spatio-temporal 4D human motion data of different actors, outfits and motions. The dataset is designed to contain different actors wearing different outfits while performing different motions in each outfit. In this way, the dataset can be seen as a cube of data containing 4D motion sequences along 3 axes with identity, outfit and motion. This rich dataset has numerous potential applications for the processing and creation of digital humans, e.g. augmented reality, avatar creation and virtual try on. 4DHumanOutfit is released for research purposes at https://kinovis.inria.fr/4dhumanoutfit/. In addition to image data and 4D reconstructions, the dataset includes reference solutions for each axis. We present independent baselines along each axis that demonstrate the value of these reference solutions for evaluation tasks.
<div id='section'>Paperid: <span id='pid'>1170, <a href='https://arxiv.org/pdf/2304.05703.pdf' target='_blank'>https://arxiv.org/pdf/2304.05703.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jayden Hong, Zengjie Zhang, Amir M. Soufi Enayati, Homayoun Najjaran
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.05703">Human-Robot Skill Transfer with Enhanced Compliance via Dynamic Movement Primitives</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Finding an efficient way to adapt robot trajectory is a priority to improve overall performance of robots. One approach for trajectory planning is through transferring human-like skills to robots by Learning from Demonstrations (LfD). The human demonstration is considered the target motion to mimic. However, human motion is typically optimal for human embodiment but not for robots because of the differences between human biomechanics and robot dynamics. The Dynamic Movement Primitives (DMP) framework is a viable solution for this limitation of LfD, but it requires tuning the second-order dynamics in the formulation. Our contribution is introducing a systematic method to extract the dynamic features from human demonstration to auto-tune the parameters in the DMP framework. In addition to its use with LfD, another utility of the proposed method is that it can readily be used in conjunction with Reinforcement Learning (RL) for robot training. In this way, the extracted features facilitate the transfer of human skills by allowing the robot to explore the possible trajectories more efficiently and increasing robot compliance significantly. We introduced a methodology to extract the dynamic features from multiple trajectories based on the optimization of human-likeness and similarity in the parametric space. Our method was implemented into an actual human-robot setup to extract human dynamic features and used to regenerate the robot trajectories following both LfD and RL with DMP. It resulted in a stable performance of the robot, maintaining a high degree of human-likeness based on accumulated distance error as good as the best heuristic tuning.
<div id='section'>Paperid: <span id='pid'>1171, <a href='https://arxiv.org/pdf/2304.02760.pdf' target='_blank'>https://arxiv.org/pdf/2304.02760.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aykut Ä°Åleyen, Nathan van de Wouw, ÃmÃ¼r Arslan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.02760">Adaptive Headway Motion Control and Motion Prediction for Safe Unicycle Motion Design</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Differential drive robots that can be modeled as a kinematic unicycle are a standard mobile base platform for many service and logistics robots. Safe and smooth autonomous motion around obstacles is a crucial skill for unicycle robots to perform diverse tasks in complex environments. A classical control approach for unicycle control is feedback linearization using a headway point at a fixed headway distance in front of the unicycle. The unicycle headway control brings the headway point to a desired goal location by embedding a linear headway reference dynamics, which often results in an undesired offset for the actual unicycle position. In this paper, we introduce a new unicycle headway control approach with an adaptive headway distance that overcomes this limitation, i.e., when the headway point reaches the goal the unicycle position is also at the goal. By systematically analyzing the closed-loop unicycle motion under the adaptive headway controller, we design analytical feedback motion prediction methods that bound the closed-loop unicycle position trajectory and so can be effectively used for safety assessment and safe unicycle motion design around obstacles. We present an application of adaptive headway motion control and motion prediction for safe unicycle path following around obstacles in numerical simulations.
<div id='section'>Paperid: <span id='pid'>1172, <a href='https://arxiv.org/pdf/2303.09558.pdf' target='_blank'>https://arxiv.org/pdf/2303.09558.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Laura Duarte, Pedro Neto
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.09558">Classification of Primitive Manufacturing Tasks from Filtered Event Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Collaborative robots are increasingly present in industry to support human activities. However, to make the human-robot collaborative process more effective, there are several challenges to be addressed. Collaborative robotic systems need to be aware of the human activities to (1) anticipate collaborative/assistive actions, (2) learn by demonstration, and (3) activate safety procedures in shared workspace. This study proposes an action classification system to recognize primitive assembly tasks from human motion events data captured by a Dynamic and Active-pixel Vision Sensor (DAVIS). Several filters are compared and combined to remove event data noise. Task patterns are classified from a continuous stream of event data using advanced deep learning and recurrent networks to classify spatial and temporal features. Experiments were conducted on a novel dataset, the dataset of manufacturing tasks (DMT22), featuring 5 classes of representative manufacturing primitives (PickUp, Place, Screw, Hold, Idle) from 5 participants. Results show that the proposed filters remove about 65\% of all events (noise) per recording, conducting to a classification accuracy up to 99,37\% for subjects that trained the system and 97.08\% for new subjects. Data from a left-handed subject were successfully classified using only right-handed training data. These results are object independent.
<div id='section'>Paperid: <span id='pid'>1173, <a href='https://arxiv.org/pdf/2302.10995.pdf' target='_blank'>https://arxiv.org/pdf/2302.10995.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>ÃmÃ¼r Arslan, Aykut Ä°Åleyen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.10995">Vandermonde Trajectory Bounds for Linear Companion Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fast and accurate safety assessment and collision checking are essential for motion planning and control of highly dynamic autonomous robotic systems. Informative, intuitive, and explicit motion trajectory bounds enable explainable and time-critical safety verification of autonomous robot motion. In this paper, we consider feedback linearization of nonlinear systems in the form of proportional-and-higher-order-derivative (PhD) control corresponding to companion dynamics. We introduce a novel analytic convex trajectory bound, called $\textit{Vandermonde simplex}$, for high-order companion systems, that is given by the convex hull of a finite weighted combination of system position, velocity, and other relevant higher-order state variables. Our construction of Vandermonde simplexes is built based on expressing the solution trajectory of companion dynamics in a newly introduced family of $\textit{Vandermonde basis functions}$ that offer new insights for understanding companion system motion compared to the classical exponential basis functions. In numerical simulations, we demonstrate that Vandermonde simplexes offer significantly more accurate motion prediction (e.g., at least an order of magnitude improvement in estimated motion volume) for describing the motion trajectory of companion systems compared to the standard invariant Lyapunov ellipsoids as well as exponential simplexes built based on exponential basis functions.
<div id='section'>Paperid: <span id='pid'>1174, <a href='https://arxiv.org/pdf/2302.08932.pdf' target='_blank'>https://arxiv.org/pdf/2302.08932.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tao Hu, Xiaoqing Guan, Yixu Wang, Yifan Liu, Bixuan Zhang, Boyu Lin, You Wang, Guang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.08932">An MPC-based Optimal Motion Control Framework for Pendulum-driven Spherical Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion control is essential for all autonomous mobile robots, and even more so for spherical robots. Due to the uniqueness of the spherical robot, its motion control must not only ensure accurate tracking of the target commands, but also minimize fluctuations in the robot's attitude and motors' current while tracking. In this paper, model predictive control (MPC) is applied to the control of spherical robots and an MPC-based motion control framework is designed. There are two controllers in the framework, an optimal velocity controller ESO-MPC which combines extend states observers (ESO) and MPC, and an optimal orientation controller that uses multilayer perceptron (MLP) to generate accurate trajectories and MPC with changing weights to achieve optimal control. Finally, the performance of individual controllers and the whole control framework are verified by physical experiments. The experimental results show that the MPC-based motion control framework proposed in this work is much better than PID in terms of rapidity and accuracy, and has great advantages over sliding mode controller (SMC) for overshoot, attitude stability, current stability and energy consumption.
<div id='section'>Paperid: <span id='pid'>1175, <a href='https://arxiv.org/pdf/2302.05041.pdf' target='_blank'>https://arxiv.org/pdf/2302.05041.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Takeru Oba, Norimichi Ukita
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.05041">Data-Driven Stochastic Motion Evaluation and Optimization with Image by Spatially-Aligned Temporal Encoding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper proposes a probabilistic motion prediction method for long motions. The motion is predicted so that it accomplishes a task from the initial state observed in the given image. While our method evaluates the task achievability by the Energy-Based Model (EBM), previous EBMs are not designed for evaluating the consistency between different domains (i.e., image and motion in our method). Our method seamlessly integrates the image and motion data into the image feature domain by spatially-aligned temporal encoding so that features are extracted along the motion trajectory projected onto the image. Furthermore, this paper also proposes a data-driven motion optimization method, Deep Motion Optimizer (DMO), that works with EBM for motion prediction. Different from previous gradient-based optimizers, our self-supervised DMO alleviates the difficulty of hyper-parameter tuning to avoid local minima. The effectiveness of the proposed method is demonstrated with a variety of experiments with similar SOTA methods.
<div id='section'>Paperid: <span id='pid'>1176, <a href='https://arxiv.org/pdf/2211.11220.pdf' target='_blank'>https://arxiv.org/pdf/2211.11220.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rongqin Liang, Yuanman Li, Jiantao Zhou, Xia Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.11220">STGlow: A Flow-based Generative Framework with Dual Graphormer for Pedestrian Trajectory Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The pedestrian trajectory prediction task is an essential component of intelligent systems. Its applications include but are not limited to autonomous driving, robot navigation, and anomaly detection of monitoring systems. Due to the diversity of motion behaviors and the complex social interactions among pedestrians, accurately forecasting their future trajectory is challenging. Existing approaches commonly adopt GANs or CVAEs to generate diverse trajectories. However, GAN-based methods do not directly model data in a latent space, which may make them fail to have full support over the underlying data distribution; CVAE-based methods optimize a lower bound on the log-likelihood of observations, which may cause the learned distribution to deviate from the underlying distribution. The above limitations make existing approaches often generate highly biased or inaccurate trajectories. In this paper, we propose a novel generative flow based framework with dual graphormer for pedestrian trajectory prediction (STGlow). Different from previous approaches, our method can more precisely model the underlying data distribution by optimizing the exact log-likelihood of motion behaviors. Besides, our method has clear physical meanings for simulating the evolution of human motion behaviors. The forward process of the flow gradually degrades complex motion behavior into simple behavior, while its reverse process represents the evolution of simple behavior into complex motion behavior. Further, we introduce a dual graphormer combining with the graph structure to more adequately model the temporal dependencies and the mutual spatial interactions. Experimental results on several benchmarks demonstrate that our method achieves much better performance compared to previous state-of-the-art approaches.
<div id='section'>Paperid: <span id='pid'>1177, <a href='https://arxiv.org/pdf/2210.12315.pdf' target='_blank'>https://arxiv.org/pdf/2210.12315.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiyuan Ren, Zhihong Pan, Xin Zhou, Le Kang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.12315">Diffusion Motion: Generate Text-Guided 3D Human Motion by Diffusion Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a simple and novel method for generating 3D human motion from complex natural language sentences, which describe different velocity, direction and composition of all kinds of actions. Different from existing methods that use classical generative architecture, we apply the Denoising Diffusion Probabilistic Model to this task, synthesizing diverse motion results under the guidance of texts. The diffusion model converts white noise into structured 3D motion by a Markov process with a series of denoising steps and is efficiently trained by optimizing a variational lower bound. To achieve the goal of text-conditioned image synthesis, we use the classifier-free guidance strategy to fuse text embedding into the model during training. Our experiments demonstrate that our model achieves competitive results on HumanML3D test set quantitatively and can generate more visually natural and diverse examples. We also show with experiments that our model is capable of zero-shot generation of motions for unseen text guidance.
<div id='section'>Paperid: <span id='pid'>1178, <a href='https://arxiv.org/pdf/2209.14887.pdf' target='_blank'>https://arxiv.org/pdf/2209.14887.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Siddhant Gangapurwala, Luigi Campanaro, Ioannis Havoutis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.14887">Learning Low-Frequency Motion Control for Robust and Dynamic Robot Locomotion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robotic locomotion is often approached with the goal of maximizing robustness and reactivity by increasing motion control frequency. We challenge this intuitive notion by demonstrating robust and dynamic locomotion with a learned motion controller executing at as low as 8 Hz on a real ANYmal C quadruped. The robot is able to robustly and repeatably achieve a high heading velocity of 1.5 m/s, traverse uneven terrain, and resist unexpected external perturbations. We further present a comparative analysis of deep reinforcement learning (RL) based motion control policies trained and executed at frequencies ranging from 5 Hz to 200 Hz. We show that low-frequency policies are less sensitive to actuation latencies and variations in system dynamics. This is to the extent that a successful sim-to-real transfer can be performed even without any dynamics randomization or actuation modeling. We support this claim through a set of rigorous empirical evaluations. Moreover, to assist reproducibility, we provide the training and deployment code along with an extended analysis at https://ori-drs.github.io/lfmc/.
<div id='section'>Paperid: <span id='pid'>1179, <a href='https://arxiv.org/pdf/2209.14009.pdf' target='_blank'>https://arxiv.org/pdf/2209.14009.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Doganay Sirintuna, Idil Ozdamar, Arash Ajoudani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.14009">Carrying the uncarriable: a deformation-agnostic and human-cooperative framework for unwieldy objects using multiple robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This manuscript introduces an object deformability-agnostic framework for co-carrying tasks that are shared between a person and multiple robots. Our approach allows the full control of the co-carrying trajectories by the person while sharing the load with multiple robots depending on the size and the weight of the object. This is achieved by merging the haptic information transferred through the object and the human motion information obtained from a motion capture system. One important advantage of the framework is that no strict internal communication is required between the robots, regardless of the object size and deformation characteristics. We validate the framework with two challenging real-world scenarios: co-transportation of a wooden rigid closet and a bulky box on top of forklift moving straps, with the latter characterizing deformable objects. In order to evaluate the generalizability of the proposed framework, a heterogenous team of two mobile manipulators that consist of an Omni-directional mobile base and a collaborative robotic arm with different DoFs is chosen for the experiments. The qualitative comparison between our controller and the baseline controller (i.e., an admittance controller) during these experiments demonstrated the effectiveness of the proposed framework especially when co-carrying deformable objects. Furthermore, we believe that the performance of our framework during the experiment with the lifting straps offers a promising solution for the co-transportation of bulky and ungraspable objects.
<div id='section'>Paperid: <span id='pid'>1180, <a href='https://arxiv.org/pdf/2209.12648.pdf' target='_blank'>https://arxiv.org/pdf/2209.12648.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aykut Ä°Åleyen, Nathan van de Wouw, ÃmÃ¼r Arslan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.12648">Feedback Motion Prediction for Safe Unicycle Robot Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As a simple and robust mobile robot base, differential drive robots that can be modelled as a kinematic unicycle find significant applications in logistics and service robotics in both industrial and domestic settings. Safe robot navigation around obstacles is an essential skill for such unicycle robots to perform diverse useful tasks in complex cluttered environments, especially around people and other robots. Fast and accurate safety assessment plays a key role in reactive and safe robot motion design. In this paper, as a more accurate and still simple alternative to the standard circular Lyapunov level sets, we introduce novel conic feedback motion prediction methods for bounding the close-loop motion trajectory of the kinematic unicycle robot model under a standard unicycle motion control approach. We present an application of unicycle feedback motion prediction for safe robot navigation around obstacles using reference governors, where the safety of a unicycle robot is continuously monitored based on the predicted future robot motion. We investigate the role of motion prediction on robot behaviour in numerical simulations and conclude that fast and accurate feedback motion prediction is key for fast, reactive, and safe robot navigation around obstacles.
<div id='section'>Paperid: <span id='pid'>1181, <a href='https://arxiv.org/pdf/2209.11886.pdf' target='_blank'>https://arxiv.org/pdf/2209.11886.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weizhuo Wang, Michael Raitor, Steve Collins, C. Karen Liu, Monroe Kennedy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.11886">Trajectory and Sway Prediction Towards Fall Prevention</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Falls are the leading cause of fatal and non-fatal injuries, particularly for older persons. Imbalance can result from the body's internal causes (illness), or external causes (active or passive perturbation). Active perturbation results from applying an external force to a person, while passive perturbation results from human motion interacting with a static obstacle. This work proposes a metric that allows for the monitoring of the person's torso and its correlation to active and passive perturbations. We show that large changes in the torso sway can be strongly correlated to active perturbations. We also show that we can reasonably predict the future path and expected change in torso sway by conditioning the expected path and torso sway on the past trajectory, torso motion, and the surrounding scene. This could have direct future applications to fall prevention. Results demonstrate that the torso sway is strongly correlated with perturbations. And our model is able to make use of the visual cues presented in the panorama and condition the prediction accordingly.
<div id='section'>Paperid: <span id='pid'>1182, <a href='https://arxiv.org/pdf/2209.05135.pdf' target='_blank'>https://arxiv.org/pdf/2209.05135.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Federico Tavella, Aphrodite Galata, Angelo Cangelosi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.05135">Signs of Language: Embodied Sign Language Fingerspelling Acquisition from Demonstrations for Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning fine-grained movements is a challenging topic in robotics, particularly in the context of robotic hands. One specific instance of this challenge is the acquisition of fingerspelling sign language in robots. In this paper, we propose an approach for learning dexterous motor imitation from video examples without additional information. To achieve this, we first build a URDF model of a robotic hand with a single actuator for each joint. We then leverage pre-trained deep vision models to extract the 3D pose of the hand from RGB videos. Next, using state-of-the-art reinforcement learning algorithms for motion imitation (namely, proximal policy optimization and soft actor-critic), we train a policy to reproduce the movement extracted from the demonstrations. We identify the optimal set of hyperparameters for imitation based on a reference motion. Finally, we demonstrate the generalizability of our approach by testing it on six different tasks, corresponding to fingerspelled letters. Our results show that our approach is able to successfully imitate these fine-grained movements without additional information, highlighting its potential for real-world applications in robotics.
<div id='section'>Paperid: <span id='pid'>1183, <a href='https://arxiv.org/pdf/2207.14556.pdf' target='_blank'>https://arxiv.org/pdf/2207.14556.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Seyed Amir Tafrishi, Ankit A. Ravankar, Yasuhisa Hirata
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2207.14556">PSM: A Predictive Safety Model for Body Motion Based On the Spring-Damper Pendulum</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Quantifying the safety of the human body orientation is an important issue in human-robot interaction. Knowing the changing physical constraints on human motion can improve inspection of safe human motions and bring essential information about stability and normality of human body orientations with real-time risk assessment. Also, this information can be used in cooperative robots and monitoring systems to evaluate and interact in the environment more freely. Furthermore, the workspace area can be more deterministic with the known physical characteristics of safety. Based on this motivation, we propose a novel predictive safety model (PSM) that relies on the information of an inertial measurement unit on the human chest. The PSM encompasses a 3-Dofs spring-damper pendulum model that predicts human motion based on a safe motion dataset. The estimated safe orientation of humans is obtained by integrating a safety dataset and an elastic spring-damper model in a way that the proposed approach can realize complex motions at different safety levels. We did experiments in a real-world scenario to verify our novel proposed model. This novel approach can be used in different guidance/assistive robots and health monitoring systems to support and evaluate the human condition, particularly elders.
<div id='section'>Paperid: <span id='pid'>1184, <a href='https://arxiv.org/pdf/2204.06862.pdf' target='_blank'>https://arxiv.org/pdf/2204.06862.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingzhe Ma, Xiaoqing Zhang, Shiqi Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2204.06862">An Identity-Preserved Framework for Human Motion Transfer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion transfer (HMT) aims to generate a video clip for the target subject by imitating the source subject's motion. Although previous methods have achieved good results in synthesizing good-quality videos, they lose sight of individualized motion information from the source and target motions, which is significant for the realism of the motion in the generated video. To address this problem, we propose a novel identity-preserved HMT network, termed \textit{IDPres}. This network is a skeleton-based approach that uniquely incorporates the target's individualized motion and skeleton information to augment identity representations. This integration significantly enhances the realism of movements in the generated videos. Our method focuses on the fine-grained disentanglement and synthesis of motion. To improve the representation learning capability in latent space and facilitate the training of \textit{IDPres}, we introduce three training schemes. These schemes enable \textit{IDPres} to concurrently disentangle different representations and accurately control them, ensuring the synthesis of ideal motions. To evaluate the proportion of individualized motion information in the generated video, we are the first to introduce a new quantitative metric called Identity Score (\textit{ID-Score}), motivated by the success of gait recognition methods in capturing identity information. Moreover, we collect an identity-motion paired dataset, $Dancer101$, consisting of solo-dance videos of 101 subjects from the public domain, providing a benchmark to prompt the development of HMT methods. Extensive experiments demonstrate that the proposed \textit{IDPres} method surpasses existing state-of-the-art techniques in terms of reconstruction accuracy, realistic motion, and identity preservation.
<div id='section'>Paperid: <span id='pid'>1185, <a href='https://arxiv.org/pdf/1608.07876.pdf' target='_blank'>https://arxiv.org/pdf/1608.07876.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hirokatsu Kataoka, Kensho Hara, Yutaka Satoh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/1608.07876">Human Action Recognition without Human</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The objective of this paper is to evaluate "human action recognition without human". Motion representation is frequently discussed in human action recognition. We have examined several sophisticated options, such as dense trajectories (DT) and the two-stream convolutional neural network (CNN). However, some features from the background could be too strong, as shown in some recent studies on human action recognition. Therefore, we considered whether a background sequence alone can classify human actions in current large-scale action datasets (e.g., UCF101). In this paper, we propose a novel concept for human action analysis that is named "human action recognition without human". An experiment clearly shows the effect of a background sequence for understanding an action label.
<div id='section'>Paperid: <span id='pid'>1186, <a href='https://arxiv.org/pdf/2510.04753.pdf' target='_blank'>https://arxiv.org/pdf/2510.04753.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Masoumeh Chapariniya, Teodora Vukovic, Sarah Ebling, Volker Dellwo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04753">Beyond Appearance: Transformer-based Person Identification from Conversational Dynamics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper investigates the performance of transformer-based architectures for person identification in natural, face-to-face conversation scenario. We implement and evaluate a two-stream framework that separately models spatial configurations and temporal motion patterns of 133 COCO WholeBody keypoints, extracted from a subset of the CANDOR conversational corpus. Our experiments compare pre-trained and from-scratch training, investigate the use of velocity features, and introduce a multi-scale temporal transformer for hierarchical motion modeling. Results demonstrate that domain-specific training significantly outperforms transfer learning, and that spatial configurations carry more discriminative information than temporal dynamics. The spatial transformer achieves 95.74% accuracy, while the multi-scale temporal transformer achieves 93.90%. Feature-level fusion pushes performance to 98.03%, confirming that postural and dynamic information are complementary. These findings highlight the potential of transformer architectures for person identification in natural interactions and provide insights for future multimodal and cross-cultural studies.
<div id='section'>Paperid: <span id='pid'>1187, <a href='https://arxiv.org/pdf/2510.02732.pdf' target='_blank'>https://arxiv.org/pdf/2510.02732.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianing Chen, Zehao Li, Yujun Cai, Hao Jiang, Shuqin Gao, Honglong Zhao, Tianlu Mao, Yucheng Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.02732">From Tokens to Nodes: Semantic-Guided Motion Control for Dynamic 3D Gaussian Splatting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dynamic 3D reconstruction from monocular videos remains difficult due to the ambiguity inferring 3D motion from limited views and computational demands of modeling temporally varying scenes. While recent sparse control methods alleviate computation by reducing millions of Gaussians to thousands of control points, they suffer from a critical limitation: they allocate points purely by geometry, leading to static redundancy and dynamic insufficiency. We propose a motion-adaptive framework that aligns control density with motion complexity. Leveraging semantic and motion priors from vision foundation models, we establish patch-token-node correspondences and apply motion-adaptive compression to concentrate control points in dynamic regions while suppressing redundancy in static backgrounds. Our approach achieves flexible representational density adaptation through iterative voxelization and motion tendency scoring, directly addressing the fundamental mismatch between control point allocation and motion complexity. To capture temporal evolution, we introduce spline-based trajectory parameterization initialized by 2D tracklets, replacing MLP-based deformation fields to achieve smoother motion representation and more stable optimization. Extensive experiments demonstrate significant improvements in reconstruction quality and efficiency over existing state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>1188, <a href='https://arxiv.org/pdf/2510.02284.pdf' target='_blank'>https://arxiv.org/pdf/2510.02284.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>David Romero, Ariana Bermudez, Hao Li, Fabio Pizzati, Ivan Laptev
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.02284">Learning to Generate Object Interactions with Physics-Guided Video Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent models for video generation have achieved remarkable progress and are now deployed in film, social media production, and advertising. Beyond their creative potential, such models also hold promise as world simulators for robotics and embodied decision making. Despite strong advances, however, current approaches still struggle to generate physically plausible object interactions and lack physics-grounded control mechanisms. To address this limitation, we introduce KineMask, an approach for physics-guided video generation that enables realistic rigid body control, interactions, and effects. Given a single image and a specified object velocity, our method generates videos with inferred motions and future object interactions. We propose a two-stage training strategy that gradually removes future motion supervision via object masks. Using this strategy we train video diffusion models (VDMs) on synthetic scenes of simple interactions and demonstrate significant improvements of object interactions in real scenes. Furthermore, KineMask integrates low-level motion control with high-level textual conditioning via predictive scene descriptions, leading to effective support for synthesis of complex dynamical phenomena. Extensive experiments show that KineMask achieves strong improvements over recent models of comparable size. Ablation studies further highlight the complementary roles of low- and high-level conditioning in VDMs. Our code, model, and data will be made publicly available.
<div id='section'>Paperid: <span id='pid'>1189, <a href='https://arxiv.org/pdf/2509.23279.pdf' target='_blank'>https://arxiv.org/pdf/2509.23279.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rohit Chowdhury, Aniruddha Bala, Rohan Jaiswal, Siddharth Roheda
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23279">Vid-Freeze: Protecting Images from Malicious Image-to-Video Generation via Temporal Freezing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid progress of image-to-video (I2V) generation models has introduced significant risks, enabling video synthesis from static images and facilitating deceptive or malicious content creation. While prior defenses such as I2VGuard attempt to immunize images, effective and principled protection to block motion remains underexplored. In this work, we introduce Vid-Freeze - a novel attention-suppressing adversarial attack that adds carefully crafted adversarial perturbations to images. Our method explicitly targets the attention mechanism of I2V models, completely disrupting motion synthesis while preserving semantic fidelity of the input image. The resulting immunized images generate stand-still or near-static videos, effectively blocking malicious content creation. Our experiments demonstrate the impressive protection provided by the proposed approach, highlighting the importance of attention attacks as a promising direction for robust and proactive defenses against misuse of I2V generation models.
<div id='section'>Paperid: <span id='pid'>1190, <a href='https://arxiv.org/pdf/2509.20696.pdf' target='_blank'>https://arxiv.org/pdf/2509.20696.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qingpeng Li, Chengrui Zhu, Yanming Wu, Xin Yuan, Zhen Zhang, Jian Yang, Yong Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20696">RuN: Residual Policy for Natural Humanoid Locomotion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Enabling humanoid robots to achieve natural and dynamic locomotion across a wide range of speeds, including smooth transitions from walking to running, presents a significant challenge. Existing deep reinforcement learning methods typically require the policy to directly track a reference motion, forcing a single policy to simultaneously learn motion imitation, velocity tracking, and stability maintenance. To address this, we introduce RuN, a novel decoupled residual learning framework. RuN decomposes the control task by pairing a pre-trained Conditional Motion Generator, which provides a kinematically natural motion prior, with a reinforcement learning policy that learns a lightweight residual correction to handle dynamical interactions. Experiments in simulation and reality on the Unitree G1 humanoid robot demonstrate that RuN achieves stable, natural gaits and smooth walk-run transitions across a broad velocity range (0-2.5 m/s), outperforming state-of-the-art methods in both training efficiency and final performance.
<div id='section'>Paperid: <span id='pid'>1191, <a href='https://arxiv.org/pdf/2509.19545.pdf' target='_blank'>https://arxiv.org/pdf/2509.19545.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Min Dai, Aaron D. Ames
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.19545">RoMoCo: Robotic Motion Control Toolbox for Reduced-Order Model-Based Locomotion on Bipedal and Humanoid Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present RoMoCo, an open-source C++ toolbox for the synthesis and evaluation of reduced-order model-based planners and whole-body controllers for bipedal and humanoid robots. RoMoCo's modular architecture unifies state-of-the-art planners and whole-body locomotion controllers under a consistent API, enabling rapid prototyping and reproducible benchmarking. By leveraging reduced-order models for platform-agnostic gait generation, RoMoCo enables flexible controller design across diverse robots. We demonstrate its versatility and performance through extensive simulations on the Cassie, Unitree H1, and G1 robots, and validate its real-world efficacy with hardware experiments on the Cassie and G1 humanoids.
<div id='section'>Paperid: <span id='pid'>1192, <a href='https://arxiv.org/pdf/2508.07376.pdf' target='_blank'>https://arxiv.org/pdf/2508.07376.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Huangbin Liang, Beatriz Moya, Francisco Chinesta, Eleni Chatzi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.07376">A Multi-Model Probabilistic Framework for Seismic Risk Assessment and Retrofit Planning of Electric Power Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Electric power networks are critical lifelines, and their disruption during earthquakes can lead to severe cascading failures and significantly hinder post-disaster recovery. Enhancing their seismic resilience requires identifying and strengthening vulnerable components in a cost-effective and system-aware manner. However, existing studies often overlook the systemic behavior of power networks under seismic loading. Common limitations include isolated component analyses that neglect network-wide interdependencies, oversimplified damage models assuming binary states or damage independence, and the exclusion of electrical operational constraints. These simplifications can result in inaccurate risk estimates and inefficient retrofit decisions. This study proposes a multi-model probabilistic framework for seismic risk assessment and retrofit planning of electric power systems. The approach integrates: (1) regional seismic hazard characterization with ground motion prediction and spatial correlation models; (2) component-level damage analysis using fragility functions and multi-state damage-functionality mappings; (3) system-level cascading impact evaluation through graph-based island detection and constrained optimal power flow analysis; and (4) retrofit planning via heuristic optimization to minimize expected annual functionality loss (EAFL) under budget constraints. Uncertainty is propagated throughout the framework using Monte Carlo simulation. The methodology is demonstrated on the IEEE 24-bus Reliability Test System, showcasing its ability to capture cascading failures, identify critical components, and generate effective retrofit strategies. Results underscore the potential of the framework as a scalable, data-informed decision-support tool for enhancing the seismic resilience of power infrastructure.
<div id='section'>Paperid: <span id='pid'>1193, <a href='https://arxiv.org/pdf/2508.05162.pdf' target='_blank'>https://arxiv.org/pdf/2508.05162.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuan Wang, Kai Ruan, Liyang Qian, Zhizhi Guo, Chang Su, Gaoang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05162">X-MoGen: Unified Motion Generation across Humans and Animals</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-driven motion generation has attracted increasing attention due to its broad applications in virtual reality, animation, and robotics. While existing methods typically model human and animal motion separately, a joint cross-species approach offers key advantages, such as a unified representation and improved generalization. However, morphological differences across species remain a key challenge, often compromising motion plausibility. To address this, we propose \textbf{X-MoGen}, the first unified framework for cross-species text-driven motion generation covering both humans and animals. X-MoGen adopts a two-stage architecture. First, a conditional graph variational autoencoder learns canonical T-pose priors, while an autoencoder encodes motion into a shared latent space regularized by morphological loss. In the second stage, we perform masked motion modeling to generate motion embeddings conditioned on textual descriptions. During training, a morphological consistency module is employed to promote skeletal plausibility across species. To support unified modeling, we construct \textbf{UniMo4D}, a large-scale dataset of 115 species and 119k motion sequences, which integrates human and animal motions under a shared skeletal topology for joint training. Extensive experiments on UniMo4D demonstrate that X-MoGen outperforms state-of-the-art methods on both seen and unseen species.
<div id='section'>Paperid: <span id='pid'>1194, <a href='https://arxiv.org/pdf/2508.04966.pdf' target='_blank'>https://arxiv.org/pdf/2508.04966.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifan Zhou, Beizhen Zhao, Pengcheng Wu, Hao Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.04966">Laplacian Analysis Meets Dynamics Modelling: Gaussian Splatting for 4D Reconstruction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While 3D Gaussian Splatting (3DGS) excels in static scene modeling, its extension to dynamic scenes introduces significant challenges. Existing dynamic 3DGS methods suffer from either over-smoothing due to low-rank decomposition or feature collision from high-dimensional grid sampling. This is because of the inherent spectral conflicts between preserving motion details and maintaining deformation consistency at different frequency. To address these challenges, we propose a novel dynamic 3DGS framework with hybrid explicit-implicit functions. Our approach contains three key innovations: a spectral-aware Laplacian encoding architecture which merges Hash encoding and Laplacian-based module for flexible frequency motion control, an enhanced Gaussian dynamics attribute that compensates for photometric distortions caused by geometric deformation, and an adaptive Gaussian split strategy guided by KDTree-based primitive control to efficiently query and optimize dynamic areas. Through extensive experiments, our method demonstrates state-of-the-art performance in reconstructing complex dynamic scenes, achieving better reconstruction fidelity.
<div id='section'>Paperid: <span id='pid'>1195, <a href='https://arxiv.org/pdf/2508.03068.pdf' target='_blank'>https://arxiv.org/pdf/2508.03068.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sirui Chen, Yufei Ye, Zi-Ang Cao, Jennifer Lew, Pei Xu, C. Karen Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.03068">Hand-Eye Autonomous Delivery: Learning Humanoid Navigation, Locomotion and Reaching</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose Hand-Eye Autonomous Delivery (HEAD), a framework that learns navigation, locomotion, and reaching skills for humanoids, directly from human motion and vision perception data. We take a modular approach where the high-level planner commands the target position and orientation of the hands and eyes of the humanoid, delivered by the low-level policy that controls the whole-body movements. Specifically, the low-level whole-body controller learns to track the three points (eyes, left hand, and right hand) from existing large-scale human motion capture data while high-level policy learns from human data collected by Aria glasses. Our modular approach decouples the ego-centric vision perception from physical actions, promoting efficient learning and scalability to novel scenes. We evaluate our method both in simulation and in the real-world, demonstrating humanoid's capabilities to navigate and reach in complex environments designed for humans.
<div id='section'>Paperid: <span id='pid'>1196, <a href='https://arxiv.org/pdf/2507.16121.pdf' target='_blank'>https://arxiv.org/pdf/2507.16121.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shanshan Zhang, Qi Zhang, Siyue Wang, Tianshui Wen, Ziheng Zhou, Lingxiang Zheng, Yu Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.16121">DWSFormer: A Lightweight Inertial Odometry Network for Complex Motion Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Inertial odometry (IO) directly estimates the position of a carrier from inertial sensor measurements and serves as a core technology for the widespread deployment of consumer grade localization systems. While existing IO methods can accurately reconstruct simple and near linear motion trajectories, they often fail to account for drift errors caused by complex motion patterns such as turning. This limitation significantly degrades localization accuracy and restricts the applicability of IO systems in real world scenarios. To address these challenges, we propose a lightweight IO framework. Specifically, inertial data is projected into a high dimensional implicit nonlinear feature space using the Star Operation method, enabling the extraction of complex motion features that are typically overlooked. We further introduce a collaborative attention mechanism that jointly models global motion dynamics across both channel and temporal dimensions. In addition, we design Multi Scale Gated Convolution Units to capture fine grained dynamic variations throughout the motion process, thereby enhancing the model's ability to learn rich and expressive motion representations. Extensive experiments demonstrate that our proposed method consistently outperforms SOTA baselines across six widely used inertial datasets. Compared to baseline models on the RoNIN dataset, it achieves reductions in ATE ranging from 2.26% to 65.78%, thereby establishing a new benchmark in the field.
<div id='section'>Paperid: <span id='pid'>1197, <a href='https://arxiv.org/pdf/2507.15649.pdf' target='_blank'>https://arxiv.org/pdf/2507.15649.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haocheng Xu, Haodong Zhang, Zhenghan Chen, Rong Xiong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.15649">EMP: Executable Motion Prior for Humanoid Robot Standing Upper-body Motion Imitation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To support humanoid robots in performing manipulation tasks, it is essential to study stable standing while accommodating upper-body motions. However, the limited controllable range of humanoid robots in a standing position affects the stability of the entire body. Thus we introduce a reinforcement learning based framework for humanoid robots to imitate human upper-body motions while maintaining overall stability. Our approach begins with designing a retargeting network that generates a large-scale upper-body motion dataset for training the reinforcement learning (RL) policy, which enables the humanoid robot to track upper-body motion targets, employing domain randomization for enhanced robustness. To avoid exceeding the robot's execution capability and ensure safety and stability, we propose an Executable Motion Prior (EMP) module, which adjusts the input target movements based on the robot's current state. This adjustment improves standing stability while minimizing changes to motion amplitude. We evaluate our framework through simulation and real-world tests, demonstrating its practical applicability.
<div id='section'>Paperid: <span id='pid'>1198, <a href='https://arxiv.org/pdf/2507.07734.pdf' target='_blank'>https://arxiv.org/pdf/2507.07734.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Michael Neumeier, Jules Lecomte, Nils Kazinski, Soubarna Banik, Bing Li, Axel von Arnim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07734">EEvAct: Early Event-Based Action Recognition with High-Rate Two-Stream Spiking Neural Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recognizing human activities early is crucial for the safety and responsiveness of human-robot and human-machine interfaces. Due to their high temporal resolution and low latency, event-based vision sensors are a perfect match for this early recognition demand. However, most existing processing approaches accumulate events to low-rate frames or space-time voxels which limits the early prediction capabilities. In contrast, spiking neural networks (SNNs) can process the events at a high-rate for early predictions, but most works still fall short on final accuracy. In this work, we introduce a high-rate two-stream SNN which closes this gap by outperforming previous work by 2% in final accuracy on the large-scale THU EACT-50 dataset. We benchmark the SNNs within a novel early event-based recognition framework by reporting Top-1 and Top-5 recognition scores for growing observation time. Finally, we exemplify the impact of these methods on a real-world task of early action triggering for human motion capture in sports.
<div id='section'>Paperid: <span id='pid'>1199, <a href='https://arxiv.org/pdf/2507.00472.pdf' target='_blank'>https://arxiv.org/pdf/2507.00472.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ying Guo, Xi Liu, Cheng Zhen, Pengfei Yan, Xiaoming Wei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.00472">ARIG: Autoregressive Interactive Head Generation for Real-time Conversations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Face-to-face communication, as a common human activity, motivates the research on interactive head generation. A virtual agent can generate motion responses with both listening and speaking capabilities based on the audio or motion signals of the other user and itself. However, previous clip-wise generation paradigm or explicit listener/speaker generator-switching methods have limitations in future signal acquisition, contextual behavioral understanding, and switching smoothness, making it challenging to be real-time and realistic. In this paper, we propose an autoregressive (AR) based frame-wise framework called ARIG to realize the real-time generation with better interaction realism. To achieve real-time generation, we model motion prediction as a non-vector-quantized AR process. Unlike discrete codebook-index prediction, we represent motion distribution using diffusion procedure, achieving more accurate predictions in continuous space. To improve interaction realism, we emphasize interactive behavior understanding (IBU) and detailed conversational state understanding (CSU). In IBU, based on dual-track dual-modal signals, we summarize short-range behaviors through bidirectional-integrated learning and perform contextual understanding over long ranges. In CSU, we use voice activity signals and context features of IBU to understand the various states (interruption, feedback, pause, etc.) that exist in actual conversations. These serve as conditions for the final progressive motion prediction. Extensive experiments have verified the effectiveness of our model.
<div id='section'>Paperid: <span id='pid'>1200, <a href='https://arxiv.org/pdf/2506.22488.pdf' target='_blank'>https://arxiv.org/pdf/2506.22488.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xi Fu, Weibang Jiang, Rui Liu, Gernot R. MÃ¼ller-Putz, Cuntai Guan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.22488">Zero-Shot EEG-to-Gait Decoding via Phase-Aware Representation Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate decoding of lower-limb motion from EEG signals is essential for advancing brain-computer interface (BCI) applications in movement intent recognition and control. However, challenges persist in achieving causal, phase-consistent predictions and in modeling both inter- and intra-subject variability. To address these issues, we propose NeuroDyGait, a domain-generalizable EEG-to-motion decoding framework that leverages structured contrastive representation learning and relational domain modeling. The proposed method employs relative contrastive learning to achieve semantic alignment between EEG and motion embeddings. Furthermore, a multi-cycle gait reconstruction objective is introduced to enforce temporal coherence and maintain biomechanical consistency. To promote inter-session generalization, during fine-tuning, a domain dynamic decoding mechanism adaptively assigns session-specific prediction heads and learns to mix their outputs based on inter-session relationships. NeuroDyGait enables zero-shot motion prediction for unseen individuals without requiring adaptation and achieves superior performance in cross-subject gait decoding on benchmark datasets. Additionally, it demonstrates strong phase-detection capabilities even without explicit phase supervision during training. These findings highlight the potential of relational domain learning in enabling scalable, target-free deployment of BCIs.
<div id='section'>Paperid: <span id='pid'>1201, <a href='https://arxiv.org/pdf/2506.21632.pdf' target='_blank'>https://arxiv.org/pdf/2506.21632.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Da Li, Donggang Jia, Markus Hadwiger, Ivan Viola
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.21632">SkinningGS: Editable Dynamic Human Scene Reconstruction Using Gaussian Splatting Based on a Skinning Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reconstructing an interactive human avatar and the background from a monocular video of a dynamic human scene is highly challenging. In this work we adopt a strategy of point cloud decoupling and joint optimization to achieve the decoupled reconstruction of backgrounds and human bodies while preserving the interactivity of human motion. We introduce a position texture to subdivide the Skinned Multi-Person Linear (SMPL) body model's surface and grow the human point cloud. To capture fine details of human dynamics and deformations, we incorporate a convolutional neural network structure to predict human body point cloud features based on texture. This strategy makes our approach free of hyperparameter tuning for densification and efficiently represents human points with half the point cloud of HUGS. This approach ensures high-quality human reconstruction and reduces GPU resource consumption during training. As a result, our method surpasses the previous state-of-the-art HUGS in reconstruction metrics while maintaining the ability to generalize to novel poses and views. Furthermore, our technique achieves real-time rendering at over 100 FPS, $\sim$6$\times$ the HUGS speed using only Linear Blend Skinning (LBS) weights for human transformation. Additionally, this work demonstrates that this framework can be extended to animal scene reconstruction when an accurately-posed model of an animal is available.
<div id='section'>Paperid: <span id='pid'>1202, <a href='https://arxiv.org/pdf/2506.03191.pdf' target='_blank'>https://arxiv.org/pdf/2506.03191.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Muhammad Islam, Tao Huang, Euijoon Ahn, Usman Naseem
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.03191">Multimodal Generative AI with Autoregressive LLMs for Human Motion Understanding and Generation: A Way Forward</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents an in-depth survey on the use of multimodal Generative Artificial Intelligence (GenAI) and autoregressive Large Language Models (LLMs) for human motion understanding and generation, offering insights into emerging methods, architectures, and their potential to advance realistic and versatile motion synthesis. Focusing exclusively on text and motion modalities, this research investigates how textual descriptions can guide the generation of complex, human-like motion sequences. The paper explores various generative approaches, including autoregressive models, diffusion models, Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), and transformer-based models, by analyzing their strengths and limitations in terms of motion quality, computational efficiency, and adaptability. It highlights recent advances in text-conditioned motion generation, where textual inputs are used to control and refine motion outputs with greater precision. The integration of LLMs further enhances these models by enabling semantic alignment between instructions and motion, improving coherence and contextual relevance. This systematic survey underscores the transformative potential of text-to-motion GenAI and LLM architectures in applications such as healthcare, humanoids, gaming, animation, and assistive technologies, while addressing ongoing challenges in generating efficient and realistic human motion.
<div id='section'>Paperid: <span id='pid'>1203, <a href='https://arxiv.org/pdf/2505.21161.pdf' target='_blank'>https://arxiv.org/pdf/2505.21161.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Leon Tolksdorf, Arturo Tejada, Christian Birkner, Nathan van de Wouw
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.21161">Collision Probability Estimation for Optimization-based Vehicular Motion Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Many motion planning algorithms for automated driving require estimating the probability of collision (POC) to account for uncertainties in the measurement and estimation of the motion of road users. Common POC estimation techniques often utilize sampling-based methods that suffer from computational inefficiency and a non-deterministic estimation, i.e., each estimation result for the same inputs is slightly different. In contrast, optimization-based motion planning algorithms require computationally efficient POC estimation, ideally using deterministic estimation, such that typical optimization algorithms for motion planning retain feasibility. Estimating the POC analytically, however, is challenging because it depends on understanding the collision conditions (e.g., vehicle's shape) and characterizing the uncertainty in motion prediction. In this paper, we propose an approach in which we estimate the POC between two vehicles by over-approximating their shapes by a multi-circular shape approximation. The position and heading of the predicted vehicle are modelled as random variables, contrasting with the literature, where the heading angle is often neglected. We guarantee that the provided POC is an over-approximation, which is essential in providing safety guarantees, and present a computationally efficient algorithm for computing the POC estimate for Gaussian uncertainty in the position and heading. This algorithm is then used in a path-following stochastic model predictive controller (SMPC) for motion planning. With the proposed algorithm, the SMPC generates reproducible trajectories while the controller retains its feasibility in the presented test cases and demonstrates the ability to handle varying levels of uncertainty.
<div id='section'>Paperid: <span id='pid'>1204, <a href='https://arxiv.org/pdf/2505.19530.pdf' target='_blank'>https://arxiv.org/pdf/2505.19530.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Amartya Purushottam, Jack Yan, Christopher Yu, Joao Ramos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.19530">Heavy lifting tasks via haptic teleoperation of a wheeled humanoid</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humanoid robots can support human workers in physically demanding environments by performing tasks that require whole-body coordination, such as lifting and transporting heavy objects.These tasks, which we refer to as Dynamic Mobile Manipulation (DMM), require the simultaneous control of locomotion, manipulation, and posture under dynamic interaction forces. This paper presents a teleoperation framework for DMM on a height-adjustable wheeled humanoid robot for carrying heavy payloads. A Human-Machine Interface (HMI) enables whole-body motion retargeting from the human pilot to the robot by capturing the motion of the human and applying haptic feedback. The pilot uses body motion to regulate robot posture and locomotion, while arm movements guide manipulation.Real time haptic feedback delivers end effector wrenches and balance related cues, closing the loop between human perception and robot environment interaction. We evaluate the different telelocomotion mappings that offer varying levels of balance assistance, allowing the pilot to either manually or automatically regulate the robot's lean in response to payload-induced disturbances. The system is validated in experiments involving dynamic lifting of barbells and boxes up to 2.5 kg (21% of robot mass), demonstrating coordinated whole-body control, height variation, and disturbance handling under pilot guidance. Video demo can be found at: https://youtu.be/jF270_bG1h8?feature=shared
<div id='section'>Paperid: <span id='pid'>1205, <a href='https://arxiv.org/pdf/2505.09731.pdf' target='_blank'>https://arxiv.org/pdf/2505.09731.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>William Xie, Max Conway, Yutong Zhang, Nikolaus Correll
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.09731">Unfettered Forceful Skill Acquisition with Physical Reasoning and Coordinate Frame Labeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision language models (VLMs) exhibit vast knowledge of the physical world, including intuition of physical and spatial properties, affordances, and motion. With fine-tuning, VLMs can also natively produce robot trajectories. We demonstrate that eliciting wrenches, not trajectories, allows VLMs to explicitly reason about forces and leads to zero-shot generalization in a series of manipulation tasks without pretraining. We achieve this by overlaying a consistent visual representation of relevant coordinate frames on robot-attached camera images to augment our query. First, we show how this addition enables a versatile motion control framework evaluated across four tasks (opening and closing a lid, pushing a cup or chair) spanning prismatic and rotational motion, an order of force and position magnitude, different camera perspectives, annotation schemes, and two robot platforms over 220 experiments, resulting in 51% success across the four tasks. Then, we demonstrate that the proposed framework enables VLMs to continually reason about interaction feedback to recover from task failure or incompletion, with and without human supervision. Finally, we observe that prompting schemes with visual annotation and embodied reasoning can bypass VLM safeguards. We characterize prompt component contribution to harmful behavior elicitation and discuss its implications for developing embodied reasoning. Our code, videos, and data are available at: https://scalingforce.github.io/.
<div id='section'>Paperid: <span id='pid'>1206, <a href='https://arxiv.org/pdf/2505.04974.pdf' target='_blank'>https://arxiv.org/pdf/2505.04974.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wanjiang Weng, Xiaofeng Tan, Hongsong Wang, Pan Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.04974">ReAlign: Bilingual Text-to-Motion Generation via Step-Aware Reward-Guided Alignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Bilingual text-to-motion generation, which synthesizes 3D human motions from bilingual text inputs, holds immense potential for cross-linguistic applications in gaming, film, and robotics. However, this task faces critical challenges: the absence of bilingual motion-language datasets and the misalignment between text and motion distributions in diffusion models, leading to semantically inconsistent or low-quality motions. To address these challenges, we propose BiHumanML3D, a novel bilingual human motion dataset, which establishes a crucial benchmark for bilingual text-to-motion generation models. Furthermore, we propose a Bilingual Motion Diffusion model (BiMD), which leverages cross-lingual aligned representations to capture semantics, thereby achieving a unified bilingual model. Building upon this, we propose Reward-guided sampling Alignment (ReAlign) method, comprising a step-aware reward model to assess alignment quality during sampling and a reward-guided strategy that directs the diffusion process toward an optimally aligned distribution. This reward model integrates step-aware tokens and combines a text-aligned module for semantic consistency and a motion-aligned module for realism, refining noisy motions at each timestep to balance probability density and alignment. Experiments demonstrate that our approach significantly improves text-motion alignment and motion quality compared to existing state-of-the-art methods. Project page: https://wengwanjiang.github.io/ReAlign-page/.
<div id='section'>Paperid: <span id='pid'>1207, <a href='https://arxiv.org/pdf/2504.16722.pdf' target='_blank'>https://arxiv.org/pdf/2504.16722.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yingjie Xi, Jian Jun Zhang, Xiaosong Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.16722">PMG: Progressive Motion Generation via Sparse Anchor Postures Curriculum Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In computer animation, game design, and human-computer interaction, synthesizing human motion that aligns with user intent remains a significant challenge. Existing methods have notable limitations: textual approaches offer high-level semantic guidance but struggle to describe complex actions accurately; trajectory-based techniques provide intuitive global motion direction yet often fall short in generating precise or customized character movements; and anchor poses-guided methods are typically confined to synthesize only simple motion patterns. To generate more controllable and precise human motions, we propose \textbf{ProMoGen (Progressive Motion Generation)}, a novel framework that integrates trajectory guidance with sparse anchor motion control. Global trajectories ensure consistency in spatial direction and displacement, while sparse anchor motions only deliver precise action guidance without displacement. This decoupling enables independent refinement of both aspects, resulting in a more controllable, high-fidelity, and sophisticated motion synthesis. ProMoGen supports both dual and single control paradigms within a unified training process. Moreover, we recognize that direct learning from sparse motions is inherently unstable, we introduce \textbf{SAP-CL (Sparse Anchor Posture Curriculum Learning)}, a curriculum learning strategy that progressively adjusts the number of anchors used for guidance, thereby enabling more precise and stable convergence. Extensive experiments demonstrate that ProMoGen excels in synthesizing vivid and diverse motions guided by predefined trajectory and arbitrary anchor frames. Our approach seamlessly integrates personalized motion with structured guidance, significantly outperforming state-of-the-art methods across multiple control scenarios.
<div id='section'>Paperid: <span id='pid'>1208, <a href='https://arxiv.org/pdf/2504.03639.pdf' target='_blank'>https://arxiv.org/pdf/2504.03639.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ting-Hsuan Liao, Yi Zhou, Yu Shen, Chun-Hao Paul Huang, Saayan Mitra, Jia-Bin Huang, Uttaran Bhattacharya
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.03639">Shape My Moves: Text-Driven Shape-Aware Synthesis of Human Motions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We explore how body shapes influence human motion synthesis, an aspect often overlooked in existing text-to-motion generation methods due to the ease of learning a homogenized, canonical body shape. However, this homogenization can distort the natural correlations between different body shapes and their motion dynamics. Our method addresses this gap by generating body-shape-aware human motions from natural language prompts. We utilize a finite scalar quantization-based variational autoencoder (FSQ-VAE) to quantize motion into discrete tokens and then leverage continuous body shape information to de-quantize these tokens back into continuous, detailed motion. Additionally, we harness the capabilities of a pretrained language model to predict both continuous shape parameters and motion tokens, facilitating the synthesis of text-aligned motions and decoding them into shape-aware motions. We evaluate our method quantitatively and qualitatively, and also conduct a comprehensive perceptual study to demonstrate its efficacy in generating shape-aware motions.
<div id='section'>Paperid: <span id='pid'>1209, <a href='https://arxiv.org/pdf/2503.19351.pdf' target='_blank'>https://arxiv.org/pdf/2503.19351.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingyu Liu, Zijie Xin, Yuhan Fu, Ruixiang Zhao, Bangxiang Lan, Xirong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.19351">Multi-Object Sketch Animation by Scene Decomposition and Motion Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Sketch animation, which brings static sketches to life by generating dynamic video sequences, has found widespread applications in GIF design, cartoon production, and daily entertainment. While current methods for sketch animation perform well in single-object sketch animation, they struggle in multi-object scenarios. By analyzing their failures, we identify two major challenges of transitioning from single-object to multi-object sketch animation: object-aware motion modeling and complex motion optimization. For multi-object sketch animation, we propose MoSketch based on iterative optimization through Score Distillation Sampling (SDS) and thus animating a multi-object sketch in a training-data free manner. To tackle the two challenges in a divide-and-conquer strategy, MoSketch has four novel modules, i.e., LLM-based scene decomposition, LLM-based motion planning, multi-grained motion refinement, and compositional SDS. Extensive qualitative and quantitative experiments demonstrate the superiority of our method over existing sketch animation approaches. MoSketch takes a pioneering step towards multi-object sketch animation, opening new avenues for future research and applications.
<div id='section'>Paperid: <span id='pid'>1210, <a href='https://arxiv.org/pdf/2503.17695.pdf' target='_blank'>https://arxiv.org/pdf/2503.17695.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yikun Ma, Yiqing Li, Jiawei Wu, Xing Luo, Zhi Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.17695">MotionDiff: Training-free Zero-shot Interactive Motion Editing via Flow-assisted Multi-view Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generative models have made remarkable advancements and are capable of producing high-quality content. However, performing controllable editing with generative models remains challenging, due to their inherent uncertainty in outputs. This challenge is praticularly pronounced in motion editing, which involves the processing of spatial information. While some physics-based generative methods have attempted to implement motion editing, they typically operate on single-view images with simple motions, such as translation and dragging. These methods struggle to handle complex rotation and stretching motions and ensure multi-view consistency, often necessitating resource-intensive retraining. To address these challenges, we propose MotionDiff, a training-free zero-shot diffusion method that leverages optical flow for complex multi-view motion editing. Specifically, given a static scene, users can interactively select objects of interest to add motion priors. The proposed Point Kinematic Model (PKM) then estimates corresponding multi-view optical flows during the Multi-view Flow Estimation Stage (MFES). Subsequently, these optical flows are utilized to generate multi-view motion results through decoupled motion representation in the Multi-view Motion Diffusion Stage (MMDS). Extensive experiments demonstrate that MotionDiff outperforms other physics-based generative motion editing methods in achieving high-quality multi-view consistent motion results. Notably, MotionDiff does not require retraining, enabling users to conveniently adapt it for various down-stream tasks.
<div id='section'>Paperid: <span id='pid'>1211, <a href='https://arxiv.org/pdf/2503.17340.pdf' target='_blank'>https://arxiv.org/pdf/2503.17340.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Congyi Fan, Jian Guan, Xuanjia Zhao, Dongli Xu, Youtian Lin, Tong Ye, Pengming Feng, Haiwei Pan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.17340">Align Your Rhythm: Generating Highly Aligned Dance Poses with Gating-Enhanced Rhythm-Aware Feature Representation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automatically generating natural, diverse and rhythmic human dance movements driven by music is vital for virtual reality and film industries. However, generating dance that naturally follows music remains a challenge, as existing methods lack proper beat alignment and exhibit unnatural motion dynamics. In this paper, we propose Danceba, a novel framework that leverages gating mechanism to enhance rhythm-aware feature representation for music-driven dance generation, which achieves highly aligned dance poses with enhanced rhythmic sensitivity. Specifically, we introduce Phase-Based Rhythm Extraction (PRE) to precisely extract rhythmic information from musical phase data, capitalizing on the intrinsic periodicity and temporal structures of music. Additionally, we propose Temporal-Gated Causal Attention (TGCA) to focus on global rhythmic features, ensuring that dance movements closely follow the musical rhythm. We also introduce Parallel Mamba Motion Modeling (PMMM) architecture to separately model upper and lower body motions along with musical features, thereby improving the naturalness and diversity of generated dance movements. Extensive experiments confirm that Danceba outperforms state-of-the-art methods, achieving significantly better rhythmic alignment and motion diversity. Project page: https://danceba.github.io/ .
<div id='section'>Paperid: <span id='pid'>1212, <a href='https://arxiv.org/pdf/2503.15127.pdf' target='_blank'>https://arxiv.org/pdf/2503.15127.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tommaso Van Der Meer, Andrea Garulli, Antonio Giannitrapani, Renato Quartullo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.15127">A Comparative Study of Human Motion Models in Reinforcement Learning Algorithms for Social Robot Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Social robot navigation is an evolving research field that aims to find efficient strategies to safely navigate dynamic environments populated by humans. A critical challenge in this domain is the accurate modeling of human motion, which directly impacts the design and evaluation of navigation algorithms. This paper presents a comparative study of two popular categories of human motion models used in social robot navigation, namely velocity-based models and force-based models. A system-theoretic representation of both model types is presented, which highlights their common feedback structure, although with different state variables. Several navigation policies based on reinforcement learning are trained and tested in various simulated environments involving pedestrian crowds modeled with these approaches. A comparative study is conducted to assess performance across multiple factors, including human motion model, navigation policy, scenario complexity and crowd density. The results highlight advantages and challenges of different approaches to modeling human behavior, as well as their role during training and testing of learning-based navigation policies. The findings offer valuable insights and guidelines for selecting appropriate human motion models when designing socially-aware robot navigation systems.
<div id='section'>Paperid: <span id='pid'>1213, <a href='https://arxiv.org/pdf/2503.14919.pdf' target='_blank'>https://arxiv.org/pdf/2503.14919.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junyu Shi, Lijiang Liu, Yong Sun, Zhiyuan Zhang, Jinni Zhou, Qiang Nie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.14919">GenM$^3$: Generative Pretrained Multi-path Motion Model for Text Conditional Human Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scaling up motion datasets is crucial to enhance motion generation capabilities. However, training on large-scale multi-source datasets introduces data heterogeneity challenges due to variations in motion content. To address this, we propose Generative Pretrained Multi-path Motion Model (GenM\(^3\)), a comprehensive framework designed to learn unified motion representations. GenM\(^3\) comprises two components: 1) a Multi-Expert VQ-VAE (MEVQ-VAE) that adapts to different dataset distributions to learn a unified discrete motion representation, and 2) a Multi-path Motion Transformer (MMT) that improves intra-modal representations by using separate modality-specific pathways, each with densely activated experts to accommodate variations within that modality, and improves inter-modal alignment by the text-motion shared pathway. To enable large-scale training, we integrate and unify 11 high-quality motion datasets (approximately 220 hours of motion data) and augment it with textual annotations (nearly 10,000 motion sequences labeled by a large language model and 300+ by human experts). After training on our integrated dataset, GenM\(^3\) achieves a state-of-the-art FID of 0.035 on the HumanML3D benchmark, surpassing state-of-the-art methods by a large margin. It also demonstrates strong zero-shot generalization on IDEA400 dataset, highlighting its effectiveness and adaptability across diverse motion scenarios.
<div id='section'>Paperid: <span id='pid'>1214, <a href='https://arxiv.org/pdf/2503.12782.pdf' target='_blank'>https://arxiv.org/pdf/2503.12782.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiming Wang, Yulong Gao, Yang Wang, Xiongwei Zhao, Yijiao Sun, Xiangyan Kong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.12782">DART: Dual-level Autonomous Robotic Topology for Efficient Exploration in Unknown Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Conventional algorithms in autonomous exploration face challenges due to their inability to accurately and efficiently identify the spatial distribution of convex regions in the real-time map. These methods often prioritize navigation toward the nearest or information-rich frontiers -- the boundaries between known and unknown areas -- resulting in incomplete convex region exploration and requiring excessive backtracking to revisit these missed areas. To address these limitations, this paper introduces an innovative dual-level topological analysis approach. First, we introduce a Low-level Topological Graph (LTG), generated through uniform sampling of the original map data, which captures essential geometric and connectivity details. Next, the LTG is transformed into a High-level Topological Graph (HTG), representing the spatial layout and exploration completeness of convex regions, prioritizing the exploration of convex regions that are not fully explored and minimizing unnecessary backtracking. Finally, an novel Local Artificial Potential Field (LAPF) method is employed for motion control, replacing conventional path planning and boosting overall efficiency. Experimental results highlight the effectiveness of our approach. Simulation tests reveal that our framework significantly reduces exploration time and travel distance, outperforming existing methods in both speed and efficiency. Ablation studies confirm the critical role of each framework component. Real-world tests demonstrate the robustness of our method in environments with poor mapping quality, surpassing other approaches in adaptability to mapping inaccuracies and inaccessible areas.
<div id='section'>Paperid: <span id='pid'>1215, <a href='https://arxiv.org/pdf/2503.00371.pdf' target='_blank'>https://arxiv.org/pdf/2503.00371.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuehao Gao, Yang Yang, Shaoyi Du, Guo-Jun Qi, Junwei Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.00371">Jointly Understand Your Command and Intention:Reciprocal Co-Evolution between Scene-Aware 3D Human Motion Synthesis and Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As two intimate reciprocal tasks, scene-aware human motion synthesis and analysis require a joint understanding between multiple modalities, including 3D body motions, 3D scenes, and textual descriptions. In this paper, we integrate these two paired processes into a Co-Evolving Synthesis-Analysis (CESA) pipeline and mutually benefit their learning. Specifically, scene-aware text-to-human synthesis generates diverse indoor motion samples from the same textual description to enrich human-scene interaction intra-class diversity, thus significantly benefiting training a robust human motion analysis system. Reciprocally, human motion analysis would enforce semantic scrutiny on each synthesized motion sample to ensure its semantic consistency with the given textual description, thus improving realistic motion synthesis. Considering that real-world indoor human motions are goal-oriented and path-guided, we propose a cascaded generation strategy that factorizes text-driven scene-specific human motion generation into three stages: goal inferring, path planning, and pose synthesizing. Coupling CESA with this powerful cascaded motion synthesis model, we jointly improve realistic human motion synthesis and robust human motion analysis in 3D scenes.
<div id='section'>Paperid: <span id='pid'>1216, <a href='https://arxiv.org/pdf/2502.16908.pdf' target='_blank'>https://arxiv.org/pdf/2502.16908.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jaehyung Kim, Jiho Kim, Dongryung Lee, Yujin Jang, Beomjoon Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.16908">A low-cost and lightweight 6 DoF bimanual arm for dynamic and contact-rich manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dynamic and contact-rich object manipulation, such as striking, snatching, or hammering, remains challenging for robotic systems due to hardware limitations. Most existing robots are constrained by high-inertia design, limited compliance, and reliance on expensive torque sensors. To address this, we introduce ARMADA (Affordable Robot for Manipulation and Dynamic Actions), a 6 degrees-of-freedom bimanual robot designed for dynamic manipulation research. ARMADA combines low-inertia, back-drivable actuators with a lightweight design, using readily available components and 3D-printed links for ease of assembly in research labs. The entire system, including both arms, is built for just $6,100. Each arm achieves speeds up to 6.16m/s, almost twice that of most collaborative robots, with a comparable payload of 2.5kg. We demonstrate ARMADA can perform dynamic manipulation like snatching, hammering, and bimanual throwing in real-world environments. We also showcase its effectiveness in reinforcement learning (RL) by training a non-prehensile manipulation policy in simulation and transferring it zero-shot to the real world, as well as human motion shadowing for dynamic bimanual object throwing. ARMADA is fully open-sourced with detailed assembly instructions, CAD models, URDFs, simulation, and learning codes. We highly recommend viewing the supplementary video at https://sites.google.com/view/im2-humanoid-arm.
<div id='section'>Paperid: <span id='pid'>1217, <a href='https://arxiv.org/pdf/2502.06212.pdf' target='_blank'>https://arxiv.org/pdf/2502.06212.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pandula Thennakoon, Mario De Silva, M. Mahesha Viduranga, Sashini Liyanage, Roshan Godaliyadda, Mervyn Parakrama Ekanayake, Vijitha Herath, Anuruddhika Rathnayake, Ganga Thilakarathne, Janaka Ekanayake, Samath Dharmarathne
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.06212">AVSim -- Realistic Simulation Framework for Airborne and Vector-Borne Disease Dynamics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Computational disease modeling plays a crucial role in understanding and controlling the transmission of infectious diseases. While agent-based models (ABMs) provide detailed insights into individual dynamics, accurately replicating human motion remains challenging due to its complex, multi-factorial nature. Most existing frameworks fail to model realistic human motion, leading to oversimplified and less realistic behavior modeling. Furthermore, many current models rely on synthetic assumptions and fail to account for realistic environmental structures, transportation systems, and behavioral heterogeneity across occupation groups. To address these limitations, we introduce AVSim, an agent-based simulation framework designed to model airborne and vector-borne disease dynamics under realistic conditions. A distinguishing feature of AVSim is its ability to accurately model the dual nature of human mobility (both the destinations individuals visit and the duration of their stay) by utilizing GPS traces from real-world participants, characterized by occupation. This enables a significantly more granular and realistic representation of human movement compared to existing approaches. Furthermore, spectral clustering combined with graph-theoretic analysis is used to uncover latent behavioral patterns within occupations, enabling fine-grained modeling of agent behavior. We validate the synthetic human mobility patterns against ground-truth GPS data and demonstrate AVSim's capabilities via simulations of COVID-19 and dengue. The results highlight AVSim's capacity to trace infection pathways, identify high-risk zones, and evaluate interventions such as vaccination, quarantine, and vector control with occupational and geographic specificity.
<div id='section'>Paperid: <span id='pid'>1218, <a href='https://arxiv.org/pdf/2502.02063.pdf' target='_blank'>https://arxiv.org/pdf/2502.02063.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Che-Jui Chang, Qingze Tony Liu, Honglu Zhou, Vladimir Pavlovic, Mubbasir Kapadia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.02063">CASIM: Composite Aware Semantic Injection for Text to Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in generative modeling and tokenization have driven significant progress in text-to-motion generation, leading to enhanced quality and realism in generated motions. However, effectively leveraging textual information for conditional motion generation remains an open challenge. We observe that current approaches, primarily relying on fixed-length text embeddings (e.g., CLIP) for global semantic injection, struggle to capture the composite nature of human motion, resulting in suboptimal motion quality and controllability. To address this limitation, we propose the Composite Aware Semantic Injection Mechanism (CASIM), comprising a composite-aware semantic encoder and a text-motion aligner that learns the dynamic correspondence between text and motion tokens. Notably, CASIM is model and representation-agnostic, readily integrating with both autoregressive and diffusion-based methods. Experiments on HumanML3D and KIT benchmarks demonstrate that CASIM consistently improves motion quality, text-motion alignment, and retrieval scores across state-of-the-art methods. Qualitative analyses further highlight the superiority of our composite-aware approach over fixed-length semantic injection, enabling precise motion control from text prompts and stronger generalization to unseen text inputs.
<div id='section'>Paperid: <span id='pid'>1219, <a href='https://arxiv.org/pdf/2501.19083.pdf' target='_blank'>https://arxiv.org/pdf/2501.19083.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lei Jiang, Ye Wei, Hao Ni
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.19083">MotionPCM: Real-Time Motion Synthesis with Phased Consistency Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion models have become a popular choice for human motion synthesis due to their powerful generative capabilities. However, their high computational complexity and large sampling steps pose challenges for real-time applications. Fortunately, the Consistency Model (CM) provides a solution to greatly reduce the number of sampling steps from hundreds to a few, typically fewer than four, significantly accelerating the synthesis of diffusion models. However, applying CM to text-conditioned human motion synthesis in latent space yields unsatisfactory generation results. In this paper, we introduce \textbf{MotionPCM}, a phased consistency model-based approach designed to improve the quality and efficiency for real-time motion synthesis in latent space. Experimental results on the HumanML3D dataset show that our model achieves real-time inference at over 30 frames per second in a single sampling step while outperforming the previous state-of-the-art with a 38.9\% improvement in FID. The code will be available for reproduction.
<div id='section'>Paperid: <span id='pid'>1220, <a href='https://arxiv.org/pdf/2412.17333.pdf' target='_blank'>https://arxiv.org/pdf/2412.17333.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jaeheun Jung, Jaehyuk Lee, Changhae Jung, Hanyoung Kim, Bosung Jung, Donghun Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.17333">Broadband Ground Motion Synthesis by Diffusion Model with Minimal Condition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Shock waves caused by earthquakes can be devastating. Generating realistic earthquake-caused ground motion waveforms help reducing losses in lives and properties, yet generative models for the task tend to generate subpar waveforms. We present High-fidelity Earthquake Groundmotion Generation System (HEGGS) and demonstrate its superior performance using earthquakes from North American, East Asian, and European regions. HEGGS exploits the intrinsic characteristics of earthquake dataset and learns the waveforms using an end-to-end differentiable generator containing conditional latent diffusion model and hi-fidelity waveform construction model. We show the learning efficiency of HEGGS by training it on a single GPU machine and validate its performance using earthquake databases from North America, East Asia, and Europe, using diverse criteria from waveform generation tasks and seismology. Once trained, HEGGS can generate three dimensional E-N-Z seismic waveforms with accurate P/S phase arrivals, envelope correlation, signal-to-noise ratio, GMPE analysis, frequency content analysis, and section plot analysis.
<div id='section'>Paperid: <span id='pid'>1221, <a href='https://arxiv.org/pdf/2412.15166.pdf' target='_blank'>https://arxiv.org/pdf/2412.15166.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junjia Liu, Zhuo Li, Minghao Yu, Zhipeng Dong, Sylvain Calinon, Darwin Caldwell, Fei Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.15166">Human-Humanoid Robots Cross-Embodiment Behavior-Skill Transfer Using Decomposed Adversarial Learning from Demonstration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humanoid robots are envisioned as embodied intelligent agents capable of performing a wide range of human-level loco-manipulation tasks, particularly in scenarios requiring strenuous and repetitive labor. However, learning these skills is challenging due to the high degrees of freedom of humanoid robots, and collecting sufficient training data for humanoid is a laborious process. Given the rapid introduction of new humanoid platforms, a cross-embodiment framework that allows generalizable skill transfer is becoming increasingly critical. To address this, we propose a transferable framework that reduces the data bottleneck by using a unified digital human model as a common prototype and bypassing the need for re-training on every new robot platform. The model learns behavior primitives from human demonstrations through adversarial imitation, and the complex robot structures are decomposed into functional components, each trained independently and dynamically coordinated. Task generalization is achieved through a human-object interaction graph, and skills are transferred to different robots via embodiment-specific kinematic motion retargeting and dynamic fine-tuning. Our framework is validated on five humanoid robots with diverse configurations, demonstrating stable loco-manipulation and highlighting its effectiveness in reducing data requirements and increasing the efficiency of skill transfer across platforms.
<div id='section'>Paperid: <span id='pid'>1222, <a href='https://arxiv.org/pdf/2411.19527.pdf' target='_blank'>https://arxiv.org/pdf/2411.19527.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jungbin Cho, Junwan Kim, Jisoo Kim, Minseo Kim, Mingu Kang, Sungeun Hong, Tae-Hyun Oh, Youngjae Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.19527">DisCoRD: Discrete Tokens to Continuous Motion via Rectified Flow Decoding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion is inherently continuous and dynamic, posing significant challenges for generative models. While discrete generation methods are widely used, they suffer from limited expressiveness and frame-wise noise artifacts. In contrast, continuous approaches produce smoother, more natural motion but often struggle to adhere to conditioning signals due to high-dimensional complexity and limited training data. To resolve this 'discord' between discrete and continuous representations we introduce DisCoRD: Discrete Tokens to Continuous Motion via Rectified Flow Decoding, a novel method that leverages rectified flow to decode discrete motion tokens in the continuous, raw motion space. Our core idea is to frame token decoding as a conditional generation task, ensuring that DisCoRD captures fine-grained dynamics and achieves smoother, more natural motions. Compatible with any discrete-based framework, our method enhances naturalness without compromising faithfulness to the conditioning signals on diverse settings. Extensive evaluations demonstrate that DisCoRD achieves state-of-the-art performance, with FID of 0.032 on HumanML3D and 0.169 on KIT-ML. These results establish DisCoRD as a robust solution for bridging the divide between discrete efficiency and continuous realism. Project website: https://whwjdqls.github.io/discord-motion/
<div id='section'>Paperid: <span id='pid'>1223, <a href='https://arxiv.org/pdf/2411.19459.pdf' target='_blank'>https://arxiv.org/pdf/2411.19459.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuhang Zhang, Yuan Zhou, Zeyu Liu, Yuxuan Cai, Qiuyue Wang, Aidong Men, Huan Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.19459">Fleximo: Towards Flexible Text-to-Human Motion Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current methods for generating human motion videos rely on extracting pose sequences from reference videos, which restricts flexibility and control. Additionally, due to the limitations of pose detection techniques, the extracted pose sequences can sometimes be inaccurate, leading to low-quality video outputs. We introduce a novel task aimed at generating human motion videos solely from reference images and natural language. This approach offers greater flexibility and ease of use, as text is more accessible than the desired guidance videos. However, training an end-to-end model for this task requires millions of high-quality text and human motion video pairs, which are challenging to obtain. To address this, we propose a new framework called Fleximo, which leverages large-scale pre-trained text-to-3D motion models. This approach is not straightforward, as the text-generated skeletons may not consistently match the scale of the reference image and may lack detailed information. To overcome these challenges, we introduce an anchor point based rescale method and design a skeleton adapter to fill in missing details and bridge the gap between text-to-motion and motion-to-video generation. We also propose a video refinement process to further enhance video quality. A large language model (LLM) is employed to decompose natural language into discrete motion sequences, enabling the generation of motion videos of any desired length. To assess the performance of Fleximo, we introduce a new benchmark called MotionBench, which includes 400 videos across 20 identities and 20 motions. We also propose a new metric, MotionScore, to evaluate the accuracy of motion following. Both qualitative and quantitative results demonstrate that our method outperforms existing text-conditioned image-to-video generation methods. All code and model weights will be made publicly available.
<div id='section'>Paperid: <span id='pid'>1224, <a href='https://arxiv.org/pdf/2411.15472.pdf' target='_blank'>https://arxiv.org/pdf/2411.15472.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pengfei Zhang, Pinxin Liu, Pablo Garrido, Hyeongwoo Kim, Bindita Chaudhuri
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.15472">KinMo: Kinematic-aware Human Motion Understanding and Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current human motion synthesis frameworks rely on global action descriptions, creating a modality gap that limits both motion understanding and generation capabilities. A single coarse description, such as run, fails to capture details such as variations in speed, limb positioning, and kinematic dynamics, leading to ambiguities between text and motion modalities. To address this challenge, we introduce KinMo, a unified framework built on a hierarchical describable motion representation that extends beyond global actions by incorporating kinematic group movements and their interactions. We design an automated annotation pipeline to generate high-quality, fine-grained descriptions for this decomposition, resulting in the KinMo dataset and offering a scalable and cost-efficient solution for dataset enrichment. To leverage these structured descriptions, we propose Hierarchical Text-Motion Alignment that progressively integrates additional motion details, thereby improving semantic motion understanding. Furthermore, we introduce a coarse-to-fine motion generation procedure to leverage enhanced spatial understanding to improve motion synthesis. Experimental results show that KinMo significantly improves motion understanding, demonstrated by enhanced text-motion retrieval performance and enabling more fine-grained motion generation and editing capabilities. Project Page: https://andypinxinliu.github.io/KinMo
<div id='section'>Paperid: <span id='pid'>1225, <a href='https://arxiv.org/pdf/2411.12831.pdf' target='_blank'>https://arxiv.org/pdf/2411.12831.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Paul Janson, Tiberiu Popa, Eugene Belilovsky
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.12831">Towards motion from video diffusion models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-conditioned video diffusion models have emerged as a powerful tool in the realm of video generation and editing. But their ability to capture the nuances of human movement remains under-explored. Indeed the ability of these models to faithfully model an array of text prompts can lead to a wide host of applications in human and character animation. In this work, we take initial steps to investigate whether these models can effectively guide the synthesis of realistic human body animations. Specifically we propose to synthesize human motion by deforming an SMPL-X body representation guided by Score distillation sampling (SDS) calculated using a video diffusion model. By analyzing the fidelity of the resulting animations, we gain insights into the extent to which we can obtain motion using publicly available text-to-video diffusion models using SDS. Our findings shed light on the potential and limitations of these models for generating diverse and plausible human motions, paving the way for further research in this exciting area.
<div id='section'>Paperid: <span id='pid'>1226, <a href='https://arxiv.org/pdf/2411.03289.pdf' target='_blank'>https://arxiv.org/pdf/2411.03289.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ananya Trivedi, Sarvesh Prajapati, Anway Shirgaonkar, Mark Zolotas, Taskin Padir
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.03289">Data-Driven Sampling Based Stochastic MPC for Skid-Steer Mobile Robot Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Traditional approaches to motion modeling for skid-steer robots struggle with capturing nonlinear tire-terrain dynamics, especially during high-speed maneuvers. In this paper, we tackle such nonlinearities by enhancing a dynamic unicycle model with Gaussian Process (GP) regression outputs. This enables us to develop an adaptive, uncertainty-informed navigation formulation. We solve the resultant stochastic optimal control problem using a chance-constrained Model Predictive Path Integral (MPPI) control method. This approach formulates both obstacle avoidance and path-following as chance constraints, accounting for residual uncertainties from the GP to ensure safety and reliability in control. Leveraging GPU acceleration, we efficiently manage the non-convex nature of the problem, ensuring real-time performance. Our approach unifies path-following and obstacle avoidance across different terrains, unlike prior works which typically focus on one or the other. We compare our GP-MPPI method against unicycle and data-driven kinematic models within the MPPI framework. In simulations, our approach shows superior tracking accuracy and obstacle avoidance. We further validate our approach through hardware experiments on a skid-steer robot platform, demonstrating its effectiveness in high-speed navigation. The GPU implementation of the proposed method and supplementary video footage are available at https: //stochasticmppi.github.io.
<div id='section'>Paperid: <span id='pid'>1227, <a href='https://arxiv.org/pdf/2410.09374.pdf' target='_blank'>https://arxiv.org/pdf/2410.09374.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junkai Niu, Sheng Zhong, Xiuyuan Lu, Shaojie Shen, Guillermo Gallego, Yi Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.09374">ESVO2: Direct Visual-Inertial Odometry with Stereo Event Cameras</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Event-based visual odometry is a specific branch of visual Simultaneous Localization and Mapping (SLAM) techniques, which aims at solving tracking and mapping subproblems (typically in parallel), by exploiting the special working principles of neuromorphic (i.e., event-based) cameras. Due to the motion-dependent nature of event data, explicit data association (i.e., feature matching) under large-baseline view-point changes is difficult to establish, making direct methods a more rational choice. However, state-of-the-art direct methods are limited by the high computational complexity of the mapping sub-problem and the degeneracy of camera pose tracking in certain degrees of freedom (DoF) in rotation. In this paper, we tackle these issues by building an event-based stereo visual-inertial odometry system on top of a direct pipeline. Specifically, to speed up the mapping operation, we propose an efficient strategy for sampling contour points according to the local dynamics of events. The mapping performance is also improved in terms of structure completeness and local smoothness by merging the temporal stereo and static stereo results. To circumvent the degeneracy of camera pose tracking in recovering the pitch and yaw components of general 6-DoF motion, we introduce IMU measurements as motion priors via pre-integration. To this end, a compact back-end is proposed for continuously updating the IMU bias and predicting the linear velocity, enabling an accurate motion prediction for camera pose tracking. The resulting system scales well with modern high-resolution event cameras and leads to better global positioning accuracy in large-scale outdoor environments. Extensive evaluations on five publicly available datasets featuring different resolutions and scenarios justify the superior performance of the proposed system against five state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>1228, <a href='https://arxiv.org/pdf/2410.05737.pdf' target='_blank'>https://arxiv.org/pdf/2410.05737.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ashish Kumar, Laxmidhar Behera
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.05737">Thrust Microstepping via Acceleration Feedback in Quadrotor Control for Aerial Grasping of Dynamic Payload</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we propose an end-to-end Thrust Microstepping and Decoupled Control (TMDC) of quadrotors. TMDC focuses on precise off-centered aerial grasping of payloads dynamically, which are attached rigidly to the UAV body via a gripper contrary to the swinging payload. The dynamic payload grasping quickly changes UAV's mass, inertia etc, causing instability while performing a grasping operation in-air. We identify that to handle unknown payload grasping, the role of thrust controller is crucial. Hence, we focus on thrust control without involving system parameters such as mass etc. TMDC is based on our novel Thrust Microstepping via Acceleration Feedback (TMAF) thrust controller and Decoupled Motion Control (DMC). TMAF precisely estimates the desired thrust even at smaller loop rates while DMC decouples the horizontal and vertical motion to counteract disturbances in the case of dynamic payloads. We prove the controller's efficacy via exhaustive experiments in practically interesting and adverse real-world cases, such as fully onboard state estimation without any positioning sensor, narrow and indoor flying workspaces with intense wind turbulence, heavy payloads, non-uniform loop rates, etc. Our TMDC outperforms recent direct acceleration feedback thrust controller (DA) and geometric tracking control (GT) in flying stably for aerial grasping and achieves RMSE below 0.04m in contrast to 0.15m of DA and 0.16m of GT.
<div id='section'>Paperid: <span id='pid'>1229, <a href='https://arxiv.org/pdf/2410.05260.pdf' target='_blank'>https://arxiv.org/pdf/2410.05260.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaifeng Zhao, Gen Li, Siyu Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.05260">DartControl: A Diffusion-Based Autoregressive Motion Model for Real-Time Text-Driven Motion Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-conditioned human motion generation, which allows for user interaction through natural language, has become increasingly popular. Existing methods typically generate short, isolated motions based on a single input sentence. However, human motions are continuous and can extend over long periods, carrying rich semantics. Creating long, complex motions that precisely respond to streams of text descriptions, particularly in an online and real-time setting, remains a significant challenge. Furthermore, incorporating spatial constraints into text-conditioned motion generation presents additional challenges, as it requires aligning the motion semantics specified by text descriptions with geometric information, such as goal locations and 3D scene geometry. To address these limitations, we propose DartControl, in short DART, a Diffusion-based Autoregressive motion primitive model for Real-time Text-driven motion control. Our model effectively learns a compact motion primitive space jointly conditioned on motion history and text inputs using latent diffusion models. By autoregressively generating motion primitives based on the preceding history and current text input, DART enables real-time, sequential motion generation driven by natural language descriptions. Additionally, the learned motion primitive space allows for precise spatial motion control, which we formulate either as a latent noise optimization problem or as a Markov decision process addressed through reinforcement learning. We present effective algorithms for both approaches, demonstrating our model's versatility and superior performance in various motion synthesis tasks. Experiments show our method outperforms existing baselines in motion realism, efficiency, and controllability. Video results are available on the project page: https://zkf1997.github.io/DART/.
<div id='section'>Paperid: <span id='pid'>1230, <a href='https://arxiv.org/pdf/2410.02510.pdf' target='_blank'>https://arxiv.org/pdf/2410.02510.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>James Gao, Jacob Lee, Yuting Zhou, Yunze Hu, Chang Liu, Pingping Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.02510">SwarmCVT: Centroidal Voronoi Tessellation-Based Path Planning for Very-Large-Scale Robotics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Swarm robotics, or very large-scale robotics (VLSR), has many meaningful applications for complicated tasks. However, the complexity of motion control and energy costs stack up quickly as the number of robots increases. In addressing this problem, our previous studies have formulated various methods employing macroscopic and microscopic approaches. These methods enable microscopic robots to adhere to a reference Gaussian mixture model (GMM) distribution observed at the macroscopic scale. As a result, optimizing the macroscopic level will result in an optimal overall result. However, all these methods require systematic and global generation of Gaussian components (GCs) within obstacle-free areas to construct the GMM trajectories. This work utilizes centroidal Voronoi tessellation to generate GCs methodically. Consequently, it demonstrates performance improvement while also ensuring consistency and reliability.
<div id='section'>Paperid: <span id='pid'>1231, <a href='https://arxiv.org/pdf/2409.20554.pdf' target='_blank'>https://arxiv.org/pdf/2409.20554.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ameya Salvi, Pardha Sai Krishna Ala, Jonathon M. Smereka, Mark Brudnak, David Gorsich, Matthias Schmid, Venkat Krovi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.20554">Online identification of skidding modes with interactive multiple model estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Skid-steered wheel mobile robots (SSWMRs) operate in a variety of outdoor environments exhibiting motion behaviors dominated by the effects of complex wheel-ground interactions. Characterizing these interactions is crucial both from the immediate robot autonomy perspective (for motion prediction and control) as well as a long-term predictive maintenance and diagnostics perspective. An ideal solution entails capturing precise state measurements for decisions and controls, which is considerably difficult, especially in increasingly unstructured outdoor regimes of operations for these robots. In this milieu, a framework to identify pre-determined discrete modes of operation can considerably simplify the motion model identification process. To this end, we propose an interactive multiple model (IMM) based filtering framework to probabilistically identify predefined robot operation modes that could arise due to traversal in different terrains or loss of wheel traction.
<div id='section'>Paperid: <span id='pid'>1232, <a href='https://arxiv.org/pdf/2409.06189.pdf' target='_blank'>https://arxiv.org/pdf/2409.06189.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yining Yao, Xi Guo, Chenjing Ding, Wei Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.06189">MyGo: Consistent and Controllable Multi-View Driving Video Generation with Camera Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>High-quality driving video generation is crucial for providing training data for autonomous driving models. However, current generative models rarely focus on enhancing camera motion control under multi-view tasks, which is essential for driving video generation. Therefore, we propose MyGo, an end-to-end framework for video generation, introducing motion of onboard cameras as conditions to make progress in camera controllability and multi-view consistency. MyGo employs additional plug-in modules to inject camera parameters into the pre-trained video diffusion model, which retains the extensive knowledge of the pre-trained model as much as possible. Furthermore, we use epipolar constraints and neighbor view information during the generation process of each view to enhance spatial-temporal consistency. Experimental results show that MyGo has achieved state-of-the-art results in both general camera-controlled video generation and multi-view driving video generation tasks, which lays the foundation for more accurate environment simulation in autonomous driving. Project page: https://metadrivescape.github.io/papers_project/MyGo/page.html
<div id='section'>Paperid: <span id='pid'>1233, <a href='https://arxiv.org/pdf/2408.07295.pdf' target='_blank'>https://arxiv.org/pdf/2408.07295.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pranay Dugar, Aayam Shrestha, Fangzhou Yu, Bart van Marum, Alan Fern
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.07295">Learning Multi-Modal Whole-Body Control for Real-World Humanoid Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The foundational capabilities of humanoid robots should include robustly standing, walking, and mimicry of whole and partial-body motions. This work introduces the Masked Humanoid Controller (MHC), which supports all of these capabilities by tracking target trajectories over selected subsets of humanoid state variables while ensuring balance and robustness against disturbances. The MHC is trained in simulation using a carefully designed curriculum that imitates partially masked motions from a library of behaviors spanning standing, walking, optimized reference trajectories, re-targeted video clips, and human motion capture data. It also allows for combining joystick-based control with partial-body motion mimicry. We showcase simulation experiments validating the MHC's ability to execute a wide variety of behaviors from partially-specified target motions. Moreover, we demonstrate sim-to-real transfer on the real-world Digit V3 humanoid robot. To our knowledge, this is the first instance of a learned controller that can realize whole-body control of a real-world humanoid for such diverse multi-modal targets.
<div id='section'>Paperid: <span id='pid'>1234, <a href='https://arxiv.org/pdf/2407.10481.pdf' target='_blank'>https://arxiv.org/pdf/2407.10481.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jordan Juravsky, Yunrong Guo, Sanja Fidler, Xue Bin Peng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.10481">SuperPADL: Scaling Language-Directed Physics-Based Control with Progressive Supervised Distillation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Physically-simulated models for human motion can generate high-quality responsive character animations, often in real-time. Natural language serves as a flexible interface for controlling these models, allowing expert and non-expert users to quickly create and edit their animations. Many recent physics-based animation methods, including those that use text interfaces, train control policies using reinforcement learning (RL). However, scaling these methods beyond several hundred motions has remained challenging. Meanwhile, kinematic animation models are able to successfully learn from thousands of diverse motions by leveraging supervised learning methods. Inspired by these successes, in this work we introduce SuperPADL, a scalable framework for physics-based text-to-motion that leverages both RL and supervised learning to train controllers on thousands of diverse motion clips. SuperPADL is trained in stages using progressive distillation, starting with a large number of specialized experts using RL. These experts are then iteratively distilled into larger, more robust policies using a combination of reinforcement learning and supervised learning. Our final SuperPADL controller is trained on a dataset containing over 5000 skills and runs in real time on a consumer GPU. Moreover, our policy can naturally transition between skills, allowing for users to interactively craft multi-stage animations. We experimentally demonstrate that SuperPADL significantly outperforms RL-based baselines at this large data scale.
<div id='section'>Paperid: <span id='pid'>1235, <a href='https://arxiv.org/pdf/2407.08049.pdf' target='_blank'>https://arxiv.org/pdf/2407.08049.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lei Cheng, Arindam Sengupta, Siyang Cao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.08049">Deep Learning-Based Robust Multi-Object Tracking via Fusion of mmWave Radar and Camera Sensors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous driving holds great promise in addressing traffic safety concerns by leveraging artificial intelligence and sensor technology. Multi-Object Tracking plays a critical role in ensuring safer and more efficient navigation through complex traffic scenarios. This paper presents a novel deep learning-based method that integrates radar and camera data to enhance the accuracy and robustness of Multi-Object Tracking in autonomous driving systems. The proposed method leverages a Bi-directional Long Short-Term Memory network to incorporate long-term temporal information and improve motion prediction. An appearance feature model inspired by FaceNet is used to establish associations between objects across different frames, ensuring consistent tracking. A tri-output mechanism is employed, consisting of individual outputs for radar and camera sensors and a fusion output, to provide robustness against sensor failures and produce accurate tracking results. Through extensive evaluations of real-world datasets, our approach demonstrates remarkable improvements in tracking accuracy, ensuring reliable performance even in low-visibility scenarios.
<div id='section'>Paperid: <span id='pid'>1236, <a href='https://arxiv.org/pdf/2406.19852.pdf' target='_blank'>https://arxiv.org/pdf/2406.19852.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guillem Capellera, Luis Ferraz, Antonio Rubio, Antonio Agudo, Francesc Moreno-Noguer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.19852">FootBots: A Transformer-based Architecture for Motion Prediction in Soccer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion prediction in soccer involves capturing complex dynamics from player and ball interactions. We present FootBots, an encoder-decoder transformer-based architecture addressing motion prediction and conditioned motion prediction through equivariance properties. FootBots captures temporal and social dynamics using set attention blocks and multi-attention block decoder. Our evaluation utilizes two datasets: a real soccer dataset and a tailored synthetic one. Insights from the synthetic dataset highlight the effectiveness of FootBots' social attention mechanism and the significance of conditioned motion prediction. Empirical results on real soccer data demonstrate that FootBots outperforms baselines in motion prediction and excels in conditioned tasks, such as predicting the players based on the ball position, predicting the offensive (defensive) team based on the ball and the defensive (offensive) team, and predicting the ball position based on all players. Our evaluation connects quantitative and qualitative findings. https://youtu.be/9kaEkfzG3L8
<div id='section'>Paperid: <span id='pid'>1237, <a href='https://arxiv.org/pdf/2406.14422.pdf' target='_blank'>https://arxiv.org/pdf/2406.14422.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingkun Wang, Xiaoguang Ren, Ruochun Jin, Minglong Li, Xiaochuan Zhang, Changqian Yu, Mingxu Wang, Wenjing Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.14422">FutureNet-LOF: Joint Trajectory Prediction and Lane Occupancy Field Prediction with Future Context Encoding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Most prior motion prediction endeavors in autonomous driving have inadequately encoded future scenarios, leading to predictions that may fail to accurately capture the diverse movements of agents (e.g., vehicles or pedestrians). To address this, we propose FutureNet, which explicitly integrates initially predicted trajectories into the future scenario and further encodes these future contexts to enhance subsequent forecasting. Additionally, most previous motion forecasting works have focused on predicting independent futures for each agent. However, safe and smooth autonomous driving requires accurately predicting the diverse future behaviors of numerous surrounding agents jointly in complex dynamic environments. Given that all agents occupy certain potential travel spaces and possess lane driving priority, we propose Lane Occupancy Field (LOF), a new representation with lane semantics for motion forecasting in autonomous driving. LOF can simultaneously capture the joint probability distribution of all road participants' future spatial-temporal positions. Due to the high compatibility between lane occupancy field prediction and trajectory prediction, we propose a novel network with future context encoding for the joint prediction of these two tasks. Our approach ranks 1st on two large-scale motion forecasting benchmarks: Argoverse 1 and Argoverse 2.
<div id='section'>Paperid: <span id='pid'>1238, <a href='https://arxiv.org/pdf/2406.02767.pdf' target='_blank'>https://arxiv.org/pdf/2406.02767.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kathrin Donandt, Dirk SÃ¶ffker
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.02767">Spatial and social situation-aware transformer-based trajectory prediction of autonomous systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous transportation systems such as road vehicles or vessels require the consideration of the static and dynamic environment to dislocate without collision. Anticipating the behavior of an agent in a given situation is required to adequately react to it in time. Developing deep learning-based models has become the dominant approach to motion prediction recently. The social environment is often considered through a CNN-LSTM-based sub-module processing a $\textit{social tensor}$ that includes information of the past trajectory of surrounding agents. For the proposed transformer-based trajectory prediction model, an alternative, computationally more efficient social tensor definition and processing is suggested. It considers the interdependencies between target and surrounding agents at each time step directly instead of relying on information of last hidden LSTM states of individually processed agents. A transformer-based sub-module, the Social Tensor Transformer, is integrated into the overall prediction model. It is responsible for enriching the target agent's dislocation features with social interaction information obtained from the social tensor. For the awareness of spatial limitations, dislocation features are defined in relation to the navigable area. This replaces additional, computationally expensive map processing sub-modules. An ablation study shows, that for longer prediction horizons, the deviation of the predicted trajectory from the ground truth is lower compared to a spatially and socially agnostic model. Even if the performance gain from a spatial-only to a spatial and social context-sensitive model is small in terms of common error measures, by visualizing the results it can be shown that the proposed model in fact is able to predict reactions to surrounding agents and explicitely allows an interpretable behavior.
<div id='section'>Paperid: <span id='pid'>1239, <a href='https://arxiv.org/pdf/2406.00636.pdf' target='_blank'>https://arxiv.org/pdf/2406.00636.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Taeryung Lee, Fabien Baradel, Thomas Lucas, Kyoung Mu Lee, Gregory Rogez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.00636">T2LM: Long-Term 3D Human Motion Generation from Multiple Sentences</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we address the challenging problem of long-term 3D human motion generation. Specifically, we aim to generate a long sequence of smoothly connected actions from a stream of multiple sentences (i.e., paragraph). Previous long-term motion generating approaches were mostly based on recurrent methods, using previously generated motion chunks as input for the next step. However, this approach has two drawbacks: 1) it relies on sequential datasets, which are expensive; 2) these methods yield unrealistic gaps between motions generated at each step. To address these issues, we introduce simple yet effective T2LM, a continuous long-term generation framework that can be trained without sequential data. T2LM comprises two components: a 1D-convolutional VQVAE, trained to compress motion to sequences of latent vectors, and a Transformer-based Text Encoder that predicts a latent sequence given an input text. At inference, a sequence of sentences is translated into a continuous stream of latent vectors. This is then decoded into a motion by the VQVAE decoder; the use of 1D convolutions with a local temporal receptive field avoids temporal inconsistencies between training and generated sequences. This simple constraint on the VQ-VAE allows it to be trained with short sequences only and produces smoother transitions. T2LM outperforms prior long-term generation models while overcoming the constraint of requiring sequential data; it is also competitive with SOTA single-action generation models.
<div id='section'>Paperid: <span id='pid'>1240, <a href='https://arxiv.org/pdf/2405.19609.pdf' target='_blank'>https://arxiv.org/pdf/2405.19609.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yujiao Jiang, Qingmin Liao, Zhaolong Wang, Xiangru Lin, Zongqing Lu, Yuxi Zhao, Hanqing Wei, Jingrui Ye, Yu Zhang, Zhijing Shao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.19609">SMPLX-Lite: A Realistic and Drivable Avatar Benchmark with Rich Geometry and Texture Annotations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recovering photorealistic and drivable full-body avatars is crucial for numerous applications, including virtual reality, 3D games, and tele-presence. Most methods, whether reconstruction or generation, require large numbers of human motion sequences and corresponding textured meshes. To easily learn a drivable avatar, a reasonable parametric body model with unified topology is paramount. However, existing human body datasets either have images or textured models and lack parametric models which fit clothes well. We propose a new parametric model SMPLX-Lite-D, which can fit detailed geometry of the scanned mesh while maintaining stable geometry in the face, hand and foot regions. We present SMPLX-Lite dataset, the most comprehensive clothing avatar dataset with multi-view RGB sequences, keypoints annotations, textured scanned meshes, and textured SMPLX-Lite-D models. With the SMPLX-Lite dataset, we train a conditional variational autoencoder model that takes human pose and facial keypoints as input, and generates a photorealistic drivable human avatar.
<div id='section'>Paperid: <span id='pid'>1241, <a href='https://arxiv.org/pdf/2405.18700.pdf' target='_blank'>https://arxiv.org/pdf/2405.18700.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuehao Gao, Yang Yang, Yang Wu, Shaoyi Du, Guo-Jun Qi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.18700">Multi-Condition Latent Diffusion Network for Scene-Aware Neural Human Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Inferring 3D human motion is fundamental in many applications, including understanding human activity and analyzing one's intention. While many fruitful efforts have been made to human motion prediction, most approaches focus on pose-driven prediction and inferring human motion in isolation from the contextual environment, thus leaving the body location movement in the scene behind. However, real-world human movements are goal-directed and highly influenced by the spatial layout of their surrounding scenes. In this paper, instead of planning future human motion in a 'dark' room, we propose a Multi-Condition Latent Diffusion network (MCLD) that reformulates the human motion prediction task as a multi-condition joint inference problem based on the given historical 3D body motion and the current 3D scene contexts. Specifically, instead of directly modeling joint distribution over the raw motion sequences, MCLD performs a conditional diffusion process within the latent embedding space, characterizing the cross-modal mapping from the past body movement and current scene context condition embeddings to the future human motion embedding. Extensive experiments on large-scale human motion prediction datasets demonstrate that our MCLD achieves significant improvements over the state-of-the-art methods on both realistic and diverse predictions.
<div id='section'>Paperid: <span id='pid'>1242, <a href='https://arxiv.org/pdf/2405.18483.pdf' target='_blank'>https://arxiv.org/pdf/2405.18483.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mengyi Shan, Lu Dong, Yutao Han, Yuan Yao, Tao Liu, Ifeoma Nwogu, Guo-Jun Qi, Mitch Hill
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.18483">Towards Open Domain Text-Driven Synthesis of Multi-Person Motions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work aims to generate natural and diverse group motions of multiple humans from textual descriptions. While single-person text-to-motion generation is extensively studied, it remains challenging to synthesize motions for more than one or two subjects from in-the-wild prompts, mainly due to the lack of available datasets. In this work, we curate human pose and motion datasets by estimating pose information from large-scale image and video datasets. Our models use a transformer-based diffusion framework that accommodates multiple datasets with any number of subjects or frames. Experiments explore both generation of multi-person static poses and generation of multi-person motion sequences. To our knowledge, our method is the first to generate multi-subject motion sequences with high diversity and fidelity from a large variety of textual prompts.
<div id='section'>Paperid: <span id='pid'>1243, <a href='https://arxiv.org/pdf/2405.16909.pdf' target='_blank'>https://arxiv.org/pdf/2405.16909.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>LÃ©ore Bensabath, Mathis Petrovich, GÃ¼l Varol
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.16909">A Cross-Dataset Study for Text-based 3D Human Motion Retrieval</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We provide results of our study on text-based 3D human motion retrieval and particularly focus on cross-dataset generalization. Due to practical reasons such as dataset-specific human body representations, existing works typically benchmarkby training and testing on partitions from the same dataset. Here, we employ a unified SMPL body format for all datasets, which allows us to perform training on one dataset, testing on the other, as well as training on a combination of datasets. Our results suggest that there exist dataset biases in standard text-motion benchmarks such as HumanML3D, KIT Motion-Language, and BABEL. We show that text augmentations help close the domain gap to some extent, but the gap remains. We further provide the first zero-shot action recognition results on BABEL, without using categorical action labels during training, opening up a new avenue for future research.
<div id='section'>Paperid: <span id='pid'>1244, <a href='https://arxiv.org/pdf/2405.16152.pdf' target='_blank'>https://arxiv.org/pdf/2405.16152.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiawei Fang, Haishan Song, Chengxu Zuo, Xiaoxia Gao, Xiaowei Chen, Shihui Guo, Yipeng Qin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.16152">SuDA: Support-based Domain Adaptation for Sim2Real Motion Capture with Flexible Sensors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Flexible sensors hold promise for human motion capture (MoCap), offering advantages such as wearability, privacy preservation, and minimal constraints on natural movement. However, existing flexible sensor-based MoCap methods rely on deep learning and necessitate large and diverse labeled datasets for training. These data typically need to be collected in MoCap studios with specialized equipment and substantial manual labor, making them difficult and expensive to obtain at scale. Thanks to the high-linearity of flexible sensors, we address this challenge by proposing a novel Sim2Real Mocap solution based on domain adaptation, eliminating the need for labeled data yet achieving comparable accuracy to supervised learning. Our solution relies on a novel Support-based Domain Adaptation method, namely SuDA, which aligns the supports of the predictive functions rather than the instance-dependent distributions between the source and target domains. Extensive experimental results demonstrate the effectiveness of our method andits superiority over state-of-the-art distribution-based domain adaptation methods in our task.
<div id='section'>Paperid: <span id='pid'>1245, <a href='https://arxiv.org/pdf/2405.06646.pdf' target='_blank'>https://arxiv.org/pdf/2405.06646.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lei Hu, Zihao Zhang, Yongjing Ye, Yiwen Xu, Shihong Xia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.06646">Diffusion-based Human Motion Style Transfer with Semantic Guidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D Human motion style transfer is a fundamental problem in computer graphic and animation processing. Existing AdaIN- based methods necessitate datasets with balanced style distribution and content/style labels to train the clustered latent space. However, we may encounter a single unseen style example in practical scenarios, but not in sufficient quantity to constitute a style cluster for AdaIN-based methods. Therefore, in this paper, we propose a novel two-stage framework for few-shot style transfer learning based on the diffusion model. Specifically, in the first stage, we pre-train a diffusion-based text-to-motion model as a generative prior so that it can cope with various content motion inputs. In the second stage, based on the single style example, we fine-tune the pre-trained diffusion model in a few-shot manner to make it capable of style transfer. The key idea is regarding the reverse process of diffusion as a motion-style translation process since the motion styles can be viewed as special motion variations. During the fine-tuning for style transfer, a simple yet effective semantic-guided style transfer loss coordinated with style example reconstruction loss is introduced to supervise the style transfer in CLIP semantic space. The qualitative and quantitative evaluations demonstrate that our method can achieve state-of-the-art performance and has practical applications.
<div id='section'>Paperid: <span id='pid'>1246, <a href='https://arxiv.org/pdf/2405.06290.pdf' target='_blank'>https://arxiv.org/pdf/2405.06290.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jin Dai, Zejiang Wang, Yebin Wang, Rien Quirynen, Stefano Di Cairano
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.06290">Path Planning and Motion Control for Accurate Positioning of Car-like Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper investigates the planning and control for accurate positioning of car-like robots. We propose a solution that integrates two modules: a motion planner, facilitated by the rapidly-exploring random tree algorithm and continuous-curvature (CC) steering technique, generates a CC trajectory as a reference; and a nonlinear model predictive controller (NMPC) regulates the robot to accurately track the reference trajectory. Based on the $Î¼$-tangency conditions in prior art, we derive explicit existence conditions and develop associated computation methods for a special class of CC paths which not only admit the same driving patterns as Reeds-Shepp paths but also consist of cusp-free clothoid turns. Afterwards, we create an autonomous vehicle parking scenario where the NMPC endeavors to follow the reference trajectory. Feasibility and computational efficiency of the CC steering are validated by numerical simulation. CarSim-Simulink joint simulations statistically verify that with exactly same NMPC, the closed-loop system with CC trajectories as references substantially outperforms the case where Reeds-Shepp trajectories are used as references.
<div id='section'>Paperid: <span id='pid'>1247, <a href='https://arxiv.org/pdf/2405.02911.pdf' target='_blank'>https://arxiv.org/pdf/2405.02911.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenyu Lou, Qiongjie Cui, Haofan Wang, Xu Tang, Hong Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.02911">Multimodal Sense-Informed Prediction of 3D Human Motions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Predicting future human pose is a fundamental application for machine intelligence, which drives robots to plan their behavior and paths ahead of time to seamlessly accomplish human-robot collaboration in real-world 3D scenarios. Despite encouraging results, existing approaches rarely consider the effects of the external scene on the motion sequence, leading to pronounced artifacts and physical implausibilities in the predictions. To address this limitation, this work introduces a novel multi-modal sense-informed motion prediction approach, which conditions high-fidelity generation on two modal information: external 3D scene, and internal human gaze, and is able to recognize their salience for future human activity. Furthermore, the gaze information is regarded as the human intention, and combined with both motion and scene features, we construct a ternary intention-aware attention to supervise the generation to match where the human wants to reach. Meanwhile, we introduce semantic coherence-aware attention to explicitly distinguish the salient point clouds and the underlying ones, to ensure a reasonable interaction of the generated sequence with the 3D scene. On two real-world benchmarks, the proposed method achieves state-of-the-art performance both in 3D human pose and trajectory prediction.
<div id='section'>Paperid: <span id='pid'>1248, <a href='https://arxiv.org/pdf/2404.16500.pdf' target='_blank'>https://arxiv.org/pdf/2404.16500.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Richard Schubert, Marvin Loba, Jasper SÃ¼nnemann, Torben Stolte, Markus Maurer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.16500">Conformal Prediction of Motion Control Performance for an Automated Vehicle in Presence of Actuator Degradations and Failures</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automated driving systems require monitoring mechanisms to ensure safe operation, especially if system components degrade or fail. Their runtime self-representation plays a key role as it provides a-priori knowledge about the system's capabilities and limitations. In this paper, we propose a data-driven approach for deriving such a self-representation model for the motion controller of an automated vehicle. A conformalized prediction model is learned and allows estimating how operational conditions as well as potential degradations and failures of the vehicle's actuators impact motion control performance. During runtime behavior generation, our predictor can provide a heuristic for determining the admissible action space.
<div id='section'>Paperid: <span id='pid'>1249, <a href='https://arxiv.org/pdf/2404.14745.pdf' target='_blank'>https://arxiv.org/pdf/2404.14745.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Runqi Wang, Caoyuan Ma, Guopeng Li, Hanrui Xu, Yuke Li, Zheng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.14745">You Think, You ACT: The New Task of Arbitrary Text to Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text to Motion aims to generate human motions from texts. Existing settings rely on limited Action Texts that include action labels, which limits flexibility and practicability in scenarios difficult to describe directly. This paper extends limited Action Texts to arbitrary ones. Scene texts without explicit action labels can enhance the practicality of models in complex and diverse industries such as virtual human interaction, robot behavior generation, and film production, while also supporting the exploration of potential implicit behavior patterns. However, newly introduced Scene Texts may yield multiple reasonable output results, causing significant challenges in existing data, framework, and evaluation. To address this practical issue, we first create a new dataset HUMANML3D++ by extending texts of the largest existing dataset HUMANML3D. Secondly, we propose a simple yet effective framework that extracts action instructions from arbitrary texts and subsequently generates motions. Furthermore, we also benchmark this new setting with multi-solution metrics to address the inadequacies of existing single-solution metrics. Extensive experiments indicate that Text to Motion in this realistic setting is challenging, fostering new research in this practical direction.
<div id='section'>Paperid: <span id='pid'>1250, <a href='https://arxiv.org/pdf/2404.14713.pdf' target='_blank'>https://arxiv.org/pdf/2404.14713.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinhao Liang, Kaidi Yang, Chaopeng Tan, Jinxiang Wang, Guodong Yin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.14713">Enhancing High-Speed Cruising Performance of Autonomous Vehicles through Integrated Deep Reinforcement Learning Framework</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>High-speed cruising scenarios with mixed traffic greatly challenge the road safety of autonomous vehicles (AVs). Unlike existing works that only look at fundamental modules in isolation, this work enhances AV safety in mixed-traffic high-speed cruising scenarios by proposing an integrated framework that synthesizes three fundamental modules, i.e., behavioral decision-making, path-planning, and motion-control modules. Considering that the integrated framework would increase the system complexity, a bootstrapped deep Q-Network (DQN) is employed to enhance the deep exploration of the reinforcement learning method and achieve adaptive decision making of AVs. Moreover, to make AV behavior understandable by surrounding HDVs to prevent unexpected operations caused by misinterpretations, we derive an inverse reinforcement learning (IRL) approach to learn the reward function of skilled drivers for the path planning of lane-changing maneuvers. Such a design enables AVs to achieve a human-like tradeoff between multi-performance requirements. Simulations demonstrate that the proposed integrated framework can guide AVs to take safe actions while guaranteeing high-speed cruising performance.
<div id='section'>Paperid: <span id='pid'>1251, <a href='https://arxiv.org/pdf/2403.15959.pdf' target='_blank'>https://arxiv.org/pdf/2403.15959.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Justin Lidard, Hang Pham, Ariel Bachman, Bryan Boateng, Anirudha Majumdar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.15959">Risk-Calibrated Human-Robot Interaction via Set-Valued Intent Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tasks where robots must anticipate human intent, such as navigating around a cluttered home or sorting everyday items, are challenging because they exhibit a wide range of valid actions that lead to similar outcomes. Moreover, zero-shot cooperation between human-robot partners is an especially challenging problem because it requires the robot to infer and adapt on the fly to a latent human intent, which could vary significantly from human to human. Recently, deep learned motion prediction models have shown promising results in predicting human intent but are prone to being confidently incorrect. In this work, we present Risk-Calibrated Interactive Planning (RCIP), which is a framework for measuring and calibrating risk associated with uncertain action selection in human-robot cooperation, with the fundamental idea that the robot should ask for human clarification when the risk associated with the uncertainty in the human's intent cannot be controlled. RCIP builds on the theory of set-valued risk calibration to provide a finite-sample statistical guarantee on the cumulative loss incurred by the robot while minimizing the cost of human clarification in complex multi-step settings. Our main insight is to frame the risk control problem as a sequence-level multi-hypothesis testing problem, allowing efficient calibration using a low-dimensional parameter that controls a pre-trained risk-aware policy. Experiments across a variety of simulated and real-world environments demonstrate RCIP's ability to predict and adapt to a diverse set of dynamic human intents.
<div id='section'>Paperid: <span id='pid'>1252, <a href='https://arxiv.org/pdf/2403.14536.pdf' target='_blank'>https://arxiv.org/pdf/2403.14536.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Charlott Vallon, Mark Pustilnik, Alessandro Pinto, Francesco Borrelli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.14536">Learning Hierarchical Control Systems for Autonomous Systems with Energy Constraints</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper focuses on the design of hierarchical control architectures for autonomous systems with energy constraints. We focus on systems where energy storage limitations and slow recharge rates drastically affect the way the autonomous systems are operated. Using examples from space robotics and public transportation, we motivate the need for formally designed learning hierarchical control systems. We propose a learning control architecture which incorporates learning mechanisms at various levels of the control hierarchy to improve performance and resource utilization. The proposed hierarchical control scheme relies on high-level energy-aware task planning and assignment, complemented by a low-level predictive control mechanism responsible for the autonomous execution of tasks, including motion control and energy management. Simulation examples show the benefits and the limitations of the proposed architecture when learning is used to obtain a more energy-efficient task allocation.
<div id='section'>Paperid: <span id='pid'>1253, <a href='https://arxiv.org/pdf/2403.13570.pdf' target='_blank'>https://arxiv.org/pdf/2403.13570.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Deng, Duomin Wang, Baoyuan Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.13570">Portrait4D-v2: Pseudo Multi-View Data Creates Better 4D Head Synthesizer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose a novel learning approach for feed-forward one-shot 4D head avatar synthesis. Different from existing methods that often learn from reconstructing monocular videos guided by 3DMM, we employ pseudo multi-view videos to learn a 4D head synthesizer in a data-driven manner, avoiding reliance on inaccurate 3DMM reconstruction that could be detrimental to the synthesis performance. The key idea is to first learn a 3D head synthesizer using synthetic multi-view images to convert monocular real videos into multi-view ones, and then utilize the pseudo multi-view videos to learn a 4D head synthesizer via cross-view self-reenactment. By leveraging a simple vision transformer backbone with motion-aware cross-attentions, our method exhibits superior performance compared to previous methods in terms of reconstruction fidelity, geometry consistency, and motion control accuracy. We hope our method offers novel insights into integrating 3D priors with 2D supervisions for improved 4D head avatar creation.
<div id='section'>Paperid: <span id='pid'>1254, <a href='https://arxiv.org/pdf/2403.13294.pdf' target='_blank'>https://arxiv.org/pdf/2403.13294.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qingyuan Jiang, Burak Susam, Jun-Jee Chao, Volkan Isler
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.13294">Map-Aware Human Pose Prediction for Robot Follow-Ahead</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the robot follow-ahead task, a mobile robot is tasked to maintain its relative position in front of a moving human actor while keeping the actor in sight. To accomplish this task, it is important that the robot understand the full 3D pose of the human (since the head orientation can be different than the torso) and predict future human poses so as to plan accordingly. This prediction task is especially tricky in a complex environment with junctions and multiple corridors. In this work, we address the problem of forecasting the full 3D trajectory of a human in such environments. Our main insight is to show that one can first predict the 2D trajectory and then estimate the full 3D trajectory by conditioning the estimator on the predicted 2D trajectory. With this approach, we achieve results comparable or better than the state-of-the-art methods three times faster. As part of our contribution, we present a new dataset where, in contrast to existing datasets, the human motion is in a much larger area than a single room. We also present a complete robot system that integrates our human pose forecasting network on the mobile robot to enable real-time robot follow-ahead and present results from real-world experiments in multiple buildings on campus. Our project page, including supplementary material and videos, can be found at: https://qingyuan-jiang.github.io/iros2024_poseForecasting/
<div id='section'>Paperid: <span id='pid'>1255, <a href='https://arxiv.org/pdf/2403.05878.pdf' target='_blank'>https://arxiv.org/pdf/2403.05878.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yorick Broens, Hans Butler, Roland TÃ³th
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.05878">Frequency Domain Auto-tuning of Structured LPV Controllers for High-Precision Motion Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion systems are a vital part of many industrial processes. However, meeting the increasingly stringent demands of these systems, especially concerning precision and throughput, requires novel control design methods that can go beyond the capabilities of traditional solutions. Traditional control methods often struggle with the complexity and position-dependent effects inherent in modern motion systems, leading to compromises in performance and a laborious task of controller design. This paper addresses these challenges by introducing a novel structured feedback control auto-tuning approach for multiple-input multiple-output (MIMO) motion systems. By leveraging frequency response function (FRF) estimates and the linear-parameter-varying (LPV) control framework, the proposed approach automates the controller design, while providing local stability and performance guarantees. Key innovations include norm-based magnitude optimization of the sensitivity functions, an automated stability check through a novel extended factorized Nyquist criterion, a modular structured MIMO LPV controller parameterization, and a controller discretization approach which preserves the continuous-time (CT) controller parameterization. The proposed approach is validated through experiments using a state-of-the-art moving-magnet planar actuator prototype.
<div id='section'>Paperid: <span id='pid'>1256, <a href='https://arxiv.org/pdf/2403.03561.pdf' target='_blank'>https://arxiv.org/pdf/2403.03561.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Peng Dai, Yang Zhang, Tao Liu, Zhen Fan, Tianyuan Du, Zhuo Su, Xiaozheng Zheng, Zeming Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.03561">HMD-Poser: On-Device Real-time Human Motion Tracking from Scalable Sparse Observations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>It is especially challenging to achieve real-time human motion tracking on a standalone VR Head-Mounted Display (HMD) such as Meta Quest and PICO. In this paper, we propose HMD-Poser, the first unified approach to recover full-body motions using scalable sparse observations from HMD and body-worn IMUs. In particular, it can support a variety of input scenarios, such as HMD, HMD+2IMUs, HMD+3IMUs, etc. The scalability of inputs may accommodate users' choices for both high tracking accuracy and easy-to-wear. A lightweight temporal-spatial feature learning network is proposed in HMD-Poser to guarantee that the model runs in real-time on HMDs. Furthermore, HMD-Poser presents online body shape estimation to improve the position accuracy of body joints. Extensive experimental results on the challenging AMASS dataset show that HMD-Poser achieves new state-of-the-art results in both accuracy and real-time performance. We also build a new free-dancing motion dataset to evaluate HMD-Poser's on-device performance and investigate the performance gap between synthetic data and real-captured sensor data. Finally, we demonstrate our HMD-Poser with a real-time Avatar-driving application on a commercial HMD. Our code and free-dancing motion dataset are available https://pico-ai-team.github.io/hmd-poser
<div id='section'>Paperid: <span id='pid'>1257, <a href='https://arxiv.org/pdf/2402.17339.pdf' target='_blank'>https://arxiv.org/pdf/2402.17339.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Xiang, Haoteng Yin, He Wang, Xiaogang Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.17339">SocialCVAE: Predicting Pedestrian Trajectory via Interaction Conditioned Latents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pedestrian trajectory prediction is the key technology in many applications for providing insights into human behavior and anticipating human future motions. Most existing empirical models are explicitly formulated by observed human behaviors using explicable mathematical terms with a deterministic nature, while recent work has focused on developing hybrid models combined with learning-based techniques for powerful expressiveness while maintaining explainability. However, the deterministic nature of the learned steering behaviors from the empirical models limits the models' practical performance. To address this issue, this work proposes the social conditional variational autoencoder (SocialCVAE) for predicting pedestrian trajectories, which employs a CVAE to explore behavioral uncertainty in human motion decisions. SocialCVAE learns socially reasonable motion randomness by utilizing a socially explainable interaction energy map as the CVAE's condition, which illustrates the future occupancy of each pedestrian's local neighborhood area. The energy map is generated using an energy-based interaction model, which anticipates the energy cost (i.e., repulsion intensity) of pedestrians' interactions with neighbors. Experimental results on two public benchmarks including 25 scenes demonstrate that SocialCVAE significantly improves prediction accuracy compared with the state-of-the-art methods, with up to 16.85% improvement in Average Displacement Error (ADE) and 69.18% improvement in Final Displacement Error (FDE).
<div id='section'>Paperid: <span id='pid'>1258, <a href='https://arxiv.org/pdf/2402.09459.pdf' target='_blank'>https://arxiv.org/pdf/2402.09459.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Javier GonzÃ¡lez-Alonso, David Oviedo-Pastor, HÃ©ctor J. Aguado, Francisco J. DÃ­az-Pernas, David GonzÃ¡lez-Ortega, Mario MartÃ­nez-Zarzuela
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.09459">Custom IMU-Based Wearable System for Robust 2.4 GHz Wireless Human Body Parts Orientation Tracking and 3D Movement Visualization on an Avatar</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent studies confirm the applicability of Inertial Measurement Unit (IMU)-based systems for human motion analysis. Notwithstanding, high-end IMU-based commercial solutions are yet too expensive and complex to democratize their use among a wide range of potential users. Less featured entry-level commercial solutions are being introduced in the market, trying to fill this gap, but still present some limitations that need to be overcome. At the same time, there is a growing number of scientific papers using not commercial, but custom do-it-yourself IMU-based systems in medical and sports applications. Even though these solutions can help to popularize the use of this technology, they have more limited features and the description on how to design and build them from scratch is yet too scarce in the literature. The aim of this work is two-fold: (1) Proving the feasibility of building an affordable custom solution aimed at simultaneous multiple body parts orientation tracking; while providing a detailed bottom-up description of the required hardware, tools, and mathematical operations to estimate and represent 3D movement in real-time. (2) Showing how the introduction of a custom 2.4 GHz communication protocol including a channel hopping strategy can address some of the current communication limitations of entry-level commercial solutions. The proposed system can be used for wireless real-time human body parts orientation tracking with up to 10 custom sensors, at least at 50 Hz. In addition, it provides a more reliable motion data acquisition in Bluetooth and Wi-Fi crowded environments, where the use of entry-level commercial solutions might be unfeasible. This system can be used as a groundwork for developing affordable human motion analysis solutions that do not require an accurate kinematic analysis.
<div id='section'>Paperid: <span id='pid'>1259, <a href='https://arxiv.org/pdf/2402.09442.pdf' target='_blank'>https://arxiv.org/pdf/2402.09442.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weixiang Wan, Wenjian Sun, Qiang Zeng, Linying Pan, Jingyu Xu, Bo Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.09442">Progress in artificial intelligence applications based on the combination of self-driven sensors and deep learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the era of Internet of Things, how to develop a smart sensor system with sustainable power supply, easy deployment and flexible use has become a difficult problem to be solved. The traditional power supply has problems such as frequent replacement or charging when in use, which limits the development of wearable devices. The contact-to-separate friction nanogenerator (TENG) was prepared by using polychotomy thy lene (PTFE) and aluminum (AI) foils. Human motion energy was collected by human body arrangement, and human motion posture was monitored according to the changes of output electrical signals. In 2012, Academician Wang Zhong lin and his team invented the triboelectric nanogenerator (TENG), which uses Maxwell displacement current as a driving force to directly convert mechanical stimuli into electrical signals, so it can be used as a self-driven sensor. Teng-based sensors have the advantages of simple structure and high instantaneous power density, which provides an important means for building intelligent sensor systems. At the same time, machine learning, as a technology with low cost, short development cycle, strong data processing ability and prediction ability, has a significant effect on the processing of a large number of electrical signals generated by TENG, and the combination with TENG sensors will promote the rapid development of intelligent sensor networks in the future. Therefore, this paper is based on the intelligent sound monitoring and recognition system of TENG, which has good sound recognition capability, and aims to evaluate the feasibility of the sound perception module architecture in ubiquitous sensor networks.
<div id='section'>Paperid: <span id='pid'>1260, <a href='https://arxiv.org/pdf/2402.04356.pdf' target='_blank'>https://arxiv.org/pdf/2402.04356.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Canyu Zhang, Youbao Tang, Ning Zhang, Ruei-Sung Lin, Mei Han, Jing Xiao, Song Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.04356">Bidirectional Autoregressive Diffusion Model for Dance Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dance serves as a powerful medium for expressing human emotions, but the lifelike generation of dance is still a considerable challenge. Recently, diffusion models have showcased remarkable generative abilities across various domains. They hold promise for human motion generation due to their adaptable many-to-many nature. Nonetheless, current diffusion-based motion generation models often create entire motion sequences directly and unidirectionally, lacking focus on the motion with local and bidirectional enhancement. When choreographing high-quality dance movements, people need to take into account not only the musical context but also the nearby music-aligned dance motions. To authentically capture human behavior, we propose a Bidirectional Autoregressive Diffusion Model (BADM) for music-to-dance generation, where a bidirectional encoder is built to enforce that the generated dance is harmonious in both the forward and backward directions. To make the generated dance motion smoother, a local information decoder is built for local motion enhancement. The proposed framework is able to generate new motions based on the input conditions and nearby motions, which foresees individual motion slices iteratively and consolidates all predictions. To further refine the synchronicity between the generated dance and the beat, the beat information is incorporated as an input to generate better music-aligned dance movements. Experimental results demonstrate that the proposed model achieves state-of-the-art performance compared to existing unidirectional approaches on the prominent benchmark for music-to-dance generation.
<div id='section'>Paperid: <span id='pid'>1261, <a href='https://arxiv.org/pdf/2402.02904.pdf' target='_blank'>https://arxiv.org/pdf/2402.02904.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Yu, Zebin Huang, Qingbo Liu, Ignacio Carlucho, Mustafa Suphi Erden
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.02904">Replication of Impedance Identification Experiments on a Reinforcement-Learning-Controlled Digital Twin of Human Elbows</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study presents a pioneering effort to replicate human neuromechanical experiments within a virtual environment utilising a digital human model. By employing MyoSuite, a state-of-the-art human motion simulation platform enhanced by Reinforcement Learning (RL), multiple types of impedance identification experiments of human elbow were replicated on a musculoskeletal model. We compared the elbow movement controlled by an RL agent with the motion of an actual human elbow in terms of the impedance identified in torque-perturbation experiments. The findings reveal that the RL agent exhibits higher elbow impedance to stabilise the target elbow motion under perturbation than a human does, likely due to its shorter reaction time and superior sensory capabilities. This study serves as a preliminary exploration into the potential of virtual environment simulations for neuromechanical research, offering an initial yet promising alternative to conventional experimental approaches. An RL-controlled digital twin with complete musculoskeletal models of the human body is expected to be useful in designing experiments and validating rehabilitation theory before experiments on real human subjects.
<div id='section'>Paperid: <span id='pid'>1262, <a href='https://arxiv.org/pdf/2402.01049.pdf' target='_blank'>https://arxiv.org/pdf/2402.01049.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zikang Leng, Amitrajit Bhattacharjee, Hrudhai Rajasekhar, Lizhe Zhang, Elizabeth Bruda, Hyeokhyen Kwon, Thomas PlÃ¶tz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.01049">IMUGPT 2.0: Language-Based Cross Modality Transfer for Sensor-Based Human Activity Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>One of the primary challenges in the field of human activity recognition (HAR) is the lack of large labeled datasets. This hinders the development of robust and generalizable models. Recently, cross modality transfer approaches have been explored that can alleviate the problem of data scarcity. These approaches convert existing datasets from a source modality, such as video, to a target modality (IMU). With the emergence of generative AI models such as large language models (LLMs) and text-driven motion synthesis models, language has become a promising source data modality as well as shown in proof of concepts such as IMUGPT. In this work, we conduct a large-scale evaluation of language-based cross modality transfer to determine their effectiveness for HAR. Based on this study, we introduce two new extensions for IMUGPT that enhance its use for practical HAR application scenarios: a motion filter capable of filtering out irrelevant motion sequences to ensure the relevance of the generated virtual IMU data, and a set of metrics that measure the diversity of the generated data facilitating the determination of when to stop generating virtual IMU data for both effective and efficient processing. We demonstrate that our diversity metrics can reduce the effort needed for the generation of virtual IMU data by at least 50%, which open up IMUGPT for practical use cases beyond a mere proof of concept.
<div id='section'>Paperid: <span id='pid'>1263, <a href='https://arxiv.org/pdf/2402.00663.pdf' target='_blank'>https://arxiv.org/pdf/2402.00663.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Raul Fernandez-Fernandez, Bartek Åukawski, Juan G. Victores, Claudio Pacchierotti
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.00663">Transferring human emotions to robot motions using Neural Policy Style Transfer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Neural Style Transfer (NST) was originally proposed to use feature extraction capabilities of Neural Networks as a way to perform Style Transfer with images. Pre-trained image classification architectures were selected for feature extraction, leading to new images showing the same content as the original but with a different style. In robotics, Style Transfer can be employed to transfer human motion styles to robot motions. The challenge lies in the lack of pre-trained classification architectures for robot motions that could be used for feature extraction. Neural Policy Style Transfer TD3 (NPST3) is proposed for the transfer of human motion styles to robot motions. This framework allows the same robot motion to be executed in different human-centered motion styles, such as in an angry, happy, calm, or sad fashion. The Twin Delayed Deep Deterministic Policy Gradient (TD3) network is introduced for the generation of control policies. An autoencoder network is in charge of feature extraction for the Style Transfer step. The Style Transfer step can be performed both offline and online: offline for the autonomous executions of human-style robot motions, and online for adapting at runtime the style of e.g., a teleoperated robot. The framework is tested using two different robotic platforms: a robotic manipulator designed for telemanipulation tasks, and a humanoid robot designed for social interaction. The proposed approach was evaluated for both platforms, performing a total of 147 questionnaires asking human subjects to recognize the human motion style transferred to the robot motion for a predefined set of actions.
<div id='section'>Paperid: <span id='pid'>1264, <a href='https://arxiv.org/pdf/2401.08559.pdf' target='_blank'>https://arxiv.org/pdf/2401.08559.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mathis Petrovich, Or Litany, Umar Iqbal, Michael J. Black, GÃ¼l Varol, Xue Bin Peng, Davis Rempe
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.08559">Multi-Track Timeline Control for Text-Driven 3D Human Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in generative modeling have led to promising progress on synthesizing 3D human motion from text, with methods that can generate character animations from short prompts and specified durations. However, using a single text prompt as input lacks the fine-grained control needed by animators, such as composing multiple actions and defining precise durations for parts of the motion. To address this, we introduce the new problem of timeline control for text-driven motion synthesis, which provides an intuitive, yet fine-grained, input interface for users. Instead of a single prompt, users can specify a multi-track timeline of multiple prompts organized in temporal intervals that may overlap. This enables specifying the exact timings of each action and composing multiple actions in sequence or at overlapping intervals. To generate composite animations from a multi-track timeline, we propose a new test-time denoising method. This method can be integrated with any pre-trained motion diffusion model to synthesize realistic motions that accurately reflect the timeline. At every step of denoising, our method processes each timeline interval (text prompt) individually, subsequently aggregating the predictions with consideration for the specific body parts engaged in each action. Experimental comparisons and ablations validate that our method produces realistic motions that respect the semantics and timing of given text prompts. Our code and models are publicly available at https://mathis.petrovich.fr/stmc.
<div id='section'>Paperid: <span id='pid'>1265, <a href='https://arxiv.org/pdf/2312.16737.pdf' target='_blank'>https://arxiv.org/pdf/2312.16737.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Enes Duran, Muhammed Kocabas, Vasileios Choutas, Zicong Fan, Michael J. Black
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.16737">HMP: Hand Motion Priors for Pose and Shape Estimation from Video</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding how humans interact with the world necessitates accurate 3D hand pose estimation, a task complicated by the hand's high degree of articulation, frequent occlusions, self-occlusions, and rapid motions. While most existing methods rely on single-image inputs, videos have useful cues to address aforementioned issues. However, existing video-based 3D hand datasets are insufficient for training feedforward models to generalize to in-the-wild scenarios. On the other hand, we have access to large human motion capture datasets which also include hand motions, e.g. AMASS. Therefore, we develop a generative motion prior specific for hands, trained on the AMASS dataset which features diverse and high-quality hand motions. This motion prior is then employed for video-based 3D hand motion estimation following a latent optimization approach. Our integration of a robust motion prior significantly enhances performance, especially in occluded scenarios. It produces stable, temporally consistent results that surpass conventional single-frame methods. We demonstrate our method's efficacy via qualitative and quantitative evaluations on the HO3D and DexYCB datasets, with special emphasis on an occlusion-focused subset of HO3D. Code is available at https://hmp.is.tue.mpg.de
<div id='section'>Paperid: <span id='pid'>1266, <a href='https://arxiv.org/pdf/2310.17649.pdf' target='_blank'>https://arxiv.org/pdf/2310.17649.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Takuma Yoneda, Tianchong Jiang, Gregory Shakhnarovich, Matthew R. Walter
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.17649">6-DoF Stability Field via Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A core capability for robot manipulation is reasoning over where and how to stably place objects in cluttered environments. Traditionally, robots have relied on object-specific, hand-crafted heuristics in order to perform such reasoning, with limited generalizability beyond a small number of object instances and object interaction patterns. Recent approaches instead learn notions of physical interaction, namely motion prediction, but require supervision in the form of labeled object information or come at the cost of high sample complexity, and do not directly reason over stability or object placement. We present 6-DoFusion, a generative model capable of generating 3D poses of an object that produces a stable configuration of a given scene. Underlying 6-DoFusion is a diffusion model that incrementally refines a randomly initialized SE(3) pose to generate a sample from a learned, context-dependent distribution over stable poses. We evaluate our model on different object placement and stacking tasks, demonstrating its ability to construct stable scenes that involve novel object classes as well as to improve the accuracy of state-of-the-art 3D pose estimation methods.
<div id='section'>Paperid: <span id='pid'>1267, <a href='https://arxiv.org/pdf/2310.12085.pdf' target='_blank'>https://arxiv.org/pdf/2310.12085.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zikang Leng, Hyeokhyen Kwon, Thomas PlÃ¶tz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.12085">On the Benefit of Generative Foundation Models for Human Activity Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In human activity recognition (HAR), the limited availability of annotated data presents a significant challenge. Drawing inspiration from the latest advancements in generative AI, including Large Language Models (LLMs) and motion synthesis models, we believe that generative AI can address this data scarcity by autonomously generating virtual IMU data from text descriptions. Beyond this, we spotlight several promising research pathways that could benefit from generative AI for the community, including the generating benchmark datasets, the development of foundational models specific to HAR, the exploration of hierarchical structures within HAR, breaking down complex activities, and applications in health sensing and activity summarization.
<div id='section'>Paperid: <span id='pid'>1268, <a href='https://arxiv.org/pdf/2310.03314.pdf' target='_blank'>https://arxiv.org/pdf/2310.03314.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aadi Kothari, Tony Tohme, Xiaotong Zhang, Kamal Youcef-Toumi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.03314">Enhanced Human-Robot Collaboration using Constrained Probabilistic Human-Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion prediction is an essential step for efficient and safe human-robot collaboration. Current methods either purely rely on representing the human joints in some form of neural network-based architecture or use regression models offline to fit hyper-parameters in the hope of capturing a model encompassing human motion. While these methods provide good initial results, they are missing out on leveraging well-studied human body kinematic models as well as body and scene constraints which can help boost the efficacy of these prediction frameworks while also explicitly avoiding implausible human joint configurations. We propose a novel human motion prediction framework that incorporates human joint constraints and scene constraints in a Gaussian Process Regression (GPR) model to predict human motion over a set time horizon. This formulation is combined with an online context-aware constraints model to leverage task-dependent motions. It is tested on a human arm kinematic model and implemented on a human-robot collaborative setup with a UR5 robot arm to demonstrate the real-time capability of our approach. Simulations were also performed on datasets like HA4M and ANDY. The simulation and experimental results demonstrate considerable improvements in a Gaussian Process framework when these constraints are explicitly considered.
<div id='section'>Paperid: <span id='pid'>1269, <a href='https://arxiv.org/pdf/2309.13742.pdf' target='_blank'>https://arxiv.org/pdf/2309.13742.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifeng Jiang, Jungdam Won, Yuting Ye, C. Karen Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.13742">DROP: Dynamics Responses from Human Motion Prior and Projective Dynamics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Synthesizing realistic human movements, dynamically responsive to the environment, is a long-standing objective in character animation, with applications in computer vision, sports, and healthcare, for motion prediction and data augmentation. Recent kinematics-based generative motion models offer impressive scalability in modeling extensive motion data, albeit without an interface to reason about and interact with physics. While simulator-in-the-loop learning approaches enable highly physically realistic behaviors, the challenges in training often affect scalability and adoption. We introduce DROP, a novel framework for modeling Dynamics Responses of humans using generative mOtion prior and Projective dynamics. DROP can be viewed as a highly stable, minimalist physics-based human simulator that interfaces with a kinematics-based generative motion prior. Utilizing projective dynamics, DROP allows flexible and simple integration of the learned motion prior as one of the projective energies, seamlessly incorporating control provided by the motion prior with Newtonian dynamics. Serving as a model-agnostic plug-in, DROP enables us to fully leverage recent advances in generative motion models for physics-based motion synthesis. We conduct extensive evaluations of our model across different motion tasks and various physical perturbations, demonstrating the scalability and diversity of responses.
<div id='section'>Paperid: <span id='pid'>1270, <a href='https://arxiv.org/pdf/2309.10226.pdf' target='_blank'>https://arxiv.org/pdf/2309.10226.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kai Wang, Xiaoyu Xu, Yinping Zhen, Da Zhou, Shihui Guo, Yipeng Qin, Xiaohu Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.10226">Computational Design of Wiring Layout on Tight Suits with Minimal Motion Resistance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>An increasing number of electronics are directly embedded on the clothing to monitor human status (e.g., skeletal motion) or provide haptic feedback. A specific challenge to prototype and fabricate such a clothing is to design the wiring layout, while minimizing the intervention to human motion. We address this challenge by formulating the topological optimization problem on the clothing surface as a deformation-weighted Steiner tree problem on a 3D clothing mesh. Our method proposed an energy function for minimizing strain energy in the wiring area under different motions, regularized by its total length. We built the physical prototype to verify the effectiveness of our method and conducted user study with participants of both design experts and smart cloth users. On three types of commercial products of smart clothing, the optimized layout design reduced wire strain energy by an average of 77% among 248 actions compared to baseline design, and 18% over the expert design.
<div id='section'>Paperid: <span id='pid'>1271, <a href='https://arxiv.org/pdf/2307.07754.pdf' target='_blank'>https://arxiv.org/pdf/2307.07754.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wing-Yin Yu, Lai-Man Po, Ray C. C. Cheung, Yuzhi Zhao, Yu Xue, Kun Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.07754">Bidirectionally Deformable Motion Modulation For Video-based Human Pose Transfer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video-based human pose transfer is a video-to-video generation task that animates a plain source human image based on a series of target human poses. Considering the difficulties in transferring highly structural patterns on the garments and discontinuous poses, existing methods often generate unsatisfactory results such as distorted textures and flickering artifacts. To address these issues, we propose a novel Deformable Motion Modulation (DMM) that utilizes geometric kernel offset with adaptive weight modulation to simultaneously perform feature alignment and style transfer. Different from normal style modulation used in style transfer, the proposed modulation mechanism adaptively reconstructs smoothed frames from style codes according to the object shape through an irregular receptive field of view. To enhance the spatio-temporal consistency, we leverage bidirectional propagation to extract the hidden motion information from a warped image sequence generated by noisy poses. The proposed feature propagation significantly enhances the motion prediction ability by forward and backward propagation. Both quantitative and qualitative experimental results demonstrate superiority over the state-of-the-arts in terms of image fidelity and visual continuity. The source code is publicly available at github.com/rocketappslab/bdmm.
<div id='section'>Paperid: <span id='pid'>1272, <a href='https://arxiv.org/pdf/2307.01938.pdf' target='_blank'>https://arxiv.org/pdf/2307.01938.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Daniele Reda, Jungdam Won, Yuting Ye, Michiel van de Panne, Alexander Winkler
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.01938">Physics-based Motion Retargeting from Sparse Inputs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Avatars are important to create interactive and immersive experiences in virtual worlds. One challenge in animating these characters to mimic a user's motion is that commercial AR/VR products consist only of a headset and controllers, providing very limited sensor data of the user's pose. Another challenge is that an avatar might have a different skeleton structure than a human and the mapping between them is unclear. In this work we address both of these challenges. We introduce a method to retarget motions in real-time from sparse human sensor data to characters of various morphologies. Our method uses reinforcement learning to train a policy to control characters in a physics simulator. We only require human motion capture data for training, without relying on artist-generated animations for each avatar. This allows us to use large motion capture datasets to train general policies that can track unseen users from real and sparse data in real-time. We demonstrate the feasibility of our approach on three characters with different skeleton structure: a dinosaur, a mouse-like creature and a human. We show that the avatar poses often match the user surprisingly well, despite having no sensor information of the lower body available. We discuss and ablate the important components in our framework, specifically the kinematic retargeting step, the imitation, contact and action reward as well as our asymmetric actor-critic observations. We further explore the robustness of our method in a variety of settings including unbalancing, dancing and sports motions.
<div id='section'>Paperid: <span id='pid'>1273, <a href='https://arxiv.org/pdf/2306.13566.pdf' target='_blank'>https://arxiv.org/pdf/2306.13566.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaogang Peng, Xiao Zhou, Yikai Luo, Hao Wen, Yu Ding, Zizhao Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.13566">The MI-Motion Dataset and Benchmark for 3D Multi-Person Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D multi-person motion prediction is a challenging task that involves modeling individual behaviors and interactions between people. Despite the emergence of approaches for this task, comparing them is difficult due to the lack of standardized training settings and benchmark datasets. In this paper, we introduce the Multi-Person Interaction Motion (MI-Motion) Dataset, which includes skeleton sequences of multiple individuals collected by motion capture systems and refined and synthesized using a game engine. The dataset contains 167k frames of interacting people's skeleton poses and is categorized into 5 different activity scenes. To facilitate research in multi-person motion prediction, we also provide benchmarks to evaluate the performance of prediction methods in three settings: short-term, long-term, and ultra-long-term prediction. Additionally, we introduce a novel baseline approach that leverages graph and temporal convolutional networks, which has demonstrated competitive results in multi-person motion prediction. We believe that the proposed MI-Motion benchmark dataset and baseline will facilitate future research in this area, ultimately leading to better understanding and modeling of multi-person interactions.
<div id='section'>Paperid: <span id='pid'>1274, <a href='https://arxiv.org/pdf/2306.11638.pdf' target='_blank'>https://arxiv.org/pdf/2306.11638.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hsu-kuang Chiu, Stephen F. Smith
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.11638">Collision Avoidance Detour for Multi-Agent Trajectory Forecasting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present our approach, Collision Avoidance Detour (CAD), which won the 3rd place award in the 2023 Waymo Open Dataset Challenge - Sim Agents, held at the 2023 CVPR Workshop on Autonomous Driving. To satisfy the motion prediction factorization requirement, we partition all the valid objects into three mutually exclusive sets: Autonomous Driving Vehicle (ADV), World-tracks-to-predict, and World-others. We use different motion models to forecast their future trajectories independently. Furthermore, we also apply collision avoidance detour resampling, additive Gaussian noise, and velocity-based heading estimation to improve the realism of our simulation result.
<div id='section'>Paperid: <span id='pid'>1275, <a href='https://arxiv.org/pdf/2306.08006.pdf' target='_blank'>https://arxiv.org/pdf/2306.08006.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lei Hu, Zihao Zhang, Chongyang Zhong, Boyuan Jiang, Shihong Xia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.08006">Pose-aware Attention Network for Flexible Motion Retargeting by Body Part</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion retargeting is a fundamental problem in computer graphics and computer vision. Existing approaches usually have many strict requirements, such as the source-target skeletons needing to have the same number of joints or share the same topology. To tackle this problem, we note that skeletons with different structure may have some common body parts despite the differences in joint numbers. Following this observation, we propose a novel, flexible motion retargeting framework. The key idea of our method is to regard the body part as the basic retargeting unit rather than directly retargeting the whole body motion. To enhance the spatial modeling capability of the motion encoder, we introduce a pose-aware attention network (PAN) in the motion encoding phase. The PAN is pose-aware since it can dynamically predict the joint weights within each body part based on the input pose, and then construct a shared latent space for each body part by feature pooling. Extensive experiments show that our approach can generate better motion retargeting results both qualitatively and quantitatively than state-of-the-art methods. Moreover, we also show that our framework can generate reasonable results even for a more challenging retargeting scenario, like retargeting between bipedal and quadrupedal skeletons because of the body part retargeting strategy and PAN. Our code is publicly available.
<div id='section'>Paperid: <span id='pid'>1276, <a href='https://arxiv.org/pdf/2305.19235.pdf' target='_blank'>https://arxiv.org/pdf/2305.19235.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Antonio Marino, Claudio Pacchierotti, Paolo Robuffo Giordano
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.19235">Input State Stability of Gated Graph Neural Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we aim to find the conditions for input-state stability (ISS) and incremental input-state stability ($Î´$ISS) of Gated Graph Neural Networks (GGNNs). We show that this recurrent version of Graph Neural Networks (GNNs) can be expressed as a dynamical distributed system and, as a consequence, can be analysed using model-based techniques to assess its stability and robustness properties. Then, the stability criteria found can be exploited as constraints during the training process to enforce the internal stability of the neural network. Two distributed control examples, flocking and multi-robot motion control, show that using these conditions increases the performance and robustness of the gated GNNs.
<div id='section'>Paperid: <span id='pid'>1277, <a href='https://arxiv.org/pdf/2305.10989.pdf' target='_blank'>https://arxiv.org/pdf/2305.10989.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>AJ Miller, Shamel Fahmi, Matthew Chignoli, Sangbae Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.10989">Reinforcement Learning for Legged Robots: Motion Imitation from Model-Based Optimal Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose MIMOC: Motion Imitation from Model-Based Optimal Control. MIMOC is a Reinforcement Learning (RL) controller that learns agile locomotion by imitating reference trajectories from model-based optimal control. MIMOC mitigates challenges faced by other motion imitation RL approaches because the references are dynamically consistent, require no motion retargeting, and include torque references. Hence, MIMOC does not require fine-tuning. MIMOC is also less sensitive to modeling and state estimation inaccuracies than model-based controllers. We validate MIMOC on the Mini-Cheetah in outdoor environments over a wide variety of challenging terrain, and on the MIT Humanoid in simulation. We show cases where MIMOC outperforms model-based optimal controllers, and show that imitating torque references improves the policy's performance.
<div id='section'>Paperid: <span id='pid'>1278, <a href='https://arxiv.org/pdf/2305.04443.pdf' target='_blank'>https://arxiv.org/pdf/2305.04443.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiarui Sun, Girish Chowdhary
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.04443">Towards Accurate Human Motion Prediction via Iterative Refinement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion prediction aims to forecast an upcoming pose sequence given a past human motion trajectory. To address the problem, in this work we propose FreqMRN, a human motion prediction framework that takes into account both the kinematic structure of the human body and the temporal smoothness nature of motion. Specifically, FreqMRN first generates a fixed-size motion history summary using a motion attention module, which helps avoid inaccurate motion predictions due to excessively long motion inputs. Then, supervised by the proposed spatial-temporal-aware, velocity-aware and global-smoothness-aware losses, FreqMRN iteratively refines the predicted motion though the proposed motion refinement module, which converts motion representations back and forth between pose space and frequency space. We evaluate FreqMRN on several standard benchmark datasets, including Human3.6M, AMASS and 3DPW. Experimental results demonstrate that FreqMRN outperforms previous methods by large margins for both short-term and long-term predictions, while demonstrating superior robustness.
<div id='section'>Paperid: <span id='pid'>1279, <a href='https://arxiv.org/pdf/2305.03187.pdf' target='_blank'>https://arxiv.org/pdf/2305.03187.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zikang Leng, Hyeokhyen Kwon, Thomas PlÃ¶tz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.03187">Generating Virtual On-body Accelerometer Data from Virtual Textual Descriptions for Human Activity Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The development of robust, generalized models in human activity recognition (HAR) has been hindered by the scarcity of large-scale, labeled data sets. Recent work has shown that virtual IMU data extracted from videos using computer vision techniques can lead to substantial performance improvements when training HAR models combined with small portions of real IMU data. Inspired by recent advances in motion synthesis from textual descriptions and connecting Large Language Models (LLMs) to various AI models, we introduce an automated pipeline that first uses ChatGPT to generate diverse textual descriptions of activities. These textual descriptions are then used to generate 3D human motion sequences via a motion synthesis model, T2M-GPT, and later converted to streams of virtual IMU data. We benchmarked our approach on three HAR datasets (RealWorld, PAMAP2, and USC-HAD) and demonstrate that the use of virtual IMU training data generated using our new approach leads to significantly improved HAR model performance compared to only using real IMU data. Our approach contributes to the growing field of cross-modality transfer methods and illustrate how HAR models can be improved through the generation of virtual training data that do not require any manual effort.
<div id='section'>Paperid: <span id='pid'>1280, <a href='https://arxiv.org/pdf/2305.00976.pdf' target='_blank'>https://arxiv.org/pdf/2305.00976.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mathis Petrovich, Michael J. Black, GÃ¼l Varol
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.00976">TMR: Text-to-Motion Retrieval Using Contrastive 3D Human Motion Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we present TMR, a simple yet effective approach for text to 3D human motion retrieval. While previous work has only treated retrieval as a proxy evaluation metric, we tackle it as a standalone task. Our method extends the state-of-the-art text-to-motion synthesis model TEMOS, and incorporates a contrastive loss to better structure the cross-modal latent space. We show that maintaining the motion generation loss, along with the contrastive training, is crucial to obtain good performance. We introduce a benchmark for evaluation and provide an in-depth analysis by reporting results on several protocols. Our extensive experiments on the KIT-ML and HumanML3D datasets show that TMR outperforms the prior work by a significant margin, for example reducing the median rank from 54 to 19. Finally, we showcase the potential of our approach on moment retrieval. Our code and models are publicly available at https://mathis.petrovich.fr/tmr.
<div id='section'>Paperid: <span id='pid'>1281, <a href='https://arxiv.org/pdf/2304.10417.pdf' target='_blank'>https://arxiv.org/pdf/2304.10417.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nikos Athanasiou, Mathis Petrovich, Michael J. Black, GÃ¼l Varol
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.10417">SINC: Spatial Composition of 3D Human Motions for Simultaneous Action Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Our goal is to synthesize 3D human motions given textual inputs describing simultaneous actions, for example 'waving hand' while 'walking' at the same time. We refer to generating such simultaneous movements as performing 'spatial compositions'. In contrast to temporal compositions that seek to transition from one action to another, spatial compositing requires understanding which body parts are involved in which action, to be able to move them simultaneously. Motivated by the observation that the correspondence between actions and body parts is encoded in powerful language models, we extract this knowledge by prompting GPT-3 with text such as "what are the body parts involved in the action <action name>?", while also providing the parts list and few-shot examples. Given this action-part mapping, we combine body parts from two motions together and establish the first automated method to spatially compose two actions. However, training data with compositional actions is always limited by the combinatorics. Hence, we further create synthetic data with this approach, and use it to train a new state-of-the-art text-to-motion generation model, called SINC ("SImultaneous actioN Compositions for 3D human motions"). In our experiments, that training with such GPT-guided synthetic data improves spatial composition generation over baselines. Our code is publicly available at https://sinc.is.tue.mpg.de/.
<div id='section'>Paperid: <span id='pid'>1282, <a href='https://arxiv.org/pdf/2303.14392.pdf' target='_blank'>https://arxiv.org/pdf/2303.14392.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yorick Broens, Hans Butler, Roland TÃ³th
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.14392">On Improved Commutation for Moving-Magnet Planar Actuators</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The demand for high-precision and high-throughput motion control systems has increased significantly in recent years. The use of moving-magnet planar actuators (MMPAs) is gaining popularity due to their advantageous characteristics, such as complete environmental decoupling and reduction of stage mass. Nonetheless, model-based commutation techniques for MMPAs are compromised by misalignment between the mover and coil array and mismatch between the ideal electromagnetic model and the physical system, often leading to decreased system performance. To address this issue, a novel improved commutation approach is proposed in this paper\YB{, which is applicable for general planar motor applications,} by means of dynamic regulation of the position dependence of the ideal model-based commutation algorithm, which allows for attenuation of magnetic misalignment, manufacturing inaccuracies and other unmodelled phenomena. The effectiveness of the proposed approach is validated through experiments using a state-of-the-art moving-magnet planar actuator prototype.
<div id='section'>Paperid: <span id='pid'>1283, <a href='https://arxiv.org/pdf/2303.01424.pdf' target='_blank'>https://arxiv.org/pdf/2303.01424.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sriyash Poddar, Christoforos Mavrogiannis, Siddhartha S. Srinivasa
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.01424">From Crowd Motion Prediction to Robot Navigation in Crowds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We focus on robot navigation in crowded environments. To navigate safely and efficiently within crowds, robots need models for crowd motion prediction. Building such models is hard due to the high dimensionality of multiagent domains and the challenge of collecting or simulating interaction-rich crowd-robot demonstrations. While there has been important progress on models for offline pedestrian motion forecasting, transferring their performance on real robots is nontrivial due to close interaction settings and novelty effects on users. In this paper, we investigate the utility of a recent state-of-the-art motion prediction model (S-GAN) for crowd navigation tasks. We incorporate this model into a model predictive controller (MPC) and deploy it on a self-balancing robot which we subject to a diverse range of crowd behaviors in the lab. We demonstrate that while S-GAN motion prediction accuracy transfers to the real world, its value is not reflected on navigation performance, measured with respect to safety and efficiency; in fact, the MPC performs indistinguishably even when using a simple constant-velocity prediction model, suggesting that substantial model improvements might be needed to yield significant gains for crowd navigation tasks. Footage from our experiments can be found at https://youtu.be/mzFiXg8KsZ0.
<div id='section'>Paperid: <span id='pid'>1284, <a href='https://arxiv.org/pdf/2302.01060.pdf' target='_blank'>https://arxiv.org/pdf/2302.01060.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Renukanandan Tumu, Lars Lindemann, Truong Nghiem, Rahul Mangharam
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.01060">Physics Constrained Motion Prediction with Uncertainty Quantification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Predicting the motion of dynamic agents is a critical task for guaranteeing the safety of autonomous systems. A particular challenge is that motion prediction algorithms should obey dynamics constraints and quantify prediction uncertainty as a measure of confidence. We present a physics-constrained approach for motion prediction which uses a surrogate dynamical model to ensure that predicted trajectories are dynamically feasible. We propose a two-step integration consisting of intent and trajectory prediction subject to dynamics constraints. We also construct prediction regions that quantify uncertainty and are tailored for autonomous driving by using conformal prediction, a popular statistical tool. Physics Constrained Motion Prediction achieves a 41% better ADE, 56% better FDE, and 19% better IoU over a baseline in experiments using an autonomous racing dataset.
<div id='section'>Paperid: <span id='pid'>1285, <a href='https://arxiv.org/pdf/2301.00114.pdf' target='_blank'>https://arxiv.org/pdf/2301.00114.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pratik K. Mishra, Alex Mihailidis, Shehroz S. Khan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.00114">Skeletal Video Anomaly Detection using Deep Learning: Survey, Challenges and Future Directions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The existing methods for video anomaly detection mostly utilize videos containing identifiable facial and appearance-based features. The use of videos with identifiable faces raises privacy concerns, especially when used in a hospital or community-based setting. Appearance-based features can also be sensitive to pixel-based noise, straining the anomaly detection methods to model the changes in the background and making it difficult to focus on the actions of humans in the foreground. Structural information in the form of skeletons describing the human motion in the videos is privacy-protecting and can overcome some of the problems posed by appearance-based features. In this paper, we present a survey of privacy-protecting deep learning anomaly detection methods using skeletons extracted from videos. We present a novel taxonomy of algorithms based on the various learning approaches. We conclude that skeleton-based approaches for anomaly detection can be a plausible privacy-protecting alternative for video anomaly detection. Lastly, we identify major open research questions and provide guidelines to address them.
<div id='section'>Paperid: <span id='pid'>1286, <a href='https://arxiv.org/pdf/2208.09224.pdf' target='_blank'>https://arxiv.org/pdf/2208.09224.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaogang Peng, Yaodi Shen, Haoran Wang, Binling Nie, Yigang Wang, Zizhao Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2208.09224">SoMoFormer: Social-Aware Motion Transformer for Multi-Person Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-person motion prediction remains a challenging problem, especially in the joint representation learning of individual motion and social interactions. Most prior methods only involve learning local pose dynamics for individual motion (without global body trajectory) and also struggle to capture complex interaction dependencies for social interactions. In this paper, we propose a novel Social-Aware Motion Transformer (SoMoFormer) to effectively model individual motion and social interactions in a joint manner. Specifically, SoMoFormer extracts motion features from sub-sequences in displacement trajectory space to effectively learn both local and global pose dynamics for each individual. In addition, we devise a novel social-aware motion attention mechanism in SoMoFormer to further optimize dynamics representations and capture interaction dependencies simultaneously via motion similarity calculation across time and social dimensions. On both short- and long-term horizons, we empirically evaluate our framework on multi-person motion datasets and demonstrate that our method greatly outperforms state-of-the-art methods of single- and multi-person motion prediction. Code will be made publicly available upon acceptance.
<div id='section'>Paperid: <span id='pid'>1287, <a href='https://arxiv.org/pdf/2103.14675.pdf' target='_blank'>https://arxiv.org/pdf/2103.14675.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anindita Ghosh, Noshaba Cheema, Cennet Oguz, Christian Theobalt, Philipp Slusallek
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2103.14675">Synthesis of Compositional Animations from Textual Descriptions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>"How can we animate 3D-characters from a movie script or move robots by simply telling them what we would like them to do?" "How unstructured and complex can we make a sentence and still generate plausible movements from it?" These are questions that need to be answered in the long-run, as the field is still in its infancy. Inspired by these problems, we present a new technique for generating compositional actions, which handles complex input sentences. Our output is a 3D pose sequence depicting the actions in the input sentence. We propose a hierarchical two-stream sequential model to explore a finer joint-level mapping between natural language sentences and 3D pose sequences corresponding to the given motion. We learn two manifold representations of the motion -- one each for the upper body and the lower body movements. Our model can generate plausible pose sequences for short sentences describing single actions as well as long compositional sentences describing multiple sequential and superimposed actions. We evaluate our proposed model on the publicly available KIT Motion-Language Dataset containing 3D pose data with human-annotated sentences. Experimental results show that our model advances the state-of-the-art on text-based motion synthesis in objective evaluations by a margin of 50%. Qualitative evaluations based on a user study indicate that our synthesized motions are perceived to be the closest to the ground-truth motion captures for both short and compositional sentences.
<div id='section'>Paperid: <span id='pid'>1288, <a href='https://arxiv.org/pdf/2510.06988.pdf' target='_blank'>https://arxiv.org/pdf/2510.06988.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Girolamo Macaluso, Lorenzo Mandelli, Mirko Bicchierai, Stefano Berretti, Andrew D. Bagdanov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.06988">No MoCap Needed: Post-Training Motion Diffusion Models with Reinforcement Learning using Only Textual Prompts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion models have recently advanced human motion generation, producing realistic and diverse animations from textual prompts. However, adapting these models to unseen actions or styles typically requires additional motion capture data and full retraining, which is costly and difficult to scale. We propose a post-training framework based on Reinforcement Learning that fine-tunes pretrained motion diffusion models using only textual prompts, without requiring any motion ground truth. Our approach employs a pretrained text-motion retrieval network as a reward signal and optimizes the diffusion policy with Denoising Diffusion Policy Optimization, effectively shifting the model's generative distribution toward the target domain without relying on paired motion data. We evaluate our method on cross-dataset adaptation and leave-one-out motion experiments using the HumanML3D and KIT-ML datasets across both latent- and joint-space diffusion architectures. Results from quantitative metrics and user studies show that our approach consistently improves the quality and diversity of generated motions, while preserving performance on the original distribution. Our approach is a flexible, data-efficient, and privacy-preserving solution for motion adaptation.
<div id='section'>Paperid: <span id='pid'>1289, <a href='https://arxiv.org/pdf/2510.05070.pdf' target='_blank'>https://arxiv.org/pdf/2510.05070.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Siheng Zhao, Yanjie Ze, Yue Wang, C. Karen Liu, Pieter Abbeel, Guanya Shi, Rocky Duan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.05070">ResMimic: From General Motion Tracking to Humanoid Whole-body Loco-Manipulation via Residual Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humanoid whole-body loco-manipulation promises transformative capabilities for daily service and warehouse tasks. While recent advances in general motion tracking (GMT) have enabled humanoids to reproduce diverse human motions, these policies lack the precision and object awareness required for loco-manipulation. To this end, we introduce ResMimic, a two-stage residual learning framework for precise and expressive humanoid control from human motion data. First, a GMT policy, trained on large-scale human-only motion, serves as a task-agnostic base for generating human-like whole-body movements. An efficient but precise residual policy is then learned to refine the GMT outputs to improve locomotion and incorporate object interaction. To further facilitate efficient training, we design (i) a point-cloud-based object tracking reward for smoother optimization, (ii) a contact reward that encourages accurate humanoid body-object interactions, and (iii) a curriculum-based virtual object controller to stabilize early training. We evaluate ResMimic in both simulation and on a real Unitree G1 humanoid. Results show substantial gains in task success, training efficiency, and robustness over strong baselines. Videos are available at https://resmimic.github.io/ .
<div id='section'>Paperid: <span id='pid'>1290, <a href='https://arxiv.org/pdf/2510.03423.pdf' target='_blank'>https://arxiv.org/pdf/2510.03423.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ethan Foss, Simone D'Amico
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03423">Efficient Input-Constrained Impulsive Optimal Control of Linear Systems with Application to Spacecraft Relative Motion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work presents a novel algorithm for impulsive optimal control of linear time-varying systems with the inclusion of input magnitude constraints. Impulsive optimal control problems, where the optimal input solution is a sum of delta functions, are typically formulated as an optimization over a normed function space subject to integral equality constraints and can be efficiently solved for linear time-varying systems in their dual formulation. In this dual setting, the problem takes the form of a semi-infinite program which is readily solvable in online scenarios for constructing maneuver plans. This work augments the approach with the inclusion of magnitude constraints on the input over time windows of interest, which is shown to preserve the impulsive nature of the optimal solution and enable efficient solution procedures via semi-infinite programming. The resulting algorithm is demonstrated on the highly relevant problem of relative motion control of spacecraft in Low Earth Orbit (LEO) and compared to several other proposed solutions from the literature.
<div id='section'>Paperid: <span id='pid'>1291, <a href='https://arxiv.org/pdf/2509.23009.pdf' target='_blank'>https://arxiv.org/pdf/2509.23009.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Masato Kobayashi, Ning Ding, Toru Tamaki
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23009">Disentangling Static and Dynamic Information for Reducing Static Bias in Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Action recognition models rely excessively on static cues rather than dynamic human motion, which is known as static bias. This bias leads to poor performance in real-world applications and zero-shot action recognition. In this paper, we propose a method to reduce static bias by separating temporal dynamic information from static scene information. Our approach uses a statistical independence loss between biased and unbiased streams, combined with a scene prediction loss. Our experiments demonstrate that this method effectively reduces static bias and confirm the importance of scene prediction loss.
<div id='section'>Paperid: <span id='pid'>1292, <a href='https://arxiv.org/pdf/2509.22058.pdf' target='_blank'>https://arxiv.org/pdf/2509.22058.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qifeng Wang, Weigang Li, Lei Nie, Xin Xu, Wenping Liu, Zhe Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22058">An Adaptive ICP LiDAR Odometry Based on Reliable Initial Pose</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As a key technology for autonomous navigation and positioning in mobile robots, light detection and ranging (LiDAR) odometry is widely used in autonomous driving applications. The Iterative Closest Point (ICP)-based methods have become the core technique in LiDAR odometry due to their efficient and accurate point cloud registration capability. However, some existing ICP-based methods do not consider the reliability of the initial pose, which may cause the method to converge to a local optimum. Furthermore, the absence of an adaptive mechanism hinders the effective handling of complex dynamic environments, resulting in a significant degradation of registration accuracy. To address these issues, this paper proposes an adaptive ICP-based LiDAR odometry method that relies on a reliable initial pose. First, distributed coarse registration based on density filtering is employed to obtain the initial pose estimation. The reliable initial pose is then selected by comparing it with the motion prediction pose, reducing the initial error between the source and target point clouds. Subsequently, by combining the current and historical errors, the adaptive threshold is dynamically adjusted to accommodate the real-time changes in the dynamic environment. Finally, based on the reliable initial pose and the adaptive threshold, point-to-plane adaptive ICP registration is performed from the current frame to the local map, achieving high-precision alignment of the source and target point clouds. Extensive experiments on the public KITTI dataset demonstrate that the proposed method outperforms existing approaches and significantly enhances the accuracy of LiDAR odometry.
<div id='section'>Paperid: <span id='pid'>1293, <a href='https://arxiv.org/pdf/2509.18046.pdf' target='_blank'>https://arxiv.org/pdf/2509.18046.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yinuo Wang, Yuanyang Qi, Jinzhao Zhou, Gavin Tao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.18046">HuMam: Humanoid Motion Control via End-to-End Deep Reinforcement Learning with Mamba</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>End-to-end reinforcement learning (RL) for humanoid locomotion is appealing for its compact perception-action mapping, yet practical policies often suffer from training instability, inefficient feature fusion, and high actuation cost. We present HuMam, a state-centric end-to-end RL framework that employs a single-layer Mamba encoder to fuse robot-centric states with oriented footstep targets and a continuous phase clock. The policy outputs joint position targets tracked by a low-level PD loop and is optimized with PPO. A concise six-term reward balances contact quality, swing smoothness, foot placement, posture, and body stability while implicitly promoting energy saving. On the JVRC-1 humanoid in mc-mujoco, HuMam consistently improves learning efficiency, training stability, and overall task performance over a strong feedforward baseline, while reducing power consumption and torque peaks. To our knowledge, this is the first end-to-end humanoid RL controller that adopts Mamba as the fusion backbone, demonstrating tangible gains in efficiency, stability, and control economy.
<div id='section'>Paperid: <span id='pid'>1294, <a href='https://arxiv.org/pdf/2509.15130.pdf' target='_blank'>https://arxiv.org/pdf/2509.15130.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenxi Song, Yanming Yang, Tong Zhao, Ruibo Li, Chi Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15130">WorldForge: Unlocking Emergent 3D/4D Generation in Video Diffusion Model via Training-Free Guidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent video diffusion models demonstrate strong potential in spatial intelligence tasks due to their rich latent world priors. However, this potential is hindered by their limited controllability and geometric inconsistency, creating a gap between their strong priors and their practical use in 3D/4D tasks. As a result, current approaches often rely on retraining or fine-tuning, which risks degrading pretrained knowledge and incurs high computational costs. To address this, we propose WorldForge, a training-free, inference-time framework composed of three tightly coupled modules. Intra-Step Recursive Refinement introduces a recursive refinement mechanism during inference, which repeatedly optimizes network predictions within each denoising step to enable precise trajectory injection. Flow-Gated Latent Fusion leverages optical flow similarity to decouple motion from appearance in the latent space and selectively inject trajectory guidance into motion-related channels. Dual-Path Self-Corrective Guidance compares guided and unguided denoising paths to adaptively correct trajectory drift caused by noisy or misaligned structural signals. Together, these components inject fine-grained, trajectory-aligned guidance without training, achieving both accurate motion control and photorealistic content generation. Extensive experiments across diverse benchmarks validate our method's superiority in realism, trajectory consistency, and visual fidelity. This work introduces a novel plug-and-play paradigm for controllable video synthesis, offering a new perspective on leveraging generative priors for spatial intelligence.
<div id='section'>Paperid: <span id='pid'>1295, <a href='https://arxiv.org/pdf/2509.15130.pdf' target='_blank'>https://arxiv.org/pdf/2509.15130.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenxi Song, Yanming Yang, Tong Zhao, Ruibo Li, Chi Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15130">WorldForge: Unlocking Emergent 3D/4D Generation in Video Diffusion Model via Training-Free Guidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent video diffusion models show immense potential for spatial intelligence tasks due to their rich world priors, but this is undermined by limited controllability, poor spatial-temporal consistency, and entangled scene-camera dynamics. Existing solutions, such as model fine-tuning and warping-based repainting, struggle with scalability, generalization, and robustness against artifacts. To address this, we propose WorldForge, a training-free, inference-time framework composed of three tightly coupled modules. 1) Intra-Step Recursive Refinement injects fine-grained trajectory guidance at denoising steps through a recursive correction loop, ensuring motion remains aligned with the target path. 2) Flow-Gated Latent Fusion leverages optical flow similarity to decouple motion from appearance in the latent space and selectively inject trajectory guidance into motion-related channels. 3) Dual-Path Self-Corrective Guidance compares guided and unguided denoising paths to adaptively correct trajectory drift caused by noisy or misaligned structural signals. Together, these components inject fine-grained, trajectory-aligned guidance without training, achieving both accurate motion control and photorealistic content generation. Our framework is plug-and-play and model-agnostic, enabling broad applicability across various 3D/4D tasks. Extensive experiments demonstrate that our method achieves state-of-the-art performance in trajectory adherence, geometric consistency, and perceptual quality, outperforming both training-intensive and inference-only baselines.
<div id='section'>Paperid: <span id='pid'>1296, <a href='https://arxiv.org/pdf/2509.09667.pdf' target='_blank'>https://arxiv.org/pdf/2509.09667.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhengdi Yu, Simone Foti, Linguang Zhang, Amy Zhao, Cem Keskin, Stefanos Zafeiriou, Tolga Birdal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09667">Geometric Neural Distance Fields for Learning Human Motion Priors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce Neural Riemannian Motion Fields (NRMF), a novel 3D generative human motion prior that enables robust, temporally consistent, and physically plausible 3D motion recovery. Unlike existing VAE or diffusion-based methods, our higher-order motion prior explicitly models the human motion in the zero level set of a collection of neural distance fields (NDFs) corresponding to pose, transition (velocity), and acceleration dynamics. Our framework is rigorous in the sense that our NDFs are constructed on the product space of joint rotations, their angular velocities, and angular accelerations, respecting the geometry of the underlying articulations. We further introduce: (i) a novel adaptive-step hybrid algorithm for projecting onto the set of plausible motions, and (ii) a novel geometric integrator to "roll out" realistic motion trajectories during test-time-optimization and generation. Our experiments show significant and consistent gains: trained on the AMASS dataset, NRMF remarkably generalizes across multiple input modalities and to diverse tasks ranging from denoising to motion in-betweening and fitting to partial 2D / 3D observations.
<div id='section'>Paperid: <span id='pid'>1297, <a href='https://arxiv.org/pdf/2509.09210.pdf' target='_blank'>https://arxiv.org/pdf/2509.09210.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xing Gao, Zherui Huang, Weiyao Lin, Xiao Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09210">ProgD: Progressive Multi-scale Decoding with Dynamic Graphs for Joint Multi-agent Motion Forecasting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate motion prediction of surrounding agents is crucial for the safe planning of autonomous vehicles. Recent advancements have extended prediction techniques from individual agents to joint predictions of multiple interacting agents, with various strategies to address complex interactions within future motions of agents. However, these methods overlook the evolving nature of these interactions. To address this limitation, we propose a novel progressive multi-scale decoding strategy, termed ProgD, with the help of dynamic heterogeneous graph-based scenario modeling. In particular, to explicitly and comprehensively capture the evolving social interactions in future scenarios, given their inherent uncertainty, we design a progressive modeling of scenarios with dynamic heterogeneous graphs. With the unfolding of such dynamic heterogeneous graphs, a factorized architecture is designed to process the spatio-temporal dependencies within future scenarios and progressively eliminate uncertainty in future motions of multiple agents. Furthermore, a multi-scale decoding procedure is incorporated to improve on the future scenario modeling and consistent prediction of agents' future motion. The proposed ProgD achieves state-of-the-art performance on the INTERACTION multi-agent prediction benchmark, ranking $1^{st}$, and the Argoverse 2 multi-world forecasting benchmark.
<div id='section'>Paperid: <span id='pid'>1298, <a href='https://arxiv.org/pdf/2509.07593.pdf' target='_blank'>https://arxiv.org/pdf/2509.07593.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gavin Tao, Yinuo Wang, Jinzhao Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.07593">Can SSD-Mamba2 Unlock Reinforcement Learning for End-to-End Motion Control?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>End-to-end reinforcement learning for motion control promises unified perception-action policies that scale across embodiments and tasks, yet most deployed controllers are either blind (proprioception-only) or rely on fusion backbones with unfavorable compute-memory trade-offs. Recurrent controllers struggle with long-horizon credit assignment, and Transformer-based fusion incurs quadratic cost in token length, limiting temporal and spatial context. We present a vision-driven cross-modal RL framework built on SSD-Mamba2, a selective state-space backbone that applies state-space duality (SSD) to enable both recurrent and convolutional scanning with hardware-aware streaming and near-linear scaling. Proprioceptive states and exteroceptive observations (e.g., depth tokens) are encoded into compact tokens and fused by stacked SSD-Mamba2 layers. The selective state-space updates retain long-range dependencies with markedly lower latency and memory use than quadratic self-attention, enabling longer look-ahead, higher token resolution, and stable training under limited compute. Policies are trained end-to-end under curricula that randomize terrain and appearance and progressively increase scene complexity. A compact, state-centric reward balances task progress, energy efficiency, and safety. Across diverse motion-control scenarios, our approach consistently surpasses strong state-of-the-art baselines in return, safety (collisions and falls), and sample efficiency, while converging faster at the same compute budget. These results suggest that SSD-Mamba2 provides a practical fusion backbone for scalable, foresightful, and efficient end-to-end motion control.
<div id='section'>Paperid: <span id='pid'>1299, <a href='https://arxiv.org/pdf/2508.19731.pdf' target='_blank'>https://arxiv.org/pdf/2508.19731.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maryam Kazemi Eskeri, Ville Kyrki, Dominik Baumann, Tomasz Piotr Kucner
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19731">Efficient Human-Aware Task Allocation for Multi-Robot Systems in Shared Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-robot systems are increasingly deployed in applications, such as intralogistics or autonomous delivery, where multiple robots collaborate to complete tasks efficiently. One of the key factors enabling their efficient cooperation is Multi-Robot Task Allocation (MRTA). Algorithms solving this problem optimize task distribution among robots to minimize the overall execution time. In shared environments, apart from the relative distance between the robots and the tasks, the execution time is also significantly impacted by the delay caused by navigating around moving people. However, most existing MRTA approaches are dynamics-agnostic, relying on static maps and neglecting human motion patterns, leading to inefficiencies and delays. In this paper, we introduce \acrfull{method name}. This method leverages Maps of Dynamics (MoDs), spatio-temporal queryable models designed to capture historical human movement patterns, to estimate the impact of humans on the task execution time during deployment. \acrshort{method name} utilizes a stochastic cost function that includes MoDs. Experimental results show that integrating MoDs enhances task allocation performance, resulting in reduced mission completion times by up to $26\%$ compared to the dynamics-agnostic method and up to $19\%$ compared to the baseline. This work underscores the importance of considering human dynamics in MRTA within shared environments and presents an efficient framework for deploying multi-robot systems in environments populated by humans.
<div id='section'>Paperid: <span id='pid'>1300, <a href='https://arxiv.org/pdf/2508.19595.pdf' target='_blank'>https://arxiv.org/pdf/2508.19595.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maryam Kazemi Eskeri, Thomas Wiedemann, Ville Kyrki, Dominik Baumann, Tomasz Piotr Kucner
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19595">A Lightweight Crowd Model for Robot Social Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robots operating in human-populated environments must navigate safely and efficiently while minimizing social disruption. Achieving this requires estimating crowd movement to avoid congested areas in real-time. Traditional microscopic models struggle to scale in dense crowds due to high computational cost, while existing macroscopic crowd prediction models tend to be either overly simplistic or computationally intensive. In this work, we propose a lightweight, real-time macroscopic crowd prediction model tailored for human motion, which balances prediction accuracy and computational efficiency. Our approach simplifies both spatial and temporal processing based on the inherent characteristics of pedestrian flow, enabling robust generalization without the overhead of complex architectures. We demonstrate a 3.6 times reduction in inference time, while improving prediction accuracy by 3.1 %. Integrated into a socially aware planning framework, the model enables efficient and socially compliant robot navigation in dynamic environments. This work highlights that efficient human crowd modeling enables robots to navigate dense environments without costly computations.
<div id='section'>Paperid: <span id='pid'>1301, <a href='https://arxiv.org/pdf/2508.19153.pdf' target='_blank'>https://arxiv.org/pdf/2508.19153.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yinuo Wang, Gavin Tao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19153">QuadKAN: KAN-Enhanced Quadruped Motion Control via End-to-End Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We address vision-guided quadruped motion control with reinforcement learning (RL) and highlight the necessity of combining proprioception with vision for robust control. We propose QuadKAN, a spline-parameterized cross-modal policy instantiated with Kolmogorov-Arnold Networks (KANs). The framework incorporates a spline encoder for proprioception and a spline fusion head for proprioception-vision inputs. This structured function class aligns the state-to-action mapping with the piecewise-smooth nature of gait, improving sample efficiency, reducing action jitter and energy consumption, and providing interpretable posture-action sensitivities. We adopt Multi-Modal Delay Randomization (MMDR) and perform end-to-end training with Proximal Policy Optimization (PPO). Evaluations across diverse terrains, including both even and uneven surfaces and scenarios with static or dynamic obstacles, demonstrate that QuadKAN achieves consistently higher returns, greater distances, and fewer collisions than state-of-the-art (SOTA) baselines. These results show that spline-parameterized policies offer a simple, effective, and interpretable alternative for robust vision-guided locomotion. A repository will be made available upon acceptance.
<div id='section'>Paperid: <span id='pid'>1302, <a href='https://arxiv.org/pdf/2508.10427.pdf' target='_blank'>https://arxiv.org/pdf/2508.10427.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Keishi Ishihara, Kento Sasaki, Tsubasa Takahashi, Daiki Shiono, Yu Yamaguchi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10427">STRIDE-QA: Visual Question Answering Dataset for Spatiotemporal Reasoning in Urban Driving Scenes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language Models (VLMs) have been applied to autonomous driving to support decision-making in complex real-world scenarios. However, their training on static, web-sourced image-text pairs fundamentally limits the precise spatiotemporal reasoning required to understand and predict dynamic traffic scenes. We address this critical gap with STRIDE-QA, a large-scale visual question answering (VQA) dataset for physically grounded reasoning from an ego-centric perspective. Constructed from 100 hours of multi-sensor driving data in Tokyo, capturing diverse and challenging conditions, STRIDE-QA is the largest VQA dataset for spatiotemporal reasoning in urban driving, offering 16 million QA pairs over 285K frames. Grounded by dense, automatically generated annotations including 3D bounding boxes, segmentation masks, and multi-object tracks, the dataset uniquely supports both object-centric and ego-centric reasoning through three novel QA tasks that require spatial localization and temporal prediction. Our benchmarks demonstrate that existing VLMs struggle significantly, achieving near-zero scores on prediction consistency. In contrast, VLMs fine-tuned on STRIDE-QA exhibit dramatic performance gains, achieving 55% success in spatial localization and 28% consistency in future motion prediction, compared to near-zero scores from general-purpose VLMs. Therefore, STRIDE-QA establishes a comprehensive foundation for developing more reliable VLMs for safety-critical autonomous systems.
<div id='section'>Paperid: <span id='pid'>1303, <a href='https://arxiv.org/pdf/2508.07146.pdf' target='_blank'>https://arxiv.org/pdf/2508.07146.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Liu, Zhijie Liu, Xiao Ren, You-Fu Li, He Kong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.07146">Intention-Aware Diffusion Model for Pedestrian Trajectory Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Predicting pedestrian motion trajectories is critical for the path planning and motion control of autonomous vehicles. Recent diffusion-based models have shown promising results in capturing the inherent stochasticity of pedestrian behavior for trajectory prediction. However, the absence of explicit semantic modelling of pedestrian intent in many diffusion-based methods may result in misinterpreted behaviors and reduced prediction accuracy. To address the above challenges, we propose a diffusion-based pedestrian trajectory prediction framework that incorporates both short-term and long-term motion intentions. Short-term intent is modelled using a residual polar representation, which decouples direction and magnitude to capture fine-grained local motion patterns. Long-term intent is estimated through a learnable, token-based endpoint predictor that generates multiple candidate goals with associated probabilities, enabling multimodal and context-aware intention modelling. Furthermore, we enhance the diffusion process by incorporating adaptive guidance and a residual noise predictor that dynamically refines denoising accuracy. The proposed framework is evaluated on the widely used ETH, UCY, and SDD benchmarks, demonstrating competitive results against state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>1304, <a href='https://arxiv.org/pdf/2508.04229.pdf' target='_blank'>https://arxiv.org/pdf/2508.04229.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Liu, Zhijie Liu, Xiao Ren, You-Fu Li, He Kong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.04229">Intention Enhanced Diffusion Model for Multimodal Pedestrian Trajectory Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Predicting pedestrian motion trajectories is critical for path planning and motion control of autonomous vehicles. However, accurately forecasting crowd trajectories remains a challenging task due to the inherently multimodal and uncertain nature of human motion. Recent diffusion-based models have shown promising results in capturing the stochasticity of pedestrian behavior for trajectory prediction. However, few diffusion-based approaches explicitly incorporate the underlying motion intentions of pedestrians, which can limit the interpretability and precision of prediction models. In this work, we propose a diffusion-based multimodal trajectory prediction model that incorporates pedestrians' motion intentions into the prediction framework. The motion intentions are decomposed into lateral and longitudinal components, and a pedestrian intention recognition module is introduced to enable the model to effectively capture these intentions. Furthermore, we adopt an efficient guidance mechanism that facilitates the generation of interpretable trajectories. The proposed framework is evaluated on two widely used human trajectory prediction benchmarks, ETH and UCY, on which it is compared against state-of-the-art methods. The experimental results demonstrate that our method achieves competitive performance.
<div id='section'>Paperid: <span id='pid'>1305, <a href='https://arxiv.org/pdf/2507.23188.pdf' target='_blank'>https://arxiv.org/pdf/2507.23188.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shiyao Yu, Zi-An Wang, Kangning Yin, Zheng Tian, Mingyuan Zhang, Weixin Si, Shihao Zou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.23188">Multi-Modal Motion Retrieval by Learning a Fine-Grained Joint Embedding Space</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion retrieval is crucial for motion acquisition, offering superior precision, realism, controllability, and editability compared to motion generation. Existing approaches leverage contrastive learning to construct a unified embedding space for motion retrieval from text or visual modality. However, these methods lack a more intuitive and user-friendly interaction mode and often overlook the sequential representation of most modalities for improved retrieval performance. To address these limitations, we propose a framework that aligns four modalities -- text, audio, video, and motion -- within a fine-grained joint embedding space, incorporating audio for the first time in motion retrieval to enhance user immersion and convenience. This fine-grained space is achieved through a sequence-level contrastive learning approach, which captures critical details across modalities for better alignment. To evaluate our framework, we augment existing text-motion datasets with synthetic but diverse audio recordings, creating two multi-modal motion retrieval datasets. Experimental results demonstrate superior performance over state-of-the-art methods across multiple sub-tasks, including an 10.16% improvement in R@10 for text-to-motion retrieval and a 25.43% improvement in R@1 for video-to-motion retrieval on the HumanML3D dataset. Furthermore, our results show that our 4-modal framework significantly outperforms its 3-modal counterpart, underscoring the potential of multi-modal motion retrieval for advancing motion acquisition.
<div id='section'>Paperid: <span id='pid'>1306, <a href='https://arxiv.org/pdf/2507.21069.pdf' target='_blank'>https://arxiv.org/pdf/2507.21069.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andreas Spilz, Heiko Oppel, Jochen Werner, Kathrin Stucke-Straub, Felix Capanni, Michael Munz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.21069">GAITEX: Human motion dataset from impaired gait and rehabilitation exercises of inertial and optical sensor data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Wearable inertial measurement units (IMUs) offer a cost-effective and scalable means to assess human movement quality in clinical and everyday settings. However, the development of robust sensor-based classification models for physiotherapeutic exercises and gait analysis requires large, diverse datasets, which are costly and time-consuming to collect. Here, we present a multimodal dataset of physiotherapeutic exercises - including correct and clinically relevant variants - and gait-related exercises - including both normal and impaired gait patterns - recorded from 19 participants using synchronized IMUs and marker-based motion capture (MoCap). The dataset includes raw data from nine IMUs and thirty-five optical markers capturing full-body kinematics. Each IMU is additionally equipped with four optical markers, enabling precise comparison between IMU-derived orientation estimates and reference values from the MoCap system. To support further analysis, we also provide processed IMU orientations aligned with common segment coordinate systems, subject-specific OpenSim models, inverse kinematics results, and tools for visualizing IMU orientations in the musculoskeletal context. Detailed annotations of movement execution quality and time-stamped segmentations support diverse analysis goals. This dataset supports the development and benchmarking of machine learning models for tasks such as automatic exercise evaluation, gait analysis, temporal activity segmentation, and biomechanical parameter estimation. To facilitate reproducibility, we provide code for postprocessing, sensor-to-segment alignment, inverse kinematics computation, and technical validation. This resource is intended to accelerate research in machine learning-driven human movement analysis.
<div id='section'>Paperid: <span id='pid'>1307, <a href='https://arxiv.org/pdf/2507.20557.pdf' target='_blank'>https://arxiv.org/pdf/2507.20557.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingting Li, Yu Qian, Lin Zhao, Su-Jing Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.20557">FED-PsyAU: Privacy-Preserving Micro-Expression Recognition via Psychological AU Coordination and Dynamic Facial Motion Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Micro-expressions (MEs) are brief, low-intensity, often localized facial expressions. They could reveal genuine emotions individuals may attempt to conceal, valuable in contexts like criminal interrogation and psychological counseling. However, ME recognition (MER) faces challenges, such as small sample sizes and subtle features, which hinder efficient modeling. Additionally, real-world applications encounter ME data privacy issues, leaving the task of enhancing recognition across settings under privacy constraints largely unexplored. To address these issues, we propose a FED-PsyAU research framework. We begin with a psychological study on the coordination of upper and lower facial action units (AUs) to provide structured prior knowledge of facial muscle dynamics. We then develop a DPK-GAT network that combines these psychological priors with statistical AU patterns, enabling hierarchical learning of facial motion features from regional to global levels, effectively enhancing MER performance. Additionally, our federated learning framework advances MER capabilities across multiple clients without data sharing, preserving privacy and alleviating the limited-sample issue for each client. Extensive experiments on commonly-used ME databases demonstrate the effectiveness of our approach.
<div id='section'>Paperid: <span id='pid'>1308, <a href='https://arxiv.org/pdf/2507.02857.pdf' target='_blank'>https://arxiv.org/pdf/2507.02857.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziye Li, Hao Luo, Xincheng Shuai, Henghui Ding
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02857">AnyI2V: Animating Any Conditional Image with Motion Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in video generation, particularly in diffusion models, have driven notable progress in text-to-video (T2V) and image-to-video (I2V) synthesis. However, challenges remain in effectively integrating dynamic motion signals and flexible spatial constraints. Existing T2V methods typically rely on text prompts, which inherently lack precise control over the spatial layout of generated content. In contrast, I2V methods are limited by their dependence on real images, which restricts the editability of the synthesized content. Although some methods incorporate ControlNet to introduce image-based conditioning, they often lack explicit motion control and require computationally expensive training. To address these limitations, we propose AnyI2V, a training-free framework that animates any conditional images with user-defined motion trajectories. AnyI2V supports a broader range of modalities as the conditional image, including data types such as meshes and point clouds that are not supported by ControlNet, enabling more flexible and versatile video generation. Additionally, it supports mixed conditional inputs and enables style transfer and editing via LoRA and text prompts. Extensive experiments demonstrate that the proposed AnyI2V achieves superior performance and provides a new perspective in spatial- and motion-controlled video generation. Code is available at https://henghuiding.com/AnyI2V/.
<div id='section'>Paperid: <span id='pid'>1309, <a href='https://arxiv.org/pdf/2506.10240.pdf' target='_blank'>https://arxiv.org/pdf/2506.10240.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rongfei Li, Francis Assadian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.10240">Innovative Adaptive Imaged Based Visual Servoing Control of 6 DoFs Industrial Robot Manipulators</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image-based visual servoing (IBVS) methods have been well developed and used in many applications, especially in pose (position and orientation) alignment. However, most research papers focused on developing control solutions when 3D point features can be detected inside the field of view. This work proposes an innovative feedforward-feedback adaptive control algorithm structure with the Youla Parameterization method. A designed feature estimation loop ensures stable and fast motion control when point features are outside the field of view. As 3D point features move inside the field of view, the IBVS feedback loop preserves the precision of the pose at the end of the control period. Also, an adaptive controller is developed in the feedback loop to stabilize the system in the entire range of operations. The nonlinear camera and robot manipulator model is linearized and decoupled online by an adaptive algorithm. The adaptive controller is then computed based on the linearized model evaluated at current linearized point. The proposed solution is robust and easy to implement in different industrial robotic systems. Various scenarios are used in simulations to validate the effectiveness and robust performance of the proposed controller.
<div id='section'>Paperid: <span id='pid'>1310, <a href='https://arxiv.org/pdf/2506.08851.pdf' target='_blank'>https://arxiv.org/pdf/2506.08851.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sepehr Samavi, Garvish Bhutani, Florian Shkurti, Angela P. Schoellig
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.08851">Deploying SICNav in the Field: Safe and Interactive Crowd Navigation using MPC and Bilevel Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Safe and efficient navigation in crowded environments remains a critical challenge for robots that provide a variety of service tasks such as food delivery or autonomous wheelchair mobility. Classical robot crowd navigation methods decouple human motion prediction from robot motion planning, which neglects the closed-loop interactions between humans and robots. This lack of a model for human reactions to the robot plan (e.g. moving out of the way) can cause the robot to get stuck. Our proposed Safe and Interactive Crowd Navigation (SICNav) method is a bilevel Model Predictive Control (MPC) framework that combines prediction and planning into one optimization problem, explicitly modeling interactions among agents. In this paper, we present a systems overview of the crowd navigation platform we use to deploy SICNav in previously unseen indoor and outdoor environments. We provide a preliminary analysis of the system's operation over the course of nearly 7 km of autonomous navigation over two hours in both indoor and outdoor environments.
<div id='section'>Paperid: <span id='pid'>1311, <a href='https://arxiv.org/pdf/2506.02733.pdf' target='_blank'>https://arxiv.org/pdf/2506.02733.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoyi Feng, Kaifeng Zou, Caichun Cen, Tao Huang, Hui Guo, Zizhou Huang, Yingli Zhao, Mingqing Zhang, Ziyuan Zheng, Diwei Wang, Yuntao Zou, Dagang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.02733">LinkTo-Anime: A 2D Animation Optical Flow Dataset from 3D Model Rendering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing optical flow datasets focus primarily on real-world simulation or synthetic human motion, but few are tailored to Celluloid(cel) anime character motion: a domain with unique visual and motion characteristics. To bridge this gap and facilitate research in optical flow estimation and downstream tasks such as anime video generation and line drawing colorization, we introduce LinkTo-Anime, the first high-quality dataset specifically designed for cel anime character motion generated with 3D model rendering. LinkTo-Anime provides rich annotations including forward and backward optical flow, occlusion masks, and Mixamo Skeleton. The dataset comprises 395 video sequences, totally 24,230 training frames, 720 validation frames, and 4,320 test frames. Furthermore, a comprehensive benchmark is constructed with various optical flow estimation methods to analyze the shortcomings and limitations across multiple datasets.
<div id='section'>Paperid: <span id='pid'>1312, <a href='https://arxiv.org/pdf/2505.20920.pdf' target='_blank'>https://arxiv.org/pdf/2505.20920.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qihang Fang, Chengcheng Tang, Bugra Tekin, Shugao Ma, Yanchao Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.20920">HuMoCon: Concept Discovery for Human Motion Understanding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present HuMoCon, a novel motion-video understanding framework designed for advanced human behavior analysis. The core of our method is a human motion concept discovery framework that efficiently trains multi-modal encoders to extract semantically meaningful and generalizable features. HuMoCon addresses key challenges in motion concept discovery for understanding and reasoning, including the lack of explicit multi-modality feature alignment and the loss of high-frequency information in masked autoencoding frameworks. Our approach integrates a feature alignment strategy that leverages video for contextual understanding and motion for fine-grained interaction modeling, further with a velocity reconstruction mechanism to enhance high-frequency feature expression and mitigate temporal over-smoothing. Comprehensive experiments on standard benchmarks demonstrate that HuMoCon enables effective motion concept discovery and significantly outperforms state-of-the-art methods in training large models for human motion understanding. We will open-source the associated code with our paper.
<div id='section'>Paperid: <span id='pid'>1313, <a href='https://arxiv.org/pdf/2505.13054.pdf' target='_blank'>https://arxiv.org/pdf/2505.13054.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Max Grobbel, Daniel FlÃ¶gel, Philipp Rigoll, SÃ¶ren Hohmann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.13054">Disentangling Coordiante Frames for Task Specific Motion Retargeting in Teleoperation using Shared Control and VR Controllers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Task performance in terms of task completion time in teleoperation is still far behind compared to humans conducting tasks directly. One large identified impact on this is the human capability to perform transformations and alignments, which is directly influenced by the point of view and the motion retargeting strategy. In modern teleoperation systems, motion retargeting is usually implemented through a one time calibration or switching modes. Complex tasks, like concatenated screwing, might be difficult, because the operator has to align (e.g. mirror) rotational and translational input commands. Recent research has shown, that the separation of translation and rotation leads to increased task performance. This work proposes a formal motion retargeting method, which separates translational and rotational input commands. This method is then included in a optimal control based trajectory planner and shown to work on a UR5e manipulator.
<div id='section'>Paperid: <span id='pid'>1314, <a href='https://arxiv.org/pdf/2505.08293.pdf' target='_blank'>https://arxiv.org/pdf/2505.08293.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhizhuo Yin, Yuk Hang Tsui, Pan Hui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.08293">M3G: Multi-Granular Gesture Generator for Audio-Driven Full-Body Human Motion Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating full-body human gestures encompassing face, body, hands, and global movements from audio is a valuable yet challenging task in virtual avatar creation. Previous systems focused on tokenizing the human gestures framewisely and predicting the tokens of each frame from the input audio. However, one observation is that the number of frames required for a complete expressive human gesture, defined as granularity, varies among different human gesture patterns. Existing systems fail to model these gesture patterns due to the fixed granularity of their gesture tokens. To solve this problem, we propose a novel framework named Multi-Granular Gesture Generator (M3G) for audio-driven holistic gesture generation. In M3G, we propose a novel Multi-Granular VQ-VAE (MGVQ-VAE) to tokenize motion patterns and reconstruct motion sequences from different temporal granularities. Subsequently, we proposed a multi-granular token predictor that extracts multi-granular information from audio and predicts the corresponding motion tokens. Then M3G reconstructs the human gestures from the predicted tokens using the MGVQ-VAE. Both objective and subjective experiments demonstrate that our proposed M3G framework outperforms the state-of-the-art methods in terms of generating natural and expressive full-body human gestures.
<div id='section'>Paperid: <span id='pid'>1315, <a href='https://arxiv.org/pdf/2505.07539.pdf' target='_blank'>https://arxiv.org/pdf/2505.07539.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Li, Sicheng Li, Xiang Gao, Abudouaihati Batuer, Lu Yu, Yiyi Liao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.07539">GIFStream: 4D Gaussian-based Immersive Video with Feature Stream</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Immersive video offers a 6-Dof-free viewing experience, potentially playing a key role in future video technology. Recently, 4D Gaussian Splatting has gained attention as an effective approach for immersive video due to its high rendering efficiency and quality, though maintaining quality with manageable storage remains challenging. To address this, we introduce GIFStream, a novel 4D Gaussian representation using a canonical space and a deformation field enhanced with time-dependent feature streams. These feature streams enable complex motion modeling and allow efficient compression by leveraging temporal correspondence and motion-aware pruning. Additionally, we incorporate both temporal and spatial compression networks for end-to-end compression. Experimental results show that GIFStream delivers high-quality immersive video at 30 Mbps, with real-time rendering and fast decoding on an RTX 4090. Project page: https://xdimlab.github.io/GIFStream
<div id='section'>Paperid: <span id='pid'>1316, <a href='https://arxiv.org/pdf/2505.07301.pdf' target='_blank'>https://arxiv.org/pdf/2505.07301.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Katsuki Shimbo, Hiromu Taketsugu, Norimichi Ukita
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.07301">Human Motion Prediction via Test-domain-aware Adaptation with Easily-available Human Motions Estimated from Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In 3D Human Motion Prediction (HMP), conventional methods train HMP models with expensive motion capture data. However, the data collection cost of such motion capture data limits the data diversity, which leads to poor generalizability to unseen motions or subjects. To address this issue, this paper proposes to enhance HMP with additional learning using estimated poses from easily available videos. The 2D poses estimated from the monocular videos are carefully transformed into motion capture-style 3D motions through our pipeline. By additional learning with the obtained motions, the HMP model is adapted to the test domain. The experimental results demonstrate the quantitative and qualitative impact of our method.
<div id='section'>Paperid: <span id='pid'>1317, <a href='https://arxiv.org/pdf/2505.04961.pdf' target='_blank'>https://arxiv.org/pdf/2505.04961.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyu Zhang, Sergey Bashkirov, Dun Yang, Yi Shi, Michael Taylor, Xue Bin Peng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.04961">Physics-Based Motion Imitation with Adversarial Differential Discriminators</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-objective optimization problems, which require the simultaneous optimization of multiple objectives, are prevalent across numerous applications. Existing multi-objective optimization methods often rely on manually-tuned aggregation functions to formulate a joint optimization objective. The performance of such hand-tuned methods is heavily dependent on careful weight selection, a time-consuming and laborious process. These limitations also arise in the setting of reinforcement-learning-based motion tracking methods for physically simulated characters, where intricately crafted reward functions are typically used to achieve high-fidelity results. Such solutions not only require domain expertise and significant manual tuning, but also limit the applicability of the resulting reward function across diverse skills. To bridge this gap, we present a novel adversarial multi-objective optimization technique that is broadly applicable to a range of multi-objective reinforcement-learning tasks, including motion tracking. Our proposed Adversarial Differential Discriminator (ADD) receives a single positive sample, yet is still effective at guiding the optimization process. We demonstrate that our technique can enable characters to closely replicate a variety of acrobatic and agile behaviors, achieving comparable quality to state-of-the-art motion-tracking methods, without relying on manually-designed reward functions. Code and results are available at https://add-moo.github.io/.
<div id='section'>Paperid: <span id='pid'>1318, <a href='https://arxiv.org/pdf/2504.10190.pdf' target='_blank'>https://arxiv.org/pdf/2504.10190.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaushik Bhargav Sivangi, Idris Zakariyya, Paul Henderson, Fani Deligianni
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.10190">Differentially Private 2D Human Pose Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human pose estimation (HPE) has become essential in numerous applications including healthcare, activity recognition, and human-computer interaction. However, the privacy implications of processing sensitive visual data present significant deployment barriers in critical domains. While traditional anonymization techniques offer limited protection and often compromise data utility for broader motion analysis, Differential Privacy (DP) provides formal privacy guarantees but typically degrades model performance when applied naively. In this work, we present the first differentially private 2D human pose estimation (2D-HPE) by applying Differentially Private Stochastic Gradient Descent (DP-SGD) to this task. To effectively balance privacy with performance, we adopt Projected DP-SGD (PDP-SGD), which projects the noisy gradients to a low-dimensional subspace. Additionally, we adapt TinyViT, a compact and efficient vision transformer for coordinate classification in HPE, providing a lightweight yet powerful backbone that enhances privacy-preserving deployment feasibility on resource-limited devices. Our approach is particularly valuable for multimedia interpretation tasks, enabling privacy-safe analysis and understanding of human motion across diverse visual media while preserving the semantic meaning required for downstream applications. Comprehensive experiments on the MPII Human Pose Dataset demonstrate significant performance enhancement with PDP-SGD achieving 78.48% PCKh@0.5 at a strict privacy budget ($Îµ=0.2$), compared to 63.85% for standard DP-SGD. This work lays foundation for privacy-preserving human pose estimation in real-world, sensitive applications.
<div id='section'>Paperid: <span id='pid'>1319, <a href='https://arxiv.org/pdf/2504.01724.pdf' target='_blank'>https://arxiv.org/pdf/2504.01724.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxuan Luo, Zhengkun Rong, Lizhen Wang, Longhao Zhang, Tianshu Hu, Yongming Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.01724">DreamActor-M1: Holistic, Expressive and Robust Human Image Animation with Hybrid Guidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While recent image-based human animation methods achieve realistic body and facial motion synthesis, critical gaps remain in fine-grained holistic controllability, multi-scale adaptability, and long-term temporal coherence, which leads to their lower expressiveness and robustness. We propose a diffusion transformer (DiT) based framework, DreamActor-M1, with hybrid guidance to overcome these limitations. For motion guidance, our hybrid control signals that integrate implicit facial representations, 3D head spheres, and 3D body skeletons achieve robust control of facial expressions and body movements, while producing expressive and identity-preserving animations. For scale adaptation, to handle various body poses and image scales ranging from portraits to full-body views, we employ a progressive training strategy using data with varying resolutions and scales. For appearance guidance, we integrate motion patterns from sequential frames with complementary visual references, ensuring long-term temporal coherence for unseen regions during complex movements. Experiments demonstrate that our method outperforms the state-of-the-art works, delivering expressive results for portraits, upper-body, and full-body generation with robust long-term consistency. Project Page: https://grisoon.github.io/DreamActor-M1/.
<div id='section'>Paperid: <span id='pid'>1320, <a href='https://arxiv.org/pdf/2503.01857.pdf' target='_blank'>https://arxiv.org/pdf/2503.01857.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi Yang, Xiao Li, Xuchen Wang, Mei Liu, Junwei Yin, Weibing Li, Richard M. Voyles, Xin Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.01857">A strictly predefined-time convergent and anti-noise fractional-order zeroing neural network for solving time-variant quadratic programming in kinematic robot control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper proposes a strictly predefined-time convergent and anti-noise fractional-order zeroing neural network (SPTC-AN-FOZNN) model, meticulously designed for addressing time-variant quadratic programming (TVQP) problems. This model marks the first variable-gain ZNN to collectively manifest strictly predefined-time convergence and noise resilience, specifically tailored for kinematic motion control of robots. The SPTC-AN-FOZNN advances traditional ZNNs by incorporating a conformable fractional derivative in accordance with the Leibniz rule, a compliance not commonly achieved by other fractional derivative definitions. It also features a novel activation function designed to ensure favorable convergence independent of the model's order. When compared to five recently published recurrent neural networks (RNNs), the SPTC-AN-FOZNN, configured with $0<Î±\leq 1$, exhibits superior positional accuracy and robustness against additive noises for TVQP applications. Extensive empirical evaluations, including simulations with two types of robotic manipulators and experiments with a Flexiv Rizon robot, have validated the SPTC-AN-FOZNN's effectiveness in precise tracking and computational efficiency, establishing its utility for robust kinematic control.
<div id='section'>Paperid: <span id='pid'>1321, <a href='https://arxiv.org/pdf/2502.19049.pdf' target='_blank'>https://arxiv.org/pdf/2502.19049.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Patrick Seifner, Kostadin Cvejoski, David Berghaus, Cesar Ojeda, Ramses J. Sanchez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.19049">Foundation Inference Models for Stochastic Differential Equations: A Transformer-based Approach for Zero-shot Function Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Stochastic differential equations (SDEs) describe dynamical systems where deterministic flows, governed by a drift function, are superimposed with random fluctuations dictated by a diffusion function. The accurate estimation (or discovery) of these functions from data is a central problem in machine learning, with wide application across natural and social sciences alike. Yet current solutions are brittle, and typically rely on symbolic regression or Bayesian non-parametrics. In this work, we introduce FIM-SDE (Foundation Inference Model for SDEs), a transformer-based recognition model capable of performing accurate zero-shot estimation of the drift and diffusion functions of SDEs, from noisy and sparse observations on empirical processes of different dimensionalities. Leveraging concepts from amortized inference and neural operators, we train FIM-SDE in a supervised fashion, to map a large set of noisy and discretely observed SDE paths to their corresponding drift and diffusion functions. We demonstrate that one and the same (pretrained) FIM-SDE achieves robust zero-shot function estimation (i.e. without any parameter fine-tuning) across a wide range of synthetic and real-world processes, from canonical SDE systems (e.g. double-well dynamics or weakly perturbed Hopf bifurcations) to human motion recordings and oil price and wind speed fluctuations.
<div id='section'>Paperid: <span id='pid'>1322, <a href='https://arxiv.org/pdf/2502.12975.pdf' target='_blank'>https://arxiv.org/pdf/2502.12975.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhexiong Wan, Bin Fan, Le Hui, Yuchao Dai, Gim Hee Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.12975">Instance-Level Moving Object Segmentation from a Single Image with Events</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Moving object segmentation plays a crucial role in understanding dynamic scenes involving multiple moving objects, while the difficulties lie in taking into account both spatial texture structures and temporal motion cues. Existing methods based on video frames encounter difficulties in distinguishing whether pixel displacements of an object are caused by camera motion or object motion due to the complexities of accurate image-based motion modeling. Recent advances exploit the motion sensitivity of novel event cameras to counter conventional images' inadequate motion modeling capabilities, but instead lead to challenges in segmenting pixel-level object masks due to the lack of dense texture structures in events. To address these two limitations imposed by unimodal settings, we propose the first instance-level moving object segmentation framework that integrates complementary texture and motion cues. Our model incorporates implicit cross-modal masked attention augmentation, explicit contrastive feature learning, and flow-guided motion enhancement to exploit dense texture information from a single image and rich motion information from events, respectively. By leveraging the augmented texture and motion features, we separate mask segmentation from motion classification to handle varying numbers of independently moving objects. Through extensive evaluations on multiple datasets, as well as ablation experiments with different input settings and real-time efficiency analysis of the proposed framework, we believe that our first attempt to incorporate image and event data for practical deployment can provide new insights for future work in event-based motion related works. The source code with model training and pre-trained weights is released at https://npucvr.github.io/EvInsMOS
<div id='section'>Paperid: <span id='pid'>1323, <a href='https://arxiv.org/pdf/2502.10585.pdf' target='_blank'>https://arxiv.org/pdf/2502.10585.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anshul Nayak, Azim Eskandarian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.10585">Prediction uncertainty-aware planning using deep ensembles and trajectory optimisation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion is stochastic and ensuring safe robot navigation in a pedestrian-rich environment requires proactive decision-making. Past research relied on incorporating deterministic future states of surrounding pedestrians which can be overconfident leading to unsafe robot behaviour. The current paper proposes a predictive uncertainty-aware planner that integrates neural network based probabilistic trajectory prediction into planning. Our method uses a deep ensemble based network for probabilistic forecasting of surrounding humans and integrates the predictive uncertainty as constraints into the planner. We compare numerous constraint satisfaction methods on the planner and evaluated its performance on real world pedestrian datasets. Further, offline robot navigation was carried out on out-of-distribution pedestrian trajectories inside a narrow corridor
<div id='section'>Paperid: <span id='pid'>1324, <a href='https://arxiv.org/pdf/2501.15860.pdf' target='_blank'>https://arxiv.org/pdf/2501.15860.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lei Wan, Hannan Ejaz Keen, Alexey Vinel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.15860">The Components of Collaborative Joint Perception and Prediction -- A Conceptual Framework</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Connected Autonomous Vehicles (CAVs) benefit from Vehicle-to-Everything (V2X) communication, which enables the exchange of sensor data to achieve Collaborative Perception (CP). To reduce cumulative errors in perception modules and mitigate the visual occlusion, this paper introduces a new task, Collaborative Joint Perception and Prediction (Co-P&P), and provides a conceptual framework for its implementation to improve motion prediction of surrounding objects, thereby enhancing vehicle awareness in complex traffic scenarios. The framework consists of two decoupled core modules, Collaborative Scene Completion (CSC) and Joint Perception and Prediction (P&P) module, which simplify practical deployment and enhance scalability. Additionally, we outline the challenges in Co-P&P and discuss future directions for this research area.
<div id='section'>Paperid: <span id='pid'>1325, <a href='https://arxiv.org/pdf/2501.10356.pdf' target='_blank'>https://arxiv.org/pdf/2501.10356.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Claire Chen, Zhongchun Yu, Hojung Choi, Mark Cutkosky, Jeannette Bohg
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.10356">DexForce: Extracting Force-informed Actions from Kinesthetic Demonstrations for Dexterous Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Imitation learning requires high-quality demonstrations consisting of sequences of state-action pairs. For contact-rich dexterous manipulation tasks that require dexterity, the actions in these state-action pairs must produce the right forces. Current widely-used methods for collecting dexterous manipulation demonstrations are difficult to use for demonstrating contact-rich tasks due to unintuitive human-to-robot motion retargeting and the lack of direct haptic feedback. Motivated by these concerns, we propose DexForce. DexForce leverages contact forces, measured during kinesthetic demonstrations, to compute force-informed actions for policy learning. We collect demonstrations for six tasks and show that policies trained on our force-informed actions achieve an average success rate of 76% across all tasks. In contrast, policies trained directly on actions that do not account for contact forces have near-zero success rates. We also conduct a study ablating the inclusion of force data in policy observations. We find that while using force data never hurts policy performance, it helps most for tasks that require advanced levels of precision and coordination, like opening an AirPods case and unscrewing a nut.
<div id='section'>Paperid: <span id='pid'>1326, <a href='https://arxiv.org/pdf/2501.07563.pdf' target='_blank'>https://arxiv.org/pdf/2501.07563.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinyu Zhang, Zicheng Duan, Dong Gong, Lingqiao Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.07563">Training-Free Motion-Guided Video Generation with Enhanced Temporal Consistency Using Motion Consistency Loss</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we address the challenge of generating temporally consistent videos with motion guidance. While many existing methods depend on additional control modules or inference-time fine-tuning, recent studies suggest that effective motion guidance is achievable without altering the model architecture or requiring extra training. Such approaches offer promising compatibility with various video generation foundation models. However, existing training-free methods often struggle to maintain consistent temporal coherence across frames or to follow guided motion accurately. In this work, we propose a simple yet effective solution that combines an initial-noise-based approach with a novel motion consistency loss, the latter being our key innovation. Specifically, we capture the inter-frame feature correlation patterns of intermediate features from a video diffusion model to represent the motion pattern of the reference video. We then design a motion consistency loss to maintain similar feature correlation patterns in the generated video, using the gradient of this loss in the latent space to guide the generation process for precise motion control. This approach improves temporal consistency across various motion control tasks while preserving the benefits of a training-free setup. Extensive experiments show that our method sets a new standard for efficient, temporally coherent video generation.
<div id='section'>Paperid: <span id='pid'>1327, <a href='https://arxiv.org/pdf/2501.04169.pdf' target='_blank'>https://arxiv.org/pdf/2501.04169.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sungjae Park, Seungho Lee, Mingi Choi, Jiye Lee, Jeonghwan Kim, Jisoo Kim, Hanbyul Joo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.04169">Learning to Transfer Human Hand Skills for Robot Manipulations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a method for teaching dexterous manipulation tasks to robots from human hand motion demonstrations. Unlike existing approaches that solely rely on kinematics information without taking into account the plausibility of robot and object interaction, our method directly infers plausible robot manipulation actions from human motion demonstrations. To address the embodiment gap between the human hand and the robot system, our approach learns a joint motion manifold that maps human hand movements, robot hand actions, and object movements in 3D, enabling us to infer one motion component from others. Our key idea is the generation of pseudo-supervision triplets, which pair human, object, and robot motion trajectories synthetically. Through real-world experiments with robot hand manipulation, we demonstrate that our data-driven retargeting method significantly outperforms conventional retargeting techniques, effectively bridging the embodiment gap between human and robotic hands. Website at https://rureadyo.github.io/MocapRobot/.
<div id='section'>Paperid: <span id='pid'>1328, <a href='https://arxiv.org/pdf/2501.01425.pdf' target='_blank'>https://arxiv.org/pdf/2501.01425.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xincheng Shuai, Henghui Ding, Zhenyuan Qin, Hao Luo, Xingjun Ma, Dacheng Tao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.01425">Free-Form Motion Control: Controlling the 6D Poses of Camera and Objects in Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Controlling the movements of dynamic objects and the camera within generated videos is a meaningful yet challenging task. Due to the lack of datasets with comprehensive 6D pose annotations, existing text-to-video methods can not simultaneously control the motions of both camera and objects in 3D-aware manner, resulting in limited controllability over generated contents. To address this issue and facilitate the research in this field, we introduce a Synthetic Dataset for Free-Form Motion Control (SynFMC). The proposed SynFMC dataset includes diverse object and environment categories and covers various motion patterns according to specific rules, simulating common and complex real-world scenarios. The complete 6D pose information facilitates models learning to disentangle the motion effects from objects and the camera in a video.~To provide precise 3D-aware motion control, we further propose a method trained on SynFMC, Free-Form Motion Control (FMC). FMC can control the 6D poses of objects and camera independently or simultaneously, producing high-fidelity videos. Moreover, it is compatible with various personalized text-to-image (T2I) models for different content styles. Extensive experiments demonstrate that the proposed FMC outperforms previous methods across multiple scenarios.
<div id='section'>Paperid: <span id='pid'>1329, <a href='https://arxiv.org/pdf/2412.00851.pdf' target='_blank'>https://arxiv.org/pdf/2412.00851.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weihang Li, Weirong Chen, Shenhan Qian, Jiajie Chen, Daniel Cremers, Haoang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.00851">DynSUP: Dynamic Gaussian Splatting from An Unposed Image Pair</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in 3D Gaussian Splatting have shown promising results. Existing methods typically assume static scenes and/or multiple images with prior poses. Dynamics, sparse views, and unknown poses significantly increase the problem complexity due to insufficient geometric constraints. To overcome this challenge, we propose a method that can use only two images without prior poses to fit Gaussians in dynamic environments. To achieve this, we introduce two technical contributions. First, we propose an object-level two-view bundle adjustment. This strategy decomposes dynamic scenes into piece-wise rigid components, and jointly estimates the camera pose and motions of dynamic objects. Second, we design an SE(3) field-driven Gaussian training method. It enables fine-grained motion modeling through learnable per-Gaussian transformations. Our method leads to high-fidelity novel view synthesis of dynamic scenes while accurately preserving temporal consistency and object motion. Experiments on both synthetic and real-world datasets demonstrate that our method significantly outperforms state-of-the-art approaches designed for the cases of static environments, multiple images, and/or known poses. Our project page is available at https://colin-de.github.io/DynSUP/.
<div id='section'>Paperid: <span id='pid'>1330, <a href='https://arxiv.org/pdf/2412.00526.pdf' target='_blank'>https://arxiv.org/pdf/2412.00526.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Matyas Bohacek, Hany Farid
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.00526">Human Action CLIPs: Detecting AI-generated Human Motion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>AI-generated video generation continues its journey through the uncanny valley to produce content that is increasingly perceptually indistinguishable from reality. To better protect individuals, organizations, and societies from its malicious applications, we describe an effective and robust technique for distinguishing real from AI-generated human motion using multi-modal semantic embeddings. Our method is robust to the types of laundering that typically confound more low- to mid-level approaches, including resolution and compression attacks. This method is evaluated against DeepAction, a custom-built, open-sourced dataset of video clips with human actions generated by seven text-to-video AI models and matching real footage. The dataset is available under an academic license at https://www.huggingface.co/datasets/faridlab/deepaction_v1.
<div id='section'>Paperid: <span id='pid'>1331, <a href='https://arxiv.org/pdf/2411.17917.pdf' target='_blank'>https://arxiv.org/pdf/2411.17917.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Boqi Li, Haojie Zhu, Henry X. Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.17917">DECODE: Domain-aware Continual Domain Expansion for Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion prediction is critical for autonomous vehicles to effectively navigate complex environments and accurately anticipate the behaviors of other traffic participants. As autonomous driving continues to evolve, the need to assimilate new and varied driving scenarios necessitates frequent model updates through retraining. To address these demands, we introduce DECODE, a novel continual learning framework that begins with a pre-trained generalized model and incrementally develops specialized models for distinct domains. Unlike existing continual learning approaches that attempt to develop a unified model capable of generalizing across diverse scenarios, DECODE uniquely balances specialization with generalization, dynamically adjusting to real-time demands. The proposed framework leverages a hypernetwork to generate model parameters, significantly reducing storage requirements, and incorporates a normalizing flow mechanism for real-time model selection based on likelihood estimation. Furthermore, DECODE merges outputs from the most relevant specialized and generalized models using deep Bayesian uncertainty estimation techniques. This integration ensures optimal performance in familiar conditions while maintaining robustness in unfamiliar scenarios. Extensive evaluations confirm the effectiveness of the framework, achieving a notably low forgetting rate of 0.044 and an average minADE of 0.584 m, significantly surpassing traditional learning strategies and demonstrating adaptability across a wide range of driving conditions.
<div id='section'>Paperid: <span id='pid'>1332, <a href='https://arxiv.org/pdf/2411.17335.pdf' target='_blank'>https://arxiv.org/pdf/2411.17335.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zeyu Ling, Bo Han, Shiyang Li, Jikang Cheng, Hongdeng Shen, Changqing Zou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.17335">VersatileMotion: A Unified Framework for Motion Synthesis and Comprehension</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models (LLMs) are, by design, inherently capable of multi-task learning: through a unified next-token prediction paradigm, they can naturally address a wide variety of downstream tasks. Prior work in the motion domain has demonstrated some generality by adapting LLMs via a Motion Tokenizer coupled with an autoregressive Transformer to generate and understand human motion. However, this generality remains limited in scope and yields only modest performance gains. We introduce VersatileMotion, a unified multimodal motion LLM that combines a novel motion tokenizer, integrating VQ-VAE with flow matching, and an autoregressive transformer backbone to seamlessly support at least nine distinct motion-related tasks. VersatileMotion is the first method to handle single-agent and multi-agent motions in a single framework and enable cross-modal conversion between motion, text, music, and speech, achieving state-of-the-art performance on seven of these tasks. Each sequence in MotionHub may include one or more of the following annotations: natural-language captions, music or audio clips, speech transcripts, and multi-agent interaction data. To facilitate evaluation, we define and release benchmark splits covering nine core tasks. Extensive experiments demonstrate the superior performance, versatility, and potential of VersatileMotion as a foundational model for future understanding and generation of motion.
<div id='section'>Paperid: <span id='pid'>1333, <a href='https://arxiv.org/pdf/2411.10582.pdf' target='_blank'>https://arxiv.org/pdf/2411.10582.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jaewoo Heo, Kuan-Chieh Wang, Karen Liu, Serena Yeung-Levy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.10582">Motion Diffusion-Guided 3D Global HMR from a Dynamic Camera</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion capture technologies have transformed numerous fields, from the film and gaming industries to sports science and healthcare, by providing a tool to capture and analyze human movement in great detail. The holy grail in the topic of monocular global human mesh and motion reconstruction (GHMR) is to achieve accuracy on par with traditional multi-view capture on any monocular videos captured with a dynamic camera, in-the-wild. This is a challenging task as the monocular input has inherent depth ambiguity, and the moving camera adds additional complexity as the rendered human motion is now a product of both human and camera movement. Not accounting for this confusion, existing GHMR methods often output motions that are unrealistic, e.g. unaccounted root translation of the human causes foot sliding. We present DiffOpt, a novel 3D global HMR method using Diffusion Optimization. Our key insight is that recent advances in human motion generation, such as the motion diffusion model (MDM), contain a strong prior of coherent human motion. The core of our method is to optimize the initial motion reconstruction using the MDM prior. This step can lead to more globally coherent human motion. Our optimization jointly optimizes the motion prior loss and reprojection loss to correctly disentangle the human and camera motions. We validate DiffOpt with video sequences from the Electromagnetic Database of Global 3D Human Pose and Shape in the Wild (EMDB) and Egobody, and demonstrate superior global human motion recovery capability over other state-of-the-art global HMR methods most prominently in long video settings.
<div id='section'>Paperid: <span id='pid'>1334, <a href='https://arxiv.org/pdf/2411.08409.pdf' target='_blank'>https://arxiv.org/pdf/2411.08409.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Franz Franco Gallo, Hui-Yin Wu, Lucile Sassatelli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.08409">DiVR: incorporating context from diverse VR scenes for human trajectory prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Virtual environments provide a rich and controlled setting for collecting detailed data on human behavior, offering unique opportunities for predicting human trajectories in dynamic scenes. However, most existing approaches have overlooked the potential of these environments, focusing instead on static contexts without considering userspecific factors. Employing the CREATTIVE3D dataset, our work models trajectories recorded in virtual reality (VR) scenes for diverse situations including road-crossing tasks with user interactions and simulated visual impairments. We propose Diverse Context VR Human Motion Prediction (DiVR), a cross-modal transformer based on the Perceiver architecture that integrates both static and dynamic scene context using a heterogeneous graph convolution network. We conduct extensive experiments comparing DiVR against existing architectures including MLP, LSTM, and transformers with gaze and point cloud context. Additionally, we also stress test our model's generalizability across different users, tasks, and scenes. Results show that DiVR achieves higher accuracy and adaptability compared to other models and to static graphs. This work highlights the advantages of using VR datasets for context-aware human trajectory modeling, with potential applications in enhancing user experiences in the metaverse. Our source code is publicly available at https://gitlab.inria.fr/ffrancog/creattive3d-divr-model.
<div id='section'>Paperid: <span id='pid'>1335, <a href='https://arxiv.org/pdf/2411.06459.pdf' target='_blank'>https://arxiv.org/pdf/2411.06459.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nian Liu, Libin Liu, Zilong Zhang, Zi Wang, Hongzhao Xie, Tengyu Liu, Xinyi Tong, Yaodong Yang, Zhaofeng He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.06459">Learning Uniformly Distributed Embedding Clusters of Stylistic Skills for Physically Simulated Characters</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning natural and diverse behaviors from human motion datasets remains challenging in physics-based character control. Existing conditional adversarial models often suffer from tight and biased embedding distributions where embeddings from the same motion are closely grouped in a small area and shorter motions occupy even less space. Our empirical observations indicate this limits the representational capacity and diversity under each skill. An ideal latent space should be maximally packed by all motion's embedding clusters. In this paper, we propose a skill-conditioned controller that learns diverse skills with expressive variations. Our approach leverages the Neural Collapse phenomenon, a natural outcome of the classification-based encoder, to uniformly distributed cluster centers. We additionally propose a novel Embedding Expansion technique to form stylistic embedding clusters for diverse skills that are uniformly distributed on a hypersphere, maximizing the representational area occupied by each skill and minimizing unmapped regions. This maximally packed and uniformly distributed embedding space ensures that embeddings within the same cluster generate behaviors conforming to the characteristics of the corresponding motion clips, yet exhibiting noticeable variations within each cluster. Compared to existing methods, our controller not only generates high-quality, diverse motions covering the entire dataset but also achieves superior controllability, motion coverage, and diversity under each skill. Both qualitative and quantitative results confirm these traits, enabling our controller to be applied to a wide range of downstream tasks and serving as a cornerstone for diverse applications.
<div id='section'>Paperid: <span id='pid'>1336, <a href='https://arxiv.org/pdf/2411.02099.pdf' target='_blank'>https://arxiv.org/pdf/2411.02099.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Idris Zakariyya, Linda Tran, Kaushik Bhargav Sivangi, Paul Henderson, Fani Deligianni
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.02099">Differentially Private Integrated Decision Gradients (IDG-DP) for Radar-based Human Activity Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion analysis offers significant potential for healthcare monitoring and early detection of diseases. The advent of radar-based sensing systems has captured the spotlight for they are able to operate without physical contact and they can integrate with pre-existing Wi-Fi networks. They are also seen as less privacy-invasive compared to camera-based systems. However, recent research has shown high accuracy in recognizing subjects or gender from radar gait patterns, raising privacy concerns. This study addresses these issues by investigating privacy vulnerabilities in radar-based Human Activity Recognition (HAR) systems and proposing a novel method for privacy preservation using Differential Privacy (DP) driven by attributions derived with Integrated Decision Gradient (IDG) algorithm. We investigate Black-box Membership Inference Attack (MIA) Models in HAR settings across various levels of attacker-accessible information. We extensively evaluated the effectiveness of the proposed IDG-DP method by designing a CNN-based HAR model and rigorously assessing its resilience against MIAs. Experimental results demonstrate the potential of IDG-DP in mitigating privacy attacks while maintaining utility across all settings, particularly excelling against label-only and shadow model black-box MIA attacks. This work represents a crucial step towards balancing the need for effective radar-based HAR with robust privacy protection in healthcare environments.
<div id='section'>Paperid: <span id='pid'>1337, <a href='https://arxiv.org/pdf/2410.09505.pdf' target='_blank'>https://arxiv.org/pdf/2410.09505.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoran Wang, Yaoru Sun, Zeshen Tang, Haibo Shi, Chenyuan Jiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.09505">HG2P: Hippocampus-inspired High-reward Graph and Model-Free Q-Gradient Penalty for Path Planning and Motion Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Goal-conditioned hierarchical reinforcement learning (HRL) decomposes complex reaching tasks into a sequence of simple subgoal-conditioned tasks, showing significant promise for addressing long-horizon planning in large-scale environments. This paper bridges the goal-conditioned HRL based on graph-based planning to brain mechanisms, proposing a hippocampus-striatum-like dual-controller hypothesis. Inspired by the brain mechanisms of organisms (i.e., the high-reward preferences observed in hippocampal replay) and instance-based theory, we propose a high-return sampling strategy for constructing memory graphs, improving sample efficiency. Additionally, we derive a model-free lower-level Q-function gradient penalty to resolve the model dependency issues present in prior work, improving the generalization of Lipschitz constraints in applications. Finally, we integrate these two extensions, High-reward Graph and model-free Gradient Penalty (HG2P), into the state-of-the-art framework ACLG, proposing a novel goal-conditioned HRL framework, HG2P+ACLG. Experimentally, the results demonstrate that our method outperforms state-of-the-art goal-conditioned HRL algorithms on a variety of long-horizon navigation tasks and robotic manipulation tasks.
<div id='section'>Paperid: <span id='pid'>1338, <a href='https://arxiv.org/pdf/2410.07543.pdf' target='_blank'>https://arxiv.org/pdf/2410.07543.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weicheng Gao, Xiaodong Qu, Xiaopeng Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.07543">Generalization Ability Analysis of Through-the-Wall Radar Human Activity Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Through-the-Wall radar (TWR) human activity recognition (HAR) is a technology that uses low-frequency ultra-wideband (UWB) signal to detect and analyze indoor human motion. However, the high dependence of existing end-to-end recognition models on the distribution of TWR training data makes it difficult to achieve good generalization across different indoor testers. In this regard, the generalization ability of TWR HAR is analyzed in this paper. In detail, an end-to-end linear neural network method for TWR HAR and its generalization error bound are first discussed. Second, a micro-Doppler corner representation method and the change of the generalization error before and after dimension reduction are presented. The appropriateness of the theoretical generalization errors is proved through numerical simulations and experiments. The results demonstrate that feature dimension reduction is effective in allowing recognition models to generalize across different indoor testers.
<div id='section'>Paperid: <span id='pid'>1339, <a href='https://arxiv.org/pdf/2410.06327.pdf' target='_blank'>https://arxiv.org/pdf/2410.06327.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rajmund Nagy, Hendric Voss, Youngwoo Yoon, Taras Kucherenko, Teodor Nikolov, Thanh Hoang-Minh, Rachel McDonnell, Stefan Kopp, Michael Neff, Gustav Eje Henter
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.06327">Towards a GENEA Leaderboard -- an Extended, Living Benchmark for Evaluating and Advancing Conversational Motion Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current evaluation practices in speech-driven gesture generation lack standardisation and focus on aspects that are easy to measure over aspects that actually matter. This leads to a situation where it is impossible to know what is the state of the art, or to know which method works better for which purpose when comparing two publications. In this position paper, we review and give details on issues with existing gesture-generation evaluation, and present a novel proposal for remedying them. Specifically, we announce an upcoming living leaderboard to benchmark progress in conversational motion synthesis. Unlike earlier gesture-generation challenges, the leaderboard will be updated with large-scale user studies of new gesture-generation systems multiple times per year, and systems on the leaderboard can be submitted to any publication venue that their authors prefer. By evolving the leaderboard evaluation data and tasks over time, the effort can keep driving progress towards the most important end goals identified by the community. We actively seek community involvement across the entire evaluation pipeline: from data and tasks for the evaluation, via tooling, to the systems evaluated. In other words, our proposal will not only make it easier for researchers to perform good evaluations, but their collective input and contributions will also help drive the future of gesture-generation research.
<div id='section'>Paperid: <span id='pid'>1340, <a href='https://arxiv.org/pdf/2409.19638.pdf' target='_blank'>https://arxiv.org/pdf/2409.19638.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chaohui Xu, Si Wang, Chip-Hong Chang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.19638">BadHMP: Backdoor Attack against Human Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Precise future human motion prediction over sub-second horizons from past observations is crucial for various safety-critical applications. To date, only a few studies have examined the vulnerability of skeleton-based neural networks to evasion and backdoor attacks. In this paper, we propose BadHMP, a novel backdoor attack that targets specifically human motion prediction tasks. Our approach involves generating poisoned training samples by embedding a localized backdoor trigger in one limb of the skeleton, causing selected joints to follow predefined motion in historical time steps. Subsequently, the future sequences are globally modified that all the joints move following the target trajectories. Our carefully designed backdoor triggers and targets guarantee the smoothness and naturalness of the poisoned samples, making them stealthy enough to evade detection by the model trainer while keeping the poisoned model unobtrusive in terms of prediction fidelity to untainted sequences. The target sequences can be successfully activated by the designed input sequences even with a low poisoned sample injection ratio. Experimental results on two datasets (Human3.6M and CMU-Mocap) and two network architectures (LTD and HRI) demonstrate the high-fidelity, effectiveness, and stealthiness of BadHMP. Robustness of our attack against fine-tuning defense is also verified.
<div id='section'>Paperid: <span id='pid'>1341, <a href='https://arxiv.org/pdf/2409.09777.pdf' target='_blank'>https://arxiv.org/pdf/2409.09777.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haisheng Su, Wei Wu, Junchi Yan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.09777">DiFSD: Ego-Centric Fully Sparse Paradigm with Uncertainty Denoising and Iterative Refinement for Efficient End-to-End Self-Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current end-to-end autonomous driving methods resort to unifying modular designs for various tasks (e.g. perception, prediction and planning). Although optimized in a planning-oriented spirit with a fully differentiable framework, existing end-to-end driving systems without ego-centric designs still suffer from unsatisfactory performance and inferior efficiency, owing to the rasterized scene representation learning and redundant information transmission. In this paper, we revisit the human driving behavior and propose an ego-centric fully sparse paradigm, named DiFSD, for end-to-end self-driving. Specifically, DiFSD mainly consists of sparse perception, hierarchical interaction and iterative motion planner. The sparse perception module performs detection, tracking and online mapping based on sparse representation of the driving scene. The hierarchical interaction module aims to select the Closest In-Path Vehicle / Stationary (CIPV / CIPS) from coarse to fine, benefiting from an additional geometric prior. As for the iterative motion planner, both selected interactive agents and ego-vehicle are considered for joint motion prediction, where the output multi-modal ego-trajectories are optimized in an iterative fashion. Besides, both position-level motion diffusion and trajectory-level planning denoising are introduced for uncertainty modeling, thus facilitating the training stability and convergence of the whole framework. Extensive experiments conducted on nuScenes and Bench2Drive datasets demonstrate the superior planning performance and great efficiency of DiFSD.
<div id='section'>Paperid: <span id='pid'>1342, <a href='https://arxiv.org/pdf/2409.01522.pdf' target='_blank'>https://arxiv.org/pdf/2409.01522.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifei Yang, Zikai Huang, Chenshu Xu, Shengfeng He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.01522">Lagrangian Motion Fields for Long-term Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Long-term motion generation is a challenging task that requires producing coherent and realistic sequences over extended durations. Current methods primarily rely on framewise motion representations, which capture only static spatial details and overlook temporal dynamics. This approach leads to significant redundancy across the temporal dimension, complicating the generation of effective long-term motion. To overcome these limitations, we introduce the novel concept of Lagrangian Motion Fields, specifically designed for long-term motion generation. By treating each joint as a Lagrangian particle with uniform velocity over short intervals, our approach condenses motion representations into a series of "supermotions" (analogous to superpixels). This method seamlessly integrates static spatial information with interpretable temporal dynamics, transcending the limitations of existing network architectures and motion sequence content types. Our solution is versatile and lightweight, eliminating the need for neural network preprocessing. Our approach excels in tasks such as long-term music-to-dance generation and text-to-motion generation, offering enhanced efficiency, superior generation quality, and greater diversity compared to existing methods. Additionally, the adaptability of Lagrangian Motion Fields extends to applications like infinite motion looping and fine-grained controlled motion generation, highlighting its broad utility. Video demonstrations are available at https://plyfager.github.io/LaMoG.
<div id='section'>Paperid: <span id='pid'>1343, <a href='https://arxiv.org/pdf/2408.17168.pdf' target='_blank'>https://arxiv.org/pdf/2408.17168.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhen Fan, Peng Dai, Zhuo Su, Xu Gao, Zheng Lv, Jiarui Zhang, Tianyuan Du, Guidong Wang, Yang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.17168">EMHI: A Multimodal Egocentric Human Motion Dataset with HMD and Body-Worn IMUs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Egocentric human pose estimation (HPE) using wearable sensors is essential for VR/AR applications. Most methods rely solely on either egocentric-view images or sparse Inertial Measurement Unit (IMU) signals, leading to inaccuracies due to self-occlusion in images or the sparseness and drift of inertial sensors. Most importantly, the lack of real-world datasets containing both modalities is a major obstacle to progress in this field. To overcome the barrier, we propose EMHI, a multimodal \textbf{E}gocentric human \textbf{M}otion dataset with \textbf{H}ead-Mounted Display (HMD) and body-worn \textbf{I}MUs, with all data collected under the real VR product suite. Specifically, EMHI provides synchronized stereo images from downward-sloping cameras on the headset and IMU data from body-worn sensors, along with pose annotations in SMPL format. This dataset consists of 885 sequences captured by 58 subjects performing 39 actions, totaling about 28.5 hours of recording. We evaluate the annotations by comparing them with optical marker-based SMPL fitting results. To substantiate the reliability of our dataset, we introduce MEPoser, a new baseline method for multimodal egocentric HPE, which employs a multimodal fusion encoder, temporal feature encoder, and MLP-based regression heads. The experiments on EMHI show that MEPoser outperforms existing single-modal methods and demonstrates the value of our dataset in solving the problem of egocentric HPE. We believe the release of EMHI and the method could advance the research of egocentric HPE and expedite the practical implementation of this technology in VR/AR products.
<div id='section'>Paperid: <span id='pid'>1344, <a href='https://arxiv.org/pdf/2408.12077.pdf' target='_blank'>https://arxiv.org/pdf/2408.12077.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaopeng Yang, Weicheng Gao, Xiaodong Qu, Zeyu Ma, Hao Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.12077">Through-the-Wall Radar Human Activity Micro-Doppler Signature Representation Method Based on Joint Boulic-Sinusoidal Pendulum Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the help of micro-Doppler signature, ultra-wideband (UWB) through-the-wall radar (TWR) enables the reconstruction of range and velocity information of limb nodes to accurately identify indoor human activities. However, existing methods are usually trained and validated directly using range-time maps (RTM) and Doppler-time maps (DTM), which have high feature redundancy and poor generalization ability. In order to solve this problem, this paper proposes a human activity micro-Doppler signature representation method based on joint Boulic-sinusoidal pendulum motion model. In detail, this paper presents a simplified joint Boulic-sinusoidal pendulum human motion model by taking head, torso, both hands and feet into consideration improved from Boulic-Thalmann kinematic model. The paper also calculates the minimum number of key points needed to describe the Doppler and micro-Doppler information sufficiently. Both numerical simulations and experiments are conducted to verify the effectiveness. The results demonstrate that the proposed number of key points of micro-Doppler signature can precisely represent the indoor human limb node motion characteristics, and substantially improve the generalization capability of the existing methods for different testers.
<div id='section'>Paperid: <span id='pid'>1345, <a href='https://arxiv.org/pdf/2408.00712.pdf' target='_blank'>https://arxiv.org/pdf/2408.00712.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nikos Athanasiou, AlpÃ¡r Cseke, Markos Diomataris, Michael J. Black, GÃ¼l Varol
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.00712">MotionFix: Text-Driven 3D Human Motion Editing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The focus of this paper is on 3D motion editing. Given a 3D human motion and a textual description of the desired modification, our goal is to generate an edited motion as described by the text. The key challenges include the scarcity of training data and the need to design a model that accurately edits the source motion. In this paper, we address both challenges. We propose a methodology to semi-automatically collect a dataset of triplets comprising (i) a source motion, (ii) a target motion, and (iii) an edit text, introducing the new MotionFix dataset. Access to this data allows us to train a conditional diffusion model, TMED, that takes both the source motion and the edit text as input. We develop several baselines to evaluate our model, comparing it against models trained solely on text-motion pair datasets, and demonstrate the superior performance of our model trained on triplets. We also introduce new retrieval-based metrics for motion editing, establishing a benchmark on the evaluation set of MotionFix. Our results are promising, paving the way for further research in fine-grained motion generation. Code, models, and data are available at https://motionfix.is.tue.mpg.de/ .
<div id='section'>Paperid: <span id='pid'>1346, <a href='https://arxiv.org/pdf/2407.18159.pdf' target='_blank'>https://arxiv.org/pdf/2407.18159.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Max Emerick, Stacy Patterson, Bassam Bamieh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.18159">Optimal Assignment and Motion Control in Two-Class Continuum Swarms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We consider optimal swarm control problems where two different classes of agents are present. Continuum idealizations of large-scale swarms are used where the dynamics describe the evolution of the spatially-distributed densities of each agent class. The problem formulation we adopt is motivated by applications where agents of one class are assigned to agents of the other class, which we refer to as demand and resource agents respectively. Assignments have costs related to the distances between mutually assigned agents, and the overall cost of an assignment is quantified by a Wasserstein distance between the densities of the two agent classes. When agents can move, the assignment cost can decrease at the expense of a physical motion cost, and this tradeoff sets up a nonlinear, infinite-dimensional optimal control problem. We show that in one spatial dimension, this problem can be converted to an infinite-dimensional, but decoupled, linear-quadratic (LQ) tracking problem when expressed in terms of the respective quantile functions. Solutions are given in the general one-dimensional case, as well as in the special cases of constant and periodically time-varying demands.
<div id='section'>Paperid: <span id='pid'>1347, <a href='https://arxiv.org/pdf/2407.10547.pdf' target='_blank'>https://arxiv.org/pdf/2407.10547.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andrea Eirale, Matteo Leonetti, Marcello Chiaberge
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.10547">Learning Social Cost Functions for Human-Aware Path Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Achieving social acceptance is one of the main goals of Social Robotic Navigation. Despite this topic has received increasing interest in recent years, most of the research has focused on driving the robotic agent along obstacle-free trajectories, planning around estimates of future human motion to respect personal distances and optimize navigation. However, social interactions in everyday life are also dictated by norms that do not strictly depend on movement, such as when standing at the end of a queue rather than cutting it. In this paper, we propose a novel method to recognize common social scenarios and modify a traditional planner's cost function to adapt to them. This solution enables the robot to carry out different social navigation behaviors that would not arise otherwise, maintaining the robustness of traditional navigation. Our approach allows the robot to learn different social norms with a single learned model, rather than having different modules for each task. As a proof of concept, we consider the tasks of queuing and respect interaction spaces of groups of people talking to one another, but the method can be extended to other human activities that do not involve motion.
<div id='section'>Paperid: <span id='pid'>1348, <a href='https://arxiv.org/pdf/2406.19798.pdf' target='_blank'>https://arxiv.org/pdf/2406.19798.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vinicius Trentin, Juan Medina-Lee, Antonio ArtuÃ±edo, Jorge Villagra
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.19798">Integrating occlusion awareness in urban motion prediction for enhanced autonomous vehicle navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion prediction is a key factor towards the full deployment of autonomous vehicles. It is fundamental in order to ensure safety while navigating through highly interactive and complex scenarios. Lack of visibility due to an obstructed view or sensor range poses a great safety issue for autonomous vehicles. The inclusion of occlusion in interaction-aware approaches is not very well explored in the literature. In this work, the MultIAMP framework, which produces multimodal probabilistic outputs from the integration of a Dynamic Bayesian Network and Markov chains, is extended to tackle occlusions. The framework is evaluated with a state-of-the-art motion planner in two realistic use cases.
<div id='section'>Paperid: <span id='pid'>1349, <a href='https://arxiv.org/pdf/2406.16638.pdf' target='_blank'>https://arxiv.org/pdf/2406.16638.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohammad Belal, Taimur Hassan, Abdelfatah Ahmed, Ahmad Aljarah, Nael Alsheikh, Irfan Hussain
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.16638">Feature Fusion for Human Activity Recognition using Parameter-Optimized Multi-Stage Graph Convolutional Network and Transformer Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human activity recognition (HAR) is a crucial area of research that involves understanding human movements using computer and machine vision technology. Deep learning has emerged as a powerful tool for this task, with models such as Convolutional Neural Networks (CNNs) and Transformers being employed to capture various aspects of human motion. One of the key contributions of this work is the demonstration of the effectiveness of feature fusion in improving HAR accuracy by capturing spatial and temporal features, which has important implications for the development of more accurate and robust activity recognition systems. The study uses sensory data from HuGaDB, PKU-MMD, LARa, and TUG datasets. Two model, the PO-MS-GCN and a Transformer were trained and evaluated, with PO-MS-GCN outperforming state-of-the-art models. HuGaDB and TUG achieved high accuracies and f1-scores, while LARa and PKU-MMD had lower scores. Feature fusion improved results across datasets.
<div id='section'>Paperid: <span id='pid'>1350, <a href='https://arxiv.org/pdf/2406.10740.pdf' target='_blank'>https://arxiv.org/pdf/2406.10740.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhikai Zhang, Yitang Li, Haofeng Huang, Mingxian Lin, Li Yi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.10740">FreeMotion: MoCap-Free Human Motion Synthesis with Multimodal Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion synthesis is a fundamental task in computer animation. Despite recent progress in this field utilizing deep learning and motion capture data, existing methods are always limited to specific motion categories, environments, and styles. This poor generalizability can be partially attributed to the difficulty and expense of collecting large-scale and high-quality motion data. At the same time, foundation models trained with internet-scale image and text data have demonstrated surprising world knowledge and reasoning ability for various downstream tasks. Utilizing these foundation models may help with human motion synthesis, which some recent works have superficially explored. However, these methods didn't fully unveil the foundation models' potential for this task and only support several simple actions and environments. In this paper, we for the first time, without any motion data, explore open-set human motion synthesis using natural language instructions as user control signals based on MLLMs across any motion task and environment. Our framework can be split into two stages: 1) sequential keyframe generation by utilizing MLLMs as a keyframe designer and animator; 2) motion filling between keyframes through interpolation and motion tracking. Our method can achieve general human motion synthesis for many downstream tasks. The promising results demonstrate the worth of mocap-free human motion synthesis aided by MLLMs and pave the way for future research.
<div id='section'>Paperid: <span id='pid'>1351, <a href='https://arxiv.org/pdf/2406.01952.pdf' target='_blank'>https://arxiv.org/pdf/2406.01952.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ricardo B. Grando, Raul Steinmetz, Victor A. Kich, Alisson H. Kolling, Pablo M. Furik, Junior C. de Jesus, Bruna V. Guterres, Daniel T. Gamarra, Rodrigo S. Guerra, Paulo L. J. Drews-Jr
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.01952">Improving Generalization in Aerial and Terrestrial Mobile Robots Control Through Delayed Policy Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep Reinforcement Learning (DRL) has emerged as a promising approach to enhancing motion control and decision-making through a wide range of robotic applications. While prior research has demonstrated the efficacy of DRL algorithms in facilitating autonomous mapless navigation for aerial and terrestrial mobile robots, these methods often grapple with poor generalization when faced with unknown tasks and environments. This paper explores the impact of the Delayed Policy Updates (DPU) technique on fostering generalization to new situations, and bolstering the overall performance of agents. Our analysis of DPU in aerial and terrestrial mobile robots reveals that this technique significantly curtails the lack of generalization and accelerates the learning process for agents, enhancing their efficiency across diverse tasks and unknown scenarios.
<div id='section'>Paperid: <span id='pid'>1352, <a href='https://arxiv.org/pdf/2406.00727.pdf' target='_blank'>https://arxiv.org/pdf/2406.00727.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Satoshi Yagi, Mitsunori Tada, Eiji Uchibe, Suguru Kanoga, Takamitsu Matsubara, Jun Morimoto
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.00727">Unsupervised Neural Motion Retargeting for Humanoid Teleoperation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study proposes an approach to human-to-humanoid teleoperation using GAN-based online motion retargeting, which obviates the need for the construction of pairwise datasets to identify the relationship between the human and the humanoid kinematics. Consequently, it can be anticipated that our proposed teleoperation system will reduce the complexity and setup requirements typically associated with humanoid controllers, thereby facilitating the development of more accessible and intuitive teleoperation systems for users without robotics knowledge. The experiments demonstrated the efficacy of the proposed method in retargeting a range of upper-body human motions to humanoid, including a body jab motion and a basketball shoot motion. Moreover, the human-in-the-loop teleoperation performance was evaluated by measuring the end-effector position errors between the human and the retargeted humanoid motions. The results demonstrated that the error was comparable to those of conventional motion retargeting methods that require pairwise motion datasets. Finally, a box pick-and-place task was conducted to demonstrate the usability of the developed humanoid teleoperation system.
<div id='section'>Paperid: <span id='pid'>1353, <a href='https://arxiv.org/pdf/2404.19283.pdf' target='_blank'>https://arxiv.org/pdf/2404.19283.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Marlon Steiner, Marvin Klemp, Christoph Stiller
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.19283">MAP-Former: Multi-Agent-Pair Gaussian Joint Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>There is a gap in risk assessment of trajectories between the trajectory information coming from a traffic motion prediction module and what is actually needed. Closing this gap necessitates advancements in prediction beyond current practices. Existing prediction models yield joint predictions of agents' future trajectories with uncertainty weights or marginal Gaussian probability density functions (PDFs) for single agents. Although, these methods achieve high accurate trajectory predictions, they only provide little or no information about the dependencies of interacting agents. Since traffic is a process of highly interdependent agents, whose actions directly influence their mutual behavior, the existing methods are not sufficient to reliably assess the risk of future trajectories. This paper addresses that gap by introducing a novel approach to motion prediction, focusing on predicting agent-pair covariance matrices in a ``scene-centric'' manner, which can then be used to model Gaussian joint PDFs for all agent-pairs in a scene. We propose a model capable of predicting those agent-pair covariance matrices, leveraging an enhanced awareness of interactions. Utilizing the prediction results of our model, this work forms the foundation for comprehensive risk assessment with statistically based methods for analyzing agents' relations by their joint PDFs.
<div id='section'>Paperid: <span id='pid'>1354, <a href='https://arxiv.org/pdf/2404.12886.pdf' target='_blank'>https://arxiv.org/pdf/2404.12886.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zeyu Ling, Bo Han, Yongkang Wongkan, Han Lin, Mohan Kankanhalli, Weidong Geng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.12886">MCM: Multi-condition Motion Synthesis Framework</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Conditional human motion synthesis (HMS) aims to generate human motion sequences that conform to specific conditions. Text and audio represent the two predominant modalities employed as HMS control conditions. While existing research has primarily focused on single conditions, the multi-condition human motion synthesis remains underexplored. In this study, we propose a multi-condition HMS framework, termed MCM, based on a dual-branch structure composed of a main branch and a control branch. This framework effectively extends the applicability of the diffusion model, which is initially predicated solely on textual conditions, to auditory conditions. This extension encompasses both music-to-dance and co-speech HMS while preserving the intrinsic quality of motion and the capabilities for semantic association inherent in the original model. Furthermore, we propose the implementation of a Transformer-based diffusion model, designated as MWNet, as the main branch. This model adeptly apprehends the spatial intricacies and inter-joint correlations inherent in motion sequences, facilitated by the integration of multi-wise self-attention modules. Extensive experiments show that our method achieves competitive results in single-condition and multi-condition HMS tasks.
<div id='section'>Paperid: <span id='pid'>1355, <a href='https://arxiv.org/pdf/2404.10240.pdf' target='_blank'>https://arxiv.org/pdf/2404.10240.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fan Zhang, Jinfeng Chen, Yu Hu, Zhiqiang Gao, Ge Lv, Qin Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.10240">Disturbance Rejection-Guarded Learning for Vibration Suppression of Two-Inertia Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Model uncertainty presents significant challenges in vibration suppression of multi-inertia systems, as these systems often rely on inaccurate nominal mathematical models due to system identification errors or unmodeled dynamics. An observer, such as an extended state observer (ESO), can estimate the discrepancy between the inaccurate nominal model and the true model, thus improving control performance via disturbance rejection. The conventional observer design is memoryless in the sense that once its estimated disturbance is obtained and sent to the controller, the datum is discarded. In this research, we propose a seamless integration of ESO and machine learning. On one hand, the machine learning model attempts to model the disturbance. With the assistance of prior information about the disturbance, the observer is expected to achieve faster convergence in disturbance estimation. On the other hand, machine learning benefits from an additional assurance layer provided by the ESO, as any imperfections in the machine learning model can be compensated for by the ESO. We validated the effectiveness of this novel learning-for-control paradigm through simulation and physical tests on two-inertial motion control systems used for vibration studies.
<div id='section'>Paperid: <span id='pid'>1356, <a href='https://arxiv.org/pdf/2404.05578.pdf' target='_blank'>https://arxiv.org/pdf/2404.05578.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mahsa Ehsanpour, Ian Reid, Hamid Rezatofighi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.05578">Social-MAE: Social Masked Autoencoder for Multi-person Motion Representation Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>For a complete comprehension of multi-person scenes, it is essential to go beyond basic tasks like detection and tracking. Higher-level tasks, such as understanding the interactions and social activities among individuals, are also crucial. Progress towards models that can fully understand scenes involving multiple people is hindered by a lack of sufficient annotated data for such high-level tasks. To address this challenge, we introduce Social-MAE, a simple yet effective transformer-based masked autoencoder framework for multi-person human motion data. The framework uses masked modeling to pre-train the encoder to reconstruct masked human joint trajectories, enabling it to learn generalizable and data efficient representations of motion in human crowded scenes. Social-MAE comprises a transformer as the MAE encoder and a lighter-weight transformer as the MAE decoder which operates on multi-person joints' trajectory in the frequency domain. After the reconstruction task, the MAE decoder is replaced with a task-specific decoder and the model is fine-tuned end-to-end for a variety of high-level social tasks. Our proposed model combined with our pre-training approach achieves the state-of-the-art results on various high-level social tasks, including multi-person pose forecasting, social grouping, and social action understanding. These improvements are demonstrated across four popular multi-person datasets encompassing both human 2D and 3D body pose.
<div id='section'>Paperid: <span id='pid'>1357, <a href='https://arxiv.org/pdf/2404.05404.pdf' target='_blank'>https://arxiv.org/pdf/2404.05404.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Meng Yuan, Ye Wang, Chris Manzie, Zhezhuang Xu, Tianyou Chai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.05404">Contouring Error Bounded Control for Biaxial Switched Linear Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Biaxial motion control systems are used extensively in manufacturing and printing industries. To improve throughput and reduce machine cost, lightweight materials are being proposed in structural components but may result in higher flexibility in the machine links. This flexibility is often position dependent and compromises precision of the end effector of the machine. To address the need for improved contouring accuracy in industrial machines with position-dependent structural flexibility, this paper introduces a novel contouring error-bounded control algorithm for biaxial switched linear systems. The proposed algorithm utilizes model predictive control to guarantee the satisfaction of state, input, and contouring error constraints for any admissible mode switching. In this paper, the switching signal remains unknown to the controller, although information about the minimum time the system is expected to stay in a specific mode is considered to be available. The proposed algorithm has the property of recursive feasibility and ensures the stability of the closed-loop system. The effectiveness of the proposed method is demonstrated by applying it to a high-fidelity simulation of a dual-drive industrial laser machine. The results show that the contouring error is successfully bounded within the given tolerance.
<div id='section'>Paperid: <span id='pid'>1358, <a href='https://arxiv.org/pdf/2404.04430.pdf' target='_blank'>https://arxiv.org/pdf/2404.04430.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yufei Zhang, Jeffrey O. Kephart, Zijun Cui, Qiang Ji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.04430">PhysPT: Physics-aware Pretrained Transformer for Estimating Human Dynamics from Monocular Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While current methods have shown promising progress on estimating 3D human motion from monocular videos, their motion estimates are often physically unrealistic because they mainly consider kinematics. In this paper, we introduce Physics-aware Pretrained Transformer (PhysPT), which improves kinematics-based motion estimates and infers motion forces. PhysPT exploits a Transformer encoder-decoder backbone to effectively learn human dynamics in a self-supervised manner. Moreover, it incorporates physics principles governing human motion. Specifically, we build a physics-based body representation and contact force model. We leverage them to impose novel physics-inspired training losses (i.e., force loss, contact loss, and Euler-Lagrange loss), enabling PhysPT to capture physical properties of the human body and the forces it experiences. Experiments demonstrate that, once trained, PhysPT can be directly applied to kinematics-based estimates to significantly enhance their physical plausibility and generate favourable motion forces. Furthermore, we show that these physically meaningful quantities translate into improved accuracy of an important downstream task: human action recognition.
<div id='section'>Paperid: <span id='pid'>1359, <a href='https://arxiv.org/pdf/2404.04419.pdf' target='_blank'>https://arxiv.org/pdf/2404.04419.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ehsan Nasiri, Long Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.04419">Hybrid Force Motion Control with Estimated Surface Normal for Manufacturing Applications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper proposes a hybrid force-motion framework that utilizes real-time surface normal updates. The surface normal is estimated via a novel method that leverages force sensing measurements and velocity commands to compensate the friction bias. This approach is critical for robust execution of precision force-controlled tasks in manufacturing, such as thermoplastic tape replacement that traces surfaces or paths on a workpiece subject to uncertainties deviated from the model. We formulated the proposed method and implemented the framework in ROS2 environment. The approach was validated using kinematic simulations and a hardware platform. Specifically, we demonstrated the approach on a 7-DoF manipulator equipped with a force/torque sensor at the end-effector.
<div id='section'>Paperid: <span id='pid'>1360, <a href='https://arxiv.org/pdf/2403.17916.pdf' target='_blank'>https://arxiv.org/pdf/2403.17916.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zehao Wang, Yuping Wang, Zhuoyuan Wu, Hengbo Ma, Zhaowei Li, Hang Qiu, Jiachen Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.17916">CMP: Cooperative Motion Prediction with Multi-Agent Communication</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The confluence of the advancement of Autonomous Vehicles (AVs) and the maturity of Vehicle-to-Everything (V2X) communication has enabled the capability of cooperative connected and automated vehicles (CAVs). Building on top of cooperative perception, this paper explores the feasibility and effectiveness of cooperative motion prediction. Our method, CMP, takes LiDAR signals as model input to enhance tracking and prediction capabilities. Unlike previous work that focuses separately on either cooperative perception or motion prediction, our framework, to the best of our knowledge, is the first to address the unified problem where CAVs share information in both perception and prediction modules. Incorporated into our design is the unique capability to tolerate realistic V2X transmission delays, while dealing with bulky perception representations. We also propose a prediction aggregation module, which unifies the predictions obtained by different CAVs and generates the final prediction. Through extensive experiments and ablation studies on the OPV2V and V2V4Real datasets, we demonstrate the effectiveness of our method in cooperative perception, tracking, and motion prediction. In particular, CMP reduces the average prediction error by 12.3% compared with the strongest baseline. Our work marks a significant step forward in the cooperative capabilities of CAVs, showcasing enhanced performance in complex scenarios. More details can be found on the project website: https://cmp-cooperative-prediction.github.io.
<div id='section'>Paperid: <span id='pid'>1361, <a href='https://arxiv.org/pdf/2403.02075.pdf' target='_blank'>https://arxiv.org/pdf/2403.02075.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weiyi Lv, Yuhang Huang, Ning Zhang, Ruei-Sung Lin, Mei Han, Dan Zeng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.02075">DiffMOT: A Real-time Diffusion-based Multiple Object Tracker with Non-linear Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In Multiple Object Tracking, objects often exhibit non-linear motion of acceleration and deceleration, with irregular direction changes. Tacking-by-detection (TBD) trackers with Kalman Filter motion prediction work well in pedestrian-dominant scenarios but fall short in complex situations when multiple objects perform non-linear and diverse motion simultaneously. To tackle the complex non-linear motion, we propose a real-time diffusion-based MOT approach named DiffMOT. Specifically, for the motion predictor component, we propose a novel Decoupled Diffusion-based Motion Predictor (D$^2$MP). It models the entire distribution of various motion presented by the data as a whole. It also predicts an individual object's motion conditioning on an individual's historical motion information. Furthermore, it optimizes the diffusion process with much fewer sampling steps. As a MOT tracker, the DiffMOT is real-time at 22.7FPS, and also outperforms the state-of-the-art on DanceTrack and SportsMOT datasets with $62.3\%$ and $76.2\%$ in HOTA metrics, respectively. To the best of our knowledge, DiffMOT is the first to introduce a diffusion probabilistic model into the MOT to tackle non-linear motion prediction.
<div id='section'>Paperid: <span id='pid'>1362, <a href='https://arxiv.org/pdf/2403.00691.pdf' target='_blank'>https://arxiv.org/pdf/2403.00691.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kangning Yin, Shihao Zou, Yuxuan Ge, Zheng Tian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.00691">Tri-Modal Motion Retrieval by Learning a Joint Embedding Space</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Information retrieval is an ever-evolving and crucial research domain. The substantial demand for high-quality human motion data especially in online acquirement has led to a surge in human motion research works. Prior works have mainly concentrated on dual-modality learning, such as text and motion tasks, but three-modality learning has been rarely explored. Intuitively, an extra introduced modality can enrich a model's application scenario, and more importantly, an adequate choice of the extra modality can also act as an intermediary and enhance the alignment between the other two disparate modalities. In this work, we introduce LAVIMO (LAnguage-VIdeo-MOtion alignment), a novel framework for three-modality learning integrating human-centric videos as an additional modality, thereby effectively bridging the gap between text and motion. Moreover, our approach leverages a specially designed attention mechanism to foster enhanced alignment and synergistic effects among text, video, and motion modalities. Empirically, our results on the HumanML3D and KIT-ML datasets show that LAVIMO achieves state-of-the-art performance in various motion-related cross-modal retrieval tasks, including text-to-motion, motion-to-text, video-to-motion and motion-to-video.
<div id='section'>Paperid: <span id='pid'>1363, <a href='https://arxiv.org/pdf/2402.14227.pdf' target='_blank'>https://arxiv.org/pdf/2402.14227.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pauline Bourigault, Dongpo Xu, Danilo P. Mandic
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.14227">Quaternion recurrent neural network with real-time recurrent learning and maximum correntropy criterion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We develop a robust quaternion recurrent neural network (QRNN) for real-time processing of 3D and 4D data with outliers. This is achieved by combining the real-time recurrent learning (RTRL) algorithm and the maximum correntropy criterion (MCC) as a loss function. While both the mean square error and maximum correntropy criterion are viable cost functions, it is shown that the non-quadratic maximum correntropy loss function is less sensitive to outliers, making it suitable for applications with multidimensional noisy or uncertain data. Both algorithms are derived based on the novel generalised HR (GHR) calculus, which allows for the differentiation of real functions of quaternion variables and offers the product and chain rules, thus enabling elegant and compact derivations. Simulation results in the context of motion prediction of chest internal markers for lung cancer radiotherapy, which includes regular and irregular breathing sequences, support the analysis.
<div id='section'>Paperid: <span id='pid'>1364, <a href='https://arxiv.org/pdf/2402.12676.pdf' target='_blank'>https://arxiv.org/pdf/2402.12676.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nikolaos Smyrnakis, Tasos Karakostas, R. James Cotton
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.12676">Advancing Monocular Video-Based Gait Analysis Using Motion Imitation with Physics-Based Simulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Gait analysis from videos obtained from a smartphone would open up many clinical opportunities for detecting and quantifying gait impairments. However, existing approaches for estimating gait parameters from videos can produce physically implausible results. To overcome this, we train a policy using reinforcement learning to control a physics simulation of human movement to replicate the movement seen in video. This forces the inferred movements to be physically plausible, while improving the accuracy of the inferred step length and walking velocity.
<div id='section'>Paperid: <span id='pid'>1365, <a href='https://arxiv.org/pdf/2402.07594.pdf' target='_blank'>https://arxiv.org/pdf/2402.07594.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Patrick Seifner, Kostadin Cvejoski, Antonia KÃ¶rner, RamsÃ©s J. SÃ¡nchez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.07594">Zero-shot Imputation with Foundation Inference Models for Dynamical Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dynamical systems governed by ordinary differential equations (ODEs) serve as models for a vast number of natural and social phenomena. In this work, we offer a fresh perspective on the classical problem of imputing missing time series data, whose underlying dynamics are assumed to be determined by ODEs. Specifically, we revisit ideas from amortized inference and neural operators, and propose a novel supervised learning framework for zero-shot time series imputation, through parametric functions satisfying some (hidden) ODEs. Our proposal consists of two components. First, a broad probability distribution over the space of ODE solutions, observation times and noise mechanisms, with which we generate a large, synthetic dataset of (hidden) ODE solutions, along with their noisy and sparse observations. Second, a neural recognition model that is trained offline, to map the generated time series onto the spaces of initial conditions and time derivatives of the (hidden) ODE solutions, which we then integrate to impute the missing data. We empirically demonstrate that one and the same (pretrained) recognition model can perform zero-shot imputation across 63 distinct time series with missing values, each sampled from widely different dynamical systems. Likewise, we demonstrate that it can perform zero-shot imputation of missing high-dimensional data in 10 vastly different settings, spanning human motion, air quality, traffic and electricity studies, as well as Navier-Stokes simulations -- without requiring any fine-tuning. What is more, our proposal often outperforms state-of-the-art methods, which are trained on the target datasets.
  Our pretrained model, repository and tutorials are available online.
<div id='section'>Paperid: <span id='pid'>1366, <a href='https://arxiv.org/pdf/2402.03559.pdf' target='_blank'>https://arxiv.org/pdf/2402.03559.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jacob K Christopher, Stephen Baek, Ferdinando Fioretto
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.03559">Constrained Synthesis with Projected Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces an approach to endow generative diffusion processes the ability to satisfy and certify compliance with constraints and physical principles. The proposed method recast the traditional sampling process of generative diffusion models as a constrained optimization problem, steering the generated data distribution to remain within a specified region to ensure adherence to the given constraints. These capabilities are validated on applications featuring both convex and challenging, non-convex, constraints as well as ordinary differential equations, in domains spanning from synthesizing new materials with precise morphometric properties, generating physics-informed motion, optimizing paths in planning scenarios, and human motion synthesis.
<div id='section'>Paperid: <span id='pid'>1367, <a href='https://arxiv.org/pdf/2402.01566.pdf' target='_blank'>https://arxiv.org/pdf/2402.01566.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiawei Wang, Yuchen Zhang, Jiaxin Zou, Yan Zeng, Guoqiang Wei, Liping Yuan, Hang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.01566">Boximator: Generating Rich and Controllable Motions for Video Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating rich and controllable motion is a pivotal challenge in video synthesis. We propose Boximator, a new approach for fine-grained motion control. Boximator introduces two constraint types: hard box and soft box. Users select objects in the conditional frame using hard boxes and then use either type of boxes to roughly or rigorously define the object's position, shape, or motion path in future frames. Boximator functions as a plug-in for existing video diffusion models. Its training process preserves the base model's knowledge by freezing the original weights and training only the control module. To address training challenges, we introduce a novel self-tracking technique that greatly simplifies the learning of box-object correlations. Empirically, Boximator achieves state-of-the-art video quality (FVD) scores, improving on two base models, and further enhanced after incorporating box constraints. Its robust motion controllability is validated by drastic increases in the bounding box alignment metric. Human evaluation also shows that users favor Boximator generation results over the base model.
<div id='section'>Paperid: <span id='pid'>1368, <a href='https://arxiv.org/pdf/2401.14879.pdf' target='_blank'>https://arxiv.org/pdf/2401.14879.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Max Bastian Mertens, Jona Ruof, Jan Strohbeck, Michael Buchholz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.14879">Fast Long-Term Multi-Scenario Prediction for Maneuver Planning at Unsignalized Intersections</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion prediction for intelligent vehicles typically focuses on estimating the most probable future evolutions of a traffic scenario. Estimating the gap acceptance, i.e., whether a vehicle merges or crosses before another vehicle with the right of way, is often handled implicitly in the prediction. However, an infrastructure-based maneuver planning can assign artificial priorities between cooperative vehicles, so it needs to evaluate many more potential scenarios. Additionally, the prediction horizon has to be long enough to assess the impact of a maneuver. We, therefore, present a novel long-term prediction approach handling the gap acceptance estimation and the velocity prediction in two separate stages. Thereby, the behavior of regular vehicles as well as priority assignments of cooperative vehicles can be considered. We train both stages on real-world traffic observations to achieve realistic prediction results. Our method has a competitive accuracy and is fast enough to predict a multitude of scenarios in a short time, making it suitable to be used in a maneuver planning framework.
<div id='section'>Paperid: <span id='pid'>1369, <a href='https://arxiv.org/pdf/2401.08570.pdf' target='_blank'>https://arxiv.org/pdf/2401.08570.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Siwei Zhang, Bharat Lal Bhatnagar, Yuanlu Xu, Alexander Winkler, Petr Kadlecek, Siyu Tang, Federica Bogo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.08570">RoHM: Robust Human Motion Reconstruction via Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose RoHM, an approach for robust 3D human motion reconstruction from monocular RGB(-D) videos in the presence of noise and occlusions. Most previous approaches either train neural networks to directly regress motion in 3D or learn data-driven motion priors and combine them with optimization at test time. The former do not recover globally coherent motion and fail under occlusions; the latter are time-consuming, prone to local minima, and require manual tuning. To overcome these shortcomings, we exploit the iterative, denoising nature of diffusion models. RoHM is a novel diffusion-based motion model that, conditioned on noisy and occluded input data, reconstructs complete, plausible motions in consistent global coordinates. Given the complexity of the problem -- requiring one to address different tasks (denoising and infilling) in different solution spaces (local and global motion) -- we decompose it into two sub-tasks and learn two models, one for global trajectory and one for local motion. To capture the correlations between the two, we then introduce a novel conditioning module, combining it with an iterative inference scheme. We apply RoHM to a variety of tasks -- from motion reconstruction and denoising to spatial and temporal infilling. Extensive experiments on three popular datasets show that our method outperforms state-of-the-art approaches qualitatively and quantitatively, while being faster at test time. The code is available at https://sanweiliti.github.io/ROHM/ROHM.html.
<div id='section'>Paperid: <span id='pid'>1370, <a href='https://arxiv.org/pdf/2401.05412.pdf' target='_blank'>https://arxiv.org/pdf/2401.05412.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xueyuan Yang, Chao Yao, Xiaojuan Ban
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.05412">Spatial-Related Sensors Matters: 3D Human Motion Reconstruction Assisted with Textual Semantics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Leveraging wearable devices for motion reconstruction has emerged as an economical and viable technique. Certain methodologies employ sparse Inertial Measurement Units (IMUs) on the human body and harness data-driven strategies to model human poses. However, the reconstruction of motion based solely on sparse IMUs data is inherently fraught with ambiguity, a consequence of numerous identical IMU readings corresponding to different poses. In this paper, we explore the spatial importance of multiple sensors, supervised by text that describes specific actions. Specifically, uncertainty is introduced to derive weighted features for each IMU. We also design a Hierarchical Temporal Transformer (HTT) and apply contrastive learning to achieve precise temporal and feature alignment of sensor data with textual semantics. Experimental results demonstrate our proposed approach achieves significant improvements in multiple metrics compared to existing methods. Notably, with textual supervision, our method not only differentiates between ambiguous actions such as sitting and standing but also produces more precise and natural motion.
<div id='section'>Paperid: <span id='pid'>1371, <a href='https://arxiv.org/pdf/2401.04872.pdf' target='_blank'>https://arxiv.org/pdf/2401.04872.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Liu, Yuexin Zhang, Kunming Li, Yongliang Qiao, Stewart Worrall, You-Fu Li, He Kong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.04872">Knowledge-aware Graph Transformer for Pedestrian Trajectory Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Predicting pedestrian motion trajectories is crucial for path planning and motion control of autonomous vehicles. Accurately forecasting crowd trajectories is challenging due to the uncertain nature of human motions in different environments. For training, recent deep learning-based prediction approaches mainly utilize information like trajectory history and interactions between pedestrians, among others. This can limit the prediction performance across various scenarios since the discrepancies between training datasets have not been properly incorporated. To overcome this limitation, this paper proposes a graph transformer structure to improve prediction performance, capturing the differences between the various sites and scenarios contained in the datasets. In particular, a self-attention mechanism and a domain adaption module have been designed to improve the generalization ability of the model. Moreover, an additional metric considering cross-dataset sequences is introduced for training and performance evaluation purposes. The proposed framework is validated and compared against existing methods using popular public datasets, i.e., ETH and UCY. Experimental results demonstrate the improved performance of our proposed scheme.
<div id='section'>Paperid: <span id='pid'>1372, <a href='https://arxiv.org/pdf/2312.12227.pdf' target='_blank'>https://arxiv.org/pdf/2312.12227.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gaoge Han, Shaoli Huang, Mingming Gong, Jinglei Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.12227">HuTuMotion: Human-Tuned Navigation of Latent Motion Diffusion Models with Minimal Feedback</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce HuTuMotion, an innovative approach for generating natural human motions that navigates latent motion diffusion models by leveraging few-shot human feedback. Unlike existing approaches that sample latent variables from a standard normal prior distribution, our method adapts the prior distribution to better suit the characteristics of the data, as indicated by human feedback, thus enhancing the quality of motion generation. Furthermore, our findings reveal that utilizing few-shot feedback can yield performance levels on par with those attained through extensive human feedback. This discovery emphasizes the potential and efficiency of incorporating few-shot human-guided optimization within latent diffusion models for personalized and style-aware human motion generation applications. The experimental results show the significantly superior performance of our method over existing state-of-the-art approaches.
<div id='section'>Paperid: <span id='pid'>1373, <a href='https://arxiv.org/pdf/2312.11994.pdf' target='_blank'>https://arxiv.org/pdf/2312.11994.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Korrawe Karunratanakul, Konpat Preechakul, Emre Aksan, Thabo Beeler, Supasorn Suwajanakorn, Siyu Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.11994">Optimizing Diffusion Noise Can Serve As Universal Motion Priors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose Diffusion Noise Optimization (DNO), a new method that effectively leverages existing motion diffusion models as motion priors for a wide range of motion-related tasks. Instead of training a task-specific diffusion model for each new task, DNO operates by optimizing the diffusion latent noise of an existing pre-trained text-to-motion model. Given the corresponding latent noise of a human motion, it propagates the gradient from the target criteria defined on the motion space through the whole denoising process to update the diffusion latent noise. As a result, DNO supports any use cases where criteria can be defined as a function of motion. In particular, we show that, for motion editing and control, DNO outperforms existing methods in both achieving the objective and preserving the motion content. DNO accommodates a diverse range of editing modes, including changing trajectory, pose, joint locations, or avoiding newly added obstacles. In addition, DNO is effective in motion denoising and completion, producing smooth and realistic motion from noisy and partial inputs. DNO achieves these results at inference time without the need for model retraining, offering great versatility for any defined reward or loss function on the motion representation.
<div id='section'>Paperid: <span id='pid'>1374, <a href='https://arxiv.org/pdf/2312.05473.pdf' target='_blank'>https://arxiv.org/pdf/2312.05473.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenhui Zuo, Kaibo He, Jing Shao, Yanan Sui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.05473">Self Model for Embodied Intelligence: Modeling Full-Body Human Musculoskeletal System and Locomotion Control with Hierarchical Low-Dimensional Representation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modeling and control of the human musculoskeletal system is important for understanding human motor functions, developing embodied intelligence, and optimizing human-robot interaction systems. However, current human musculoskeletal models are restricted to a limited range of body parts and often with a reduced number of muscles. There is also a lack of algorithms capable of controlling over 600 muscles to generate reasonable human movements. To fill this gap, we build a musculoskeletal model (MS-Human-700) with 90 body segments, 206 joints, and 700 muscle-tendon units, allowing simulation of full-body dynamics and interaction with various devices. We develop a new algorithm using low-dimensional representation and hierarchical deep reinforcement learning to achieve state-of-the-art full-body control. We validate the effectiveness of our model and algorithm in simulations with real human locomotion data. The musculoskeletal model, along with its control algorithm, will be made available to the research community to promote a deeper understanding of human motion control and better design of interactive robots.
  Project page: https://lnsgroup.cc/research/MS-Human-700
<div id='section'>Paperid: <span id='pid'>1375, <a href='https://arxiv.org/pdf/2312.02711.pdf' target='_blank'>https://arxiv.org/pdf/2312.02711.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jakub Rozlivek, Alessandro Roncone, Ugo Pattacini, Matej Hoffmann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.02711">HARMONIOUS -- Human-like reactive motion control and multimodal perception for humanoid robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>For safe and effective operation of humanoid robots in human-populated environments, the problem of commanding a large number of Degrees of Freedom (DoF) while simultaneously considering dynamic obstacles and human proximity has still not been solved. We present a new reactive motion controller that commands two arms of a humanoid robot and three torso joints (17 DoF in total). We formulate a quadratic program that seeks joint velocity commands respecting multiple constraints while minimizing the magnitude of the velocities. We introduce a new unified treatment of obstacles that dynamically maps visual and proximity (pre-collision) and tactile (post-collision) obstacles as additional constraints to the motion controller, in a distributed fashion over the surface of the upper body of the iCub robot (with 2000 pressure-sensitive receptors). This results in a bio-inspired controller that: (i) gives rise to a robot with whole-body visuo-tactile awareness, resembling peripersonal space representations, and (ii) produces human-like minimum jerk movement profiles. The controller was extensively experimentally validated, including a physical human-robot interaction scenario.
<div id='section'>Paperid: <span id='pid'>1376, <a href='https://arxiv.org/pdf/2311.08880.pdf' target='_blank'>https://arxiv.org/pdf/2311.08880.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Li Tan, Wei Ren, Xi-Ming Sun, Junlin Xiong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.08880">Motion Control of Two Mobile Robots under Allowable Collisions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This letter investigates the motion control problem of two mobile robots under allowable collisions. Here, the allowable collisions mean that the collisions do not damage the mobile robots. The occurrence of the collisions is discussed and the effects of the collisions on the mobile robots are analyzed to develop a hybrid model of each mobile robot under allowable collisions. Based on the effects of the collisions, we show the necessity of redesigning the motion control strategy for mobile robots. Furthermore, impulsive control techniques are applied to redesign the motion control strategy to guarantee the task accomplishment for each mobile robot. Finally, an example is used to illustrate the redesigned motion control strategy.
<div id='section'>Paperid: <span id='pid'>1377, <a href='https://arxiv.org/pdf/2310.14038.pdf' target='_blank'>https://arxiv.org/pdf/2310.14038.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Juan Moreno Nadales, Astghik Hakobyan, David MuÃ±oz de la PeÃ±a, Daniel Limon, Insoon Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.14038">Risk-Aware Wasserstein Distributionally Robust Control of Vessels in Natural Waterways</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the realm of maritime transportation, autonomous vessel navigation in natural inland waterways faces persistent challenges due to unpredictable natural factors. Existing scheduling algorithms fall short in handling these uncertainties, compromising both safety and efficiency. Moreover, these algorithms are primarily designed for non-autonomous vessels, leading to labor-intensive operations vulnerable to human error. To address these issues, this study proposes a risk-aware motion control approach for vessels that accounts for the dynamic and uncertain nature of tide islands in a distributionally robust manner. Specifically, a model predictive control method is employed to follow the reference trajectory in the time-space map while incorporating a risk constraint to prevent grounding accidents. To address uncertainties in tide islands, a novel modeling technique represents them as stochastic polytopes. Additionally, potential inaccuracies in waterway depth are addressed through a risk constraint that considers the worst-case uncertainty distribution within a Wasserstein ambiguity set around the empirical distribution. Using sensor data collected in the Guadalquivir River, we empirically demonstrate the performance of the proposed method through simulations on a vessel. As a result, the vessel successfully navigates the waterway while avoiding grounding accidents, even with a limited dataset of observations. This stands in contrast to existing non-robust controllers, highlighting the robustness and practical applicability of the proposed approach.
<div id='section'>Paperid: <span id='pid'>1378, <a href='https://arxiv.org/pdf/2310.12650.pdf' target='_blank'>https://arxiv.org/pdf/2310.12650.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tomoya Shiba, Akinobu Mizutani, Yuga Yano, Tomohiro Ono, Shoshi Tokuno, Daiju Kanaoka, Yukiya Fukuda, Hayato Amano, Mayu Koresawa, Yoshifumi Sakai, Ryogo Takemoto, Katsunori Tamai, Kazuo Nakahara, Hiroyuki Hayashi, Satsuki Fujimatsu, Yusuke Mizoguchi, Moeno Anraku, Mayo Suzuka, Lu Shen, Kohei Maeda, Fumiya Matsuzaki, Ikuya Matsumoto, Kazuya Murai, Kosei Isomoto, Kim Minje, Yuichiro Tanaka, Takashi Morie, Hakaru Tamukoh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.12650">Hibikino-Musashi@Home 2023 Team Description Paper</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper describes an overview of the techniques of Hibikino-Musashi@Home, which intends to participate in the domestic standard platform league. The team has developed a dataset generator for the training of a robot vision system and an open-source development environment running on a human support robot simulator. The robot system comprises self-developed libraries including those for motion synthesis and open-source software works on the robot operating system. The team aims to realize a home service robot that assists humans in a home, and continuously attend the competition to evaluate the developed system. The brain-inspired artificial intelligence system is also proposed for service robots which are expected to work in a real home environment.
<div id='section'>Paperid: <span id='pid'>1379, <a href='https://arxiv.org/pdf/2310.11792.pdf' target='_blank'>https://arxiv.org/pdf/2310.11792.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Noriaki Takasugi, Masaya Kinoshita, Yasuhisa Kamikawa, Ryoichi Tsuzaki, Atsushi Sakamoto, Toshimitsu Kai, Yasunori Kawanami
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.11792">Real-time Perceptive Motion Control using Control Barrier Functions with Analytical Smoothing for Six-Wheeled-Telescopic-Legged Robot Tachyon 3</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To achieve safe legged locomotion, it is important to generate motion in real-time considering various constraints in robots and environments. In this study, we propose a lightweight real-time perspective motion control system for the newly developed six-wheeled-telescopic-legged robot, Tachyon 3. In the proposed method, analytically smoothed constraints including Smooth Separating Axis Theorem (Smooth SAT) as a novel higher order differentiable collision detection for 3D shapes is applied to the Control Barrier Function (CBF). The proposed system integrating the CBF achieves online motion generation in a short control cycle of 1 ms that satisfies joint limitations, environmental collision avoidance and safe convex foothold constraints. The efficiency of Smooth SAT is shown from the collision detection time of 1 us or less and the CBF constraint computation time for Tachyon3 of several us. Furthermore, the effectiveness of the proposed system is verified through the stair-climbing motion, integrating online recognition in a simulation and a real machine.
<div id='section'>Paperid: <span id='pid'>1380, <a href='https://arxiv.org/pdf/2310.05507.pdf' target='_blank'>https://arxiv.org/pdf/2310.05507.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yilong Li, Ramanujan K Sheshadri, Karthik Sundaresan, Eugene Chai, Suman Banerjee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.05507">MEDUSA: Scalable Biometric Sensing in the Wild through Distributed MIMO Radars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Radar-based techniques for detecting vital signs have shown promise for continuous contactless vital sign sensing and healthcare applications. However, real-world indoor environments face significant challenges for existing vital sign monitoring systems. These include signal blockage in non-line-of-sight (NLOS) situations, movement of human subjects, and alterations in location and orientation. Additionally, these existing systems failed to address the challenge of tracking multiple targets simultaneously. To overcome these challenges, we present MEDUSA, a novel coherent ultra-wideband (UWB) based distributed multiple-input multiple-output (MIMO) radar system, especially it allows users to customize and disperse the $16 \times 16$ into sub-arrays. MEDUSA takes advantage of the diversity benefits of distributed yet wirelessly synchronized MIMO arrays to enable robust vital sign monitoring in real-world and daily living environments where human targets are moving and surrounded by obstacles. We've developed a scalable, self-supervised contrastive learning model which integrates seamlessly with our hardware platform. Each attention weight within the model corresponds to a specific antenna pair of Tx and Rx. The model proficiently recovers accurate vital sign waveforms by decomposing and correlating the mixed received signals, including comprising human motion, mobility, noise, and vital signs. Through extensive evaluations involving 21 participants and over 200 hours of collected data (3.75 TB in total, with 1.89 TB for static subjects and 1.86 TB for moving subjects), MEDUSA's performance has been validated, showing an average gain of 20% compared to existing systems employing COTS radar sensors. This demonstrates MEDUSA's spatial diversity gain for real-world vital sign monitoring, encompassing target and environmental dynamics in familiar and unfamiliar indoor environments.
<div id='section'>Paperid: <span id='pid'>1381, <a href='https://arxiv.org/pdf/2310.00615.pdf' target='_blank'>https://arxiv.org/pdf/2310.00615.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chaoyue Xing, Wei Mao, Miaomiao Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.00615">Scene-aware Human Motion Forecasting via Mutual Distance Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we tackle the problem of scene-aware 3D human motion forecasting. A key challenge of this task is to predict future human motions that are consistent with the scene by modeling the human-scene interactions. While recent works have demonstrated that explicit constraints on human-scene interactions can prevent the occurrence of ghost motion, they only provide constraints on partial human motion e.g., the global motion of the human or a few joints contacting the scene, leaving the rest of the motion unconstrained. To address this limitation, we propose to model the human-scene interaction with the mutual distance between the human body and the scene. Such mutual distances constrain both the local and global human motion, resulting in a whole-body motion constrained prediction. In particular, mutual distance constraints consist of two components, the signed distance of each vertex on the human mesh to the scene surface and the distance of basis scene points to the human mesh. We further introduce a global scene representation learned from a signed distance function (SDF) volume to ensure coherence between the global scene representation and the explicit constraint from the mutual distance. We develop a pipeline with two sequential steps: predicting the future mutual distances first, followed by forecasting future human motion. During training, we explicitly encourage consistency between predicted poses and mutual distances. Extensive evaluations on the existing synthetic and real datasets demonstrate that our approach consistently outperforms the state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>1382, <a href='https://arxiv.org/pdf/2309.15289.pdf' target='_blank'>https://arxiv.org/pdf/2309.15289.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiqian Lan, Yuxuan Jiang, Yao Mu, Chen Chen, Shengbo Eben Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.15289">SEPT: Towards Efficient Scene Representation Learning for Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion prediction is crucial for autonomous vehicles to operate safely in complex traffic environments. Extracting effective spatiotemporal relationships among traffic elements is key to accurate forecasting. Inspired by the successful practice of pretrained large language models, this paper presents SEPT, a modeling framework that leverages self-supervised learning to develop powerful spatiotemporal understanding for complex traffic scenes. Specifically, our approach involves three masking-reconstruction modeling tasks on scene inputs including agents' trajectories and road network, pretraining the scene encoder to capture kinematics within trajectory, spatial structure of road network, and interactions among roads and agents. The pretrained encoder is then finetuned on the downstream forecasting task. Extensive experiments demonstrate that SEPT, without elaborate architectural design or manual feature engineering, achieves state-of-the-art performance on the Argoverse 1 and Argoverse 2 motion forecasting benchmarks, outperforming previous methods on all main metrics by a large margin.
<div id='section'>Paperid: <span id='pid'>1383, <a href='https://arxiv.org/pdf/2309.14720.pdf' target='_blank'>https://arxiv.org/pdf/2309.14720.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Chen, Gong Chen, Jing Ye, Chenglong Fu, Bin Liang, Xiang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.14720">Learning to Assist Different Wearers in Multitasks: Efficient and Individualized Human-In-the-Loop Adaption Framework for Exoskeleton Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>One of the typical purposes of using lower-limb exoskeleton robots is to provide assistance to the wearer by supporting their weight and augmenting their physical capabilities according to a given task and human motion intentions. The generalizability of robots across different wearers in multiple tasks is important to ensure that the robot can provide correct and effective assistance in actual implementation. However, most lower-limb exoskeleton robots exhibit only limited generalizability. Therefore, this paper proposes a human-in-the-loop learning and adaptation framework for exoskeleton robots to improve their performance in various tasks and for different wearers. To suit different wearers, an individualized walking trajectory is generated online using dynamic movement primitives and Bayes optimization. To accommodate various tasks, a task translator is constructed using a neural network to generalize a trajectory to more complex scenarios. These generalization techniques are integrated into a unified variable impedance model, which regulates the exoskeleton to provide assistance while ensuring safety. In addition, an anomaly detection network is developed to quantitatively evaluate the wearer's comfort, which is considered in the trajectory learning procedure and contributes to the relaxation of conflicts in impedance control. The proposed framework is easy to implement, because it requires proprioceptive sensors only to perform and deploy data-efficient learning schemes. This makes the exoskeleton practical for deployment in complex scenarios, accommodating different walking patterns, habits, tasks, and conflicts. Experiments and comparative studies on a lower-limb exoskeleton robot are performed to demonstrate the effectiveness of the proposed framework.
<div id='section'>Paperid: <span id='pid'>1384, <a href='https://arxiv.org/pdf/2309.11735.pdf' target='_blank'>https://arxiv.org/pdf/2309.11735.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingjie Wu, Lei Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.11735">FleXstage: Lightweight Magnetically Levitated Precision Stage with Over-Actuation towards High-Throughput IC Manufacturing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Precision motion stages play a critical role in various manufacturing and inspection equipment, for example, the wafer/reticle scanning in photolithography scanners and positioning stages in wafer inspection systems. To meet the growing demand for higher throughput in chip manufacturing and inspection, it is critical to create new precision motion stages with higher acceleration capability with high control bandwidth, which calls for the development of lightweight precision stages. However, in today's precision motion systems, only the rigid body motion of the system are under control, and the flexible dynamic systems are in open loop. For these systems, the motion control bandwidth is limited by the first structural resonance frequency of the stage, which enforces a fundamental trade-off between the stage's bandwidth and acceleration capability. Aiming to overcome this trade-off, we have introduced a sequential structure and control design framework for lightweight stages with the low-frequency flexible modes of the stage are under active control. To facilitate the controller design, we further propose to minimize the resonance frequency of the stage's mode being controlled and to maximize the resonance frequency of the uncontrolled mode. The system's control bandwidth is placed in between the resonance frequencies. This paper presents the design, optimization, building, and experimental evaluations for a lightweight magnetically levitated planar stage, which we call FleXstage, with first flexible mode actively controlled via over-actuation. Simulations show the proposed design is highly promising in enabling stages with lightweight without sacrificing control bandwidth. We have some preliminary results now and are still working on the experimental evaluations for the closed-loop system, and will present the results in the oral presentation.
<div id='section'>Paperid: <span id='pid'>1385, <a href='https://arxiv.org/pdf/2309.03031.pdf' target='_blank'>https://arxiv.org/pdf/2309.03031.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zeyu Ling, Bo Han, Yongkang Wong, Mohan Kangkanhalli, Weidong Geng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.03031">MCM: Multi-condition Motion Synthesis Framework for Multi-scenario</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The objective of the multi-condition human motion synthesis task is to incorporate diverse conditional inputs, encompassing various forms like text, music, speech, and more. This endows the task with the capability to adapt across multiple scenarios, ranging from text-to-motion and music-to-dance, among others. While existing research has primarily focused on single conditions, the multi-condition human motion generation remains underexplored. In this paper, we address these challenges by introducing MCM, a novel paradigm for motion synthesis that spans multiple scenarios under diverse conditions. The MCM framework is able to integrate with any DDPM-like diffusion model to accommodate multi-conditional information input while preserving its generative capabilities. Specifically, MCM employs two-branch architecture consisting of a main branch and a control branch. The control branch shares the same structure as the main branch and is initialized with the parameters of the main branch, effectively maintaining the generation ability of the main branch and supporting multi-condition input. We also introduce a Transformer-based diffusion model MWNet (DDPM-like) as our main branch that can capture the spatial complexity and inter-joint correlations in motion sequences through a channel-dimension self-attention module. Quantitative comparisons demonstrate that our approach achieves SoTA results in both text-to-motion and competitive results in music-to-dance tasks, comparable to task-specific methods. Furthermore, the qualitative evaluation shows that MCM not only streamlines the adaptation of methodologies originally designed for text-to-motion tasks to domains like music-to-dance and speech-to-gesture, eliminating the need for extensive network re-configurations but also enables effective multi-condition modal control, realizing "once trained is motion need".
<div id='section'>Paperid: <span id='pid'>1386, <a href='https://arxiv.org/pdf/2308.09811.pdf' target='_blank'>https://arxiv.org/pdf/2308.09811.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ricardo B. Grando, Junior C. de Jesus, Victor A. Kich, Alisson H. Kolling, Rodrigo S. Guerra, Paulo L. J. Drews-Jr
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.09811">DoCRL: Double Critic Deep Reinforcement Learning for Mapless Navigation of a Hybrid Aerial Underwater Vehicle with Medium Transition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep Reinforcement Learning (Deep-RL) techniques for motion control have been continuously used to deal with decision-making problems for a wide variety of robots. Previous works showed that Deep-RL can be applied to perform mapless navigation, including the medium transition of Hybrid Unmanned Aerial Underwater Vehicles (HUAUVs). These are robots that can operate in both air and water media, with future potential for rescue tasks in robotics. This paper presents new approaches based on the state-of-the-art Double Critic Actor-Critic algorithms to address the navigation and medium transition problems for a HUAUV. We show that double-critic Deep-RL with Recurrent Neural Networks using range data and relative localization solely improves the navigation performance of HUAUVs. Our DoCRL approaches achieved better navigation and transitioning capability, outperforming previous approaches.
<div id='section'>Paperid: <span id='pid'>1387, <a href='https://arxiv.org/pdf/2308.02266.pdf' target='_blank'>https://arxiv.org/pdf/2308.02266.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kai Storms, Ken Mori, Steven Peters
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.02266">SURE-Val: Safe Urban Relevance Extension and Validation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To evaluate perception components of an automated driving system, it is necessary to define the relevant objects. While the urban domain is popular among perception datasets, relevance is insufficiently specified for this domain. Therefore, this work adopts an existing method to define relevance in the highway domain and expands it to the urban domain. While different conceptualizations and definitions of relevance are present in literature, there is a lack of methods to validate these definitions. Therefore, this work presents a novel relevance validation method leveraging a motion prediction component. The validation leverages the idea that removing irrelevant objects should not influence a prediction component which reflects human driving behavior. The influence on the prediction is quantified by considering the statistical distribution of prediction performance across a large-scale dataset. The validation procedure is verified using criteria specifically designed to exclude relevant objects. The validation method is successfully applied to the relevance criteria from this work, thus supporting their validity.
<div id='section'>Paperid: <span id='pid'>1388, <a href='https://arxiv.org/pdf/2307.15042.pdf' target='_blank'>https://arxiv.org/pdf/2307.15042.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zihan Zhang, Richard Liu, Kfir Aberman, Rana Hanocka
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.15042">TEDi: Temporally-Entangled Diffusion for Long-Term Motion Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The gradual nature of a diffusion process that synthesizes samples in small increments constitutes a key ingredient of Denoising Diffusion Probabilistic Models (DDPM), which have presented unprecedented quality in image synthesis and been recently explored in the motion domain. In this work, we propose to adapt the gradual diffusion concept (operating along a diffusion time-axis) into the temporal-axis of the motion sequence. Our key idea is to extend the DDPM framework to support temporally varying denoising, thereby entangling the two axes. Using our special formulation, we iteratively denoise a motion buffer that contains a set of increasingly-noised poses, which auto-regressively produces an arbitrarily long stream of frames. With a stationary diffusion time-axis, in each diffusion step we increment only the temporal-axis of the motion such that the framework produces a new, clean frame which is removed from the beginning of the buffer, followed by a newly drawn noise vector that is appended to it. This new mechanism paves the way towards a new framework for long-term motion synthesis with applications to character animation and other domains.
<div id='section'>Paperid: <span id='pid'>1389, <a href='https://arxiv.org/pdf/2307.07511.pdf' target='_blank'>https://arxiv.org/pdf/2307.07511.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nilesh Kulkarni, Davis Rempe, Kyle Genova, Abhijit Kundu, Justin Johnson, David Fouhey, Leonidas Guibas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.07511">NIFTY: Neural Object Interaction Fields for Guided Human Motion Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We address the problem of generating realistic 3D motions of humans interacting with objects in a scene. Our key idea is to create a neural interaction field attached to a specific object, which outputs the distance to the valid interaction manifold given a human pose as input. This interaction field guides the sampling of an object-conditioned human motion diffusion model, so as to encourage plausible contacts and affordance semantics. To support interactions with scarcely available data, we propose an automated synthetic data pipeline. For this, we seed a pre-trained motion model, which has priors for the basics of human movement, with interaction-specific anchor poses extracted from limited motion capture data. Using our guided diffusion model trained on generated synthetic data, we synthesize realistic motions for sitting and lifting with several objects, outperforming alternative approaches in terms of motion quality and successful action completion. We call our framework NIFTY: Neural Interaction Fields for Trajectory sYnthesis.
<div id='section'>Paperid: <span id='pid'>1390, <a href='https://arxiv.org/pdf/2306.13992.pdf' target='_blank'>https://arxiv.org/pdf/2306.13992.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaocong Zhao, Jian Sun, Meng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.13992">Measuring Sociality in Driving Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Interacting with other human road users is one of the most challenging tasks for autonomous vehicles. For congruent driving behaviors, it is essential to recognize and comprehend sociality, encompassing both implicit social norms and individualized social preferences of human drivers. To understand and quantify the complex sociality in driving interactions, we propose a Virtual-Game-based Interaction Model (VGIM) that is parameterized by a social preference measurement, Interaction Preference Value (IPV). The IPV is designed to capture the driver's relative inclination towards individual rewards over group rewards. A method for identifying IPV from observed driving trajectory is also developed, with which we assessed human drivers' IPV using driving data recorded in a typical interactive driving scenario, the unprotected left turn. Our findings reveal that (1) human drivers exhibit particular social preference patterns while undertaking specific tasks, such as turning left or proceeding straight; (2) competitive actions could be strategically conducted by human drivers in order to coordinate with others. Finally, we discuss the potential of learning sociality-aware navigation from human demonstrations by incorporating a rule-based humanlike IPV expressing strategy into VGIM and optimization-based motion planners. Simulation experiments demonstrate that (1) IPV identification improves the motion prediction performance in interactive driving scenarios and (2) the dynamic IPV expressing strategy extracted from human driving data makes it possible to reproduce humanlike coordination patterns in the driving interaction.
<div id='section'>Paperid: <span id='pid'>1391, <a href='https://arxiv.org/pdf/2306.03374.pdf' target='_blank'>https://arxiv.org/pdf/2306.03374.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanwen Fang, Jintai Chen, Peng-Tao Jiang, Chao Li, Yifeng Geng, Eddy K. F. Lam, Guodong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.03374">PGformer: Proxy-Bridged Game Transformer for Multi-Person Highly Interactive Extreme Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-person motion prediction is a challenging task, especially for real-world scenarios of highly interacted persons. Most previous works have been devoted to studying the case of weak interactions (e.g., walking together), in which typically forecasting each human pose in isolation can still achieve good performances. This paper focuses on collaborative motion prediction for multiple persons with extreme motions and attempts to explore the relationships between the highly interactive persons' pose trajectories. Specifically, a novel cross-query attention (XQA) module is proposed to bilaterally learn the cross-dependencies between the two pose sequences tailored for this situation. A proxy unit is additionally introduced to bridge the involved persons, which cooperates with our proposed XQA module and subtly controls the bidirectional spatial information flows. These designs are then integrated into a Transformer-based architecture and the resulting model is called Proxy-bridged Game Transformer (PGformer) for multi-person interactive motion prediction. Its effectiveness has been evaluated on the challenging ExPI dataset, which involves highly interactive actions. Our PGformer consistently outperforms the state-of-the-art methods in both short- and long-term predictions by a large margin. Besides, our approach can also be compatible with the weakly interacted CMU-Mocap and MuPoTS-3D datasets and extended to the case of more than 2 individuals with encouraging results.
<div id='section'>Paperid: <span id='pid'>1392, <a href='https://arxiv.org/pdf/2306.00605.pdf' target='_blank'>https://arxiv.org/pdf/2306.00605.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Marcel Hallgarten, Ismail Kisa, Martin Stoll, Andreas Zell
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.00605">Stay on Track: A Frenet Wrapper to Overcome Off-road Trajectories in Vehicle Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Predicting the future motion of observed vehicles is a crucial enabler for safe autonomous driving. The field of motion prediction has seen large progress recently with state-of-the-Art (sotA) models achieving impressive results on large-scale public benchmarks. However, recent work revealed that learning-based methods are prone to predict off-road trajectories in challenging scenarios. These can be created by perturbing existing scenarios with additional turns in front of the target vehicle while the motion history is left unchanged. We argue that this indicates that SotA models do not consider the map information sufficiently and demonstrate how this can be solved by representing model inputs and outputs in a Frenet frame defined by lane centreline sequences. To this end, we present a general wrapper that leverages a Frenet representation of the scene, and that can be applied to SotA models without changing their architecture. We demonstrate the effectiveness of this approach in a comprehensive benchmark using two SotA motion prediction models. Our experiments show that this reduces the off-road rate on challenging scenarios by more than 90% without sacrificing average performance.
<div id='section'>Paperid: <span id='pid'>1393, <a href='https://arxiv.org/pdf/2306.00378.pdf' target='_blank'>https://arxiv.org/pdf/2306.00378.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weiyu Li, Xuelin Chen, Peizhuo Li, Olga Sorkine-Hornung, Baoquan Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.00378">Example-based Motion Synthesis via Generative Motion Matching</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present GenMM, a generative model that "mines" as many diverse motions as possible from a single or few example sequences. In stark contrast to existing data-driven methods, which typically require long offline training time, are prone to visual artifacts, and tend to fail on large and complex skeletons, GenMM inherits the training-free nature and the superior quality of the well-known Motion Matching method. GenMM can synthesize a high-quality motion within a fraction of a second, even with highly complex and large skeletal structures. At the heart of our generative framework lies the generative motion matching module, which utilizes the bidirectional visual similarity as a generative cost function to motion matching, and operates in a multi-stage framework to progressively refine a random guess using exemplar motion matches. In addition to diverse motion generation, we show the versatility of our generative framework by extending it to a number of scenarios that are not possible with motion matching alone, including motion completion, key frame-guided generation, infinite looping, and motion reassembly. Code and data for this paper are at https://wyysf-98.github.io/GenMM/
<div id='section'>Paperid: <span id='pid'>1394, <a href='https://arxiv.org/pdf/2305.12577.pdf' target='_blank'>https://arxiv.org/pdf/2305.12577.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Korrawe Karunratanakul, Konpat Preechakul, Supasorn Suwajanakorn, Siyu Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.12577">Guided Motion Diffusion for Controllable Human Motion Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Denoising diffusion models have shown great promise in human motion synthesis conditioned on natural language descriptions. However, integrating spatial constraints, such as pre-defined motion trajectories and obstacles, remains a challenge despite being essential for bridging the gap between isolated human motion and its surrounding environment. To address this issue, we propose Guided Motion Diffusion (GMD), a method that incorporates spatial constraints into the motion generation process. Specifically, we propose an effective feature projection scheme that manipulates motion representation to enhance the coherency between spatial information and local poses. Together with a new imputation formulation, the generated motion can reliably conform to spatial constraints such as global motion trajectories. Furthermore, given sparse spatial constraints (e.g. sparse keyframes), we introduce a new dense guidance approach to turn a sparse signal, which is susceptible to being ignored during the reverse steps, into denser signals to guide the generated motion to the given constraints. Our extensive experiments justify the development of GMD, which achieves a significant improvement over state-of-the-art methods in text-based motion generation while allowing control of the synthesized motions with spatial constraints.
<div id='section'>Paperid: <span id='pid'>1395, <a href='https://arxiv.org/pdf/2305.02859.pdf' target='_blank'>https://arxiv.org/pdf/2305.02859.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Timur Akhtyamov, Aleksandr Kashirin, Aleksey Postnikov, Gonzalo Ferrer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.02859">Social Robot Navigation through Constrained Optimization: a Comparative Study of Uncertainty-based Objectives and Constraints</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work is dedicated to the study of how uncertainty estimation of the human motion prediction can be embedded into constrained optimization techniques, such as Model Predictive Control (MPC) for the social robot navigation. We propose several cost objectives and constraint functions obtained from the uncertainty of predicting pedestrian positions and related to the probability of the collision that can be applied to the MPC, and all the different variants are compared in challenging scenes with multiple agents. The main question this paper tries to answer is: what are the most important uncertainty-based criteria for social MPC? For that, we evaluate the proposed approaches with several social navigation metrics in an extensive set of scenarios of different complexity in reproducible synthetic environments. The main outcome of our study is a foundation for a practical guide on when and how to use uncertainty-aware approaches for social robot navigation in practice and what are the most effective criteria.
<div id='section'>Paperid: <span id='pid'>1396, <a href='https://arxiv.org/pdf/2303.15331.pdf' target='_blank'>https://arxiv.org/pdf/2303.15331.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Arnaud Klipfel, Nitish Sontakke, Ren Liu, Sehoon Ha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.15331">Learning a Single Policy for Diverse Behaviors on a Quadrupedal Robot using Scalable Motion Imitation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning various motor skills for quadrupedal robots is a challenging problem that requires careful design of task-specific mathematical models or reward descriptions. In this work, we propose to learn a single capable policy using deep reinforcement learning by imitating a large number of reference motions, including walking, turning, pacing, jumping, sitting, and lying. On top of the existing motion imitation framework, we first carefully design the observation space, the action space, and the reward function to improve the scalability of the learning as well as the robustness of the final policy. In addition, we adopt a novel adaptive motion sampling (AMS) method, which maintains a balance between successful and unsuccessful behaviors. This technique allows the learning algorithm to focus on challenging motor skills and avoid catastrophic forgetting. We demonstrate that the learned policy can exhibit diverse behaviors in simulation by successfully tracking both the training dataset and out-of-distribution trajectories. We also validate the importance of the proposed learning formulation and the adaptive motion sampling scheme by conducting experiments.
<div id='section'>Paperid: <span id='pid'>1397, <a href='https://arxiv.org/pdf/2303.10035.pdf' target='_blank'>https://arxiv.org/pdf/2303.10035.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuzheng Qu, Mohammed Abouheaf, Wail Gueaieb, Davide Spinello
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.10035">A Policy Iteration Approach for Flock Motion Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The flocking motion control is concerned with managing the possible conflicts between local and team objectives of multi-agent systems. The overall control process guides the agents while monitoring the flock-cohesiveness and localization. The underlying mechanisms may degrade due to overlooking the unmodeled uncertainties associated with the flock dynamics and formation. On another side, the efficiencies of the various control designs rely on how quickly they can adapt to different dynamic situations in real-time. An online model-free policy iteration mechanism is developed here to guide a flock of agents to follow an independent command generator over a time-varying graph topology. The strength of connectivity between any two agents or the graph edge weight is decided using a position adjacency dependent function. An online recursive least squares approach is adopted to tune the guidance strategies without knowing the dynamics of the agents or those of the command generator. It is compared with another reinforcement learning approach from the literature which is based on a value iteration technique. The simulation results of the policy iteration mechanism revealed fast learning and convergence behaviors with less computational effort.
<div id='section'>Paperid: <span id='pid'>1398, <a href='https://arxiv.org/pdf/2303.08737.pdf' target='_blank'>https://arxiv.org/pdf/2303.08737.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Taras Kucherenko, Pieter Wolfert, Youngwoo Yoon, Carla Viegas, Teodor Nikolov, Mihail Tsakov, Gustav Eje Henter
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.08737">Evaluating gesture generation in a large-scale open challenge: The GENEA Challenge 2022</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper reports on the second GENEA Challenge to benchmark data-driven automatic co-speech gesture generation. Participating teams used the same speech and motion dataset to build gesture-generation systems. Motion generated by all these systems was rendered to video using a standardised visualisation pipeline and evaluated in several large, crowdsourced user studies. Unlike when comparing different research papers, differences in results are here only due to differences between methods, enabling direct comparison between systems. The dataset was based on 18 hours of full-body motion capture, including fingers, of different persons engaging in a dyadic conversation. Ten teams participated in the challenge across two tiers: full-body and upper-body gesticulation. For each tier, we evaluated both the human-likeness of the gesture motion and its appropriateness for the specific speech signal. Our evaluations decouple human-likeness from gesture appropriateness, which has been a difficult problem in the field.
  The evaluation results show some synthetic gesture conditions being rated as significantly more human-like than 3D human motion capture. To the best of our knowledge, this has not been demonstrated before. On the other hand, all synthetic motion is found to be vastly less appropriate for the speech than the original motion-capture recordings. We also find that conventional objective metrics do not correlate well with subjective human-likeness ratings in this large evaluation. The one exception is the FrÃ©chet gesture distance (FGD), which achieves a Kendall's tau rank correlation of around $-0.5$. Based on the challenge results we formulate numerous recommendations for system building and evaluation.
<div id='section'>Paperid: <span id='pid'>1399, <a href='https://arxiv.org/pdf/2303.07655.pdf' target='_blank'>https://arxiv.org/pdf/2303.07655.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kourosh Darvish, Serena Ivaldi, Daniele Pucci
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.07655">Simultaneous Action Recognition and Human Whole-Body Motion and Dynamics Prediction from Wearable Sensors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a novel approach to solve simultaneously the problems of human activity recognition and whole-body motion and dynamics prediction for real-time applications. Starting from the dynamics of human motion and motor system theory, the notion of mixture of experts from deep learning has been extended to address this problem. In the proposed approach, experts are modelled as a sequence-to-sequence recurrent neural networks (RNN) architecture. Experiments show the results of 66-DoF real-world human motion prediction and action recognition during different tasks like walking and rotating. The code associated with this paper is available at: \url{github.com/ami-iit/paper_darvish_2022_humanoids_action-kindyn-predicition}
<div id='section'>Paperid: <span id='pid'>1400, <a href='https://arxiv.org/pdf/2303.06987.pdf' target='_blank'>https://arxiv.org/pdf/2303.06987.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Georges GagnerÃ©, Andy Lavender, CÃ©dric Plessiet, Tim White
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.06987">Challenges of movement quality using motion capture in theatre</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We describe1 two case studies of AvatarStaging theatrical mixed reality framework combining avatars and performers acting in an artistic context. We outline a qualitative approach toward the condition for stage presence for the avatars. We describe the motion control solutions we experimented with from the perspective of building a protocol of avatar direction in a mixed reality appropriate to live performance.
<div id='section'>Paperid: <span id='pid'>1401, <a href='https://arxiv.org/pdf/2302.07753.pdf' target='_blank'>https://arxiv.org/pdf/2302.07753.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Marcel Hallgarten, Martin Stoll, Andreas Zell
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.07753">From Prediction to Planning With Goal Conditioned Lane Graph Traversals</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The field of motion prediction for automated driving has seen tremendous progress recently, bearing ever-more mighty neural network architectures. Leveraging these powerful models bears great potential for the closely related planning task. In this letter we propose a novel goal-conditioning method and show its potential to transform a state-of-the-art prediction model into a goal-directed planner. Our key insight is that conditioning prediction on a navigation goal at the behaviour level outperforms other widely adopted methods, with the additional benefit of increased model interpretability. We train our model on a large open-source dataset and show promising performance in a comprehensive benchmark.
<div id='section'>Paperid: <span id='pid'>1402, <a href='https://arxiv.org/pdf/2301.02667.pdf' target='_blank'>https://arxiv.org/pdf/2301.02667.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiye Lee, Hanbyul Joo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.02667">Locomotion-Action-Manipulation: Synthesizing Human-Scene Interactions in Complex 3D Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Synthesizing interaction-involved human motions has been challenging due to the high complexity of 3D environments and the diversity of possible human behaviors within. We present LAMA, Locomotion-Action-MAnipulation, to synthesize natural and plausible long-term human movements in complex indoor environments. The key motivation of LAMA is to build a unified framework to encompass a series of everyday motions including locomotion, scene interaction, and object manipulation. Unlike existing methods that require motion data "paired" with scanned 3D scenes for supervision, we formulate the problem as a test-time optimization by using human motion capture data only for synthesis. LAMA leverages a reinforcement learning framework coupled with a motion matching algorithm for optimization, and further exploits a motion editing framework via manifold learning to cover possible variations in interaction and manipulation. Throughout extensive experiments, we demonstrate that LAMA outperforms previous approaches in synthesizing realistic motions in various challenging scenarios. Project page: https://jiyewise.github.io/projects/LAMA/ .
<div id='section'>Paperid: <span id='pid'>1403, <a href='https://arxiv.org/pdf/2301.02488.pdf' target='_blank'>https://arxiv.org/pdf/2301.02488.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weicheng Gao, Xiaopeng Yang, Xiaodong Qu, Tian Lan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.02488">TWR-MCAE: A Data Augmentation Method for Through-the-Wall Radar Human Motion Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To solve the problems of reduced accuracy and prolonging convergence time of through-the-wall radar (TWR) human motion due to wall attenuation, multipath effect, and system interference, we propose a multilink auto-encoding neural network (TWR-MCAE) data augmentation method. Specifically, the TWR-MCAE algorithm is jointly constructed by a singular value decomposition (SVD)-based data preprocessing module, an improved coordinate attention module, a compressed sensing learnable iterative shrinkage threshold reconstruction algorithm (LISTA) module, and an adaptive weight module. The data preprocessing module achieves wall clutter, human motion features, and noise subspaces separation. The improved coordinate attention module achieves clutter and noise suppression. The LISTA module achieves human motion feature enhancement. The adaptive weight module learns the weights and fuses the three subspaces. The TWR-MCAE can suppress the low-rank characteristics of wall clutter and enhance the sparsity characteristics in human motion at the same time. It can be linked before the classification step to improve the feature extraction capability without adding other prior knowledge or recollecting more data. Experiments show that the proposed algorithm gets a better peak signal-to-noise ratio (PSNR), which increases the recognition accuracy and speeds up the training process of the back-end classifiers.
<div id='section'>Paperid: <span id='pid'>1404, <a href='https://arxiv.org/pdf/2212.02978.pdf' target='_blank'>https://arxiv.org/pdf/2212.02978.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mia Chiquier, Carl Vondrick
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.02978">Muscles in Action</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion is created by, and constrained by, our muscles. We take a first step at building computer vision methods that represent the internal muscle activity that causes motion. We present a new dataset, Muscles in Action (MIA), to learn to incorporate muscle activity into human motion representations. The dataset consists of 12.5 hours of synchronized video and surface electromyography (sEMG) data of 10 subjects performing various exercises. Using this dataset, we learn a bidirectional representation that predicts muscle activation from video, and conversely, reconstructs motion from muscle activation. We evaluate our model on in-distribution subjects and exercises, as well as on out-of-distribution subjects and exercises. We demonstrate how advances in modeling both modalities jointly can serve as conditioning for muscularly consistent motion generation. Putting muscles into computer vision systems will enable richer models of virtual humans, with applications in sports, fitness, and AR/VR.
<div id='section'>Paperid: <span id='pid'>1405, <a href='https://arxiv.org/pdf/2211.08609.pdf' target='_blank'>https://arxiv.org/pdf/2211.08609.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sehwan Choi, Jungho Kim, Junyong Yun, Jun Won Choi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.08609">R-Pred: Two-Stage Motion Prediction Via Tube-Query Attention-Based Trajectory Refinement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Predicting the future motion of dynamic agents is of paramount importance to ensuring safety and assessing risks in motion planning for autonomous robots. In this study, we propose a two-stage motion prediction method, called R-Pred, designed to effectively utilize both scene and interaction context using a cascade of the initial trajectory proposal and trajectory refinement networks. The initial trajectory proposal network produces M trajectory proposals corresponding to the M modes of the future trajectory distribution. The trajectory refinement network enhances each of the M proposals using 1) tube-query scene attention (TQSA) and 2) proposal-level interaction attention (PIA) mechanisms. TQSA uses tube-queries to aggregate local scene context features pooled from proximity around trajectory proposals of interest. PIA further enhances the trajectory proposals by modeling inter-agent interactions using a group of trajectory proposals selected by their distances from neighboring agents. Our experiments conducted on Argoverse and nuScenes datasets demonstrate that the proposed refinement network provides significant performance improvements compared to the single-stage baseline and that R-Pred achieves state-of-the-art performance in some categories of the benchmarks.
<div id='section'>Paperid: <span id='pid'>1406, <a href='https://arxiv.org/pdf/2210.16144.pdf' target='_blank'>https://arxiv.org/pdf/2210.16144.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sandra Carrasco Limeros, Sylwia Majchrowska, Joakim Johnander, Christoffer Petersson, Miguel Ãngel Sotelo, David FernÃ¡ndez Llorca
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.16144">Towards trustworthy multi-modal motion prediction: Holistic evaluation and interpretability of outputs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Predicting the motion of other road agents enables autonomous vehicles to perform safe and efficient path planning. This task is very complex, as the behaviour of road agents depends on many factors and the number of possible future trajectories can be considerable (multi-modal). Most prior approaches proposed to address multi-modal motion prediction are based on complex machine learning systems that have limited interpretability. Moreover, the metrics used in current benchmarks do not evaluate all aspects of the problem, such as the diversity and admissibility of the output. In this work, we aim to advance towards the design of trustworthy motion prediction systems, based on some of the requirements for the design of Trustworthy Artificial Intelligence. We focus on evaluation criteria, robustness, and interpretability of outputs. First, we comprehensively analyse the evaluation metrics, identify the main gaps of current benchmarks, and propose a new holistic evaluation framework. We then introduce a method for the assessment of spatial and temporal robustness by simulating noise in the perception system. To enhance the interpretability of the outputs and generate more balanced results in the proposed evaluation framework, we propose an intent prediction layer that can be attached to multi-modal motion prediction models. The effectiveness of this approach is assessed through a survey that explores different elements in the visualization of the multi-modal trajectories and intentions. The proposed approach and findings make a significant contribution to the development of trustworthy motion prediction systems for autonomous vehicles, advancing the field towards greater safety and reliability.
<div id='section'>Paperid: <span id='pid'>1407, <a href='https://arxiv.org/pdf/2209.08864.pdf' target='_blank'>https://arxiv.org/pdf/2209.08864.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bo-Siang Lu, Tung-I Chen, Hsin-Ying Lee, Winston H. Hsu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.08864">CFVS: Coarse-to-Fine Visual Servoing for 6-DoF Object-Agnostic Peg-In-Hole Assembly</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robotic peg-in-hole assembly remains a challenging task due to its high accuracy demand. Previous work tends to simplify the problem by restricting the degree of freedom of the end-effector, or limiting the distance between the target and the initial pose position, which prevents them from being deployed in real-world manufacturing. Thus, we present a Coarse-to-Fine Visual Servoing (CFVS) peg-in-hole method, achieving 6-DoF end-effector motion control based on 3D visual feedback. CFVS can handle arbitrary tilt angles and large initial alignment errors through a fast pose estimation before refinement. Furthermore, by introducing a confidence map to ignore the irrelevant contour of objects, CFVS is robust against noise and can deal with various targets beyond training data. Extensive experiments show CFVS outperforms state-of-the-art methods and obtains 100%, 91%, and 82% average success rates in 3-DoF, 4-DoF, and 6-DoF peg-in-hole, respectively.
<div id='section'>Paperid: <span id='pid'>1408, <a href='https://arxiv.org/pdf/2209.07600.pdf' target='_blank'>https://arxiv.org/pdf/2209.07600.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohammad Mahdavian, Payam Nikdel, Mahdi TaherAhmadi, Mo Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.07600">STPOTR: Simultaneous Human Trajectory and Pose Prediction Using a Non-Autoregressive Transformer for Robot Following Ahead</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we develop a neural network model to predict future human motion from an observed human motion history. We propose a non-autoregressive transformer architecture to leverage its parallel nature for easier training and fast, accurate predictions at test time. The proposed architecture divides human motion prediction into two parts: 1) the human trajectory, which is the hip joint 3D position over time and 2) the human pose which is the all other joints 3D positions over time with respect to a fixed hip joint. We propose to make the two predictions simultaneously, as the shared representation can improve the model performance. Therefore, the model consists of two sets of encoders and decoders. First, a multi-head attention module applied to encoder outputs improves human trajectory. Second, another multi-head self-attention module applied to encoder outputs concatenated with decoder outputs facilitates learning of temporal dependencies. Our model is well-suited for robotic applications in terms of test accuracy and speed, and compares favorably with respect to state-of-the-art methods. We demonstrate the real-world applicability of our work via the Robot Follow-Ahead task, a challenging yet practical case study for our proposed model.
<div id='section'>Paperid: <span id='pid'>1409, <a href='https://arxiv.org/pdf/2208.01862.pdf' target='_blank'>https://arxiv.org/pdf/2208.01862.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Phillip Karle, Ferenc TÃ¶rÃ¶k, Maximilian Geisslinger, Markus Lienkamp
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2208.01862">MixNet: Structured Deep Neural Motion Prediction for Autonomous Racing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reliably predicting the motion of contestant vehicles surrounding an autonomous racecar is crucial for effective and performant planning. Although highly expressive, deep neural networks are black-box models, making their usage challenging in safety-critical applications, such as autonomous driving. In this paper, we introduce a structured way of forecasting the movement of opposing racecars with deep neural networks. The resulting set of possible output trajectories is constrained. Hence quality guarantees about the prediction can be given. We report the performance of the model by evaluating it together with an LSTM-based encoder-decoder architecture on data acquired from high-fidelity Hardware-in-the-Loop simulations. The proposed approach outperforms the baseline regarding the prediction accuracy but still fulfills the quality guarantees. Thus, a robust real-world application of the model is proven. The presented model was deployed on the racecar of the Technical University of Munich for the Indy Autonomous Challenge 2021. The code used in this research is available as open-source software at www.github.com/TUMFTM/MixNet.
<div id='section'>Paperid: <span id='pid'>1410, <a href='https://arxiv.org/pdf/2208.00946.pdf' target='_blank'>https://arxiv.org/pdf/2208.00946.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xing Zhao, Haoran Liang, Peipei Li, Guodao Sun, Dongdong Zhao, Ronghua Liang, Xiaofei He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2208.00946">Motion-aware Memory Network for Fast Video Salient Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Previous methods based on 3DCNN, convLSTM, or optical flow have achieved great success in video salient object detection (VSOD). However, they still suffer from high computational costs or poor quality of the generated saliency maps. To solve these problems, we design a space-time memory (STM)-based network, which extracts useful temporal information of the current frame from adjacent frames as the temporal branch of VSOD. Furthermore, previous methods only considered single-frame prediction without temporal association. As a result, the model may not focus on the temporal information sufficiently. Thus, we initially introduce object motion prediction between inter-frame into VSOD. Our model follows standard encoder--decoder architecture. In the encoding stage, we generate high-level temporal features by using high-level features from the current and its adjacent frames. This approach is more efficient than the optical flow-based methods. In the decoding stage, we propose an effective fusion strategy for spatial and temporal branches. The semantic information of the high-level features is used to fuse the object details in the low-level features, and then the spatiotemporal features are obtained step by step to reconstruct the saliency maps. Moreover, inspired by the boundary supervision commonly used in image salient object detection (ISOD), we design a motion-aware loss for predicting object boundary motion and simultaneously perform multitask learning for VSOD and object motion prediction, which can further facilitate the model to extract spatiotemporal features accurately and maintain the object integrity. Extensive experiments on several datasets demonstrated the effectiveness of our method and can achieve state-of-the-art metrics on some datasets. The proposed model does not require optical flow or other preprocessing, and can reach a speed of nearly 100 FPS during inference.
<div id='section'>Paperid: <span id='pid'>1411, <a href='https://arxiv.org/pdf/2204.10777.pdf' target='_blank'>https://arxiv.org/pdf/2204.10777.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xu Shen, Matthew Lacayo, Nidhir Guggilla, Francesco Borrelli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2204.10777">ParkPredict+: Multimodal Intent and Motion Prediction for Vehicles in Parking Lots with CNN and Transformer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The problem of multimodal intent and trajectory prediction for human-driven vehicles in parking lots is addressed in this paper. Using models designed with CNN and Transformer networks, we extract temporal-spatial and contextual information from trajectory history and local bird's eye view (BEV) semantic images, and generate predictions about intent distribution and future trajectory sequences. Our methods outperform existing models in accuracy, while allowing an arbitrary number of modes, encoding complex multi-agent scenarios, and adapting to different parking maps. To train and evaluate our method, we present the first public 4K video dataset of human driving in parking lots with accurate annotation, high frame rate, and rich traffic scenarios.
<div id='section'>Paperid: <span id='pid'>1412, <a href='https://arxiv.org/pdf/2204.06776.pdf' target='_blank'>https://arxiv.org/pdf/2204.06776.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haolong Li, Joerg Stueckler
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2204.06776">Visual-Inertial Odometry with Online Calibration of Velocity-Control Based Kinematic Motion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual-inertial odometry (VIO) is an important technology for autonomous robots with power and payload constraints. In this paper, we propose a novel approach for VIO with stereo cameras which integrates and calibrates the velocity-control based kinematic motion model of wheeled mobile robots online. Including such a motion model can help to improve the accuracy of VIO. Compared to several previous approaches proposed to integrate wheel odometer measurements for this purpose, our method does not require wheel encoders and can be applied when the robot motion can be modeled with velocity-control based kinematic motion model. We use radial basis function (RBF) kernels to compensate for the time delay and deviations between control commands and actual robot motion. The motion model is calibrated online by the VIO system and can be used as a forward model for motion control and planning. We evaluate our approach with data obtained in variously sized indoor environments, demonstrate improvements over a pure VIO method, and evaluate the prediction accuracy of the online calibrated model.
<div id='section'>Paperid: <span id='pid'>1413, <a href='https://arxiv.org/pdf/2109.13338.pdf' target='_blank'>https://arxiv.org/pdf/2109.13338.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nitish Sontakke, Sehoon Ha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2109.13338">Solving Challenging Control Problems Using Two-Staged Deep Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a deep reinforcement learning (deep RL) algorithm that consists of learning-based motion planning and imitation to tackle challenging control problems. Deep RL has been an effective tool for solving many high-dimensional continuous control problems, but it cannot effectively solve challenging problems with certain properties, such as sparse reward functions or sensitive dynamics. In this work, we propose an approach that decomposes the given problem into two deep RL stages: motion planning and motion imitation. The motion planning stage seeks to compute a feasible motion plan by leveraging the powerful planning capability of deep RL. Subsequently, the motion imitation stage learns a control policy that can imitate the given motion plan with realistic sensors and actuation models. This new formulation requires only a nominal added cost to the user because both stages require minimal changes to the original problem. We demonstrate that our approach can solve challenging control problems, rocket navigation, and quadrupedal locomotion, which cannot be solved by the monolithic deep RL formulation or the version with Probabilistic Roadmap.
<div id='section'>Paperid: <span id='pid'>1414, <a href='https://arxiv.org/pdf/2108.09393.pdf' target='_blank'>https://arxiv.org/pdf/2108.09393.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kayla-Jade Butkow, Ting Dang, Andrea Ferlini, Dong Ma, Cecilia Mascolo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2108.09393">hEARt: Motion-resilient Heart Rate Monitoring with In-ear Microphones</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the soaring adoption of in-ear wearables, the research community has started investigating suitable in-ear heart rate (HR) detection systems. HR is a key physiological marker of cardiovascular health and physical fitness. Continuous and reliable HR monitoring with wearable devices has therefore gained increasing attention in recent years. Existing HR detection systems in wearables mainly rely on photoplethysmography (PPG) sensors, however, these are notorious for poor performance in the presence of human motion. In this work, leveraging the occlusion effect that enhances low-frequency bone-conducted sounds in the ear canal, we investigate for the first time \textit{in-ear audio-based motion-resilient} HR monitoring. We first collected HR-induced sounds in the ear canal leveraging an in-ear microphone under stationary and three different activities (i.e., walking, running, and speaking). Then, we devised a novel deep learning based motion artefact (MA) mitigation framework to denoise the in-ear audio signals, followed by an HR estimation algorithm to extract HR. With data collected from 20 subjects over four activities, we demonstrate that hEARt, our end-to-end approach, achieves a mean absolute error (MAE) of 3.02 $\pm$ 2.97~BPM, 8.12 $\pm$ 6.74~BPM, 11.23 $\pm$ 9.20~BPM and 9.39 $\pm$ 6.97~BPM for stationary, walking, running and speaking, respectively, opening the door to a new non-invasive and affordable HR monitoring with usable performance for daily activities. Not only does hEARt outperform previous in-ear HR monitoring work, but it outperforms reported in-ear PPG performance.
<div id='section'>Paperid: <span id='pid'>1415, <a href='https://arxiv.org/pdf/2107.03147.pdf' target='_blank'>https://arxiv.org/pdf/2107.03147.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andreas Spilz, Michael Munz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2107.03147">Novel Approach To Synchronisation Of Wearable IMUs Based On Magnetometers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Synchronisation of wireless inertial measurement units in human movement analysis is often achieved using event-based synchronisation techniques. However, these techniques lack precise event generation and accuracy. An inaccurate synchronisation could lead to large errors in motion estimation and reconstruction and therefore wrong analysis outputs. We propose a novel event-based synchronisation technique based on a magnetic field, which allows sub-sample accuracy. A setup featuring Shimmer3 inertial measurement units is designed to test the approach. The proposed technique shows to be able to synchronise with a maximum offset of below 2.6~ms with sensors measuring at 100~Hz. Also, the results indicate a reliable event generation and detection. The investigated parameters suggest a required synchronisation time of eight seconds. Further research should investigate the temperature changes that the sensors are exposed to during human motion analysis and their influence on the internal time measurement of the sensors. In addition, the approach should be tested using inertial measurement units from different manufacturers to investigate an identified constant offset in the accuracy measurements.
<div id='section'>Paperid: <span id='pid'>1416, <a href='https://arxiv.org/pdf/2510.02968.pdf' target='_blank'>https://arxiv.org/pdf/2510.02968.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Amir Habel, Fawad Mehboob, Jeffrin Sam, Clement Fortin, Dzmitry Tsetserukou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.02968">YawSitter: Modeling and Controlling a Tail-Sitter UAV with Enhanced Yaw Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Achieving precise lateral motion modeling and decoupled control in hover remains a significant challenge for tail-sitter Unmanned Aerial Vehicles (UAVs), primarily due to complex aerodynamic couplings and the absence of welldefined lateral dynamics. This paper presents a novel modeling and control strategy that enhances yaw authority and lateral motion by introducing a sideslip force model derived from differential propeller slipstream effects acting on the fuselage under differential thrust. The resulting lateral force along the body y-axis enables yaw-based lateral position control without inducing roll coupling. The control framework employs a YXZ Euler rotation formulation to accurately represent attitude and incorporate gravitational components while directly controlling yaw in the yaxis, thereby improving lateral dynamic behavior and avoiding singularities. The proposed approach is validated through trajectory-tracking simulations conducted in a Unity-based environment. Tests on both rectangular and circular paths in hover mode demonstrate stable performance, with low mean absolute position errors and yaw deviations constrained within 5.688 degrees. These results confirm the effectiveness of the proposed lateral force generation model and provide a foundation for the development of agile, hover-capable tail-sitter UAVs.
<div id='section'>Paperid: <span id='pid'>1417, <a href='https://arxiv.org/pdf/2509.24099.pdf' target='_blank'>https://arxiv.org/pdf/2509.24099.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Prerit Gupta, Shourya Verma, Ananth Grama, Aniket Bera
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24099">Unified Multi-Modal Interactive & Reactive 3D Motion Generation via Rectified Flow</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating realistic, context-aware two-person motion conditioned on diverse modalities remains a central challenge in computer graphics, animation, and human-computer interaction. We introduce DualFlow, a unified and efficient framework for multi-modal two-person motion generation. DualFlow conditions 3D motion synthesis on diverse inputs, including text, music, and prior motion sequences. Leveraging rectified flow, it achieves deterministic straight-line sampling paths between noise and data, reducing inference time and mitigating error accumulation common in diffusion-based models. To enhance semantic grounding, DualFlow employs a Retrieval-Augmented Generation (RAG) module that retrieves motion exemplars using music features and LLM-based text decompositions of spatial relations, body movements, and rhythmic patterns. We use contrastive objective that further strengthens alignment with conditioning signals and introduce synchronization loss that improves inter-person coordination. Extensive evaluations across text-to-motion, music-to-motion, and multi-modal interactive benchmarks show consistent gains in motion quality, responsiveness, and efficiency. DualFlow produces temporally coherent and rhythmically synchronized motions, setting state-of-the-art in multi-modal human motion generation.
<div id='section'>Paperid: <span id='pid'>1418, <a href='https://arxiv.org/pdf/2509.15443.pdf' target='_blank'>https://arxiv.org/pdf/2509.15443.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xingyu Chen, Hanyu Wu, Sikai Wu, Mingliang Zhou, Diyun Xiang, Haodong Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15443">Implicit Kinodynamic Motion Retargeting for Human-to-humanoid Imitation Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human-to-humanoid imitation learning aims to learn a humanoid whole-body controller from human motion. Motion retargeting is a crucial step in enabling robots to acquire reference trajectories when exploring locomotion skills. However, current methods focus on motion retargeting frame by frame, which lacks scalability. Could we directly convert large-scale human motion into robot-executable motion through a more efficient approach? To address this issue, we propose Implicit Kinodynamic Motion Retargeting (IKMR), a novel efficient and scalable retargeting framework that considers both kinematics and dynamics. In kinematics, IKMR pretrains motion topology feature representation and a dual encoder-decoder architecture to learn a motion domain mapping. In dynamics, IKMR integrates imitation learning with the motion retargeting network to refine motion into physically feasible trajectories. After fine-tuning using the tracking results, IKMR can achieve large-scale physically feasible motion retargeting in real time, and a whole-body controller could be directly trained and deployed for tracking its retargeted trajectories. We conduct our experiments both in the simulator and the real robot on a full-size humanoid robot. Extensive experiments and evaluation results verify the effectiveness of our proposed framework.
<div id='section'>Paperid: <span id='pid'>1419, <a href='https://arxiv.org/pdf/2509.14010.pdf' target='_blank'>https://arxiv.org/pdf/2509.14010.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zong Chen, Shaoyang Li, Ben Liu, Min Li, Zhouping Yin, Yiqun Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14010">Whole-body Motion Control of an Omnidirectional Wheel-Legged Mobile Manipulator via Contact-Aware Dynamic Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Wheel-legged robots with integrated manipulators hold great promise for mobile manipulation in logistics, industrial automation, and human-robot collaboration. However, unified control of such systems remains challenging due to the redundancy in degrees of freedom, complex wheel-ground contact dynamics, and the need for seamless coordination between locomotion and manipulation. In this work, we present the design and whole-body motion control of an omnidirectional wheel-legged quadrupedal robot equipped with a dexterous manipulator. The proposed platform incorporates independently actuated steering modules and hub-driven wheels, enabling agile omnidirectional locomotion with high maneuverability in structured environments. To address the challenges of contact-rich interaction, we develop a contact-aware whole-body dynamic optimization framework that integrates point-contact modeling for manipulation with line-contact modeling for wheel-ground interactions. A warm-start strategy is introduced to accelerate online optimization, ensuring real-time feasibility for high-dimensional control. Furthermore, a unified kinematic model tailored for the robot's 4WIS-4WID actuation scheme eliminates the need for mode switching across different locomotion strategies, improving control consistency and robustness. Simulation and experimental results validate the effectiveness of the proposed framework, demonstrating agile terrain traversal, high-speed omnidirectional mobility, and precise manipulation under diverse scenarios, underscoring the system's potential for factory automation, urban logistics, and service robotics in semi-structured environments.
<div id='section'>Paperid: <span id='pid'>1420, <a href='https://arxiv.org/pdf/2509.12151.pdf' target='_blank'>https://arxiv.org/pdf/2509.12151.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zongyao Yi, Joachim Hertzberg, Martin Atzmueller
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.12151">Learning Contact Dynamics for Control with Action-conditioned Face Interaction Graph Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a learnable physics simulator that provides accurate motion and force-torque prediction of robot end effectors in contact-rich manipulation. The proposed model extends the state-of-the-art GNN-based simulator (FIGNet) with novel node and edge types, enabling action-conditional predictions for control and state estimation tasks. In simulation, the MPC agent using our model matches the performance of the same controller with the ground truth dynamics model in a challenging peg-in-hole task, while in the real-world experiment, our model achieves a 50% improvement in motion prediction accuracy and 3$\times$ increase in force-torque prediction precision over the baseline physics simulator. Source code and data are publicly available.
<div id='section'>Paperid: <span id='pid'>1421, <a href='https://arxiv.org/pdf/2509.11453.pdf' target='_blank'>https://arxiv.org/pdf/2509.11453.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>BaiChen Fan, Sifan Zhou, Jian Li, Shibo Zhao, Muqing Cao, Qin Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11453">Beyond Frame-wise Tracking: A Trajectory-based Paradigm for Efficient Point Cloud Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>LiDAR-based 3D single object tracking (3D SOT) is a critical task in robotics and autonomous systems. Existing methods typically follow frame-wise motion estimation or a sequence-based paradigm. However, the two-frame methods are efficient but lack long-term temporal context, making them vulnerable in sparse or occluded scenes, while sequence-based methods that process multiple point clouds gain robustness at a significant computational cost. To resolve this dilemma, we propose a novel trajectory-based paradigm and its instantiation, TrajTrack. TrajTrack is a lightweight framework that enhances a base two-frame tracker by implicitly learning motion continuity from historical bounding box trajectories alone-without requiring additional, costly point cloud inputs. It first generates a fast, explicit motion proposal and then uses an implicit motion modeling module to predict the future trajectory, which in turn refines and corrects the initial proposal. Extensive experiments on the large-scale NuScenes benchmark show that TrajTrack achieves new state-of-the-art performance, dramatically improving tracking precision by 4.48% over a strong baseline while running at 56 FPS. Besides, we also demonstrate the strong generalizability of TrajTrack across different base trackers. Video is available at https://www.bilibili.com/video/BV1ahYgzmEWP.
<div id='section'>Paperid: <span id='pid'>1422, <a href='https://arxiv.org/pdf/2509.06573.pdf' target='_blank'>https://arxiv.org/pdf/2509.06573.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jie Zhou, Linzi Qu, Miu-Ling Lam, Hongbo Fu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.06573">From Rigging to Waving: 3D-Guided Diffusion for Natural Animation of Hand-Drawn Characters</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Hand-drawn character animation is a vibrant field in computer graphics, presenting challenges in achieving geometric consistency while conveying expressive motion. Traditional skeletal animation methods maintain geometric consistency but struggle with complex non-rigid elements like flowing hair and skirts, leading to unnatural deformation. Conversely, video diffusion models synthesize realistic dynamics but often create geometric distortions in stylized drawings due to domain gaps. This work proposes a hybrid animation system that combines skeletal animation and video diffusion. Initially, coarse images are generated from characters retargeted with skeletal animations for geometric guidance. These images are then enhanced in texture and secondary dynamics using video diffusion priors, framing this enhancement as an inpainting task. A domain-adapted diffusion model refines user-masked regions needing improvement, especially for secondary dynamics. To enhance motion realism further, we introduce a Secondary Dynamics Injection (SDI) strategy in the denoising process, incorporating features from a pre-trained diffusion model enriched with human motion priors. Additionally, to tackle unnatural deformations from low-poly single-mesh character modeling, we present a Hair Layering Modeling (HLM) technique that uses segmentation maps to separate hair from the body, allowing for more natural animation of long-haired characters. Extensive experiments show that our system outperforms state-of-the-art methods in both quantitative and qualitative evaluations.
<div id='section'>Paperid: <span id='pid'>1423, <a href='https://arxiv.org/pdf/2509.04058.pdf' target='_blank'>https://arxiv.org/pdf/2509.04058.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lei Zhong, Yi Yang, Changjian Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.04058">SMooGPT: Stylized Motion Generation using Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Stylized motion generation is actively studied in computer graphics, especially benefiting from the rapid advances in diffusion models. The goal of this task is to produce a novel motion respecting both the motion content and the desired motion style, e.g., ``walking in a loop like a Monkey''. Existing research attempts to address this problem via motion style transfer or conditional motion generation. They typically embed the motion style into a latent space and guide the motion implicitly in a latent space as well. Despite the progress, their methods suffer from low interpretability and control, limited generalization to new styles, and fail to produce motions other than ``walking'' due to the strong bias in the public stylization dataset. In this paper, we propose to solve the stylized motion generation problem from a new perspective of reasoning-composition-generation, based on our observations: i) human motion can often be effectively described using natural language in a body-part centric manner, ii) LLMs exhibit a strong ability to understand and reason about human motion, and iii) human motion has an inherently compositional nature, facilitating the new motion content or style generation via effective recomposing. We thus propose utilizing body-part text space as an intermediate representation, and present SMooGPT, a fine-tuned LLM, acting as a reasoner, composer, and generator when generating the desired stylized motion. Our method executes in the body-part text space with much higher interpretability, enabling fine-grained motion control, effectively resolving potential conflicts between motion content and style, and generalizes well to new styles thanks to the open-vocabulary ability of LLMs. Comprehensive experiments and evaluations, and a user perceptual study, demonstrate the effectiveness of our approach, especially under the pure text-driven stylized motion generation.
<div id='section'>Paperid: <span id='pid'>1424, <a href='https://arxiv.org/pdf/2509.02983.pdf' target='_blank'>https://arxiv.org/pdf/2509.02983.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinghe Yang, Minh-Quan Le, Mingming Gong, Ye Pu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.02983">DUViN: Diffusion-Based Underwater Visual Navigation via Knowledge-Transferred Depth Features</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous underwater navigation remains a challenging problem due to limited sensing capabilities and the difficulty of constructing accurate maps in underwater environments. In this paper, we propose a Diffusion-based Underwater Visual Navigation policy via knowledge-transferred depth features, named DUViN, which enables vision-based end-to-end 4-DoF motion control for underwater vehicles in unknown environments. DUViN guides the vehicle to avoid obstacles and maintain a safe and perception awareness altitude relative to the terrain without relying on pre-built maps. To address the difficulty of collecting large-scale underwater navigation datasets, we propose a method that ensures robust generalization under domain shifts from in-air to underwater environments by leveraging depth features and introducing a novel model transfer strategy. Specifically, our training framework consists of two phases: we first train the diffusion-based visual navigation policy on in-air datasets using a pre-trained depth feature extractor. Secondly, we retrain the extractor on an underwater depth estimation task and integrate the adapted extractor into the trained navigation policy from the first step. Experiments in both simulated and real-world underwater environments demonstrate the effectiveness and generalization of our approach. The experimental videos are available at https://www.youtube.com/playlist?list=PLqt2s-RyCf1gfXJgFzKjmwIqYhrP4I-7Y.
<div id='section'>Paperid: <span id='pid'>1425, <a href='https://arxiv.org/pdf/2508.20553.pdf' target='_blank'>https://arxiv.org/pdf/2508.20553.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alexander Gräfe, Joram Eickhoff, Marco Zimmerling, Sebastian Trimpe
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.20553">DMPC-Swarm: Distributed Model Predictive Control on Nano UAV Swarms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Swarms of unmanned aerial vehicles (UAVs) are increasingly becoming vital to our society, undertaking tasks such as search and rescue, surveillance and delivery. A special variant of Distributed Model Predictive Control (DMPC) has emerged as a promising approach for the safe management of these swarms by combining the scalability of distributed computation with dynamic swarm motion control. In this DMPC method, multiple agents solve local optimization problems with coupled anti-collision constraints, periodically exchanging their solutions. Despite its potential, existing methodologies using this DMPC variant have yet to be deployed on distributed hardware that fully utilize true distributed computation and wireless communication. This is primarily due to the lack of a communication system tailored to meet the unique requirements of mobile swarms and an architecture that supports distributed computation while adhering to the payload constraints of UAVs. We present DMPC-SWARM, a new swarm control methodology that integrates an efficient, stateless low-power wireless communication protocol with a novel DMPC algorithm that provably avoids UAV collisions even under message loss. By utilizing event-triggered and distributed off-board computing, DMPC-SWARM supports nano UAVs, allowing them to benefit from additional computational resources while retaining scalability and fault tolerance. In a detailed theoretical analysis, we prove that DMPC-SWARM guarantees collision avoidance under realistic conditions, including communication delays and message loss. Finally, we present DMPC-SWARM's implementation on a swarm of up to 16 nano-quadcopters, demonstrating the first realization of these DMPC variants with computation distributed on multiple physical devices interconnected by a real wireless mesh networks. A video showcasing DMPC-SWARM is available at http://tiny.cc/DMPCSwarm.
<div id='section'>Paperid: <span id='pid'>1426, <a href='https://arxiv.org/pdf/2508.20553.pdf' target='_blank'>https://arxiv.org/pdf/2508.20553.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alexander Gräfe, Joram Eickhoff, Marco Zimmerling, Sebastian Trimpe
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.20553">DMPC-Swarm: Distributed Model Predictive Control on Nano UAV Swarms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Swarms of unmanned aerial vehicles (UAVs) are increasingly becoming vital to our society, undertaking tasks such as search and rescue, surveillance and delivery. A special variant of Distributed Model Predictive Control (DMPC) has emerged as a promising approach for the safe management of these swarms by combining the scalability of distributed computation with dynamic swarm motion control. In this DMPC method, multiple agents solve local optimization problems with coupled anti-collision constraints, periodically exchanging their solutions. Despite its potential, existing methodologies using this DMPC variant have yet to be deployed on distributed hardware that fully utilize true distributed computation and wireless communication. This is primarily due to the lack of a communication system tailored to meet the unique requirements of mobile swarms and an architecture that supports distributed computation while adhering to the payload constraints of UAVs. We present DMPC-SWARM, a new swarm control methodology that integrates an efficient, stateless low-power wireless communication protocol with a novel DMPC algorithm that provably avoids UAV collisions even under message loss. By utilizing event-triggered and distributed off-board computing, DMPC-SWARM supports nano UAVs, allowing them to benefit from additional computational resources while retaining scalability and fault tolerance. In a detailed theoretical analysis, we prove that DMPC-SWARM guarantees collision avoidance under realistic conditions, including communication delays and message loss. Finally, we present DMPC-SWARM's implementation on a swarm of up to 16 nano-quadcopters, demonstrating the first realization of these DMPC variants with computation distributed on multiple physical devices interconnected by a real wireless mesh networks. A video showcasing DMPC-SWARM is available at http://tiny.cc/DMPCSwarm.
<div id='section'>Paperid: <span id='pid'>1427, <a href='https://arxiv.org/pdf/2508.14309.pdf' target='_blank'>https://arxiv.org/pdf/2508.14309.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaohai Hu, Jason Laks, Guoxiao Guo, Xu Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.14309">Iterative Youla-Kucera Loop Shaping For Precision Motion Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a numerically robust approach to multi-band disturbance rejection using an iterative Youla-Kucera parameterization technique. The proposed method offers precise control over shaping the frequency response of a feedback loop while maintaining numerical stability through a systematic design process. By implementing an iterative approach, we overcome a critical numerical issue in rejecting vibrations with multiple frequency bands. Meanwhile, our proposed modification of the all-stabilizing Youla-Kucera architecture enables intuitive design while respecting fundamental performance trade-offs and minimizing undesired waterbed amplifications. Numerical validation on a hard disk drive servo system demonstrates significant performance improvements, enabling enhanced positioning precision for increased storage density. The design methodology extends beyond storage systems to various high-precision control applications where multi-band disturbance rejection is critical.
<div id='section'>Paperid: <span id='pid'>1428, <a href='https://arxiv.org/pdf/2508.01894.pdf' target='_blank'>https://arxiv.org/pdf/2508.01894.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haozhe Zhou, Riku Arakawa, Yuvraj Agarwal, Mayank Goel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01894">IMUCoCo: Enabling Flexible On-Body IMU Placement for Human Pose Estimation and Activity Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>IMUs are regularly used to sense human motion, recognize activities, and estimate full-body pose. Users are typically required to place sensors in predefined locations that are often dictated by common wearable form factors and the machine learning model's training process. Consequently, despite the increasing number of everyday devices equipped with IMUs, the limited adaptability has seriously constrained the user experience to only using a few well-explored device placements (e.g., wrist and ears). In this paper, we rethink IMU-based motion sensing by acknowledging that signals can be captured from any point on the human body. We introduce IMU over Continuous Coordinates (IMUCoCo), a novel framework that maps signals from a variable number of IMUs placed on the body surface into a unified feature space based on their spatial coordinates. These features can be plugged into downstream models for pose estimation and activity recognition. Our evaluations demonstrate that IMUCoCo supports accurate pose estimation in a wide range of typical and atypical sensor placements. Overall, IMUCoCo supports significantly more flexible use of IMUs for motion sensing than the state-of-the-art, allowing users to place their sensors-laden devices according to their needs and preferences. The framework also supports the ability to change device locations depending on the context and suggests placement depending on the use case.
<div id='section'>Paperid: <span id='pid'>1429, <a href='https://arxiv.org/pdf/2507.17445.pdf' target='_blank'>https://arxiv.org/pdf/2507.17445.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haichuan Li, Changda Tian, Panos Trahanias, Tomi Westerlund
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.17445">IndoorBEV: Joint Detection and Footprint Completion of Objects via Mask-based Prediction in Indoor Scenarios for Bird's-Eye View Perception</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Detecting diverse objects within complex indoor 3D point clouds presents significant challenges for robotic perception, particularly with varied object shapes, clutter, and the co-existence of static and dynamic elements where traditional bounding box methods falter. To address these limitations, we propose IndoorBEV, a novel mask-based Bird's-Eye View (BEV) method for indoor mobile robots.
  In a BEV method, a 3D scene is projected into a 2D BEV grid which handles naturally occlusions and provides a consistent top-down view aiding to distinguish static obstacles from dynamic agents. The obtained 2D BEV results is directly usable to downstream robotic tasks like navigation, motion prediction, and planning. Our architecture utilizes an axis compact encoder and a window-based backbone to extract rich spatial features from this BEV map. A query-based decoder head then employs learned object queries to concurrently predict object classes and instance masks in the BEV space. This mask-centric formulation effectively captures the footprint of both static and dynamic objects regardless of their shape, offering a robust alternative to bounding box regression. We demonstrate the effectiveness of IndoorBEV on a custom indoor dataset featuring diverse object classes including static objects
  and dynamic elements like robots and miscellaneous items, showcasing its potential for robust indoor scene understanding.
<div id='section'>Paperid: <span id='pid'>1430, <a href='https://arxiv.org/pdf/2507.15194.pdf' target='_blank'>https://arxiv.org/pdf/2507.15194.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yilin Lyu, Fan Yang, Xiaoyue Liu, Zichen Jiang, Joshua Dillon, Debbie Zhao, Martyn Nash, Charlene Mauger, Alistair Young, Ching-Hui Sia, Mark YY Chan, Lei Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.15194">Personalized 3D Myocardial Infarct Geometry Reconstruction from Cine MRI with Explicit Cardiac Motion Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate representation of myocardial infarct geometry is crucial for patient-specific cardiac modeling in MI patients. While Late gadolinium enhancement (LGE) MRI is the clinical gold standard for infarct detection, it requires contrast agents, introducing side effects and patient discomfort. Moreover, infarct reconstruction from LGE often relies on sparsely sampled 2D slices, limiting spatial resolution and accuracy. In this work, we propose a novel framework for automatically reconstructing high-fidelity 3D myocardial infarct geometry from 2D clinically standard cine MRI, eliminating the need for contrast agents. Specifically, we first reconstruct the 4D biventricular mesh from multi-view cine MRIs via an automatic deep shape fitting model, biv-me. Then, we design a infarction reconstruction model, CMotion2Infarct-Net, to explicitly utilize the motion patterns within this dynamic geometry to localize infarct regions. Evaluated on 205 cine MRI scans from 126 MI patients, our method shows reasonable agreement with manual delineation. This study demonstrates the feasibility of contrast-free, cardiac motion-driven 3D infarct reconstruction, paving the way for efficient digital twin of MI.
<div id='section'>Paperid: <span id='pid'>1431, <a href='https://arxiv.org/pdf/2507.03227.pdf' target='_blank'>https://arxiv.org/pdf/2507.03227.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruoshi Wen, Jiajun Zhang, Guangzeng Chen, Zhongren Cui, Min Du, Yang Gou, Zhigang Han, Junkai Hu, Liqun Huang, Hao Niu, Wei Xu, Haoxiang Zhang, Zhengming Zhu, Hang Li, Zeyu Ren
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.03227">Dexterous Teleoperation of 20-DoF ByteDexter Hand via Human Motion Retargeting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Replicating human--level dexterity remains a fundamental robotics challenge, requiring integrated solutions from mechatronic design to the control of high degree--of--freedom (DoF) robotic hands. While imitation learning shows promise in transferring human dexterity to robots, the efficacy of trained policies relies on the quality of human demonstration data. We bridge this gap with a hand--arm teleoperation system featuring: (1) a 20--DoF linkage--driven anthropomorphic robotic hand for biomimetic dexterity, and (2) an optimization--based motion retargeting for real--time, high--fidelity reproduction of intricate human hand motions and seamless hand--arm coordination. We validate the system via extensive empirical evaluations, including dexterous in-hand manipulation tasks and a long--horizon task requiring the organization of a cluttered makeup table randomly populated with nine objects. Experimental results demonstrate its intuitive teleoperation interface with real--time control and the ability to generate high--quality demonstration data. Please refer to the accompanying video for further details.
<div id='section'>Paperid: <span id='pid'>1432, <a href='https://arxiv.org/pdf/2507.01491.pdf' target='_blank'>https://arxiv.org/pdf/2507.01491.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>S. Ali Hosseini, Fabian R. Quinten, Luke F. van Eijk, Dragan Kostic, S. Hassan HosseinNia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.01491">Frequency Domain Design of a Reset-Based Filter: An Add-On Nonlinear Filter for Industrial Motion Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study introduces a modified version of the Constant-in-Gain, Lead-in-Phase (CgLp) filter, which incorporates a feedthrough term in the First-Order Reset Element (FORE) to reduce the undesirable nonlinearities and achieve an almost constant gain across all frequencies. A backward calculation approach is proposed to derive the additional parameter introduced by the feedthrough term, enabling designers to easily tune the filter to generate the required phase. The paper also presents an add-on filter structure that can enhance the performance of an existing LTI controller without altering its robustness margins. A sensitivity improvement indicator is proposed to guide the tuning process, enabling designers to visualize the improvements in closed-loop performance. The proposed methodology is demonstrated through a case study of an industrial wire bonder machine, showcasing its effectiveness in addressing low-frequency vibrations and improving overall control performance.
<div id='section'>Paperid: <span id='pid'>1433, <a href='https://arxiv.org/pdf/2507.01308.pdf' target='_blank'>https://arxiv.org/pdf/2507.01308.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Muhammad Atta ur Rahman, Dooseop Choi, KyoungWook Min
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.01308">LANet: A Lane Boundaries-Aware Approach For Robust Trajectory Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate motion forecasting is critical for safe and efficient autonomous driving, enabling vehicles to predict future trajectories and make informed decisions in complex traffic scenarios. Most of the current designs of motion prediction models are based on the major representation of lane centerlines, which limits their capability to capture critical road environments and traffic rules and constraints. In this work, we propose an enhanced motion forecasting model informed by multiple vector map elements, including lane boundaries and road edges, that facilitates a richer and more complete representation of driving environments. An effective feature fusion strategy is developed to merge information in different vector map components, where the model learns holistic information on road structures and their interactions with agents. Since encoding more information about the road environment increases memory usage and is computationally expensive, we developed an effective pruning mechanism that filters the most relevant map connections to the target agent, ensuring computational efficiency while maintaining essential spatial and semantic relationships for accurate trajectory prediction. Overcoming the limitations of lane centerline-based models, our method provides a more informative and efficient representation of the driving environment and advances the state of the art for autonomous vehicle motion forecasting. We verify our approach with extensive experiments on the Argoverse 2 motion forecasting dataset, where our method maintains competitiveness on AV2 while achieving improved performance.
  Index Terms-Autonomous driving, trajectory prediction, vector map elements, road topology, connection pruning, Argoverse 2.
<div id='section'>Paperid: <span id='pid'>1434, <a href='https://arxiv.org/pdf/2506.14198.pdf' target='_blank'>https://arxiv.org/pdf/2506.14198.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jeremy A. Collins, LorÃ¡nd Cheng, Kunal Aneja, Albert Wilcox, Benjamin Joffe, Animesh Garg
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.14198">AMPLIFY: Actionless Motion Priors for Robot Learning from Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Action-labeled data for robotics is scarce and expensive, limiting the generalization of learned policies. In contrast, vast amounts of action-free video data are readily available, but translating these observations into effective policies remains a challenge. We introduce AMPLIFY, a novel framework that leverages large-scale video data by encoding visual dynamics into compact, discrete motion tokens derived from keypoint trajectories. Our modular approach separates visual motion prediction from action inference, decoupling the challenges of learning what motion defines a task from how robots can perform it. We train a forward dynamics model on abundant action-free videos and an inverse dynamics model on a limited set of action-labeled examples, allowing for independent scaling. Extensive evaluations demonstrate that the learned dynamics are both accurate, achieving up to 3.7x better MSE and over 2.5x better pixel prediction accuracy compared to prior approaches, and broadly useful. In downstream policy learning, our dynamics predictions enable a 1.2-2.2x improvement in low-data regimes, a 1.4x average improvement by learning from action-free human videos, and the first generalization to LIBERO tasks from zero in-distribution action data. Beyond robotic control, we find the dynamics learned by AMPLIFY to be a versatile latent world model, enhancing video prediction quality. Our results present a novel paradigm leveraging heterogeneous data sources to build efficient, generalizable world models. More information can be found at https://amplify-robotics.github.io/.
<div id='section'>Paperid: <span id='pid'>1435, <a href='https://arxiv.org/pdf/2506.10016.pdf' target='_blank'>https://arxiv.org/pdf/2506.10016.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Longzhen Han, Awes Mubarak, Almas Baimagambetov, Nikolaos Polatidis, Thar Baker
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.10016">A Survey of Generative Categories and Techniques in Multimodal Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multimodal Large Language Models (MLLMs) have rapidly evolved beyond text generation, now spanning diverse output modalities including images, music, video, human motion, and 3D objects, by integrating language with other sensory modalities under unified architectures. This survey categorises six primary generative modalities and examines how foundational techniques, namely Self-Supervised Learning (SSL), Mixture of Experts (MoE), Reinforcement Learning from Human Feedback (RLHF), and Chain-of-Thought (CoT) prompting, enable cross-modal capabilities. We analyze key models, architectural trends, and emergent cross-modal synergies, while highlighting transferable techniques and unresolved challenges. Architectural innovations like transformers and diffusion models underpin this convergence, enabling cross-modal transfer and modular specialization. We highlight emerging patterns of synergy, and identify open challenges in evaluation, modularity, and structured reasoning. This survey offers a unified perspective on MLLM development and identifies critical paths toward more general-purpose, adaptive, and interpretable multimodal systems.
<div id='section'>Paperid: <span id='pid'>1436, <a href='https://arxiv.org/pdf/2506.07076.pdf' target='_blank'>https://arxiv.org/pdf/2506.07076.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinyi Wu, Haohong Wang, Aggelos K. Katsaggelos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.07076">Harmony-Aware Music-driven Motion Synthesis with Perceptual Constraint on UGC Datasets</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the popularity of video-based user-generated content (UGC) on social media, harmony, as dictated by human perceptual principles, is critical in assessing the rhythmic consistency of audio-visual UGCs for better user engagement. In this work, we propose a novel harmony-aware GAN framework, following a specifically designed harmony evaluation strategy to enhance rhythmic synchronization in the automatic music-to-motion synthesis using a UGC dance dataset. This harmony strategy utilizes refined cross-modal beat detection to capture closely correlated audio and visual rhythms in an audio-visual pair. To mimic human attention mechanism, we introduce saliency-based beat weighting and interval-driven beat alignment, which ensures accurate harmony score estimation consistent with human perception. Building on this strategy, our model, employing efficient encoder-decoder and depth-lifting designs, is adversarially trained based on categorized musical meter segments to generate realistic and rhythmic 3D human motions. We further incorporate our harmony evaluation strategy as a weakly supervised perceptual constraint to flexibly guide the synchronized audio-visual rhythms during the generation process. Experimental results show that our proposed model significantly outperforms other leading music-to-motion methods in rhythmic harmony, both quantitatively and qualitatively, even with limited UGC training data. Live samples 15 can be watched at: https://youtu.be/tWwz7yq4aUs
<div id='section'>Paperid: <span id='pid'>1437, <a href='https://arxiv.org/pdf/2506.05952.pdf' target='_blank'>https://arxiv.org/pdf/2506.05952.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongjie Fu, Tengjiao Sun, Pengcheng Fang, Xiaohao Cai, Hansung Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.05952">MOGO: Residual Quantized Hierarchical Causal Transformer for High-Quality and Real-Time 3D Human Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in transformer-based text-to-motion generation have led to impressive progress in synthesizing high-quality human motion. Nevertheless, jointly achieving high fidelity, streaming capability, real-time responsiveness, and scalability remains a fundamental challenge. In this paper, we propose MOGO (Motion Generation with One-pass), a novel autoregressive framework tailored for efficient and real-time 3D motion generation. MOGO comprises two key components: (1) MoSA-VQ, a motion scale-adaptive residual vector quantization module that hierarchically discretizes motion sequences with learnable scaling to produce compact yet expressive representations; and (2) RQHC-Transformer, a residual quantized hierarchical causal transformer that generates multi-layer motion tokens in a single forward pass, significantly reducing inference latency. To enhance semantic fidelity, we further introduce a text condition alignment mechanism that improves motion decoding under textual control. Extensive experiments on benchmark datasets including HumanML3D, KIT-ML, and CMP demonstrate that MOGO achieves competitive or superior generation quality compared to state-of-the-art transformer-based methods, while offering substantial improvements in real-time performance, streaming generation, and generalization under zero-shot settings.
<div id='section'>Paperid: <span id='pid'>1438, <a href='https://arxiv.org/pdf/2505.23465.pdf' target='_blank'>https://arxiv.org/pdf/2505.23465.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zi-An Wang, Shihao Zou, Shiyao Yu, Mingyuan Zhang, Chao Dong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.23465">Semantics-Aware Human Motion Generation from Audio Instructions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in interactive technologies have highlighted the prominence of audio signals for semantic encoding. This paper explores a new task, where audio signals are used as conditioning inputs to generate motions that align with the semantics of the audio. Unlike text-based interactions, audio provides a more natural and intuitive communication method. However, existing methods typically focus on matching motions with music or speech rhythms, which often results in a weak connection between the semantics of the audio and generated motions. We propose an end-to-end framework using a masked generative transformer, enhanced by a memory-retrieval attention module to handle sparse and lengthy audio inputs. Additionally, we enrich existing datasets by converting descriptions into conversational style and generating corresponding audio with varied speaker identities. Experiments demonstrate the effectiveness and efficiency of the proposed framework, demonstrating that audio instructions can convey semantics similar to text while providing more practical and user-friendly interactions.
<div id='section'>Paperid: <span id='pid'>1439, <a href='https://arxiv.org/pdf/2505.12774.pdf' target='_blank'>https://arxiv.org/pdf/2505.12774.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zichen Geng, Zeeshan Hayder, Wei Liu, Ajmal Mian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.12774">UniHM: Universal Human Motion Generation with Object Interactions in Indoor Scenes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion synthesis in complex scenes presents a fundamental challenge, extending beyond conventional Text-to-Motion tasks by requiring the integration of diverse modalities such as static environments, movable objects, natural language prompts, and spatial waypoints. Existing language-conditioned motion models often struggle with scene-aware motion generation due to limitations in motion tokenization, which leads to information loss and fails to capture the continuous, context-dependent nature of 3D human movement. To address these issues, we propose UniHM, a unified motion language model that leverages diffusion-based generation for synthesizing scene-aware human motion. UniHM is the first framework to support both Text-to-Motion and Text-to-Human-Object Interaction (HOI) in complex 3D scenes. Our approach introduces three key contributions: (1) a mixed-motion representation that fuses continuous 6DoF motion with discrete local motion tokens to improve motion realism; (2) a novel Look-Up-Free Quantization VAE (LFQ-VAE) that surpasses traditional VQ-VAEs in both reconstruction accuracy and generative performance; and (3) an enriched version of the Lingo dataset augmented with HumanML3D annotations, providing stronger supervision for scene-specific motion learning. Experimental results demonstrate that UniHM achieves comparative performance on the OMOMO benchmark for text-to-HOI synthesis and yields competitive results on HumanML3D for general text-conditioned motion generation.
<div id='section'>Paperid: <span id='pid'>1440, <a href='https://arxiv.org/pdf/2505.08235.pdf' target='_blank'>https://arxiv.org/pdf/2505.08235.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hanle Zheng, Xujie Han, Zegang Peng, Shangbin Zhang, Guangxun Du, Zhuo Zou, Xilin Wang, Jibin Wu, Hao Guo, Lei Deng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.08235">EventDiff: A Unified and Efficient Diffusion Model Framework for Event-based Video Frame Interpolation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video Frame Interpolation (VFI) is a fundamental yet challenging task in computer vision, particularly under conditions involving large motion, occlusion, and lighting variation. Recent advancements in event cameras have opened up new opportunities for addressing these challenges. While existing event-based VFI methods have succeeded in recovering large and complex motions by leveraging handcrafted intermediate representations such as optical flow, these designs often compromise high-fidelity image reconstruction under subtle motion scenarios due to their reliance on explicit motion modeling. Meanwhile, diffusion models provide a promising alternative for VFI by reconstructing frames through a denoising process, eliminating the need for explicit motion estimation or warping operations. In this work, we propose EventDiff, a unified and efficient event-based diffusion model framework for VFI. EventDiff features a novel Event-Frame Hybrid AutoEncoder (HAE) equipped with a lightweight Spatial-Temporal Cross Attention (STCA) module that effectively fuses dynamic event streams with static frames. Unlike previous event-based VFI methods, EventDiff performs interpolation directly in the latent space via a denoising diffusion process, making it more robust across diverse and challenging VFI scenarios. Through a two-stage training strategy that first pretrains the HAE and then jointly optimizes it with the diffusion model, our method achieves state-of-the-art performance across multiple synthetic and real-world event VFI datasets. The proposed method outperforms existing state-of-the-art event-based VFI methods by up to 1.98dB in PSNR on Vimeo90K-Triplet and shows superior performance in SNU-FILM tasks with multiple difficulty levels. Compared to the emerging diffusion-based VFI approach, our method achieves up to 5.72dB PSNR gain on Vimeo90K-Triplet and 4.24X faster inference.
<div id='section'>Paperid: <span id='pid'>1441, <a href='https://arxiv.org/pdf/2505.02668.pdf' target='_blank'>https://arxiv.org/pdf/2505.02668.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Antonio Grotta, Francesco De Lellis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.02668">Online Phase Estimation of Human Oscillatory Motions using Deep Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurately estimating the phase of oscillatory systems is essential for analyzing cyclic activities such as repetitive gestures in human motion. In this work we introduce a learning-based approach for online phase estimation in three-dimensional motion trajectories, using a Long Short- Term Memory (LSTM) network. A calibration procedure is applied to standardize trajectory position and orientation, ensuring invariance to spatial variations. The proposed model is evaluated on motion capture data and further tested in a dynamical system, where the estimated phase is used as input to a reinforcement learning (RL)-based control to assess its impact on the synchronization of a network of Kuramoto oscillators.
<div id='section'>Paperid: <span id='pid'>1442, <a href='https://arxiv.org/pdf/2504.19189.pdf' target='_blank'>https://arxiv.org/pdf/2504.19189.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lei Zhong, Chuan Guo, Yiming Xie, Jiawei Wang, Changjian Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.19189">Sketch2Anim: Towards Transferring Sketch Storyboards into 3D Animation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Storyboarding is widely used for creating 3D animations. Animators use the 2D sketches in storyboards as references to craft the desired 3D animations through a trial-and-error process. The traditional approach requires exceptional expertise and is both labor-intensive and time-consuming. Consequently, there is a high demand for automated methods that can directly translate 2D storyboard sketches into 3D animations. This task is under-explored to date and inspired by the significant advancements of motion diffusion models, we propose to address it from the perspective of conditional motion synthesis. We thus present Sketch2Anim, composed of two key modules for sketch constraint understanding and motion generation. Specifically, due to the large domain gap between the 2D sketch and 3D motion, instead of directly conditioning on 2D inputs, we design a 3D conditional motion generator that simultaneously leverages 3D keyposes, joint trajectories, and action words, to achieve precise and fine-grained motion control. Then, we invent a neural mapper dedicated to aligning user-provided 2D sketches with their corresponding 3D keyposes and trajectories in a shared embedding space, enabling, for the first time, direct 2D control of motion generation. Our approach successfully transfers storyboards into high-quality 3D motions and inherently supports direct 3D animation editing, thanks to the flexibility of our multi-conditional motion generator. Comprehensive experiments and evaluations, and a user perceptual study demonstrate the effectiveness of our approach.
<div id='section'>Paperid: <span id='pid'>1443, <a href='https://arxiv.org/pdf/2504.08181.pdf' target='_blank'>https://arxiv.org/pdf/2504.08181.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruineng Li, Daitao Xing, Huiming Sun, Yuanzhou Ha, Jinglin Shen, Chiuman Ho
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.08181">TokenMotion: Decoupled Motion Control via Token Disentanglement for Human-centric Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human-centric motion control in video generation remains a critical challenge, particularly when jointly controlling camera movements and human poses in scenarios like the iconic Grammy Glambot moment. While recent video diffusion models have made significant progress, existing approaches struggle with limited motion representations and inadequate integration of camera and human motion controls. In this work, we present TokenMotion, the first DiT-based video diffusion framework that enables fine-grained control over camera motion, human motion, and their joint interaction. We represent camera trajectories and human poses as spatio-temporal tokens to enable local control granularity. Our approach introduces a unified modeling framework utilizing a decouple-and-fuse strategy, bridged by a human-aware dynamic mask that effectively handles the spatially-and-temporally varying nature of combined motion signals. Through extensive experiments, we demonstrate TokenMotion's effectiveness across both text-to-video and image-to-video paradigms, consistently outperforming current state-of-the-art methods in human-centric motion control tasks. Our work represents a significant advancement in controllable video generation, with particular relevance for creative production applications.
<div id='section'>Paperid: <span id='pid'>1444, <a href='https://arxiv.org/pdf/2504.06176.pdf' target='_blank'>https://arxiv.org/pdf/2504.06176.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ian Groves, Andrew Campbell, James Fernandes, Diego RamÃ­rez RodrÃ­guez, Paul Murray, Massimiliano Vasile, Victoria Nockles
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.06176">A Self-Supervised Framework for Space Object Behaviour Characterisation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Foundation Models, pre-trained on large unlabelled datasets before task-specific fine-tuning, are increasingly being applied to specialised domains. Recent examples include ClimaX for climate and Clay for satellite Earth observation, but a Foundation Model for Space Object Behavioural Analysis has not yet been developed. As orbital populations grow, automated methods for characterising space object behaviour are crucial for space safety. We present a Space Safety and Sustainability Foundation Model focusing on space object behavioural analysis using light curves (LCs). We implemented a Perceiver-Variational Autoencoder (VAE) architecture, pre-trained with self-supervised reconstruction and masked reconstruction on 227,000 LCs from the MMT-9 observatory. The VAE enables anomaly detection, motion prediction, and LC generation. We fine-tuned the model for anomaly detection & motion prediction using two independent LC simulators (CASSANDRA and GRIAL respectively), using CAD models of boxwing, Sentinel-3, SMOS, and Starlink platforms. Our pre-trained model achieved a reconstruction error of 0.01%, identifying potentially anomalous light curves through reconstruction difficulty. After fine-tuning, the model scored 88% and 82% accuracy, with 0.90 and 0.95 ROC AUC scores respectively in both anomaly detection and motion mode prediction (sun-pointing, spin, etc.). Analysis of high-confidence anomaly predictions on real data revealed distinct patterns including characteristic object profiles and satellite glinting. Here, we demonstrate how self-supervised learning can simultaneously enable anomaly detection, motion prediction, and synthetic data generation from rich representations learned in pre-training. Our work therefore supports space safety and sustainability through automated monitoring and simulation capabilities.
<div id='section'>Paperid: <span id='pid'>1445, <a href='https://arxiv.org/pdf/2503.24272.pdf' target='_blank'>https://arxiv.org/pdf/2503.24272.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yizhou Huang, Yihua Cheng, Kezhi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.24272">Learning Velocity and Acceleration: Self-Supervised Motion Consistency for Pedestrian Trajectory Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding human motion is crucial for accurate pedestrian trajectory prediction. Conventional methods typically rely on supervised learning, where ground-truth labels are directly optimized against predicted trajectories. This amplifies the limitations caused by long-tailed data distributions, making it difficult for the model to capture abnormal behaviors. In this work, we propose a self-supervised pedestrian trajectory prediction framework that explicitly models position, velocity, and acceleration. We leverage velocity and acceleration information to enhance position prediction through feature injection and a self-supervised motion consistency mechanism. Our model hierarchically injects velocity features into the position stream. Acceleration features are injected into the velocity stream. This enables the model to predict position, velocity, and acceleration jointly. From the predicted position, we compute corresponding pseudo velocity and acceleration, allowing the model to learn from data-generated pseudo labels and thus achieve self-supervised learning. We further design a motion consistency evaluation strategy grounded in physical principles; it selects the most reasonable predicted motion trend by comparing it with historical dynamics and uses this trend to guide and constrain trajectory generation. We conduct experiments on the ETH-UCY and Stanford Drone datasets, demonstrating that our method achieves state-of-the-art performance on both datasets.
<div id='section'>Paperid: <span id='pid'>1446, <a href='https://arxiv.org/pdf/2503.21775.pdf' target='_blank'>https://arxiv.org/pdf/2503.21775.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyu Guo, Young Yoon Lee, Joseph Liu, Yizhak Ben-Shabat, Victor Zordan, Mubbasir Kapadia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.21775">StyleMotif: Multi-Modal Motion Stylization using Style-Content Cross Fusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present StyleMotif, a novel Stylized Motion Latent Diffusion model, generating motion conditioned on both content and style from multiple modalities. Unlike existing approaches that either focus on generating diverse motion content or transferring style from sequences, StyleMotif seamlessly synthesizes motion across a wide range of content while incorporating stylistic cues from multi-modal inputs, including motion, text, image, video, and audio. To achieve this, we introduce a style-content cross fusion mechanism and align a style encoder with a pre-trained multi-modal model, ensuring that the generated motion accurately captures the reference style while preserving realism. Extensive experiments demonstrate that our framework surpasses existing methods in stylized motion generation and exhibits emergent capabilities for multi-modal motion stylization, enabling more nuanced motion synthesis. Source code and pre-trained models will be released upon acceptance. Project Page: https://stylemotif.github.io
<div id='section'>Paperid: <span id='pid'>1447, <a href='https://arxiv.org/pdf/2503.19914.pdf' target='_blank'>https://arxiv.org/pdf/2503.19914.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sangwon Baik, Hyeonwoo Kim, Hanbyul Joo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.19914">Learning 3D Object Spatial Relationships from Pre-trained 2D Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a method for learning 3D spatial relationships between object pairs, referred to as object-object spatial relationships (OOR), by leveraging synthetically generated 3D samples from pre-trained 2D diffusion models. We hypothesize that images synthesized by 2D diffusion models inherently capture realistic OOR cues, enabling efficient collection of a 3D dataset to learn OOR for various unbounded object categories. Our approach synthesizes diverse images that capture plausible OOR cues, which we then uplift into 3D samples. Leveraging our diverse collection of 3D samples for the object pairs, we train a score-based OOR diffusion model to learn the distribution of their relative spatial relationships. Additionally, we extend our pairwise OOR to multi-object OOR by enforcing consistency across pairwise relations and preventing object collisions. Extensive experiments demonstrate the robustness of our method across various object-object spatial relationships, along with its applicability to 3D scene arrangement tasks and human motion synthesis using our OOR diffusion model.
<div id='section'>Paperid: <span id='pid'>1448, <a href='https://arxiv.org/pdf/2503.18950.pdf' target='_blank'>https://arxiv.org/pdf/2503.18950.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Taeksoo Kim, Hanbyul Joo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.18950">Target-Aware Video Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a target-aware video diffusion model that generates videos from an input image in which an actor interacts with a specified target while performing a desired action. The target is defined by a segmentation mask and the desired action is described via a text prompt. Unlike existing controllable image-to-video diffusion models that often rely on dense structural or motion cues to guide the actor's movements toward the target, our target-aware model requires only a simple mask to indicate the target, leveraging the generalization capabilities of pretrained models to produce plausible actions. This makes our method particularly effective for human-object interaction (HOI) scenarios, where providing precise action guidance is challenging, and further enables the use of video diffusion models for high-level action planning in applications such as robotics. We build our target-aware model by extending a baseline model to incorporate the target mask as an additional input. To enforce target awareness, we introduce a special token that encodes the target's spatial information within the text prompt. We then fine-tune the model with our curated dataset using a novel cross-attention loss that aligns the cross-attention maps associated with this token with the input target mask. To further improve performance, we selectively apply this loss to the most semantically relevant transformer blocks and attention regions. Experimental results show that our target-aware model outperforms existing solutions in generating videos where actors interact accurately with the specified targets. We further demonstrate its efficacy in two downstream applications: video content creation and zero-shot 3D HOI motion synthesis.
<div id='section'>Paperid: <span id='pid'>1449, <a href='https://arxiv.org/pdf/2503.16801.pdf' target='_blank'>https://arxiv.org/pdf/2503.16801.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zichen Geng, Zeeshan Hayder, Wei Liu, Ajmal Saeed Mian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.16801">Auto-Regressive Diffusion for Generating 3D Human-Object Interactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-driven Human-Object Interaction (Text-to-HOI) generation is an emerging field with applications in animation, video games, virtual reality, and robotics. A key challenge in HOI generation is maintaining interaction consistency in long sequences. Existing Text-to-Motion-based approaches, such as discrete motion tokenization, cannot be directly applied to HOI generation due to limited data in this domain and the complexity of the modality. To address the problem of interaction consistency in long sequences, we propose an autoregressive diffusion model (ARDHOI) that predicts the next continuous token. Specifically, we introduce a Contrastive Variational Autoencoder (cVAE) to learn a physically plausible space of continuous HOI tokens, thereby ensuring that generated human-object motions are realistic and natural. For generating sequences autoregressively, we develop a Mamba-based context encoder to capture and maintain consistent sequential actions. Additionally, we implement an MLP-based denoiser to generate the subsequent token conditioned on the encoded context. Our model has been evaluated on the OMOMO and BEHAVE datasets, where it outperforms existing state-of-the-art methods in terms of both performance and inference speed. This makes ARDHOI a robust and efficient solution for text-driven HOI tasks
<div id='section'>Paperid: <span id='pid'>1450, <a href='https://arxiv.org/pdf/2503.14040.pdf' target='_blank'>https://arxiv.org/pdf/2503.14040.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Binjie Liu, Lina Liu, Sanyi Zhang, Songen Gu, Yihao Zhi, Tianyi Zhu, Lei Yang, Long Ye
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.14040">MAG: Multi-Modal Aligned Autoregressive Co-Speech Gesture Generation without Vector Quantization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work focuses on full-body co-speech gesture generation. Existing methods typically employ an autoregressive model accompanied by vector-quantized tokens for gesture generation, which results in information loss and compromises the realism of the generated gestures. To address this, inspired by the natural continuity of real-world human motion, we propose MAG, a novel multi-modal aligned framework for high-quality and diverse co-speech gesture synthesis without relying on discrete tokenization. Specifically, (1) we introduce a motion-text-audio-aligned variational autoencoder (MTA-VAE), which leverages pre-trained WavCaps' text and audio embeddings to enhance both semantic and rhythmic alignment with motion, ultimately producing more realistic gestures. (2) Building on this, we propose a multimodal masked autoregressive model (MMAG) that enables autoregressive modeling in continuous motion embeddings through diffusion without vector quantization. To further ensure multi-modal consistency, MMAG incorporates a hybrid granularity audio-text fusion block, which serves as conditioning for diffusion process. Extensive experiments on two benchmark datasets demonstrate that MAG achieves stateof-the-art performance both quantitatively and qualitatively, producing highly realistic and diverse co-speech gestures.The code will be released to facilitate future research.
<div id='section'>Paperid: <span id='pid'>1451, <a href='https://arxiv.org/pdf/2503.13025.pdf' target='_blank'>https://arxiv.org/pdf/2503.13025.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>ChangHee Yang, Hyeonseop Song, Seokhun Choi, Seungwoo Lee, Jaechul Kim, Hoseok Do
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.13025">PoseSyn: Synthesizing Diverse 3D Pose Data from In-the-Wild 2D Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite considerable efforts to enhance the generalization of 3D pose estimators without costly 3D annotations, existing data augmentation methods struggle in real world scenarios with diverse human appearances and complex poses. We propose PoseSyn, a novel data synthesis framework that transforms abundant in the wild 2D pose dataset into diverse 3D pose image pairs. PoseSyn comprises two key components: Error Extraction Module (EEM), which identifies challenging poses from the 2D pose datasets, and Motion Synthesis Module (MSM), which synthesizes motion sequences around the challenging poses. Then, by generating realistic 3D training data via a human animation model aligned with challenging poses and appearances PoseSyn boosts the accuracy of various 3D pose estimators by up to 14% across real world benchmarks including various backgrounds and occlusions, challenging poses, and multi view scenarios. Extensive experiments further confirm that PoseSyn is a scalable and effective approach for improving generalization without relying on expensive 3D annotations, regardless of the pose estimator's model size or design.
<div id='section'>Paperid: <span id='pid'>1452, <a href='https://arxiv.org/pdf/2503.10898.pdf' target='_blank'>https://arxiv.org/pdf/2503.10898.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yizhou Huang, Yihua Cheng, Kezhi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.10898">Trajectory Mamba: Efficient Attention-Mamba Forecasting Model Based on Selective SSM</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion prediction is crucial for autonomous driving, as it enables accurate forecasting of future vehicle trajectories based on historical inputs. This paper introduces Trajectory Mamba, a novel efficient trajectory prediction framework based on the selective state-space model (SSM). Conventional attention-based models face the challenge of computational costs that grow quadratically with the number of targets, hindering their application in highly dynamic environments. In response, we leverage the SSM to redesign the self-attention mechanism in the encoder-decoder architecture, thereby achieving linear time complexity. To address the potential reduction in prediction accuracy resulting from modifications to the attention mechanism, we propose a joint polyline encoding strategy to better capture the associations between static and dynamic contexts, ultimately enhancing prediction accuracy. Additionally, to balance prediction accuracy and inference speed, we adopted the decoder that differs entirely from the encoder. Through cross-state space attention, all target agents share the scene context, allowing the SSM to interact with the shared scene representation during decoding, thus inferring different trajectories over the next prediction steps. Our model achieves state-of-the-art results in terms of inference speed and parameter efficiency on both the Argoverse 1 and Argoverse 2 datasets. It demonstrates a four-fold reduction in FLOPs compared to existing methods and reduces parameter count by over 40% while surpassing the performance of the vast majority of previous methods. These findings validate the effectiveness of Trajectory Mamba in trajectory prediction tasks.
<div id='section'>Paperid: <span id='pid'>1453, <a href='https://arxiv.org/pdf/2503.06151.pdf' target='_blank'>https://arxiv.org/pdf/2503.06151.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zixi Kang, Xinghan Wang, Yadong Mu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.06151">Biomechanics-Guided Residual Approach to Generalizable Human Motion Generation and Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human pose, action, and motion generation are critical for applications in digital humans, character animation, and humanoid robotics. However, many existing methods struggle to produce physically plausible movements that are consistent with biomechanical principles. Although recent autoregressive and diffusion models deliver impressive visual quality, they often neglect key biodynamic features and fail to ensure physically realistic motions. Reinforcement Learning (RL) approaches can address these shortcomings but are highly dependent on simulation environments, limiting their generalizability. To overcome these challenges, we propose BioVAE, a biomechanics-aware framework with three core innovations: (1) integration of muscle electromyography (EMG) signals and kinematic features with acceleration constraints to enable physically plausible motion without simulations; (2) seamless coupling with diffusion models for stable end-to-end training; and (3) biomechanical priors that promote strong generalization across diverse motion generation and estimation tasks. Extensive experiments demonstrate that BioVAE achieves state-of-the-art performance on multiple benchmarks, bridging the gap between data-driven motion synthesis and biomechanical authenticity while setting new standards for physically accurate motion generation and pose estimation.
<div id='section'>Paperid: <span id='pid'>1454, <a href='https://arxiv.org/pdf/2503.05092.pdf' target='_blank'>https://arxiv.org/pdf/2503.05092.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Adam Labiosa, Josiah P. Hanna
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.05092">Multi-Robot Collaboration through Reinforcement Learning and Abstract Simulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Teams of people coordinate to perform complex tasks by forming abstract mental models of world and agent dynamics. The use of abstract models contrasts with much recent work in robot learning that uses a high-fidelity simulator and reinforcement learning (RL) to obtain policies for physical robots. Motivated by this difference, we investigate the extent to which so-called abstract simulators can be used for multi-agent reinforcement learning (MARL) and the resulting policies successfully deployed on teams of physical robots. An abstract simulator models the robot's target task at a high-level of abstraction and discards many details of the world that could impact optimal decision-making. Policies are trained in an abstract simulator then transferred to the physical robot by making use of separately-obtained low-level perception and motion control modules. We identify three key categories of modifications to the abstract simulator that enable policy transfer to physical robots: simulation fidelity enhancements, training optimizations and simulation stochasticity. We then run an empirical study with extensive ablations to determine the value of each modification category for enabling policy transfer in cooperative robot soccer tasks. We also compare the performance of policies produced by our method with a well-tuned non-learning-based behavior architecture from the annual RoboCup competition and find that our approach leads to a similar level of performance. Broadly we show that MARL can be use to train cooperative physical robot behaviors using highly abstract models of the world.
<div id='section'>Paperid: <span id='pid'>1455, <a href='https://arxiv.org/pdf/2503.03474.pdf' target='_blank'>https://arxiv.org/pdf/2503.03474.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Varsha Suresh, M. Hamza Mughal, Christian Theobalt, Vera Demberg
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.03474">Enhancing Spoken Discourse Modeling in Language Models Using Gestural Cues</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Research in linguistics shows that non-verbal cues, such as gestures, play a crucial role in spoken discourse. For example, speakers perform hand gestures to indicate topic shifts, helping listeners identify transitions in discourse. In this work, we investigate whether the joint modeling of gestures using human motion sequences and language can improve spoken discourse modeling in language models. To integrate gestures into language models, we first encode 3D human motion sequences into discrete gesture tokens using a VQ-VAE. These gesture token embeddings are then aligned with text embeddings through feature alignment, mapping them into the text embedding space. To evaluate the gesture-aligned language model on spoken discourse, we construct text infilling tasks targeting three key discourse cues grounded in linguistic research: discourse connectives, stance markers, and quantifiers. Results show that incorporating gestures enhances marker prediction accuracy across the three tasks, highlighting the complementary information that gestures can offer in modeling spoken discourse. We view this work as an initial step toward leveraging non-verbal cues to advance spoken language modeling in language models.
<div id='section'>Paperid: <span id='pid'>1456, <a href='https://arxiv.org/pdf/2503.00389.pdf' target='_blank'>https://arxiv.org/pdf/2503.00389.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuto Shibata, Yusuke Oumi, Go Irie, Akisato Kimura, Yoshimitsu Aoki, Mariko Isogawa
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.00389">BGM2Pose: Active 3D Human Pose Estimation with Non-Stationary Sounds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose BGM2Pose, a non-invasive 3D human pose estimation method using arbitrary music (e.g., background music) as active sensing signals. Unlike existing approaches that significantly limit practicality by employing intrusive chirp signals within the audible range, our method utilizes natural music that causes minimal discomfort to humans. Estimating human poses from standard music presents significant challenges. In contrast to sound sources specifically designed for measurement, regular music varies in both volume and pitch. These dynamic changes in signals caused by music are inevitably mixed with alterations in the sound field resulting from human motion, making it hard to extract reliable cues for pose estimation. To address these challenges, BGM2Pose introduces a Contrastive Pose Extraction Module that employs contrastive learning and hard negative sampling to eliminate musical components from the recorded data, isolating the pose information. Additionally, we propose a Frequency-wise Attention Module that enables the model to focus on subtle acoustic variations attributable to human movement by dynamically computing attention across frequency bands. Experiments suggest that our method outperforms the existing methods, demonstrating substantial potential for real-world applications. Our datasets and code will be made publicly available.
<div id='section'>Paperid: <span id='pid'>1457, <a href='https://arxiv.org/pdf/2502.17372.pdf' target='_blank'>https://arxiv.org/pdf/2502.17372.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Stella DumenÄiÄ, Luka LanÄa, Karlo Jakac, Stefan IviÄ
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.17372">Experimental validation of UAV search and detection system in real wilderness environment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Search and rescue (SAR) missions require reliable search methods to locate survivors, especially in challenging or inaccessible environments. This is why introducing unmanned aerial vehicles (UAVs) can be of great help to enhance the efficiency of SAR missions while simultaneously increasing the safety of everyone involved in the mission. Motivated by this, we design and experiment with autonomous UAV search for humans in a Mediterranean karst environment. The UAVs are directed using Heat equation-driven area coverage (HEDAC) ergodic control method according to known probability density and detection function. The implemented sensing framework consists of a probabilistic search model, motion control system, and computer vision object detection. It enables calculation of the probability of the target being detected in the SAR mission, and this paper focuses on experimental validation of proposed probabilistic framework and UAV control. The uniform probability density to ensure the even probability of finding the targets in the desired search area is achieved by assigning suitably thought-out tasks to 78 volunteers. The detection model is based on YOLO and trained with a previously collected ortho-photo image database. The experimental search is carefully planned and conducted, while as many parameters as possible are recorded. The thorough analysis consists of the motion control system, object detection, and the search validation. The assessment of the detection and search performance provides strong indication that the designed detection model in the UAV control algorithm is aligned with real-world results.
<div id='section'>Paperid: <span id='pid'>1458, <a href='https://arxiv.org/pdf/2502.14574.pdf' target='_blank'>https://arxiv.org/pdf/2502.14574.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinrui Zhang, Lu Xiong, Peizhi Zhang, Junpeng Huang, Yining Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.14574">Real-world Troublemaker: A 5G Cloud-controlled Track Testing Framework for Automated Driving Systems in Safety-critical Interaction Scenarios</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Track testing plays a critical role in the safety evaluation of autonomous driving systems (ADS), as it provides a real-world interaction environment. However, the inflexibility in motion control of object targets and the absence of intelligent interactive testing methods often result in pre-fixed and limited testing scenarios. To address these limitations, we propose a novel 5G cloud-controlled track testing framework, Real-world Troublemaker. This framework overcomes the rigidity of traditional pre-programmed control by leveraging 5G cloud-controlled object targets integrated with the Internet of Things (IoT) and vehicle teleoperation technologies. Unlike conventional testing methods that rely on pre-set conditions, we propose a dynamic game strategy based on a quadratic risk interaction utility function, facilitating intelligent interactions with the vehicle under test (VUT) and creating a more realistic and dynamic interaction environment. The proposed framework has been successfully implemented at the Tongji University Intelligent Connected Vehicle Evaluation Base. Field test results demonstrate that Troublemaker can perform dynamic interactive testing of ADS accurately and effectively. Compared to traditional methods, Troublemaker improves scenario reproduction accuracy by 65.2\%, increases the diversity of interaction strategies by approximately 9.2 times, and enhances exposure frequency of safety-critical scenarios by 3.5 times in unprotected left-turn scenarios.
<div id='section'>Paperid: <span id='pid'>1459, <a href='https://arxiv.org/pdf/2502.10429.pdf' target='_blank'>https://arxiv.org/pdf/2502.10429.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhang Minghao, Yang Xiaojun, Wang Zhihe, Wang Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.10429">Real Time Control of Tandem-Wing Experimental Platform Using Concerto Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces the CRL2RT algorithm, an advanced reinforcement learning method aimed at improving the real-time control performance of the Direct-Drive Tandem-Wing Experimental Platform (DDTWEP). Inspired by dragonfly flight, DDTWEP's tandem wing structure causes nonlinear and unsteady aerodynamic interactions, leading to complex load behaviors during pitch, roll, and yaw maneuvers. These complexities challenge stable motion control at high frequencies (2000 Hz). To overcome these issues, we developed the CRL2RT algorithm, which combines classical control elements with reinforcement learning-based controllers using a time-interleaved architecture and a rule-based policy composer. This integration ensures finite-time convergence and single-life adaptability. Experimental results under various conditions, including different flapping frequencies and yaw disturbances, show that CRL2RT achieves a control frequency surpassing 2500 Hz on standard CPUs. Additionally, when integrated with classical controllers like PID, Adaptive PID, and Model Reference Adaptive Control (MRAC), CRL2RT enhances tracking performance by 18.3% to 60.7%. These findings demonstrate CRL2RT's broad applicability and superior performance in complex real-time control scenarios, validating its effectiveness in overcoming existing control strategy limitations and advancing robust, efficient real-time control for biomimetic aerial vehicles.
<div id='section'>Paperid: <span id='pid'>1460, <a href='https://arxiv.org/pdf/2501.16778.pdf' target='_blank'>https://arxiv.org/pdf/2501.16778.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Arvin Tashakori, Arash Tashakori, Gongbo Yang, Z. Jane Wang, Peyman Servati
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.16778">FlexMotion: Lightweight, Physics-Aware, and Controllable Human Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Lightweight, controllable, and physically plausible human motion synthesis is crucial for animation, virtual reality, robotics, and human-computer interaction applications. Existing methods often compromise between computational efficiency, physical realism, or spatial controllability. We propose FlexMotion, a novel framework that leverages a computationally lightweight diffusion model operating in the latent space, eliminating the need for physics simulators and enabling fast and efficient training. FlexMotion employs a multimodal pre-trained Transformer encoder-decoder, integrating joint locations, contact forces, joint actuations and muscle activations to ensure the physical plausibility of the generated motions. FlexMotion also introduces a plug-and-play module, which adds spatial controllability over a range of motion parameters (e.g., joint locations, joint actuations, contact forces, and muscle activations). Our framework achieves realistic motion generation with improved efficiency and control, setting a new benchmark for human motion synthesis. We evaluate FlexMotion on extended datasets and demonstrate its superior performance in terms of realism, physical plausibility, and controllability.
<div id='section'>Paperid: <span id='pid'>1461, <a href='https://arxiv.org/pdf/2501.08333.pdf' target='_blank'>https://arxiv.org/pdf/2501.08333.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hyeonwoo Kim, Sangwon Baik, Hanbyul Joo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.08333">DAViD: Modeling Dynamic Affordance of 3D Objects Using Pre-trained Video Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modeling how humans interact with objects is crucial for AI to effectively assist or mimic human behaviors. Existing studies for learning such ability primarily focus on static human-object interaction (HOI) patterns, such as contact and spatial relationships, while dynamic HOI patterns, capturing the movement of humans and objects over time, remain relatively underexplored. In this paper, we present a novel framework for learning Dynamic Affordance across various target object categories. To address the scarcity of 4D HOI datasets, our method learns the 3D dynamic affordance from synthetically generated 4D HOI samples. Specifically, we propose a pipeline that first generates 2D HOI videos from a given 3D target object using a pre-trained video diffusion model, then lifts them into 3D to generate 4D HOI samples. Leveraging these synthesized 4D HOI samples, we train DAViD, our generative 4D human-object interaction model, which is composed of two key components: (1) a human motion diffusion model (MDM) with Low-Rank Adaptation (LoRA) module to fine-tune a pre-trained MDM to learn the HOI motion concepts from limited HOI motion samples, (2) a motion diffusion model for 4D object poses conditioned by produced human interaction motions. Interestingly, DAViD can integrate newly learned HOI motion concepts with pre-trained human motions to create novel HOI motions, even for multiple HOI motion concepts, demonstrating the advantage of our pipeline with LoRA in integrating dynamic HOI concepts. Through extensive experiments, we demonstrate that DAViD outperforms baselines in synthesizing HOI motion.
<div id='section'>Paperid: <span id='pid'>1462, <a href='https://arxiv.org/pdf/2501.01449.pdf' target='_blank'>https://arxiv.org/pdf/2501.01449.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Avinash Amballa, Gayathri Akkinapalli, Vinitra Muralikrishnan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.01449">LS-GAN: Human Motion Synthesis with Latent-space GANs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion synthesis conditioned on textual input has gained significant attention in recent years due to its potential applications in various domains such as gaming, film production, and virtual reality. Conditioned Motion synthesis takes a text input and outputs a 3D motion corresponding to the text. While previous works have explored motion synthesis using raw motion data and latent space representations with diffusion models, these approaches often suffer from high training and inference times. In this paper, we introduce a novel framework that utilizes Generative Adversarial Networks (GANs) in the latent space to enable faster training and inference while achieving results comparable to those of the state-of-the-art diffusion methods. We perform experiments on the HumanML3D, HumanAct12 benchmarks and demonstrate that a remarkably simple GAN in the latent space achieves a FID of 0.482 with more than 91% in FLOPs reduction compared to latent diffusion model. Our work opens up new possibilities for efficient and high-quality motion synthesis using latent space GANs.
<div id='section'>Paperid: <span id='pid'>1463, <a href='https://arxiv.org/pdf/2412.10458.pdf' target='_blank'>https://arxiv.org/pdf/2412.10458.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiayi Zhao, Dongdong Weng, Qiuxin Du, Zeyu Tian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.10458">Motion Generation Review: Exploring Deep Learning for Lifelike Animation with Manifold</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion generation involves creating natural sequences of human body poses, widely used in gaming, virtual reality, and human-computer interaction. It aims to produce lifelike virtual characters with realistic movements, enhancing virtual agents and immersive experiences. While previous work has focused on motion generation based on signals like movement, music, text, or scene background, the complexity of human motion and its relationships with these signals often results in unsatisfactory outputs. Manifold learning offers a solution by reducing data dimensionality and capturing subspaces of effective motion. In this review, we present a comprehensive overview of manifold applications in human motion generation, one of the first in this domain. We explore methods for extracting manifolds from unstructured data, their application in motion generation, and discuss their advantages and future directions. This survey aims to provide a broad perspective on the field and stimulate new approaches to ongoing challenges.
<div id='section'>Paperid: <span id='pid'>1464, <a href='https://arxiv.org/pdf/2412.07922.pdf' target='_blank'>https://arxiv.org/pdf/2412.07922.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinyue Hu, Wei Ye, Jiaxiang Tang, Eman Ramadan, Zhi-Li Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.07922">Robust Multiple Description Neural Video Codec with Masked Transformer for Dynamic and Noisy Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multiple Description Coding (MDC) is a promising error-resilient source coding method that is particularly suitable for dynamic networks with multiple (yet noisy and unreliable) paths. However, conventional MDC video codecs suffer from cumbersome architectures, poor scalability, limited loss resilience, and lower compression efficiency. As a result, MDC has never been widely adopted. Inspired by the potential of neural video codecs, this paper rethinks MDC design. We propose a novel MDC video codec, NeuralMDC, demonstrating how bidirectional transformers trained for masked token prediction can vastly simplify the design of MDC video codec. To compress a video, NeuralMDC starts by tokenizing each frame into its latent representation and then splits the latent tokens to create multiple descriptions containing correlated information. Instead of using motion prediction and warping operations, NeuralMDC trains a bidirectional masked transformer to model the spatial-temporal dependencies of latent representations and predict the distribution of the current representation based on the past. The predicted distribution is used to independently entropy code each description and infer any potentially lost tokens. Extensive experiments demonstrate NeuralMDC achieves state-of-the-art loss resilience with minimal sacrifices in compression efficiency, significantly outperforming the best existing residual-coding-based error-resilient neural video codec.
<div id='section'>Paperid: <span id='pid'>1465, <a href='https://arxiv.org/pdf/2412.05560.pdf' target='_blank'>https://arxiv.org/pdf/2412.05560.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenqing Wang, Yun Fu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.05560">Text-to-3D Gaussian Splatting with Physics-Grounded Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-to-3D generation is a valuable technology in virtual reality and digital content creation. While recent works have pushed the boundaries of text-to-3D generation, producing high-fidelity 3D objects with inefficient prompts and simulating their physics-grounded motion accurately still remain unsolved challenges. To address these challenges, we present an innovative framework that utilizes the Large Language Model (LLM)-refined prompts and diffusion priors-guided Gaussian Splatting (GS) for generating 3D models with accurate appearances and geometric structures. We also incorporate a continuum mechanics-based deformation map and color regularization to synthesize vivid physics-grounded motion for the generated 3D Gaussians, adhering to the conservation of mass and momentum. By integrating text-to-3D generation with physics-grounded motion synthesis, our framework renders photo-realistic 3D objects that exhibit physics-aware motion, accurately reflecting the behaviors of the objects under various forces and constraints across different materials. Extensive experiments demonstrate that our approach achieves high-quality 3D generations with realistic physics-grounded motion.
<div id='section'>Paperid: <span id='pid'>1466, <a href='https://arxiv.org/pdf/2412.05460.pdf' target='_blank'>https://arxiv.org/pdf/2412.05460.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qihang Fang, Chengcheng Tang, Bugra Tekin, Yanchao Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.05460">CigTime: Corrective Instruction Generation Through Inverse Motion Editing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in models linking natural language with human motions have shown significant promise in motion generation and editing based on instructional text. Motivated by applications in sports coaching and motor skill learning, we investigate the inverse problem: generating corrective instructional text, leveraging motion editing and generation models. We introduce a novel approach that, given a user's current motion (source) and the desired motion (target), generates text instructions to guide the user towards achieving the target motion. We leverage large language models to generate corrective texts and utilize existing motion generation and editing frameworks to compile datasets of triplets (source motion, target motion, and corrective text). Using this data, we propose a new motion-language model for generating corrective instructions. We present both qualitative and quantitative results across a diverse range of applications that largely improve upon baselines. Our approach demonstrates its effectiveness in instructional scenarios, offering text-based guidance to correct and enhance user performance.
<div id='section'>Paperid: <span id='pid'>1467, <a href='https://arxiv.org/pdf/2412.01930.pdf' target='_blank'>https://arxiv.org/pdf/2412.01930.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anirudh S Chakravarthy, Shuai Kyle Zheng, Xin Huang, Sachithra Hemachandra, Xiao Zhang, Yuning Chai, Zhao Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.01930">PROFIT: A Specialized Optimizer for Deep Fine Tuning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fine-tuning pre-trained models has become invaluable in computer vision and robotics. Recent fine-tuning approaches focus on improving efficiency rather than accuracy by using a mixture of smaller learning rates or frozen backbones. To return the spotlight to model accuracy, we present PROFIT (Proximally Restricted Optimizer For Iterative Training), one of the first optimizers specifically designed for incrementally fine-tuning converged models on new tasks or datasets. Unlike traditional optimizers such as SGD or Adam, which make minimal assumptions due to random initialization, PROFIT leverages the structure of a converged model to regularize the optimization process, leading to improved results. By employing a simple temporal gradient orthogonalization process, PROFIT outperforms traditional fine-tuning methods across various tasks: image classification, representation learning, and large-scale motion prediction. Moreover, PROFIT is encapsulated within the optimizer logic, making it easily integrated into any training pipeline with minimal engineering effort. A new class of fine-tuning optimizers like PROFIT can drive advancements as fine-tuning and incremental training become increasingly prevalent, reducing reliance on costly model training from scratch.
<div id='section'>Paperid: <span id='pid'>1468, <a href='https://arxiv.org/pdf/2412.00112.pdf' target='_blank'>https://arxiv.org/pdf/2412.00112.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Seong-Eun Hong, Soobin Lim, Juyeong Hwang, Minwook Chang, Hyeongyeop Kang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.00112">BiPO: Bidirectional Partial Occlusion Network for Text-to-Motion Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating natural and expressive human motions from textual descriptions is challenging due to the complexity of coordinating full-body dynamics and capturing nuanced motion patterns over extended sequences that accurately reflect the given text. To address this, we introduce BiPO, Bidirectional Partial Occlusion Network for Text-to-Motion Synthesis, a novel model that enhances text-to-motion synthesis by integrating part-based generation with a bidirectional autoregressive architecture. This integration allows BiPO to consider both past and future contexts during generation while enhancing detailed control over individual body parts without requiring ground-truth motion length. To relax the interdependency among body parts caused by the integration, we devise the Partial Occlusion technique, which probabilistically occludes the certain motion part information during training. In our comprehensive experiments, BiPO achieves state-of-the-art performance on the HumanML3D dataset, outperforming recent methods such as ParCo, MoMask, and BAMM in terms of FID scores and overall motion quality. Notably, BiPO excels not only in the text-to-motion generation task but also in motion editing tasks that synthesize motion based on partially generated motion sequences and textual descriptions. These results reveal the BiPO's effectiveness in advancing text-to-motion synthesis and its potential for practical applications.
<div id='section'>Paperid: <span id='pid'>1469, <a href='https://arxiv.org/pdf/2411.18281.pdf' target='_blank'>https://arxiv.org/pdf/2411.18281.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haopeng Fang, Di Qiu, Binjie Mao, Pengfei Yan, He Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.18281">MotionCharacter: Identity-Preserving and Motion Controllable Human Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in personalized Text-to-Video (T2V) generation highlight the importance of integrating character-specific identities and actions. However, previous T2V models struggle with identity consistency and controllable motion dynamics, mainly due to limited fine-grained facial and action-based textual prompts, and datasets that overlook key human attributes and actions. To address these challenges, we propose MotionCharacter, an efficient and high-fidelity human video generation framework designed for identity preservation and fine-grained motion control. We introduce an ID-preserving module to maintain identity fidelity while allowing flexible attribute modifications, and further integrate ID-consistency and region-aware loss mechanisms, significantly enhancing identity consistency and detail fidelity. Additionally, our approach incorporates a motion control module that prioritizes action-related text while maintaining subject consistency, along with a dataset, Human-Motion, which utilizes large language models to generate detailed motion descriptions. For simplify user control during inference, we parameterize motion intensity through a single coefficient, allowing for easy adjustments. Extensive experiments highlight the effectiveness of MotionCharacter, demonstrating significant improvements in ID-preserving, high-quality video generation.
<div id='section'>Paperid: <span id='pid'>1470, <a href='https://arxiv.org/pdf/2411.13327.pdf' target='_blank'>https://arxiv.org/pdf/2411.13327.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kilian Freitag, Yiannis Karayiannidis, Jan Zbinden, Rita Laezza
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.13327">Fine-tuning Myoelectric Control through Reinforcement Learning in a Game Environment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Objective: Enhancing the reliability of myoelectric controllers that decode motor intent is a pressing challenge in the field of bionic prosthetics. State-of-the-art research has mostly focused on Supervised Learning (SL) techniques to tackle this problem. However, obtaining high-quality labeled data that accurately represents muscle activity during daily usage remains difficult. We investigate the potential of Reinforcement Learning (RL) to further improve the decoding of human motion intent by incorporating usage-based data. Methods: The starting point of our method is a SL control policy, pretrained on a static recording of electromyographic (EMG) ground truth data. We then apply RL to fine-tune the pretrained classifier with dynamic EMG data obtained during interaction with a game environment developed for this work. We conducted real-time experiments to evaluate our approach and achieved significant improvements in human-in-the-loop performance. Results: The method effectively predicts simultaneous finger movements, leading to a two-fold increase in decoding accuracy during gameplay and a 39\% improvement in a separate motion test. Conclusion: By employing RL and incorporating usage-based EMG data during fine-tuning, our method achieves significant improvements in accuracy and robustness. Significance: These results showcase the potential of RL for enhancing the reliability of myoelectric controllers, of particular importance for advanced bionic limbs. See our project page for visual demonstrations: https://sites.google.com/view/bionic-limb-rl
<div id='section'>Paperid: <span id='pid'>1471, <a href='https://arxiv.org/pdf/2411.07261.pdf' target='_blank'>https://arxiv.org/pdf/2411.07261.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Arthur Candalot, James Hurrell, Malik Manel Hashim, Brigid Hickey, Mickael Laine, Kazuya Yoshida
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.07261">Sinkage Study in Granular Material for Space Exploration Legged Robot Gripper</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Wheeled rovers have been the primary choice for lunar exploration due to their speed and efficiency. However, deeper areas, such as lunar caves and craters, require the mobility of legged robots. To do so, appropriate end effectors must be designed to enable climbing and walking on the granular surface of the Moon. This paper investigates the behavior of an underactuated soft gripper on deformable granular material when a legged robot is walking in soft soil. A modular test bench and a simulation model were developed to observe the gripper sinkage behavior under load. The gripper uses tendon-driven fingers to match its target shape and grasp on the target surface using multiple micro-spines. The sinkage of the gripper in silica sand was measured by comparing the axial displacement of the gripper with the nominal load of the robot mass. Multiple experiments were performed to observe the sinkage of the gripper over a range of slope angles. A simulation model accounting for the degrees of compliance of the gripper fingers was created using Altair MotionSolve software and coupled to Altair EDEM to compute the gripper interaction with particles utilizing the discrete element method. After validation of the model, complementary simulations using Lunar gravity and a regolith particle model were performed. The results show that a satisfactory gripper model with accurate freedom of motion can be created in simulation using the Altair simulation packages and expected sinkage under load in a particle-filled environment can be estimated using this model. By computing the sinkage of the end effector of legged robots, the results can be directly integrated into the motion control algorithm and improve the accuracy of mobility in a granular material environment.
<div id='section'>Paperid: <span id='pid'>1472, <a href='https://arxiv.org/pdf/2410.15554.pdf' target='_blank'>https://arxiv.org/pdf/2410.15554.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhang Minghao, Song Bifeng, Yang Xiaojun, Wang Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.15554">A Plug-and-Play Fully On-the-Job Real-Time Reinforcement Learning Algorithm for a Direct-Drive Tandem-Wing Experiment Platforms Under Multiple Random Operating Conditions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The nonlinear and unstable aerodynamic interference generated by the tandem wings of such biomimetic systems poses substantial challenges for motion control, especially under multiple random operating conditions. To address these challenges, the Concerto Reinforcement Learning Extension (CRL2E) algorithm has been developed. This plug-and-play, fully on-the-job, real-time reinforcement learning algorithm incorporates a novel Physics-Inspired Rule-Based Policy Composer Strategy with a Perturbation Module alongside a lightweight network optimized for real-time control. To validate the performance and the rationality of the module design, experiments were conducted under six challenging operating conditions, comparing seven different algorithms. The results demonstrate that the CRL2E algorithm achieves safe and stable training within the first 500 steps, improving tracking accuracy by 14 to 66 times compared to the Soft Actor-Critic, Proximal Policy Optimization, and Twin Delayed Deep Deterministic Policy Gradient algorithms. Additionally, CRL2E significantly enhances performance under various random operating conditions, with improvements in tracking accuracy ranging from 8.3% to 60.4% compared to the Concerto Reinforcement Learning (CRL) algorithm. The convergence speed of CRL2E is 36.11% to 57.64% faster than the CRL algorithm with only the Composer Perturbation and 43.52% to 65.85% faster than the CRL algorithm when both the Composer Perturbation and Time-Interleaved Capability Perturbation are introduced, especially in conditions where the standard CRL struggles to converge. Hardware tests indicate that the optimized lightweight network structure excels in weight loading and average inference time, meeting real-time control requirements.
<div id='section'>Paperid: <span id='pid'>1473, <a href='https://arxiv.org/pdf/2410.01628.pdf' target='_blank'>https://arxiv.org/pdf/2410.01628.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aron Distelzweig, Andreas Look, Eitan Kosman, Faris JanjoÅ¡, JÃ¶rg Wagner, Abhinav Valada
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.01628">Stochasticity in Motion: An Information-Theoretic Approach to Trajectory Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In autonomous driving, accurate motion prediction is crucial for safe and efficient motion planning. To ensure safety, planners require reliable uncertainty estimates of the predicted behavior of surrounding agents, yet this aspect has received limited attention. In particular, decomposing uncertainty into its aleatoric and epistemic components is essential for distinguishing between inherent environmental randomness and model uncertainty, thereby enabling more robust and informed decision-making. This paper addresses the challenge of uncertainty modeling in trajectory prediction with a holistic approach that emphasizes uncertainty quantification, decomposition, and the impact of model composition. Our method, grounded in information theory, provides a theoretically principled way to measure uncertainty and decompose it into aleatoric and epistemic components. Unlike prior work, our approach is compatible with state-of-the-art motion predictors, allowing for broader applicability. We demonstrate its utility by conducting extensive experiments on the nuScenes dataset, which shows how different architectures and configurations influence uncertainty quantification and model robustness.
<div id='section'>Paperid: <span id='pid'>1474, <a href='https://arxiv.org/pdf/2408.09178.pdf' target='_blank'>https://arxiv.org/pdf/2408.09178.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Changcheng Xiao, Qiong Cao, Zhigang Luo, Long Lan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.09178">MambaTrack: A Simple Baseline for Multiple Object Tracking with State Space Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tracking by detection has been the prevailing paradigm in the field of Multi-object Tracking (MOT). These methods typically rely on the Kalman Filter to estimate the future locations of objects, assuming linear object motion. However, they fall short when tracking objects exhibiting nonlinear and diverse motion in scenarios like dancing and sports. In addition, there has been limited focus on utilizing learning-based motion predictors in MOT. To address these challenges, we resort to exploring data-driven motion prediction methods. Inspired by the great expectation of state space models (SSMs), such as Mamba, in long-term sequence modeling with near-linear complexity, we introduce a Mamba-based motion model named Mamba moTion Predictor (MTP). MTP is designed to model the complex motion patterns of objects like dancers and athletes. Specifically, MTP takes the spatial-temporal location dynamics of objects as input, captures the motion pattern using a bi-Mamba encoding layer, and predicts the next motion. In real-world scenarios, objects may be missed due to occlusion or motion blur, leading to premature termination of their trajectories. To tackle this challenge, we further expand the application of MTP. We employ it in an autoregressive way to compensate for missing observations by utilizing its own predictions as inputs, thereby contributing to more consistent trajectories. Our proposed tracker, MambaTrack, demonstrates advanced performance on benchmarks such as Dancetrack and SportsMOT, which are characterized by complex motion and severe occlusion.
<div id='section'>Paperid: <span id='pid'>1475, <a href='https://arxiv.org/pdf/2407.15675.pdf' target='_blank'>https://arxiv.org/pdf/2407.15675.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rabbia Asghar, Wenqian Liu, Lukas Rummelhard, Anne Spalanzani, Christian Laugier
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.15675">Flow-guided Motion Prediction with Semantics and Dynamic Occupancy Grid Maps</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate prediction of driving scenes is essential for road safety and autonomous driving. Occupancy Grid Maps (OGMs) are commonly employed for scene prediction due to their structured spatial representation, flexibility across sensor modalities and integration of uncertainty. Recent studies have successfully combined OGMs with deep learning methods to predict the evolution of scene and learn complex behaviours. These methods, however, do not consider prediction of flow or velocity vectors in the scene. In this work, we propose a novel multi-task framework that leverages dynamic OGMs and semantic information to predict both future vehicle semantic grids and the future flow of the scene. This incorporation of semantic flow not only offers intermediate scene features but also enables the generation of warped semantic grids. Evaluation on the real-world NuScenes dataset demonstrates improved prediction capabilities and enhanced ability of the model to retain dynamic vehicles within the scene.
<div id='section'>Paperid: <span id='pid'>1476, <a href='https://arxiv.org/pdf/2407.15408.pdf' target='_blank'>https://arxiv.org/pdf/2407.15408.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kent Fujiwara, Mikihiro Tanaka, Qing Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.15408">Chronologically Accurate Retrieval for Temporal Grounding of Motion-Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the release of large-scale motion datasets with textual annotations, the task of establishing a robust latent space for language and 3D human motion has recently witnessed a surge of interest. Methods have been proposed to convert human motion and texts into features to achieve accurate correspondence between them. Despite these efforts to align language and motion representations, we claim that the temporal element is often overlooked, especially for compound actions, resulting in chronological inaccuracies. To shed light on the temporal alignment in motion-language latent spaces, we propose Chronologically Accurate Retrieval (CAR) to evaluate the chronological understanding of the models. We decompose textual descriptions into events, and prepare negative text samples by shuffling the order of events in compound action descriptions. We then design a simple task for motion-language models to retrieve the more likely text from the ground truth and its chronologically shuffled version. CAR reveals many cases where current motion-language models fail to distinguish the event chronology of human motion, despite their impressive performance in terms of conventional evaluation metrics. To achieve better temporal alignment between text and motion, we further propose to use these texts with shuffled sequence of events as negative samples during training to reinforce the motion-language models. We conduct experiments on text-motion retrieval and text-to-motion generation using the reinforced motion-language models, which demonstrate improved performance over conventional approaches, indicating the necessity to consider temporal elements in motion-language alignment.
<div id='section'>Paperid: <span id='pid'>1477, <a href='https://arxiv.org/pdf/2407.11962.pdf' target='_blank'>https://arxiv.org/pdf/2407.11962.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jaehyeok Kim, Dongyoon Wee, Dan Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.11962">Motion-Oriented Compositional Neural Radiance Fields for Monocular Dynamic Human Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces Motion-oriented Compositional Neural Radiance Fields (MoCo-NeRF), a framework designed to perform free-viewpoint rendering of monocular human videos via novel non-rigid motion modeling approach. In the context of dynamic clothed humans, complex cloth dynamics generate non-rigid motions that are intrinsically distinct from skeletal articulations and critically important for the rendering quality. The conventional approach models non-rigid motions as spatial (3D) deviations in addition to skeletal transformations. However, it is either time-consuming or challenging to achieve optimal quality due to its high learning complexity without a direct supervision. To target this problem, we propose a novel approach of modeling non-rigid motions as radiance residual fields to benefit from more direct color supervision in the rendering and utilize the rigid radiance fields as a prior to reduce the complexity of the learning process. Our approach utilizes a single multiresolution hash encoding (MHE) to concurrently learn the canonical T-pose representation from rigid skeletal motions and the radiance residual field for non-rigid motions. Additionally, to further improve both training efficiency and usability, we extend MoCo-NeRF to support simultaneous training of multiple subjects within a single framework, thanks to our effective design for modeling non-rigid motions. This scalability is achieved through the integration of a global MHE and learnable identity codes in addition to multiple local MHEs. We present extensive results on ZJU-MoCap and MonoCap, clearly demonstrating state-of-the-art performance in both single- and multi-subject settings. The code and model will be made publicly available at the project page: https://stevejaehyeok.github.io/publications/moco-nerf.
<div id='section'>Paperid: <span id='pid'>1478, <a href='https://arxiv.org/pdf/2406.18537.pdf' target='_blank'>https://arxiv.org/pdf/2406.18537.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Keenon Werling, Janelle Kaneda, Alan Tan, Rishi Agarwal, Six Skov, Tom Van Wouwe, Scott Uhlrich, Nicholas Bianco, Carmichael Ong, Antoine Falisse, Shardul Sapkota, Aidan Chandra, Joshua Carter, Ezio Preatoni, Benjamin Fregly, Jennifer Hicks, Scott Delp, C. Karen Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.18537">AddBiomechanics Dataset: Capturing the Physics of Human Motion at Scale</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While reconstructing human poses in 3D from inexpensive sensors has advanced significantly in recent years, quantifying the dynamics of human motion, including the muscle-generated joint torques and external forces, remains a challenge. Prior attempts to estimate physics from reconstructed human poses have been hampered by a lack of datasets with high-quality pose and force data for a variety of movements. We present the AddBiomechanics Dataset 1.0, which includes physically accurate human dynamics of 273 human subjects, over 70 hours of motion and force plate data, totaling more than 24 million frames. To construct this dataset, novel analytical methods were required, which are also reported here. We propose a benchmark for estimating human dynamics from motion using this dataset, and present several baseline results. The AddBiomechanics Dataset is publicly available at https://addbiomechanics.org/download_data.html.
<div id='section'>Paperid: <span id='pid'>1479, <a href='https://arxiv.org/pdf/2406.18159.pdf' target='_blank'>https://arxiv.org/pdf/2406.18159.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaolin Hong, Hongwei Yi, Fazhi He, Qiong Cao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.18159">Human-Aware 3D Scene Generation with Spatially-constrained Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating 3D scenes from human motion sequences supports numerous applications, including virtual reality and architectural design. However, previous auto-regression-based human-aware 3D scene generation methods have struggled to accurately capture the joint distribution of multiple objects and input humans, often resulting in overlapping object generation in the same space. To address this limitation, we explore the potential of diffusion models that simultaneously consider all input humans and the floor plan to generate plausible 3D scenes. Our approach not only satisfies all input human interactions but also adheres to spatial constraints with the floor plan. Furthermore, we introduce two spatial collision guidance mechanisms: human-object collision avoidance and object-room boundary constraints. These mechanisms help avoid generating scenes that conflict with human motions while respecting layout constraints. To enhance the diversity and accuracy of human-guided scene generation, we have developed an automated pipeline that improves the variety and plausibility of human-object interactions in the existing 3D FRONT HUMAN dataset. Extensive experiments on both synthetic and real-world datasets demonstrate that our framework can generate more natural and plausible 3D scenes with precise human-scene interactions, while significantly reducing human-object collisions compared to previous state-of-the-art methods. Our code and data will be made publicly available upon publication of this work.
<div id='section'>Paperid: <span id='pid'>1480, <a href='https://arxiv.org/pdf/2406.10126.pdf' target='_blank'>https://arxiv.org/pdf/2406.10126.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chen Hou, Zhibo Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.10126">Training-free Camera Control for Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a training-free and robust solution to offer camera movement control for off-the-shelf video diffusion models. Unlike previous work, our method does not require any supervised finetuning on camera-annotated datasets or self-supervised training via data augmentation. Instead, it can be plug-and-play with most pretrained video diffusion models and generate camera-controllable videos with a single image or text prompt as input. The inspiration for our work comes from the layout prior that intermediate latents encode for the generated results, thus rearranging noisy pixels in them will cause the output content to relocate as well. As camera moving could also be seen as a type of pixel rearrangement caused by perspective change, videos can be reorganized following specific camera motion if their noisy latents change accordingly. Building on this, we propose CamTrol, which enables robust camera control for video diffusion models. It is achieved by a two-stage process. First, we model image layout rearrangement through explicit camera movement in 3D point cloud space. Second, we generate videos with camera motion by leveraging the layout prior of noisy latents formed by a series of rearranged images. Extensive experiments have demonstrated its superior performance in both video generation and camera motion alignment compared with other finetuned methods. Furthermore, we show the capability of CamTrol to generalize to various base models, as well as its impressive applications in scalable motion control, dealing with complicated trajectories and unsupervised 3D video generation. Videos available at https://lifedecoder.github.io/CamTrol/.
<div id='section'>Paperid: <span id='pid'>1481, <a href='https://arxiv.org/pdf/2406.01136.pdf' target='_blank'>https://arxiv.org/pdf/2406.01136.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Konstantinos Roditakis, Spyridon Thermos, Nikolaos Zioulis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.01136">Towards Practical Single-shot Motion Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite the recent advances in the so-called "cold start" generation from text prompts, their needs in data and computing resources, as well as the ambiguities around intellectual property and privacy concerns pose certain counterarguments for their utility. An interesting and relatively unexplored alternative has been the introduction of unconditional synthesis from a single sample, which has led to interesting generative applications. In this paper we focus on single-shot motion generation and more specifically on accelerating the training time of a Generative Adversarial Network (GAN). In particular, we tackle the challenge of GAN's equilibrium collapse when using mini-batch training by carefully annealing the weights of the loss functions that prevent mode collapse. Additionally, we perform statistical analysis in the generator and discriminator models to identify correlations between training stages and enable transfer learning. Our improved GAN achieves competitive quality and diversity on the Mixamo benchmark when compared to the original GAN architecture and a single-shot diffusion model, while being up to x6.8 faster in training time from the former and x1.75 from the latter. Finally, we demonstrate the ability of our improved GAN to mix and compose motion with a single forward pass. Project page available at https://moverseai.github.io/single-shot.
<div id='section'>Paperid: <span id='pid'>1482, <a href='https://arxiv.org/pdf/2406.00960.pdf' target='_blank'>https://arxiv.org/pdf/2406.00960.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Takara E. Truong, Michael Piseno, Zhaoming Xie, C. Karen Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.00960">PDP: Physics-Based Character Animation via Diffusion Policy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating diverse and realistic human motion that can physically interact with an environment remains a challenging research area in character animation. Meanwhile, diffusion-based methods, as proposed by the robotics community, have demonstrated the ability to capture highly diverse and multi-modal skills. However, naively training a diffusion policy often results in unstable motions for high-frequency, under-actuated control tasks like bipedal locomotion due to rapidly accumulating compounding errors, pushing the agent away from optimal training trajectories. The key idea lies in using RL policies not just for providing optimal trajectories but for providing corrective actions in sub-optimal states, giving the policy a chance to correct for errors caused by environmental stimulus, model errors, or numerical errors in simulation. Our method, Physics-Based Character Animation via Diffusion Policy (PDP), combines reinforcement learning (RL) and behavior cloning (BC) to create a robust diffusion policy for physics-based character animation. We demonstrate PDP on perturbation recovery, universal motion tracking, and physics-based text-to-motion synthesis.
<div id='section'>Paperid: <span id='pid'>1483, <a href='https://arxiv.org/pdf/2406.00211.pdf' target='_blank'>https://arxiv.org/pdf/2406.00211.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yufei Huang, Yulin Li, Andrea Matta, Mohsen Jafari
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.00211">Navigating Autonomous Vehicle on Unmarked Roads with Diffusion-Based Motion Prediction and Active Inference</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a novel approach to improving autonomous vehicle control in environments lacking clear road markings by integrating a diffusion-based motion predictor within an Active Inference Framework (AIF). Using a simulated parking lot environment as a parallel to unmarked roads, we develop and test our model to predict and guide vehicle movements effectively. The diffusion-based motion predictor forecasts vehicle actions by leveraging probabilistic dynamics, while AIF aids in decision-making under uncertainty. Unlike traditional methods such as Model Predictive Control (MPC) and Reinforcement Learning (RL), our approach reduces computational demands and requires less extensive training, enhancing navigation safety and efficiency. Our results demonstrate the model's capability to navigate complex scenarios, marking significant progress in autonomous driving technology.
<div id='section'>Paperid: <span id='pid'>1484, <a href='https://arxiv.org/pdf/2405.18438.pdf' target='_blank'>https://arxiv.org/pdf/2405.18438.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>ZoltÃ¡n Ã. Milacski, Koichiro Niinuma, Ryosuke Kawamura, Fernando de la Torre, LÃ¡szlÃ³ A. Jeni
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.18438">GHOST: Grounded Human Motion Generation with Open Vocabulary Scene-and-Text Contexts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The connection between our 3D surroundings and the descriptive language that characterizes them would be well-suited for localizing and generating human motion in context but for one problem. The complexity introduced by multiple modalities makes capturing this connection challenging with a fixed set of descriptors. Specifically, closed vocabulary scene encoders, which require learning text-scene associations from scratch, have been favored in the literature, often resulting in inaccurate motion grounding. In this paper, we propose a method that integrates an open vocabulary scene encoder into the architecture, establishing a robust connection between text and scene. Our two-step approach starts with pretraining the scene encoder through knowledge distillation from an existing open vocabulary semantic image segmentation model, ensuring a shared text-scene feature space. Subsequently, the scene encoder is fine-tuned for conditional motion generation, incorporating two novel regularization losses that regress the category and size of the goal object. Our methodology achieves up to a 30% reduction in the goal object distance metric compared to the prior state-of-the-art baseline model on the HUMANISE dataset. This improvement is demonstrated through evaluations conducted using three implementations of our framework and a perceptual study. Additionally, our method is designed to seamlessly accommodate future 2D segmentation methods that provide per-pixel text-aligned features for distillation.
<div id='section'>Paperid: <span id='pid'>1485, <a href='https://arxiv.org/pdf/2405.12460.pdf' target='_blank'>https://arxiv.org/pdf/2405.12460.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianan Li, Tao Huang, Qingxu Zhu, Tien-Tsin Wong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.12460">Physics-based Scene Layout Generation from Human Motion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Creating scenes for captured motions that achieve realistic human-scene interaction is crucial for 3D animation in movies or video games. As character motion is often captured in a blue-screened studio without real furniture or objects in place, there may be a discrepancy between the planned motion and the captured one. This gives rise to the need for automatic scene layout generation to relieve the burdens of selecting and positioning furniture and objects. Previous approaches cannot avoid artifacts like penetration and floating due to the lack of physical constraints. Furthermore, some heavily rely on specific data to learn the contact affordances, restricting the generalization ability to different motions. In this work, we present a physics-based approach that simultaneously optimizes a scene layout generator and simulates a moving human in a physics simulator. To attain plausible and realistic interaction motions, our method explicitly introduces physical constraints. To automatically recover and generate the scene layout, we minimize the motion tracking errors to identify the objects that can afford interaction. We use reinforcement learning to perform a dual-optimization of both the character motion imitation controller and the scene layout generator. To facilitate the optimization, we reshape the tracking rewards and devise pose prior guidance obtained from our estimated pseudo-contact labels. We evaluate our method using motions from SAMP and PROX, and demonstrate physically plausible scene layout reconstruction compared with the previous kinematics-based method.
<div id='section'>Paperid: <span id='pid'>1486, <a href='https://arxiv.org/pdf/2405.10134.pdf' target='_blank'>https://arxiv.org/pdf/2405.10134.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tobias Demmler, Andreas Tamke, Thao Dang, Karsten Haug, Lars Mikelsons
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.10134">Towards Consistent and Explainable Motion Prediction using Heterogeneous Graph Attention</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In autonomous driving, accurately interpreting the movements of other road users and leveraging this knowledge to forecast future trajectories is crucial. This is typically achieved through the integration of map data and tracked trajectories of various agents. Numerous methodologies combine this information into a singular embedding for each agent, which is then utilized to predict future behavior. However, these approaches have a notable drawback in that they may lose exact location information during the encoding process. The encoding still includes general map information. However, the generation of valid and consistent trajectories is not guaranteed. This can cause the predicted trajectories to stray from the actual lanes. This paper introduces a new refinement module designed to project the predicted trajectories back onto the actual map, rectifying these discrepancies and leading towards more consistent predictions. This versatile module can be readily incorporated into a wide range of architectures. Additionally, we propose a novel scene encoder that handles all relations between agents and their environment in a single unified heterogeneous graph attention network. By analyzing the attention values on the different edges in this graph, we can gain unique insights into the neural network's inner workings leading towards a more explainable prediction.
<div id='section'>Paperid: <span id='pid'>1487, <a href='https://arxiv.org/pdf/2405.04771.pdf' target='_blank'>https://arxiv.org/pdf/2405.04771.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qing Yu, Mikihiro Tanaka, Kent Fujiwara
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.04771">Exploring Vision Transformers for 3D Human Motion-Language Models with Motion Patches</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To build a cross-modal latent space between 3D human motion and language, acquiring large-scale and high-quality human motion data is crucial. However, unlike the abundance of image data, the scarcity of motion data has limited the performance of existing motion-language models. To counter this, we introduce "motion patches", a new representation of motion sequences, and propose using Vision Transformers (ViT) as motion encoders via transfer learning, aiming to extract useful knowledge from the image domain and apply it to the motion domain. These motion patches, created by dividing and sorting skeleton joints based on body parts in motion sequences, are robust to varying skeleton structures, and can be regarded as color image patches in ViT. We find that transfer learning with pre-trained weights of ViT obtained through training with 2D image data can boost the performance of motion analysis, presenting a promising direction for addressing the issue of limited motion data. Our extensive experiments show that the proposed motion patches, used jointly with ViT, achieve state-of-the-art performance in the benchmarks of text-to-motion retrieval, and other novel challenging tasks, such as cross-skeleton recognition, zero-shot motion classification, and human interaction recognition, which are currently impeded by the lack of data.
<div id='section'>Paperid: <span id='pid'>1488, <a href='https://arxiv.org/pdf/2404.11375.pdf' target='_blank'>https://arxiv.org/pdf/2404.11375.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinghan Wang, Zixi Kang, Yadong Mu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.11375">Text-controlled Motion Mamba: Text-Instructed Temporal Grounding of Human Motion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion understanding is a fundamental task with diverse practical applications, facilitated by the availability of large-scale motion capture datasets. Recent studies focus on text-motion tasks, such as text-based motion generation, editing and question answering. In this study, we introduce the novel task of text-based human motion grounding (THMG), aimed at precisely localizing temporal segments corresponding to given textual descriptions within untrimmed motion sequences. Capturing global temporal information is crucial for the THMG task. However, transformer-based models that rely on global temporal self-attention face challenges when handling long untrimmed sequences due to the quadratic computational cost. We address these challenges by proposing Text-controlled Motion Mamba (TM-Mamba), a unified model that integrates temporal global context, language query control, and spatial graph topology with only linear memory cost. The core of the model is a text-controlled selection mechanism which dynamically incorporates global temporal information based on text query. The model is further enhanced to be topology-aware through the integration of relational embeddings. For evaluation, we introduce BABEL-Grounding, the first text-motion dataset that provides detailed textual descriptions of human actions along with their corresponding temporal segments. Extensive evaluations demonstrate the effectiveness of TM-Mamba on BABEL-Grounding.
<div id='section'>Paperid: <span id='pid'>1489, <a href='https://arxiv.org/pdf/2404.03584.pdf' target='_blank'>https://arxiv.org/pdf/2404.03584.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pengxiang Ding, Jianqin Yin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.03584">Towards more realistic human motion prediction with attention to motion coordination</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Joint relation modeling is a curial component in human motion prediction. Most existing methods rely on skeletal-based graphs to build the joint relations, where local interactive relations between joint pairs are well learned. However, the motion coordination, a global joint relation reflecting the simultaneous cooperation of all joints, is usually weakened because it is learned from part to whole progressively and asynchronously. Thus, the final predicted motions usually appear unrealistic. To tackle this issue, we learn a medium, called coordination attractor (CA), from the spatiotemporal features of motion to characterize the global motion features, which is subsequently used to build new relative joint relations. Through the CA, all joints are related simultaneously, and thus the motion coordination of all joints can be better learned. Based on this, we further propose a novel joint relation modeling module, Comprehensive Joint Relation Extractor (CJRE), to combine this motion coordination with the local interactions between joint pairs in a unified manner. Additionally, we also present a Multi-timescale Dynamics Extractor (MTDE) to extract enriched dynamics from the raw position information for effective prediction. Extensive experiments show that the proposed framework outperforms state-of-the-art methods in both short- and long-term predictions on H3.6M, CMU-Mocap, and 3DPW.
<div id='section'>Paperid: <span id='pid'>1490, <a href='https://arxiv.org/pdf/2403.18695.pdf' target='_blank'>https://arxiv.org/pdf/2403.18695.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Luyao Zhang, George Pantazis, Shaohang Han, Sergio Grammatico
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.18695">An Efficient Risk-aware Branch MPC for Automated Driving that is Robust to Uncertain Vehicle Behaviors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>One of the critical challenges in automated driving is ensuring safety of automated vehicles despite the unknown behavior of the other vehicles. Although motion prediction modules are able to generate a probability distribution associated with various behavior modes, their probabilistic estimates are often inaccurate, thus leading to a possibly unsafe trajectory. To overcome this challenge, we propose a risk-aware motion planning framework that appropriately accounts for the ambiguity in the estimated probability distribution. We formulate the risk-aware motion planning problem as a min-max optimization problem and develop an efficient iterative method by incorporating a regularization term in the probability update step. Via extensive numerical studies, we validate the convergence of our method and demonstrate its advantages compared to the state-of-the-art approaches.
<div id='section'>Paperid: <span id='pid'>1491, <a href='https://arxiv.org/pdf/2403.08363.pdf' target='_blank'>https://arxiv.org/pdf/2403.08363.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Karthikeya Puttur Venkatraj, Wo Meijer, Monica PerusquÃ­a-HernÃ¡ndez, Gijs Huisman, Abdallah El Ali
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.08363">ShareYourReality: Investigating Haptic Feedback and Agency in Virtual Avatar Co-embodiment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Virtual co-embodiment enables two users to share a single avatar in Virtual Reality (VR). During such experiences, the illusion of shared motion control can break during joint-action activities, highlighting the need for position-aware feedback mechanisms. Drawing on the perceptual crossing paradigm, we explore how haptics can enable non-verbal coordination between co-embodied participants. In a within-subjects study (20 participant pairs), we examined the effects of vibrotactile haptic feedback (None, Present) and avatar control distribution (25-75%, 50-50%, 75-25%) across two VR reaching tasks (Targeted, Free-choice) on participants Sense of Agency (SoA), co-presence, body ownership, and motion synchrony. We found (a) lower SoA in the free-choice with haptics than without, (b) higher SoA during the shared targeted task, (c) co-presence and body ownership were significantly higher in the free-choice task, (d) players hand motions synchronized more in the targeted task. We provide cautionary considerations when including haptic feedback mechanisms for avatar co-embodiment experiences.
<div id='section'>Paperid: <span id='pid'>1492, <a href='https://arxiv.org/pdf/2403.05478.pdf' target='_blank'>https://arxiv.org/pdf/2403.05478.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mengsha Hu, Jinzhou Li, Runxiang Jin, Chao Shi, Lei Xu, Rui Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.05478">HGIC: A Hand Gesture Based Interactive Control System for Efficient and Scalable Multi-UAV Operations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As technological advancements continue to expand the capabilities of multi unmanned-aerial-vehicle systems (mUAV), human operators face challenges in scalability and efficiency due to the complex cognitive load and operations associated with motion adjustments and team coordination. Such cognitive demands limit the feasible size of mUAV teams and necessitate extensive operator training, impeding broader adoption. This paper developed a Hand Gesture Based Interactive Control (HGIC), a novel interface system that utilize computer vision techniques to intuitively translate hand gestures into modular commands for robot teaming. Through learning control models, these commands enable efficient and scalable mUAV motion control and adjustments. HGIC eliminates the need for specialized hardware and offers two key benefits: 1) Minimal training requirements through natural gestures; and 2) Enhanced scalability and efficiency via adaptable commands. By reducing the cognitive burden on operators, HGIC opens the door for more effective large-scale mUAV applications in complex, dynamic, and uncertain scenarios. HGIC will be open-sourced after the paper being published online for the research community, aiming to drive forward innovations in human-mUAV interactions.
<div id='section'>Paperid: <span id='pid'>1493, <a href='https://arxiv.org/pdf/2403.03681.pdf' target='_blank'>https://arxiv.org/pdf/2403.03681.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chuanyu Luo, Nuo Cheng, Ren Zhong, Haipeng Jiang, Wenyu Chen, Aoli Wang, Pu Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.03681">3D Object Visibility Prediction in Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapid advancement of hardware and software technologies, research in autonomous driving has seen significant growth. The prevailing framework for multi-sensor autonomous driving encompasses sensor installation, perception, path planning, decision-making, and motion control. At the perception phase, a common approach involves utilizing neural networks to infer 3D bounding box (Bbox) attributes from raw sensor data, including classification, size, and orientation. In this paper, we present a novel attribute and its corresponding algorithm: 3D object visibility. By incorporating multi-task learning, the introduction of this attribute, visibility, negligibly affects the model's effectiveness and efficiency. Our proposal of this attribute and its computational strategy aims to expand the capabilities for downstream tasks, thereby enhancing the safety and reliability of real-time autonomous driving in real-world scenarios.
<div id='section'>Paperid: <span id='pid'>1494, <a href='https://arxiv.org/pdf/2402.17790.pdf' target='_blank'>https://arxiv.org/pdf/2402.17790.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Niklas Kueper, Su Kyoung Kim, Elsa Andrea Kirchner
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.17790">EEG classifier cross-task transfer to avoid training sessions in robot-assisted rehabilitation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Background: For an individualized support of patients during rehabilitation, learning of individual machine learning models from the human electroencephalogram (EEG) is required. Our approach allows labeled training data to be recorded without the need for a specific training session. For this, the planned exoskeleton-assisted rehabilitation enables bilateral mirror therapy, in which movement intentions can be inferred from the activity of the unaffected arm. During this therapy, labeled EEG data can be collected to enable movement predictions of only the affected arm of a patient. Methods: A study was conducted with 8 healthy subjects and the performance of the classifier transfer approach was evaluated. Each subject performed 3 runs of 40 self-intended unilateral and bilateral reaching movements toward a target while EEG data was recorded from 64 channels. A support vector machine (SVM) classifier was trained under both movement conditions to make predictions for the same type of movement. Furthermore, the classifier was evaluated to predict unilateral movements by only beeing trained on the data of the bilateral movement condition. Results: The results show that the performance of the classifier trained on selected EEG channels evoked by bilateral movement intentions is not significantly reduced compared to a classifier trained directly on EEG data including unilateral movement intentions. Moreover, the results show that our approach also works with only 8 or even 4 channels. Conclusion: It was shown that the proposed classifier transfer approach enables motion prediction without explicit collection of training data. Since the approach can be applied even with a small number of EEG channels, this speaks for the feasibility of the approach in real therapy sessions with patients and motivates further investigations with stroke patients.
<div id='section'>Paperid: <span id='pid'>1495, <a href='https://arxiv.org/pdf/2402.04820.pdf' target='_blank'>https://arxiv.org/pdf/2402.04820.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Arjun S. Lakshmipathy, Jessica K. Hodgins, Nancy S. Pollard
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.04820">Kinematic Motion Retargeting for Contact-Rich Anthropomorphic Manipulations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Hand motion capture data is now relatively easy to obtain, even for complicated grasps; however this data is of limited use without the ability to retarget it onto the hands of a specific character or robot. The target hand may differ dramatically in geometry, number of degrees of freedom (DOFs), or number of fingers. We present a simple, but effective framework capable of kinematically retargeting multiple human hand-object manipulations from a publicly available dataset to a wide assortment of kinematically and morphologically diverse target hands through the exploitation of contact areas. We do so by formulating the retarget operation as a non-isometric shape matching problem and use a combination of both surface contact and marker data to progressively estimate, refine, and fit the final target hand trajectory using inverse kinematics (IK). Foundational to our framework is the introduction of a novel shape matching process, which we show enables predictable and robust transfer of contact data over full manipulations while providing an intuitive means for artists to specify correspondences with relatively few inputs. We validate our framework through thirty demonstrations across five different hand shapes and six motions of different objects. We additionally compare our method against existing hand retargeting approaches. Finally, we demonstrate our method enabling novel capabilities such as object substitution and the ability to visualize the impact of design choices over full trajectories.
<div id='section'>Paperid: <span id='pid'>1496, <a href='https://arxiv.org/pdf/2401.13414.pdf' target='_blank'>https://arxiv.org/pdf/2401.13414.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xingyu Song, Zhan Li, Shi Chen, Kazuyuki Demachi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.13414">GTAutoAct: An Automatic Datasets Generation Framework Based on Game Engine Redevelopment for Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current datasets for action recognition tasks face limitations stemming from traditional collection and generation methods, including the constrained range of action classes, absence of multi-viewpoint recordings, limited diversity, poor video quality, and labor-intensive manually collection. To address these challenges, we introduce GTAutoAct, a innovative dataset generation framework leveraging game engine technology to facilitate advancements in action recognition. GTAutoAct excels in automatically creating large-scale, well-annotated datasets with extensive action classes and superior video quality. Our framework's distinctive contributions encompass: (1) it innovatively transforms readily available coordinate-based 3D human motion into rotation-orientated representation with enhanced suitability in multiple viewpoints; (2) it employs dynamic segmentation and interpolation of rotation sequences to create smooth and realistic animations of action; (3) it offers extensively customizable animation scenes; (4) it implements an autonomous video capture and processing pipeline, featuring a randomly navigating camera, with auto-trimming and labeling functionalities. Experimental results underscore the framework's robustness and highlights its potential to significantly improve action recognition model training.
<div id='section'>Paperid: <span id='pid'>1497, <a href='https://arxiv.org/pdf/2401.11115.pdf' target='_blank'>https://arxiv.org/pdf/2401.11115.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nhat M. Hoang, Kehong Gong, Chuan Guo, Michael Bi Mi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.11115">MotionMix: Weakly-Supervised Diffusion for Controllable Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Controllable generation of 3D human motions becomes an important topic as the world embraces digital transformation. Existing works, though making promising progress with the advent of diffusion models, heavily rely on meticulously captured and annotated (e.g., text) high-quality motion corpus, a resource-intensive endeavor in the real world. This motivates our proposed MotionMix, a simple yet effective weakly-supervised diffusion model that leverages both noisy and unannotated motion sequences. Specifically, we separate the denoising objectives of a diffusion model into two stages: obtaining conditional rough motion approximations in the initial $T-T^*$ steps by learning the noisy annotated motions, followed by the unconditional refinement of these preliminary motions during the last $T^*$ steps using unannotated motions. Notably, though learning from two sources of imperfect data, our model does not compromise motion generation quality compared to fully supervised approaches that access gold data. Extensive experiments on several benchmarks demonstrate that our MotionMix, as a versatile framework, consistently achieves state-of-the-art performances on text-to-motion, action-to-motion, and music-to-dance tasks. Project page: https://nhathoang2002.github.io/MotionMix-page/
<div id='section'>Paperid: <span id='pid'>1498, <a href='https://arxiv.org/pdf/2401.02899.pdf' target='_blank'>https://arxiv.org/pdf/2401.02899.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Luka LanÄa, Karlo Jakac, Stefan IviÄ
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.02899">Model predictive altitude and velocity control in ergodic potential field directed multi-UAV search</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This research addresses the challenge of executing multi-UAV survey missions over diverse terrains characterized by varying elevations. The approach integrates advanced two-dimensional ergodic search technique with model predictive control of UAV altitude and velocity. Optimization of altitude and velocity is performed along anticipated UAV ground routes, considering multiple objectives and constraints. This yields a flight regimen tailored to the terrain, as well as the motion and sensing characteristics of the UAVs. The proposed UAV motion control strategy is assessed through simulations of realistic search missions and actual terrain models. Results demonstrate the successful integration of model predictive altitude and velocity control with a two-dimensional potential field-guided ergodic search. Adjusting UAV altitudes to near-ideal levels facilitates the utilization of sensing ranges, thereby enhancing the effectiveness of the search. Furthermore, the control algorithm is capable of real-time computation, encouraging its practical application in real-world scenarios.
<div id='section'>Paperid: <span id='pid'>1499, <a href='https://arxiv.org/pdf/2312.11538.pdf' target='_blank'>https://arxiv.org/pdf/2312.11538.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Purvi Goel, Kuan-Chieh Wang, C. Karen Liu, Kayvon Fatahalian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.11538">Iterative Motion Editing with Natural Language</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-to-motion diffusion models can generate realistic animations from text prompts, but do not support fine-grained motion editing controls. In this paper, we present a method for using natural language to iteratively specify local edits to existing character animations, a task that is common in most computer animation workflows. Our key idea is to represent a space of motion edits using a set of kinematic motion editing operators (MEOs) whose effects on the source motion is well-aligned with user expectations. We provide an algorithm that leverages pre-existing language models to translate textual descriptions of motion edits into source code for programs that define and execute sequences of MEOs on a source animation. We execute MEOs by first translating them into keyframe constraints, and then use diffusion-based motion models to generate output motions that respect these constraints. Through a user study and quantitative evaluation, we demonstrate that our system can perform motion edits that respect the animator's editing intent, remain faithful to the original animation (it edits the original animation, but does not dramatically change it), and yield realistic character animation results.
<div id='section'>Paperid: <span id='pid'>1500, <a href='https://arxiv.org/pdf/2312.10993.pdf' target='_blank'>https://arxiv.org/pdf/2312.10993.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zeping Ren, Shaoli Huang, Xiu Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.10993">Realistic Human Motion Generation with Cross-Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce the Cross Human Motion Diffusion Model (CrossDiff), a novel approach for generating high-quality human motion based on textual descriptions. Our method integrates 3D and 2D information using a shared transformer network within the training of the diffusion model, unifying motion noise into a single feature space. This enables cross-decoding of features into both 3D and 2D motion representations, regardless of their original dimension. The primary advantage of CrossDiff is its cross-diffusion mechanism, which allows the model to reverse either 2D or 3D noise into clean motion during training. This capability leverages the complementary information in both motion representations, capturing intricate human movement details often missed by models relying solely on 3D information. Consequently, CrossDiff effectively combines the strengths of both representations to generate more realistic motion sequences. In our experiments, our model demonstrates competitive state-of-the-art performance on text-to-motion benchmarks. Moreover, our method consistently provides enhanced motion generation quality, capturing complex full-body movement intricacies. Additionally, with a pretrained model,our approach accommodates using in the wild 2D motion data without 3D motion ground truth during training to generate 3D motion, highlighting its potential for broader applications and efficient use of available data resources. Project page: https://wonderno.github.io/CrossDiff-webpage/.
<div id='section'>Paperid: <span id='pid'>1501, <a href='https://arxiv.org/pdf/2312.10155.pdf' target='_blank'>https://arxiv.org/pdf/2312.10155.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Feng Han, Jingang Yi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.10155">Gaussian Process-Based Learning Control of Underactuated Balance Robots with an External and Internal Convertible Modeling Structure</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>External and internal convertible (EIC) form-based motion control is one of the effective designs of simultaneously trajectory tracking and balance for underactuated balance robots. Under certain conditions, the EIC-based control design however leads to uncontrolled robot motion. We present a Gaussian process (GP)-based data-driven learning control for underactuated balance robots with the EIC modeling structure. Two GP-based learning controllers are presented by using the EIC structure property. The partial EIC (PEIC)-based control design partitions the robotic dynamics into a fully actuated subsystem and one reduced-order underactuated system. The null-space EIC (NEIC)-based control compensates for the uncontrolled motion in a subspace, while the other closed-loop dynamics are not affected. Under the PEIC- and NEIC-based, the tracking and balance tasks are guaranteed and convergence rate and bounded errors are achieved without causing any uncontrolled motion by the original EIC-based control. We validate the results and demonstrate the GP-based learning control design performance using two inverted pendulum platforms.
<div id='section'>Paperid: <span id='pid'>1502, <a href='https://arxiv.org/pdf/2312.07531.pdf' target='_blank'>https://arxiv.org/pdf/2312.07531.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Soyong Shin, Juyong Kim, Eni Halilaj, Michael J. Black
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.07531">WHAM: Reconstructing World-grounded Humans with Accurate 3D Motion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The estimation of 3D human motion from video has progressed rapidly but current methods still have several key limitations. First, most methods estimate the human in camera coordinates. Second, prior work on estimating humans in global coordinates often assumes a flat ground plane and produces foot sliding. Third, the most accurate methods rely on computationally expensive optimization pipelines, limiting their use to offline applications. Finally, existing video-based methods are surprisingly less accurate than single-frame methods. We address these limitations with WHAM (World-grounded Humans with Accurate Motion), which accurately and efficiently reconstructs 3D human motion in a global coordinate system from video. WHAM learns to lift 2D keypoint sequences to 3D using motion capture data and fuses this with video features, integrating motion context and visual information. WHAM exploits camera angular velocity estimated from a SLAM method together with human motion to estimate the body's global trajectory. We combine this with a contact-aware trajectory refinement method that lets WHAM capture human motion in diverse conditions, such as climbing stairs. WHAM outperforms all existing 3D human motion recovery methods across multiple in-the-wild benchmarks. Code will be available for research purposes at http://wham.is.tue.mpg.de/
<div id='section'>Paperid: <span id='pid'>1503, <a href='https://arxiv.org/pdf/2311.17587.pdf' target='_blank'>https://arxiv.org/pdf/2311.17587.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Armin Ghanbarzadeh, Esmaeil Najafi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.17587">Deep Reinforcement Learning Graphs: Feedback Motion Planning via Neural Lyapunov Verification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in model-free deep reinforcement learning have enabled efficient agent training. However, challenges arise when determining the region of attraction for these controllers, especially if the region does not fully cover the desired area. This paper addresses this issue by introducing a feedback motion control algorithm that utilizes data-driven techniques and neural networks. The algorithm constructs a graph of connected reinforcement-learning based controllers, each with its own defined region of attraction. This incremental approach effectively covers a bounded region of interest, creating a trajectory of interconnected nodes that guide the system from an initial state to the goal. Two approaches are presented for connecting nodes within the algorithm. The first is a tree-structured method, facilitating "point-to-point" control by constructing a tree connecting the initial state to the goal state. The second is a graph-structured method, enabling "space-to-space" control by building a graph within a bounded region. This approach allows for control from arbitrary initial and goal states. The proposed method's performance is evaluated on a first-order dynamic system, considering scenarios both with and without obstacles. The results demonstrate the effectiveness of the proposed algorithm in achieving the desired control objectives.
<div id='section'>Paperid: <span id='pid'>1504, <a href='https://arxiv.org/pdf/2311.16471.pdf' target='_blank'>https://arxiv.org/pdf/2311.16471.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zixiang Zhou, Yu Wan, Baoyuan Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.16471">A Unified Framework for Multimodal, Multi-Part Human Motion Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The field has made significant progress in synthesizing realistic human motion driven by various modalities. Yet, the need for different methods to animate various body parts according to different control signals limits the scalability of these techniques in practical scenarios. In this paper, we introduce a cohesive and scalable approach that consolidates multimodal (text, music, speech) and multi-part (hand, torso) human motion generation. Our methodology unfolds in several steps: We begin by quantizing the motions of diverse body parts into separate codebooks tailored to their respective domains. Next, we harness the robust capabilities of pre-trained models to transcode multimodal signals into a shared latent space. We then translate these signals into discrete motion tokens by iteratively predicting subsequent tokens to form a complete sequence. Finally, we reconstruct the continuous actual motion from this tokenized sequence. Our method frames the multimodal motion generation challenge as a token prediction task, drawing from specialized codebooks based on the modality of the control signal. This approach is inherently scalable, allowing for the easy integration of new modalities. Extensive experiments demonstrated the effectiveness of our design, emphasizing its potential for broad application.
<div id='section'>Paperid: <span id='pid'>1505, <a href='https://arxiv.org/pdf/2311.16468.pdf' target='_blank'>https://arxiv.org/pdf/2311.16468.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zixiang Zhou, Yu Wan, Baoyuan Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.16468">AvatarGPT: All-in-One Framework for Motion Understanding, Planning, Generation and Beyond</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models(LLMs) have shown remarkable emergent abilities in unifying almost all (if not every) NLP tasks. In the human motion-related realm, however, researchers still develop siloed models for each task. Inspired by InstuctGPT, and the generalist concept behind Gato, we introduce AvatarGPT, an All-in-One framework for motion understanding, planning, generations as well as other tasks such as motion in-between synthesis. AvatarGPT treats each task as one type of instruction fine-tuned on the shared LLM. All the tasks are seamlessly interconnected with language as the universal interface, constituting a closed-loop within the framework. To achieve this, human motion sequences are first encoded as discrete tokens, which serve as the extended vocabulary of LLM. Then, an unsupervised pipeline to generate natural language descriptions of human action sequences from in-the-wild videos is developed. Finally, all tasks are jointly trained. Extensive experiments show that AvatarGPT achieves SOTA on low-level tasks, and promising results on high-level tasks, demonstrating the effectiveness of our proposed All-in-One framework. Moreover, for the first time, AvatarGPT enables a principled approach by iterative traversal of the tasks within the closed-loop for unlimited long-motion synthesis.
<div id='section'>Paperid: <span id='pid'>1506, <a href='https://arxiv.org/pdf/2311.10834.pdf' target='_blank'>https://arxiv.org/pdf/2311.10834.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pere GirÃ³, Enric Celaya, LluÃ­s Ros
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.10834">The Otbot project: Dynamic modelling, parameter identification, and motion control of an omnidirectional tire-wheeled robot</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, autonomous mobile platforms are finding an increasing range of applications in inspection or surveillance tasks, or to the transport of objects, in places such as smart warehouses, factories or hospitals. In these environments it is useful for the robot to have omnidirectional capability in the plane, so it can navigate through narrow or cluttered areas, or make position and orientation changes without having to maneuver. While this capability is usually achieved with directional sliding wheels, this work studies a particular robot that achieves omnidirectionality using conventional wheels, which are easier to manufacture and maintain, and support larger loads in general. This robot, which we call ``Otbot'' (for omnidirectional tire-wheeled robot), was already conceived in the late 1990s, but all the controllers that have been proposed for it are based on purely kinematic models so far. These controllers may be sufficient if the robot is light, or if its motors are powerful, but on platforms that have to carry large loads, or that have more limited motors, it is necessary to resort to control laws based on dynamic models if the full acceleration capacities are to be exploited. This work develops a dynamic model of Otbot, proposes a plausible methodology to identify its parameters, and designs a control law that, using this model, is able to track prescribed trajectories in an accurate and robust manner.
<div id='section'>Paperid: <span id='pid'>1507, <a href='https://arxiv.org/pdf/2310.12733.pdf' target='_blank'>https://arxiv.org/pdf/2310.12733.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiming Wang, Qian Huang, Bin Tang, Huashan Sun, Xing Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.12733">Multiscale Motion-Aware and Spatial-Temporal-Channel Contextual Coding Network for Learned Video Compression</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, learned video compression has achieved exciting performance. Following the traditional hybrid prediction coding framework, most learned methods generally adopt the motion estimation motion compensation (MEMC) method to remove inter-frame redundancy. However, inaccurate motion vector (MV) usually lead to the distortion of reconstructed frame. In addition, most approaches ignore the spatial and channel redundancy. To solve above problems, we propose a motion-aware and spatial-temporal-channel contextual coding based video compression network (MASTC-VC), which learns the latent representation and uses variational autoencoders (VAEs) to capture the characteristics of intra-frame pixels and inter-frame motion. Specifically, we design a multiscale motion-aware module (MS-MAM) to estimate spatial-temporal-channel consistent motion vector by utilizing the multiscale motion prediction information in a coarse-to-fine way. On the top of it, we further propose a spatial-temporal-channel contextual module (STCCM), which explores the correlation of latent representation to reduce the bit consumption from spatial, temporal and channel aspects respectively. Comprehensive experiments show that our proposed MASTC-VC is surprior to previous state-of-the-art (SOTA) methods on three public benchmark datasets. More specifically, our method brings average 10.15\% BD-rate savings against H.265/HEVC (HM-16.20) in PSNR metric and average 23.93\% BD-rate savings against H.266/VVC (VTM-13.2) in MS-SSIM metric.
<div id='section'>Paperid: <span id='pid'>1508, <a href='https://arxiv.org/pdf/2309.15784.pdf' target='_blank'>https://arxiv.org/pdf/2309.15784.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Feng Han, Jingang Yi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.15784">Gaussian Process-Enhanced, External and Internal Convertible (EIC) Form-Based Control of Underactuated Balance Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>External and internal convertible (EIC) form-based motion control (i.e., EIC-based control) is one of the effective approaches for underactuated balance robots. By sequentially controller design, trajectory tracking of the actuated subsystem and balance of the unactuated subsystem can be achieved simultaneously. However, with certain conditions, there exists uncontrolled robot motion under the EIC-based control. We first identify these conditions and then propose an enhanced EIC-based control with a Gaussian process data-driven robot dynamic model. Under the new enhanced EIC-based control, the stability and performance of the closed-loop system is guaranteed. We demonstrate the GP-enhanced EIC-based control experimentally using two examples of underactuated balance robots.
<div id='section'>Paperid: <span id='pid'>1509, <a href='https://arxiv.org/pdf/2309.04418.pdf' target='_blank'>https://arxiv.org/pdf/2309.04418.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sergio MartÃ­n Serrano, David FernÃ¡ndez Llorca, IvÃ¡n GarcÃ­a Daza, Miguel Ãngel Sotelo VÃ¡zquez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.04418">Realistic pedestrian behaviour in the CARLA simulator using VR and mocap</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Simulations are gaining increasingly significance in the field of autonomous driving due to the demand for rapid prototyping and extensive testing. Employing physics-based simulation brings several benefits at an affordable cost, while mitigating potential risks to prototypes, drivers, and vulnerable road users. However, there exit two primary limitations. Firstly, the reality gap which refers to the disparity between reality and simulation and prevents the simulated autonomous driving systems from having the same performance in the real world. Secondly, the lack of empirical understanding regarding the behavior of real agents, such as backup drivers or passengers, as well as other road users such as vehicles, pedestrians, or cyclists. Agent simulation is commonly implemented through deterministic or randomized probabilistic pre-programmed models, or generated from real-world data; but it fails to accurately represent the behaviors adopted by real agents while interacting within a specific simulated scenario. This paper extends the description of our proposed framework to enable real-time interaction between real agents and simulated environments, by means immersive virtual reality and human motion capture systems within the CARLA simulator for autonomous driving. We have designed a set of usability examples that allow the analysis of the interactions between real pedestrians and simulated autonomous vehicles and we provide a first measure of the user's sensation of presence in the virtual environment.
<div id='section'>Paperid: <span id='pid'>1510, <a href='https://arxiv.org/pdf/2308.16682.pdf' target='_blank'>https://arxiv.org/pdf/2308.16682.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tom Van Wouwe, Seunghwan Lee, Antoine Falisse, Scott Delp, C. Karen Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.16682">DiffusionPoser: Real-time Human Motion Reconstruction From Arbitrary Sparse Sensors Using Autoregressive Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion capture from a limited number of body-worn sensors, such as inertial measurement units (IMUs) and pressure insoles, has important applications in health, human performance, and entertainment. Recent work has focused on accurately reconstructing whole-body motion from a specific sensor configuration using six IMUs. While a common goal across applications is to use the minimal number of sensors to achieve required accuracy, the optimal arrangement of the sensors might differ from application to application. We propose a single diffusion model, DiffusionPoser, which reconstructs human motion in real-time from an arbitrary combination of sensors, including IMUs placed at specified locations, and, pressure insoles. Unlike existing methods, our model grants users the flexibility to determine the number and arrangement of sensors tailored to the specific activity of interest, without the need for retraining. A novel autoregressive inferencing scheme ensures real-time motion reconstruction that closely aligns with measured sensor signals. The generative nature of DiffusionPoser ensures realistic behavior, even for degrees-of-freedom not directly measured. Qualitative results can be found on our website: https://diffusionposer.github.io/.
<div id='section'>Paperid: <span id='pid'>1511, <a href='https://arxiv.org/pdf/2308.04303.pdf' target='_blank'>https://arxiv.org/pdf/2308.04303.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rabbia Asghar, Manuel Diaz-Zapata, Lukas Rummelhard, Anne Spalanzani, Christian Laugier
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.04303">Vehicle Motion Forecasting using Prior Information and Semantic-assisted Occupancy Grid Maps</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion prediction is a challenging task for autonomous vehicles due to uncertainty in the sensor data, the non-deterministic nature of future, and complex behavior of agents. In this paper, we tackle this problem by representing the scene as dynamic occupancy grid maps (DOGMs), associating semantic labels to the occupied cells and incorporating map information. We propose a novel framework that combines deep-learning-based spatio-temporal and probabilistic approaches to predict vehicle behaviors.Contrary to the conventional OGM prediction methods, evaluation of our work is conducted against the ground truth annotations. We experiment and validate our results on real-world NuScenes dataset and show that our model shows superior ability to predict both static and dynamic vehicles compared to OGM predictions. Furthermore, we perform an ablation study and assess the role of semantic labels and map in the architecture.
<div id='section'>Paperid: <span id='pid'>1512, <a href='https://arxiv.org/pdf/2307.00019.pdf' target='_blank'>https://arxiv.org/pdf/2307.00019.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Klaus M. Frahm, Dima L. Shepelyansky
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.00019">Goal quest for an intelligent surfer moving in a chaotic flow</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We consider a model of an intelligent surfer moving on the Ulam network generated by a chaotic dynamics in the Chirikov standard map. This directed network is obtained by the Ulam method with a division of the phase space in cells of fixed size forming the nodes of a Markov chain. The goal quest for this surfer is to determine the network path from an initial node A to a final node B with minimal resistance given by the sum of inverse transition probabilities. We develop an algorithm for the intelligent surfer that allows to perform the quest in a small number of transitions which grows only logarithmically with the network size. The optimal path search is done on a fractal intersection set formed by nodes with small ErdÃ¶s numbers of the forward and inverted networks. The intelligent surfer exponentially outperforms a naive surfer who tries to minimize its phase space distance to the target B. We argue that such an algorithm provides new hints for motion control in chaotic flows.
<div id='section'>Paperid: <span id='pid'>1513, <a href='https://arxiv.org/pdf/2306.15852.pdf' target='_blank'>https://arxiv.org/pdf/2306.15852.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Meenakshi Sarkar, Vinayak Honkote, Dibyendu Das, Debasish Ghose
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.15852">Action-conditioned Deep Visual Prediction with RoAM, a new Indoor Human Motion Dataset for Autonomous Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the increasing adoption of robots across industries, it is crucial to focus on developing advanced algorithms that enable robots to anticipate, comprehend, and plan their actions effectively in collaboration with humans. We introduce the Robot Autonomous Motion (RoAM) video dataset, which is collected with a custom-made turtlebot3 Burger robot in a variety of indoor environments recording various human motions from the robot's ego-vision. The dataset also includes synchronized records of the LiDAR scan and all control actions taken by the robot as it navigates around static and moving human agents. The unique dataset provides an opportunity to develop and benchmark new visual prediction frameworks that can predict future image frames based on the action taken by the recording agent in partially observable scenarios or cases where the imaging sensor is mounted on a moving platform. We have benchmarked the dataset on our novel deep visual prediction framework called ACPNet where the approximated future image frames are also conditioned on action taken by the robot and demonstrated its potential for incorporating robot dynamics into the video prediction paradigm for mobile robotics and autonomous navigation research.
<div id='section'>Paperid: <span id='pid'>1514, <a href='https://arxiv.org/pdf/2306.09532.pdf' target='_blank'>https://arxiv.org/pdf/2306.09532.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhaoming Xie, Jonathan Tseng, Sebastian Starke, Michiel van de Panne, C. Karen Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.09532">Hierarchical Planning and Control for Box Loco-Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humans perform everyday tasks using a combination of locomotion and manipulation skills. Building a system that can handle both skills is essential to creating virtual humans. We present a physically-simulated human capable of solving box rearrangement tasks, which requires a combination of both skills. We propose a hierarchical control architecture, where each level solves the task at a different level of abstraction, and the result is a physics-based simulated virtual human capable of rearranging boxes in a cluttered environment. The control architecture integrates a planner, diffusion models, and physics-based motion imitation of sparse motion clips using deep reinforcement learning. Boxes can vary in size, weight, shape, and placement height. Code and trained control policies are provided.
<div id='section'>Paperid: <span id='pid'>1515, <a href='https://arxiv.org/pdf/2306.02705.pdf' target='_blank'>https://arxiv.org/pdf/2306.02705.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nils Mandischer, Frederik Schicks, Burkhard Corves
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.02705">Situational Adaptive Motion Prediction for Firefighting Squads in Indoor Search and Rescue</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Firefighting is a complex, yet low automated task. To mitigate ergonomic and safety related risks on the human operators, robots could be deployed in a collaborative approach. To allow human-robot teams in firefighting, important basics are missing. Amongst other aspects, the robot must predict the human motion as occlusion is ever-present. In this work, we propose a novel motion prediction pipeline for firefighters' squads in indoor search and rescue. The squad paths are generated with an optimal graph-based planning approach representing firefighters' tactics. Paths are generated per room which allows to dynamically adapt the path locally without global re-planning. The motion of singular agents is simulated using a modification of the headed social force model. We evaluate the pipeline for feasibility with a novel data set generated from real footage and show the computational efficiency.
<div id='section'>Paperid: <span id='pid'>1516, <a href='https://arxiv.org/pdf/2305.13773.pdf' target='_blank'>https://arxiv.org/pdf/2305.13773.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dong Wei, Xiaoning Sun, Huaijiang Sun, Bin Li, Shengxiang Hu, Weiqing Li, Jianfeng Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.13773">Enhanced Fine-grained Motion Diffusion for Text-driven Human Motion Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The emergence of text-driven motion synthesis technique provides animators with great potential to create efficiently. However, in most cases, textual expressions only contain general and qualitative motion descriptions, while lack fine depiction and sufficient intensity, leading to the synthesized motions that either (a) semantically compliant but uncontrollable over specific pose details, or (b) even deviates from the provided descriptions, bringing animators with undesired cases. In this paper, we propose DiffKFC, a conditional diffusion model for text-driven motion synthesis with KeyFrames Collaborated, enabling realistic generation with collaborative and efficient dual-level control: coarse guidance at semantic level, with only few keyframes for direct and fine-grained depiction down to body posture level. Unlike existing inference-editing diffusion models that incorporate conditions without training, our conditional diffusion model is explicitly trained and can fully exploit correlations among texts, keyframes and the diffused target frames. To preserve the control capability of discrete and sparse keyframes, we customize dilated mask attention modules where only partial valid tokens participate in local-to-global attention, indicated by the dilated keyframe mask. Additionally, we develop a simple yet effective smoothness prior, which steers the generated frames towards seamless keyframe transitions at inference. Extensive experiments show that our model not only achieves state-of-the-art performance in terms of semantic fidelity, but more importantly, is able to satisfy animator requirements through fine-grained guidance without tedious labor.
<div id='section'>Paperid: <span id='pid'>1517, <a href='https://arxiv.org/pdf/2305.09662.pdf' target='_blank'>https://arxiv.org/pdf/2305.09662.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Samaneh Azadi, Akbar Shah, Thomas Hayes, Devi Parikh, Sonal Gupta
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.09662">Make-An-Animation: Large-Scale Text-conditional 3D Human Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-guided human motion generation has drawn significant interest because of its impactful applications spanning animation and robotics. Recently, application of diffusion models for motion generation has enabled improvements in the quality of generated motions. However, existing approaches are limited by their reliance on relatively small-scale motion capture data, leading to poor performance on more diverse, in-the-wild prompts. In this paper, we introduce Make-An-Animation, a text-conditioned human motion generation model which learns more diverse poses and prompts from large-scale image-text datasets, enabling significant improvement in performance over prior works. Make-An-Animation is trained in two stages. First, we train on a curated large-scale dataset of (text, static pseudo-pose) pairs extracted from image-text datasets. Second, we fine-tune on motion capture data, adding additional layers to model the temporal dimension. Unlike prior diffusion models for motion generation, Make-An-Animation uses a U-Net architecture similar to recent text-to-video generation models. Human evaluation of motion realism and alignment with input text shows that our model reaches state-of-the-art performance on text-to-motion generation.
<div id='section'>Paperid: <span id='pid'>1518, <a href='https://arxiv.org/pdf/2305.09156.pdf' target='_blank'>https://arxiv.org/pdf/2305.09156.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zitang Sun, Yen-Ju Chen, Yung-hao Yang, Shin'ya Nishida
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.09156">Modelling Human Visual Motion Processing with Trainable Motion Energy Sensing and a Self-attention Network</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual motion processing is essential for humans to perceive and interact with dynamic environments. Despite extensive research in cognitive neuroscience, image-computable models that can extract informative motion flow from natural scenes in a manner consistent with human visual processing have yet to be established. Meanwhile, recent advancements in computer vision (CV), propelled by deep learning, have led to significant progress in optical flow estimation, a task closely related to motion perception. Here we propose an image-computable model of human motion perception by bridging the gap between biological and CV models. Specifically, we introduce a novel two-stages approach that combines trainable motion energy sensing with a recurrent self-attention network for adaptive motion integration and segregation. This model architecture aims to capture the computations in V1-MT, the core structure for motion perception in the biological visual system, while providing the ability to derive informative motion flow for a wide range of stimuli, including complex natural scenes. In silico neurophysiology reveals that our model's unit responses are similar to mammalian neural recordings regarding motion pooling and speed tuning. The proposed model can also replicate human responses to a range of stimuli examined in past psychophysical studies. The experimental results on the Sintel benchmark demonstrate that our model predicts human responses better than the ground truth, whereas the state-of-the-art CV models show the opposite. Our study provides a computational architecture consistent with human visual motion processing, although the physiological correspondence may not be exact.
<div id='section'>Paperid: <span id='pid'>1519, <a href='https://arxiv.org/pdf/2305.05375.pdf' target='_blank'>https://arxiv.org/pdf/2305.05375.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingyue Liu, Pablo Borja, Cosimo Della Santina
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.05375">Physics-informed Neural Networks to Model and Control Robots: a Theoretical and Experimental Investigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work concerns the application of physics-informed neural networks to the modeling and control of complex robotic systems. Achieving this goal required extending Physics Informed Neural Networks to handle non-conservative effects. We propose to combine these learned models with model-based controllers originally developed with first-principle models in mind. By combining standard and new techniques, we can achieve precise control performance while proving theoretical stability bounds. These validations include real-world experiments of motion prediction with a soft robot and of trajectory tracking with a Franka Emika manipulator.
<div id='section'>Paperid: <span id='pid'>1520, <a href='https://arxiv.org/pdf/2304.07494.pdf' target='_blank'>https://arxiv.org/pdf/2304.07494.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yibo Zhou, Dongfei Cui, Xiangming Dong, Zongkai Wu, Zhenyu Wei, Donglin Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.07494">BVIP Guiding System with Adaptability to Individual Differences</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Guiding robots can not only detect close-range obstacles like other guiding tools, but also extend its range to perceive the environment when making decisions. However, most existing works over-simplified the interaction between human agents and robots, ignoring the differences between individuals, resulting in poor experiences for different users. To solve the problem, we propose a data-driven guiding system to cope with the effect brighten by individual differences. In our guiding system, we design a Human Motion Predictor (HMP) and a Robot Dynamics Model (RDM) based on deep neural network, the time convolutional network (TCN) is verified to have the best performance, to predict differences in interaction between different human agents and robots. To train our models, we collected datasets that records the interactions from different human agents. Moreover, given the predictive information of the specific user, we propose a waypoints selector that allows the robot to naturally adapt to the user's state changes, which are mainly reflected in the walking speed. We compare the performance of our models with previous works and achieve significant performance improvements. On this basis, our guiding system demonstrated good adaptability to different human agents. Our guiding system is deployed on a real quadruped robot to verify the practicability.
<div id='section'>Paperid: <span id='pid'>1521, <a href='https://arxiv.org/pdf/2304.07410.pdf' target='_blank'>https://arxiv.org/pdf/2304.07410.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Samaneh Azadi, Thomas Hayes, Akbar Shah, Guan Pang, Devi Parikh, Sonal Gupta
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.07410">Text-Conditional Contextualized Avatars For Zero-Shot Personalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent large-scale text-to-image generation models have made significant improvements in the quality, realism, and diversity of the synthesized images and enable users to control the created content through language. However, the personalization aspect of these generative models is still challenging and under-explored. In this work, we propose a pipeline that enables personalization of image generation with avatars capturing a user's identity in a delightful way. Our pipeline is zero-shot, avatar texture and style agnostic, and does not require training on the avatar at all - it is scalable to millions of users who can generate a scene with their avatar. To render the avatar in a pose faithful to the given text prompt, we propose a novel text-to-3D pose diffusion model trained on a curated large-scale dataset of in-the-wild human poses improving the performance of the SOTA text-to-motion models significantly. We show, for the first time, how to leverage large-scale image datasets to learn human 3D pose parameters and overcome the limitations of motion capture datasets.
<div id='section'>Paperid: <span id='pid'>1522, <a href='https://arxiv.org/pdf/2304.04496.pdf' target='_blank'>https://arxiv.org/pdf/2304.04496.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoning Sun, Huaijiang Sun, Bin Li, Dong Wei, Weiqing Li, Jianfeng Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.04496">DeFeeNet: Consecutive 3D Human Motion Prediction with Deviation Feedback</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Let us rethink the real-world scenarios that require human motion prediction techniques, such as human-robot collaboration. Current works simplify the task of predicting human motions into a one-off process of forecasting a short future sequence (usually no longer than 1 second) based on a historical observed one. However, such simplification may fail to meet practical needs due to the neglect of the fact that motion prediction in real applications is not an isolated ``observe then predict'' unit, but a consecutive process composed of many rounds of such unit, semi-overlapped along the entire sequence. As time goes on, the predicted part of previous round has its corresponding ground truth observable in the new round, but their deviation in-between is neither exploited nor able to be captured by existing isolated learning fashion. In this paper, we propose DeFeeNet, a simple yet effective network that can be added on existing one-off prediction models to realize deviation perception and feedback when applied to consecutive motion prediction task. At each prediction round, the deviation generated by previous unit is first encoded by our DeFeeNet, and then incorporated into the existing predictor to enable a deviation-aware prediction manner, which, for the first time, allows for information transmit across adjacent prediction units. We design two versions of DeFeeNet as MLP-based and GRU-based, respectively. On Human3.6M and more complicated BABEL, experimental results indicate that our proposed network improves consecutive human motion prediction performance regardless of the basic model.
<div id='section'>Paperid: <span id='pid'>1523, <a href='https://arxiv.org/pdf/2303.15869.pdf' target='_blank'>https://arxiv.org/pdf/2303.15869.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Albin Dahlin, Yiannis Karayiannidis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.15869">Obstacle Avoidance in Dynamic Environments via Tunnel-following MPC with Adaptive Guiding Vector Fields</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper proposes a motion control scheme for robots operating in a dynamic environment with concave obstacles. A Model Predictive Controller (MPC) is constructed to drive the robot towards a goal position while ensuring collision avoidance without direct use of obstacle information in the optimization problem. This is achieved by guaranteeing tracking performance of an appropriately designed receding horizon path. The path is computed using a guiding vector field defined in a subspace of the free workspace where each point in the subspace satisfies a criteria for minimum distance to all obstacles. The effectiveness of the control scheme is illustrated by means of simulation.
<div id='section'>Paperid: <span id='pid'>1524, <a href='https://arxiv.org/pdf/2302.13578.pdf' target='_blank'>https://arxiv.org/pdf/2302.13578.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fabian Woitschek, Georg Schneider
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.13578">Online Black-Box Confidence Estimation of Deep Neural Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous driving (AD) and advanced driver assistance systems (ADAS) increasingly utilize deep neural networks (DNNs) for improved perception or planning. Nevertheless, DNNs are quite brittle when the data distribution during inference deviates from the data distribution during training. This represents a challenge when deploying in partly unknown environments like in the case of ADAS. At the same time, the standard confidence of DNNs remains high even if the classification reliability decreases. This is problematic since following motion control algorithms consider the apparently confident prediction as reliable even though it might be considerably wrong. To reduce this problem real-time capable confidence estimation is required that better aligns with the actual reliability of the DNN classification. Additionally, the need exists for black-box confidence estimation to enable the homogeneous inclusion of externally developed components to an entire system. In this work we explore this use case and introduce the neighborhood confidence (NHC) which estimates the confidence of an arbitrary DNN for classification. The metric can be used for black-box systems since only the top-1 class output is required and does not need access to the gradients, the training dataset or a hold-out validation dataset. Evaluation on different data distributions, including small in-domain distribution shifts, out-of-domain data or adversarial attacks, shows that the NHC performs better or on par with a comparable method for online white-box confidence estimation in low data regimes which is required for real-time capable AD/ADAS.
<div id='section'>Paperid: <span id='pid'>1525, <a href='https://arxiv.org/pdf/2302.08505.pdf' target='_blank'>https://arxiv.org/pdf/2302.08505.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Renjie Li, Chun Yu Lao, Rebecca St. George, Katherine Lawler, Saurabh Garg, Son N. Tran, Quan Bai, Jane Alty
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.08505">Rapid-Motion-Track: Markerless Tracking of Fast Human Motion with Deeper Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Objective The coordination of human movement directly reflects function of the central nervous system. Small deficits in movement are often the first sign of an underlying neurological problem. The objective of this research is to develop a new end-to-end, deep learning-based system, Rapid-Motion-Track (RMT) that can track the fastest human movement accurately when webcams or laptop cameras are used.
  Materials and Methods We applied RMT to finger tapping, a well-validated test of motor control that is one of the most challenging human motions to track with computer vision due to the small keypoints of digits and the high velocities that are generated. We recorded 160 finger tapping assessments simultaneously with a standard 2D laptop camera (30 frames/sec) and a high-speed wearable sensor-based 3D motion tracking system (250 frames/sec). RMT and a range of DLC models were applied to the video data with tapping frequencies up to 8Hz to extract movement features.
  Results The movement features (e.g. speed, rhythm, variance) identified with the new RMT system exhibited very high concurrent validity with the gold-standard measurements (97.3\% of RMT measures were within +/-0.5Hz of the Optotrak measures), and outperformed DLC and other advanced computer vision tools (around 88.2\% of DLC measures were within +/-0.5Hz of the Optotrak measures). RMT also accurately tracked a range of other rapid human movements such as foot tapping, head turning and sit-to -stand movements.
  Conclusion: With the ubiquity of video technology in smart devices, the RMT method holds potential to transform access and accuracy of human movement assessment.
<div id='section'>Paperid: <span id='pid'>1526, <a href='https://arxiv.org/pdf/2302.00556.pdf' target='_blank'>https://arxiv.org/pdf/2302.00556.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rim Rekik, Mathieu Marsot, Anne-HÃ©lÃ¨ne Olivier, Jean-SÃ©bastien Franco, Stefanie Wuhrer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.00556">Correspondence-free online human motion retargeting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a data-driven framework for unsupervised human motion retargeting that animates a target subject with the motion of a source subject. Our method is correspondence-free, requiring neither spatial correspondences between the source and target shapes nor temporal correspondences between different frames of the source motion. This allows to animate a target shape with arbitrary sequences of humans in motion, possibly captured using 4D acquisition platforms or consumer devices. Our method unifies the advantages of two existing lines of work, namely skeletal motion retargeting, which leverages long-term temporal context, and surface-based retargeting, which preserves surface details, by combining a geometry-aware deformation model with a skeleton-aware motion transfer approach. This allows to take into account long-term temporal context while accounting for surface details. During inference, our method runs online, i.e. input can be processed in a serial way, and retargeting is performed in a single forward pass per frame. Experiments show that including long-term temporal context during training improves the method's accuracy for skeletal motion and detail preservation. Furthermore, our method generalizes to unobserved motions and body shapes. We demonstrate that our method achieves state-of-the-art results on two test datasets and that it can be used to animate human models with the output of a multi-view acquisition platform. Code is available at \url{https://gitlab.inria.fr/rrekikdi/human-motion-retargeting2023}.
<div id='section'>Paperid: <span id='pid'>1527, <a href='https://arxiv.org/pdf/2301.05575.pdf' target='_blank'>https://arxiv.org/pdf/2301.05575.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Carolina GonÃ§alves, JoÃ£o M. Lopes, Sara Moccia, Daniele Berardini, Lucia Migliorelli, Cristina P. Santos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.05575">Deep learning-based approaches for human motion decoding in smart walkers for rehabilitation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Gait disabilities are among the most frequent worldwide. Their treatment relies on rehabilitation therapies, in which smart walkers are being introduced to empower the user's recovery and autonomy, while reducing the clinicians effort. For that, these should be able to decode human motion and needs, as early as possible. Current walkers decode motion intention using information of wearable or embedded sensors, namely inertial units, force and hall sensors, and lasers, whose main limitations imply an expensive solution or hinder the perception of human movement. Smart walkers commonly lack a seamless human-robot interaction, which intuitively understands human motions. A contactless approach is proposed in this work, addressing human motion decoding as an early action recognition/detection problematic, using RGB-D cameras. We studied different deep learning-based algorithms, organised in three different approaches, to process lower body RGB-D video sequences, recorded from an embedded camera of a smart walker, and classify them into 4 classes (stop, walk, turn right/left). A custom dataset involving 15 healthy participants walking with the device was acquired and prepared, resulting in 28800 balanced RGB-D frames, to train and evaluate the deep networks. The best results were attained by a convolutional neural network with a channel attention mechanism, reaching accuracy values of 99.61% and above 93%, for offline early detection/recognition and trial simulations, respectively. Following the hypothesis that human lower body features encode prominent information, fostering a more robust prediction towards real-time applications, the algorithm focus was also evaluated using Dice metric, leading to values slightly higher than 30%. Promising results were attained for early action detection as a human motion decoding strategy, with enhancements in the focus of the proposed architectures.
<div id='section'>Paperid: <span id='pid'>1528, <a href='https://arxiv.org/pdf/2210.16068.pdf' target='_blank'>https://arxiv.org/pdf/2210.16068.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Samaneh Manavi Roodsari, Antal Huck-Horvath, Sara Freund, Azhar Zam, Georg Rauter, Wolfgang Schade, Philippe C. Cattin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.16068">Using Supervised Deep-Learning to Model Edge-FBG Shape Sensors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Continuum robots in robot-assisted minimally invasive surgeries provide adequate access to target anatomies that are not directly reachable through small incisions. Achieving precise and reliable motion control of such snake-like manipulators necessitates an accurate navigation system that requires no line-of-sight and is immune to electromagnetic noises. Fiber Bragg Grating (FBG) shape sensors, particularly edge-FBGs, are promising tools for this task. However, in edge-FBG sensors, the intensity ratio between Bragg wavelengths carries the strain information that can be affected by undesired bending-related phenomena, making standard characterization techniques less suitable for these sensors. We showed in our previous work that a deep learning model has the potential to extract the strain information from the full edge-FBG spectrum and accurately predict the sensor's shape. In this paper, we conduct a more thorough investigation to find a suitable architectural design with lower prediction errors. We use the Hyperband algorithm to search for optimal hyperparameters in two steps. First, we limit the search space to layer settings, where the best-performing configuration gets selected. Then, we modify the search space for tuning the training and loss calculation hyperparameters. We also analyze various data transformations on the input and output variables, as data rescaling can directly influence the model's performance. Moreover, we performed discriminative training using Siamese network architecture that employs two CNNs with identical parameters to learn similarity metrics between the spectra of similar target values. The best-performing network architecture among all evaluated configurations can predict the sensor's shape with a median tip error of 3.11 mm.
<div id='section'>Paperid: <span id='pid'>1529, <a href='https://arxiv.org/pdf/2210.12035.pdf' target='_blank'>https://arxiv.org/pdf/2210.12035.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>JoÃ£o Carmona, TamÃ¡s KarÃ¡csony, JoÃ£o Paulo Silva Cunha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.12035">BlanketGen - A synthetic blanket occlusion augmentation pipeline for MoCap datasets</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion analysis has seen drastic improvements recently, however, due to the lack of representative datasets, for clinical in-bed scenarios it is still lagging behind. To address this issue, we implemented BlanketGen, a pipeline that augments videos with synthetic blanket occlusions. With this pipeline, we generated an augmented version of the pose estimation dataset 3DPW called BlanketGen-3DPW. We then used this new dataset to fine-tune a Deep Learning model to improve its performance in these scenarios with promising results. Code and further information are available at https://gitlab.inesctec.pt/brain-lab/brain-lab-public/blanket-gen-releases.
<div id='section'>Paperid: <span id='pid'>1530, <a href='https://arxiv.org/pdf/2210.03600.pdf' target='_blank'>https://arxiv.org/pdf/2210.03600.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>JoÃ£o Carmona, TamÃ¡s KarÃ¡csony, JoÃ£o Paulo Silva Cunha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.03600">BlanketSet -- A clinical real-world in-bed action recognition and qualitative semi-synchronised MoCap dataset</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Clinical in-bed video-based human motion analysis is a very relevant computer vision topic for several relevant biomedical applications. Nevertheless, the main public large datasets (e.g. ImageNet or 3DPW) used for deep learning approaches lack annotated examples for these clinical scenarios. To address this issue, we introduce BlanketSet, an RGB-IR-D action recognition dataset of sequences performed in a hospital bed. This dataset has the potential to help bridge the improvements attained in more general large datasets to these clinical scenarios. Information on how to access the dataset is available at https://rdm.inesctec.pt/dataset/nis-2022-004.
<div id='section'>Paperid: <span id='pid'>1531, <a href='https://arxiv.org/pdf/2205.10236.pdf' target='_blank'>https://arxiv.org/pdf/2205.10236.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zenan Zhu, Seyed Mostafa Rezayat Sorkhabadi, Yan Gu, Wenlong Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2205.10236">Invariant Extended Kalman Filtering for Human Motion Estimation with Imperfect Sensor Placement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces a new invariant extended Kalman filter design that produces real-time state estimates and rapid error convergence for the estimation of the human body movement even in the presence of sensor misalignment and initial state estimation errors. The filter fuses the data returned by an inertial measurement unit (IMU) attached to the body (e.g., pelvis or chest) and a virtual measurement of zero stance-foot velocity (i.e., leg odometry). The key novelty of the proposed filter lies in that its process model meets the group affine property while the filter explicitly addresses the IMU placement error by formulating its stochastic process model as Brownian motions and incorporating the error in the leg odometry. Although the measurement model is imperfect (i.e., it does not possess an invariant observation form) and thus its linearization relies on the state estimate, experimental results demonstrate fast convergence of the proposed filter (within 0.2 seconds) during squatting motions even under significant IMU placement inaccuracy and initial estimation errors.
<div id='section'>Paperid: <span id='pid'>1532, <a href='https://arxiv.org/pdf/2205.07800.pdf' target='_blank'>https://arxiv.org/pdf/2205.07800.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zenan Zhu, Seyed Mostafa Rezayat Sorkhabadi, Yan Gu, Wenlong Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2205.07800">Design and Evaluation of an Invariant Extended Kalman Filter for Trunk Motion Estimation with Sensor Misalignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding human motion is of critical importance for health monitoring and control of assistive robots, yet many human kinematic variables cannot be directly or accurately measured by wearable sensors. In recent years, invariant extended Kalman filtering (InEKF) has shown a great potential in nonlinear state estimation, but its applications to human poses new challenges, including imperfect placement of wearable sensors and inaccurate measurement models. To address these challenges, this paper proposes an augmented InEKF design which considers the misalignment of the inertial sensor at the trunk as part of the states and preserves the group affine property for the process model. Personalized lower-extremity forward kinematic models are built and employed as the measurement model for the augmented InEKF. Observability analysis for the new InEKF design is presented. The filter is evaluated with three subjects in squatting, rolling-foot walking, and ladder-climbing motions. Experimental results validate the superior performance of the proposed InEKF over the state-of-the-art InEKF. Improved accuracy and faster convergence in estimating the velocity and orientation of human, in all three motions, are achieved despite the significant initial estimation errors and the uncertainties associated with the forward kinematic measurement model.
<div id='section'>Paperid: <span id='pid'>1533, <a href='https://arxiv.org/pdf/2510.04233.pdf' target='_blank'>https://arxiv.org/pdf/2510.04233.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kai Yang, Yuqi Huang, Junheng Tao, Wanyu Wang, Qitian Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04233">Physics-Inspired All-Pair Interaction Learning for 3D Dynamics Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modeling 3D dynamics is a fundamental problem in multi-body systems across scientific and engineering domains and has important practical implications in trajectory prediction and simulation. While recent GNN-based approaches have achieved strong performance by enforcing geometric symmetries, encoding high-order features or incorporating neural-ODE mechanics, they typically depend on explicitly observed structures and inherently fail to capture the unobserved interactions that are crucial to complex physical behaviors and dynamics mechanism. In this paper, we propose PAINET, a principled SE(3)-equivariant neural architecture for learning all-pair interactions in multi-body systems. The model comprises: (1) a novel physics-inspired attention network derived from the minimization trajectory of an energy function, and (2) a parallel decoder that preserves equivariance while enabling efficient inference. Empirical results on diverse real-world benchmarks, including human motion capture, molecular dynamics, and large-scale protein simulations, show that PAINET consistently outperforms recently proposed models, yielding 4.7% to 41.5% error reductions in 3D dynamics prediction with comparable computation costs in terms of time and memory.
<div id='section'>Paperid: <span id='pid'>1534, <a href='https://arxiv.org/pdf/2510.02526.pdf' target='_blank'>https://arxiv.org/pdf/2510.02526.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anamika J H, Anujith Muraleedharan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.02526">U-LAG: Uncertainty-Aware, Lag-Adaptive Goal Retargeting for Robotic Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robots manipulating in changing environments must act on percepts that are late, noisy, or stale. We present U-LAG, a mid-execution goal-retargeting layer that leaves the low-level controller unchanged while re-aiming task goals (pre-contact, contact, post) as new observations arrive. Unlike motion retargeting or generic visual servoing, U-LAG treats in-flight goal re-aiming as a first-class, pluggable module between perception and control. Our main technical contribution is UAR-PF, an uncertainty-aware retargeter that maintains a distribution over object pose under sensing lag and selects goals that maximize expected progress. We instantiate a reproducible Shift x Lag stress test in PyBullet/PandaGym for pick, push, stacking, and peg insertion, where the object undergoes abrupt in-plane shifts while synthetic perception lag is injected during approach. Across 0-10 cm shifts and 0-400 ms lags, UAR-PF and ICP degrade gracefully relative to a no-retarget baseline, achieving higher success with modest end-effector travel and fewer aborts; simple operational safeguards further improve stability. Contributions: (1) UAR-PF for lag-adaptive, uncertainty-aware goal retargeting; (2) a pluggable retargeting interface; and (3) a reproducible Shift x Lag benchmark with evaluation on pick, push, stacking, and peg insertion.
<div id='section'>Paperid: <span id='pid'>1535, <a href='https://arxiv.org/pdf/2509.25443.pdf' target='_blank'>https://arxiv.org/pdf/2509.25443.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zewen He, Chenyuan Chen, Dilshod Azizov, Yoshihiko Nakamura
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25443">CoTaP: Compliant Task Pipeline and Reinforcement Learning of Its Controller with Compliance Modulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humanoid whole-body locomotion control is a critical approach for humanoid robots to leverage their inherent advantages. Learning-based control methods derived from retargeted human motion data provide an effective means of addressing this issue. However, because most current human datasets lack measured force data, and learning-based robot control is largely position-based, achieving appropriate compliance during interaction with real environments remains challenging. This paper presents Compliant Task Pipeline (CoTaP): a pipeline that leverages compliance information in the learning-based structure of humanoid robots. A two-stage dual-agent reinforcement learning framework combined with model-based compliance control for humanoid robots is proposed. In the training process, first a base policy with a position-based controller is trained; then in the distillation, the upper-body policy is combined with model-based compliance control, and the lower-body agent is guided by the base policy. In the upper-body control, adjustable task-space compliance can be specified and integrated with other controllers through compliance modulation on the symmetric positive definite (SPD) manifold, ensuring system stability. We validated the feasibility of the proposed strategy in simulation, primarily comparing the responses to external disturbances under different compliance settings.
<div id='section'>Paperid: <span id='pid'>1536, <a href='https://arxiv.org/pdf/2509.08534.pdf' target='_blank'>https://arxiv.org/pdf/2509.08534.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shubham Singh, Anoop Jain
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.08534">Phase-Coordinated Multi-Agent Circular Formation Control with Non-Concentric Boundary Constraints</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper addresses the problem of collective circular motion control for unicycle agents, with the objective of achieving phase coordination of their velocity vectors while ensuring that their trajectories remain confined within a prescribed non-concentric circular boundary. To accommodate such nonuniform motion constraints, we build upon our earlier work and extend the use of Mobius transformation to a multi-agent framework. The Mobius transformation maps two nonconcentric circles to concentric ones, thereby converting spatially nonuniform constraints into uniform ones in the transformed plane. Leveraging this property, we introduce the notion of a phase-shifted order parameter, along with the associated concepts of Mobius phase-shift coupled synchronization and balancing, which characterize the phase-coordinated patterns studied in this paper. We establish an equivalence between the unicycle dynamics in the original and transformed planes under the Mobius transformation and its inverse, and show that synchronization is preserved across both planes, whereas balancing is generally not. Distributed control laws are then designed in the transformed plane using barrier Lyapunov functions, under the assumption of an undirected and connected communication topology among agents. These controllers are subsequently mapped back to the original plane to obtain the linear acceleration and turn-rate control inputs applied to the actual agents. Both simulations and experimental results are provided to illustrate the proposed framework.
<div id='section'>Paperid: <span id='pid'>1537, <a href='https://arxiv.org/pdf/2509.04600.pdf' target='_blank'>https://arxiv.org/pdf/2509.04600.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qijun Ying, Zhongyuan Hu, Rui Zhang, Ronghui Li, Yu Lu, Zijiao Zeng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.04600">WATCH: World-aware Allied Trajectory and pose reconstruction for Camera and Human</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Global human motion reconstruction from in-the-wild monocular videos is increasingly demanded across VR, graphics, and robotics applications, yet requires accurate mapping of human poses from camera to world coordinates-a task challenged by depth ambiguity, motion ambiguity, and the entanglement between camera and human movements. While human-motion-centric approaches excel in preserving motion details and physical plausibility, they suffer from two critical limitations: insufficient exploitation of camera orientation information and ineffective integration of camera translation cues. We present WATCH (World-aware Allied Trajectory and pose reconstruction for Camera and Human), a unified framework addressing both challenges. Our approach introduces an analytical heading angle decomposition technique that offers superior efficiency and extensibility compared to existing geometric methods. Additionally, we design a camera trajectory integration mechanism inspired by world models, providing an effective pathway for leveraging camera translation information beyond naive hard-decoding approaches. Through experiments on in-the-wild benchmarks, WATCH achieves state-of-the-art performance in end-to-end trajectory reconstruction. Our work demonstrates the effectiveness of jointly modeling camera-human motion relationships and offers new insights for addressing the long-standing challenge of camera translation integration in global human motion reconstruction. The code will be available publicly.
<div id='section'>Paperid: <span id='pid'>1538, <a href='https://arxiv.org/pdf/2508.20734.pdf' target='_blank'>https://arxiv.org/pdf/2508.20734.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Reza Akbari Movahed, Abuzar Rezaee, Arezoo Zakeri, Colin Berry, Edmond S. L. Ho, Ali Gooya
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.20734">CardioMorphNet: Cardiac Motion Prediction Using a Shape-Guided Bayesian Recurrent Deep Network</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate cardiac motion estimation from cine cardiac magnetic resonance (CMR) images is vital for assessing cardiac function and detecting its abnormalities. Existing methods often struggle to capture heart motion accurately because they rely on intensity-based image registration similarity losses that may overlook cardiac anatomical regions. To address this, we propose CardioMorphNet, a recurrent Bayesian deep learning framework for 3D cardiac shape-guided deformable registration using short-axis (SAX) CMR images. It employs a recurrent variational autoencoder to model spatio-temporal dependencies over the cardiac cycle and two posterior models for bi-ventricular segmentation and motion estimation. The derived loss function from the Bayesian formulation guides the framework to focus on anatomical regions by recursively registering segmentation maps without using intensity-based image registration similarity loss, while leveraging sequential SAX volumes and spatio-temporal features. The Bayesian modelling also enables computation of uncertainty maps for the estimated motion fields. Validated on the UK Biobank dataset by comparing warped mask shapes with ground truth masks, CardioMorphNet demonstrates superior performance in cardiac motion estimation, outperforming state-of-the-art methods. Uncertainty assessment shows that it also yields lower uncertainty values for estimated motion fields in the cardiac region compared with other probabilistic-based cardiac registration methods, indicating higher confidence in its predictions.
<div id='section'>Paperid: <span id='pid'>1539, <a href='https://arxiv.org/pdf/2508.18525.pdf' target='_blank'>https://arxiv.org/pdf/2508.18525.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Eleni Tselepi, Spyridon Thermos, Gerasimos Potamianos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.18525">Controllable Single-shot Animation Blending with Temporal Conditioning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Training a generative model on a single human skeletal motion sequence without being bound to a specific kinematic tree has drawn significant attention from the animation community. Unlike text-to-motion generation, single-shot models allow animators to controllably generate variations of existing motion patterns without requiring additional data or extensive retraining. However, existing single-shot methods do not explicitly offer a controllable framework for blending two or more motions within a single generative pass. In this paper, we present the first single-shot motion blending framework that enables seamless blending by temporally conditioning the generation process. Our method introduces a skeleton-aware normalization mechanism to guide the transition between motions, allowing smooth, data-driven control over when and how motions blend. We perform extensive quantitative and qualitative evaluations across various animation styles and different kinematic skeletons, demonstrating that our approach produces plausible, smooth, and controllable motion blends in a unified and efficient manner.
<div id='section'>Paperid: <span id='pid'>1540, <a href='https://arxiv.org/pdf/2508.17173.pdf' target='_blank'>https://arxiv.org/pdf/2508.17173.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chao Ning, Han Wang, Longyan Li, Yang Shi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17173">Collaborative-Online-Learning-Enabled Distributionally Robust Motion Control for Multi-Robot Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper develops a novel COllaborative-Online-Learning (COOL)-enabled motion control framework for multi-robot systems to avoid collision amid randomly moving obstacles whose motion distributions are partially observable through decentralized data streams. To address the notable challenge of data acquisition due to occlusion, a COOL approach based on the Dirichlet process mixture model is proposed to efficiently extract motion distribution information by exchanging among robots selected learning structures. By leveraging the fine-grained local-moment information learned through COOL, a data-stream-driven ambiguity set for obstacle motion is constructed. We then introduce a novel ambiguity set propagation method, which theoretically admits the derivation of the ambiguity sets for obstacle positions over the entire prediction horizon by utilizing obstacle current positions and the ambiguity set for obstacle motion. Additionally, we develop a compression scheme with its safety guarantee to automatically adjust the complexity and granularity of the ambiguity set by aggregating basic ambiguity sets that are close in a measure space, thereby striking an attractive trade-off between control performance and computation time. Then the probabilistic collision-free trajectories are generated through distributionally robust optimization problems. The distributionally robust obstacle avoidance constraints based on the compressed ambiguity set are equivalently reformulated by deriving separating hyperplanes through tractable semi-definite programming. Finally, we establish the probabilistic collision avoidance guarantee and the long-term tracking performance guarantee for the proposed framework. The numerical simulations are used to demonstrate the efficacy and superiority of the proposed approach compared with state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>1541, <a href='https://arxiv.org/pdf/2508.14561.pdf' target='_blank'>https://arxiv.org/pdf/2508.14561.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sukhyun Jeong, Hong-Gi Shin, Yong-Hoon Choi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.14561">Making Pose Representations More Expressive and Disentangled via Residual Vector Quantization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent progress in text-to-motion has advanced both 3D human motion generation and text-based motion control. Controllable motion generation (CoMo), which enables intuitive control, typically relies on pose code representations, but discrete pose codes alone cannot capture fine-grained motion details, limiting expressiveness. To overcome this, we propose a method that augments pose code-based latent representations with continuous motion features using residual vector quantization (RVQ). This design preserves the interpretability and manipulability of pose codes while effectively capturing subtle motion characteristics such as high-frequency details. Experiments on the HumanML3D dataset show that our model reduces Frechet inception distance (FID) from 0.041 to 0.015 and improves Top-1 R-Precision from 0.508 to 0.510. Qualitative analysis of pairwise direction similarity between pose codes further confirms the model's controllability for motion editing.
<div id='section'>Paperid: <span id='pid'>1542, <a href='https://arxiv.org/pdf/2508.12184.pdf' target='_blank'>https://arxiv.org/pdf/2508.12184.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rhea Malhotra, William Chong, Catie Cuan, Oussama Khatib
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.12184">Humanoid Motion Scripting with Postural Synergies</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating sequences of human-like motions for humanoid robots presents challenges in collecting and analyzing reference human motions, synthesizing new motions based on these reference motions, and mapping the generated motion onto humanoid robots. To address these issues, we introduce SynSculptor, a humanoid motion analysis and editing framework that leverages postural synergies for training-free human-like motion scripting. To analyze human motion, we collect 3+ hours of motion capture data across 20 individuals where a real-time operational space controller mimics human motion on a simulated humanoid robot. The major postural synergies are extracted using principal component analysis (PCA) for velocity trajectories segmented by changes in robot momentum, constructing a style-conditioned synergy library for free-space motion generation. To evaluate generated motions using the synergy library, the foot-sliding ratio and proposed metrics for motion smoothness involving total momentum and kinetic energy deviations are computed for each generated motion, and compared with reference motions. Finally, we leverage the synergies with a motion-language transformer, where the humanoid, during execution of motion tasks with its end-effectors, adapts its posture based on the chosen synergy. Supplementary material, code, and videos are available at https://rhea-mal.github.io/humanoidsynergies.io.
<div id='section'>Paperid: <span id='pid'>1543, <a href='https://arxiv.org/pdf/2508.12176.pdf' target='_blank'>https://arxiv.org/pdf/2508.12176.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiwei Zheng, Dongyin Hu, Mingmin Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.12176">Scalable RF Simulation in Generative 4D Worlds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Radio Frequency (RF) sensing has emerged as a powerful, privacy-preserving alternative to vision-based methods for indoor perception tasks. However, collecting high-quality RF data in dynamic and diverse indoor environments remains a major challenge. To address this, we introduce WaveVerse, a prompt-based, scalable framework that simulates realistic RF signals from generated indoor scenes with human motions. WaveVerse introduces a language-guided 4D world generator, which includes a state-aware causal transformer for human motion generation conditioned on spatial constraints and texts, and a phase-coherent ray tracing simulator that enables the simulation of accurate and coherent RF signals. Experiments demonstrate the effectiveness of our approach in conditioned human motion generation and highlight how phase coherence is applied to beamforming and respiration monitoring. We further present two case studies in ML-based high-resolution imaging and human activity recognition, demonstrating that WaveVerse not only enables data generation for RF imaging for the first time, but also consistently achieves performance gain in both data-limited and data-adequate scenarios.
<div id='section'>Paperid: <span id='pid'>1544, <a href='https://arxiv.org/pdf/2508.09960.pdf' target='_blank'>https://arxiv.org/pdf/2508.09960.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifei Yao, Chengyuan Luo, Jiaheng Du, Wentao He, Jun-Guo Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09960">GBC: Generalized Behavior-Cloning Framework for Whole-Body Humanoid Imitation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The creation of human-like humanoid robots is hindered by a fundamental fragmentation: data processing and learning algorithms are rarely universal across different robot morphologies. This paper introduces the Generalized Behavior Cloning (GBC) framework, a comprehensive and unified solution designed to solve this end-to-end challenge. GBC establishes a complete pathway from human motion to robot action through three synergistic innovations. First, an adaptive data pipeline leverages a differentiable IK network to automatically retarget any human MoCap data to any humanoid. Building on this foundation, our novel DAgger-MMPPO algorithm with its MMTransformer architecture learns robust, high-fidelity imitation policies. To complete the ecosystem, the entire framework is delivered as an efficient, open-source platform based on Isaac Lab, empowering the community to deploy the full workflow via simple configuration scripts. We validate the power and generality of GBC by training policies on multiple heterogeneous humanoids, demonstrating excellent performance and transfer to novel motions. This work establishes the first practical and unified pathway for creating truly generalized humanoid controllers.
<div id='section'>Paperid: <span id='pid'>1545, <a href='https://arxiv.org/pdf/2508.05514.pdf' target='_blank'>https://arxiv.org/pdf/2508.05514.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zewei Wu, CÃ©sar Teixeira, Wei Ke, Zhang Xiong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05514">Head Anchor Enhanced Detection and Association for Crowded Pedestrian Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual pedestrian tracking represents a promising research field, with extensive applications in intelligent surveillance, behavior analysis, and human-computer interaction. However, real-world applications face significant occlusion challenges. When multiple pedestrians interact or overlap, the loss of target features severely compromises the tracker's ability to maintain stable trajectories. Traditional tracking methods, which typically rely on full-body bounding box features extracted from {Re-ID} models and linear constant-velocity motion assumptions, often struggle in severe occlusion scenarios. To address these limitations, this work proposes an enhanced tracking framework that leverages richer feature representations and a more robust motion model. Specifically, the proposed method incorporates detection features from both the regression and classification branches of an object detector, embedding spatial and positional information directly into the feature representations. To further mitigate occlusion challenges, a head keypoint detection model is introduced, as the head is less prone to occlusion compared to the full body. In terms of motion modeling, we propose an iterative Kalman filtering approach designed to align with modern detector assumptions, integrating 3D priors to better complete motion trajectories in complex scenes. By combining these advancements in appearance and motion modeling, the proposed method offers a more robust solution for multi-object tracking in crowded environments where occlusions are prevalent.
<div id='section'>Paperid: <span id='pid'>1546, <a href='https://arxiv.org/pdf/2508.03578.pdf' target='_blank'>https://arxiv.org/pdf/2508.03578.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jonas Leo Mueller, Lukas Engel, Eva Dorschky, Daniel Krauss, Ingrid Ullmann, Martin Vossiek, Bjoern M. Eskofier
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.03578">RadProPoser: A Framework for Human Pose Estimation with Uncertainty Quantification from Raw Radar Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Radar-based human pose estimation (HPE) provides a privacy-preserving, illumination-invariant sensing modality but is challenged by noisy, multipath-affected measurements. We introduce RadProPoser, a probabilistic encoder-decoder architecture that processes complex-valued radar tensors from a compact 3-transmitter, 4-receiver MIMO radar. By incorporating variational inference into keypoint regression, RadProPoser jointly predicts 26 three-dimensional joint locations alongside heteroscedastic aleatoric uncertainties and can be recalibrated to predict total uncertainty. We explore different probabilistic formulations using both Gaussian and Laplace distributions for latent priors and likelihoods. On our newly released dataset with optical motion-capture ground truth, RadProPoser achieves an overall mean per-joint position error (MPJPE) of 6.425 cm, with 5.678 cm at the 45 degree aspect angle. The learned uncertainties exhibit strong alignment with actual pose errors and can be calibrated to produce reliable prediction intervals, with our best configuration achieving an expected calibration error of 0.021. As an additional demonstration, sampling from these latent distributions enables effective data augmentation for downstream activity classification, resulting in an F1 score of 0.870. To our knowledge, this is the first end-to-end radar tensor-based HPE system to explicitly model and quantify per-joint uncertainty from raw radar tensor data, establishing a foundation for explainable and reliable human motion analysis in radar applications.
<div id='section'>Paperid: <span id='pid'>1547, <a href='https://arxiv.org/pdf/2508.00939.pdf' target='_blank'>https://arxiv.org/pdf/2508.00939.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haodong Huang, Shilong Sun, Yuanpeng Wang, Chiyao Li, Hailin Huang, Wenfu Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.00939">BarlowWalk: Self-supervised Representation Learning for Legged Robot Terrain-adaptive Locomotion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reinforcement learning (RL), driven by data-driven methods, has become an effective solution for robot leg motion control problems. However, the mainstream RL methods for bipedal robot terrain traversal, such as teacher-student policy knowledge distillation, suffer from long training times, which limit development efficiency. To address this issue, this paper proposes BarlowWalk, an improved Proximal Policy Optimization (PPO) method integrated with self-supervised representation learning. This method employs the Barlow Twins algorithm to construct a decoupled latent space, mapping historical observation sequences into low-dimensional representations and implementing self-supervision. Meanwhile, the actor requires only proprioceptive information to achieve self-supervised learning over continuous time steps, significantly reducing the dependence on external terrain perception. Simulation experiments demonstrate that this method has significant advantages in complex terrain scenarios. To enhance the credibility of the evaluation, this study compares BarlowWalk with advanced algorithms through comparative tests, and the experimental results verify the effectiveness of the proposed method.
<div id='section'>Paperid: <span id='pid'>1548, <a href='https://arxiv.org/pdf/2508.00299.pdf' target='_blank'>https://arxiv.org/pdf/2508.00299.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Danzhen Fu, Jiagao Hu, Daiguo Zhou, Fei Wang, Zepeng Wang, Wenhua Liao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.00299">Controllable Pedestrian Video Editing for Multi-View Driving Scenarios via Motion Sequence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pedestrian detection models in autonomous driving systems often lack robustness due to insufficient representation of dangerous pedestrian scenarios in training datasets. To address this limitation, we present a novel framework for controllable pedestrian video editing in multi-view driving scenarios by integrating video inpainting and human motion control techniques. Our approach begins by identifying pedestrian regions of interest across multiple camera views, expanding detection bounding boxes with a fixed ratio, and resizing and stitching these regions into a unified canvas while preserving cross-view spatial relationships. A binary mask is then applied to designate the editable area, within which pedestrian editing is guided by pose sequence control conditions. This enables flexible editing functionalities, including pedestrian insertion, replacement, and removal. Extensive experiments demonstrate that our framework achieves high-quality pedestrian editing with strong visual realism, spatiotemporal coherence, and cross-view consistency. These results establish the proposed method as a robust and versatile solution for multi-view pedestrian video generation, with broad potential for applications in data augmentation and scenario simulation in autonomous driving.
<div id='section'>Paperid: <span id='pid'>1549, <a href='https://arxiv.org/pdf/2507.20562.pdf' target='_blank'>https://arxiv.org/pdf/2507.20562.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hyung Kyu Kim, Sangmin Lee, Hak Gu Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.20562">MemoryTalker: Personalized Speech-Driven 3D Facial Animation via Audio-Guided Stylization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Speech-driven 3D facial animation aims to synthesize realistic facial motion sequences from given audio, matching the speaker's speaking style. However, previous works often require priors such as class labels of a speaker or additional 3D facial meshes at inference, which makes them fail to reflect the speaking style and limits their practical use. To address these issues, we propose MemoryTalker which enables realistic and accurate 3D facial motion synthesis by reflecting speaking style only with audio input to maximize usability in applications. Our framework consists of two training stages: 1-stage is storing and retrieving general motion (i.e., Memorizing), and 2-stage is to perform the personalized facial motion synthesis (i.e., Animating) with the motion memory stylized by the audio-driven speaking style feature. In this second stage, our model learns about which facial motion types should be emphasized for a particular piece of audio. As a result, our MemoryTalker can generate a reliable personalized facial animation without additional prior information. With quantitative and qualitative evaluations, as well as user study, we show the effectiveness of our model and its performance enhancement for personalized facial animation over state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>1550, <a href='https://arxiv.org/pdf/2507.14097.pdf' target='_blank'>https://arxiv.org/pdf/2507.14097.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hari Iyer, Neel Macwan, Atharva Jitendra Hude, Heejin Jeong, Shenghan Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.14097">Generative AI-Driven High-Fidelity Human Motion Simulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion simulation (HMS) supports cost-effective evaluation of worker behavior, safety, and productivity in industrial tasks. However, existing methods often suffer from low motion fidelity. This study introduces Generative-AI-Enabled HMS (G-AI-HMS), which integrates text-to-text and text-to-motion models to enhance simulation quality for physical tasks. G-AI-HMS tackles two key challenges: (1) translating task descriptions into motion-aware language using Large Language Models aligned with MotionGPT's training vocabulary, and (2) validating AI-enhanced motions against real human movements using computer vision. Posture estimation algorithms are applied to real-time videos to extract joint landmarks, and motion similarity metrics are used to compare them with AI-enhanced sequences. In a case study involving eight tasks, the AI-enhanced motions showed lower error than human created descriptions in most scenarios, performing better in six tasks based on spatial accuracy, four tasks based on alignment after pose normalization, and seven tasks based on overall temporal similarity. Statistical analysis showed that AI-enhanced prompts significantly (p $<$ 0.0001) reduced joint error and temporal misalignment while retaining comparable posture accuracy.
<div id='section'>Paperid: <span id='pid'>1551, <a href='https://arxiv.org/pdf/2507.09704.pdf' target='_blank'>https://arxiv.org/pdf/2507.09704.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaotang Zhang, Ziyi Chang, Qianhui Men, Hubert Shum
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.09704">Real-time and Controllable Reactive Motion Synthesis via Intention Guidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a real-time method for reactive motion synthesis based on the known trajectory of input character, predicting instant reactions using only historical, user-controlled motions. Our method handles the uncertainty of future movements by introducing an intention predictor, which forecasts key joint intentions to make pose prediction more deterministic from the historical interaction. The intention is later encoded into the latent space of its reactive motion, matched with a codebook which represents mappings between input and output. It samples a categorical distribution for pose generation and strengthens model robustness through adversarial training. Unlike previous offline approaches, the system can recursively generate intentions and reactive motions using feedback from earlier steps, enabling real-time, long-term realistic interactive synthesis. Both quantitative and qualitative experiments show our approach outperforms other matching-based motion synthesis approaches, delivering superior stability and generalizability. In our method, user can also actively influence the outcome by controlling the moving directions, creating a personalized interaction path that deviates from predefined trajectories.
<div id='section'>Paperid: <span id='pid'>1552, <a href='https://arxiv.org/pdf/2507.07805.pdf' target='_blank'>https://arxiv.org/pdf/2507.07805.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kim P. Wabersich, Felix Berkel, Felix Gruber, Sven Reimann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07805">Set-Based Control Barrier Functions and Safety Filters</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>High performance and formal safety guarantees are common requirements for industrial control applications. Control barrier function (CBF) methods provide a systematic approach to the modularization of safety and performance. However, the design of such CBFs can be challenging, which limits their applicability to large-scale or data-driven systems. This paper introduces the concept of a set-based CBF for linear systems with convex constraints. By leveraging control invariant sets from reachability analysis and predictive control, the set-based CBF is defined implicitly through the minimal scaling of such a set to contain the current system state. This approach enables the development of implicit, data-driven, and high-dimensional CBF representations. The paper demonstrates the design of a safety filter using set-based CBFs, which is suitable for real-time implementations and learning-based approximations to reduce online computational demands. The effectiveness of the method is illustrated through comprehensive simulations on a high-dimensional mass-spring-damper system and a motion control task, and it is validated experimentally using an electric drive application with short sampling times, highlighting its practical benefits for safety-critical control.
<div id='section'>Paperid: <span id='pid'>1553, <a href='https://arxiv.org/pdf/2507.04955.pdf' target='_blank'>https://arxiv.org/pdf/2507.04955.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fathinah Izzati, Xinyue Li, Gus Xia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.04955">EXPOTION: Facial Expression and Motion Control for Multimodal Music Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose Expotion (Facial Expression and Motion Control for Multimodal Music Generation), a generative model leveraging multimodal visual controls - specifically, human facial expressions and upper-body motion - as well as text prompts to produce expressive and temporally accurate music. We adopt parameter-efficient fine-tuning (PEFT) on the pretrained text-to-music generation model, enabling fine-grained adaptation to the multimodal controls using a small dataset. To ensure precise synchronization between video and music, we introduce a temporal smoothing strategy to align multiple modalities. Experiments demonstrate that integrating visual features alongside textual descriptions enhances the overall quality of generated music in terms of musicality, creativity, beat-tempo consistency, temporal alignment with the video, and text adherence, surpassing both proposed baselines and existing state-of-the-art video-to-music generation models. Additionally, we introduce a novel dataset consisting of 7 hours of synchronized video recordings capturing expressive facial and upper-body gestures aligned with corresponding music, providing significant potential for future research in multimodal and interactive music generation.
<div id='section'>Paperid: <span id='pid'>1554, <a href='https://arxiv.org/pdf/2507.04062.pdf' target='_blank'>https://arxiv.org/pdf/2507.04062.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianwei Tang, Hong Yang, Tengyue Chen, Jian-Fang Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.04062">Stochastic Human Motion Prediction with Memory of Action Transition and Action Characteristic</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Action-driven stochastic human motion prediction aims to generate future motion sequences of a pre-defined target action based on given past observed sequences performing non-target actions. This task primarily presents two challenges. Firstly, generating smooth transition motions is hard due to the varying transition speeds of different actions. Secondly, the action characteristic is difficult to be learned because of the similarity of some actions. These issues cause the predicted results to be unreasonable and inconsistent. As a result, we propose two memory banks, the Soft-transition Action Bank (STAB) and Action Characteristic Bank (ACB), to tackle the problems above. The STAB stores the action transition information. It is equipped with the novel soft searching approach, which encourages the model to focus on multiple possible action categories of observed motions. The ACB records action characteristic, which produces more prior information for predicting certain actions. To fuse the features retrieved from the two banks better, we further propose the Adaptive Attention Adjustment (AAA) strategy. Extensive experiments on four motion prediction datasets demonstrate that our approach consistently outperforms the previous state-of-the-art. The demo and code are available at https://hyqlat.github.io/STABACB.github.io/.
<div id='section'>Paperid: <span id='pid'>1555, <a href='https://arxiv.org/pdf/2506.22459.pdf' target='_blank'>https://arxiv.org/pdf/2506.22459.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wending Heng, Chaoyuan Liang, Yihui Zhao, Zhiqiang Zhang, Glen Cooper, Zhenhong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.22459">Physics-Embedded Neural Networks for sEMG-based Continuous Motion Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurately decoding human motion intentions from surface electromyography (sEMG) is essential for myoelectric control and has wide applications in rehabilitation robotics and assistive technologies. However, existing sEMG-based motion estimation methods often rely on subject-specific musculoskeletal (MSK) models that are difficult to calibrate, or purely data-driven models that lack physiological consistency. This paper introduces a novel Physics-Embedded Neural Network (PENN) that combines interpretable MSK forward-dynamics with data-driven residual learning, thereby preserving physiological consistency while achieving accurate motion estimation. The PENN employs a recursive temporal structure to propagate historical estimates and a lightweight convolutional neural network for residual correction, leading to robust and temporally coherent estimations. A two-phase training strategy is designed for PENN. Experimental evaluations on six healthy subjects show that PENN outperforms state-of-the-art baseline methods in both root mean square error (RMSE) and $R^2$ metrics.
<div id='section'>Paperid: <span id='pid'>1556, <a href='https://arxiv.org/pdf/2506.11419.pdf' target='_blank'>https://arxiv.org/pdf/2506.11419.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bin Sun, Boao Zhang, Jiayi Lu, Xinjie Feng, Jiachen Shang, Rui Cao, Mengchao Zheng, Chuanye Wang, Shichun Yang, Yaoguang Cao, Ziying Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.11419">FocalAD: Local Motion Planning for End-to-End Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In end-to-end autonomous driving,the motion prediction plays a pivotal role in ego-vehicle planning. However, existing methods often rely on globally aggregated motion features, ignoring the fact that planning decisions are primarily influenced by a small number of locally interacting agents. Failing to attend to these critical local interactions can obscure potential risks and undermine planning reliability. In this work, we propose FocalAD, a novel end-to-end autonomous driving framework that focuses on critical local neighbors and refines planning by enhancing local motion representations. Specifically, FocalAD comprises two core modules: the Ego-Local-Agents Interactor (ELAI) and the Focal-Local-Agents Loss (FLA Loss). ELAI conducts a graph-based ego-centric interaction representation that captures motion dynamics with local neighbors to enhance both ego planning and agent motion queries. FLA Loss increases the weights of decision-critical neighboring agents, guiding the model to prioritize those more relevant to planning. Extensive experiments show that FocalAD outperforms existing state-of-the-art methods on the open-loop nuScenes datasets and closed-loop Bench2Drive benchmark. Notably, on the robustness-focused Adv-nuScenes dataset, FocalAD achieves even greater improvements, reducing the average colilision rate by 41.9% compared to DiffusionDrive and by 15.6% compared to SparseDrive.
<div id='section'>Paperid: <span id='pid'>1557, <a href='https://arxiv.org/pdf/2506.09411.pdf' target='_blank'>https://arxiv.org/pdf/2506.09411.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vaclav Knapp, Matyas Bohacek
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.09411">Synthetic Human Action Video Data Generation with Pose Transfer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In video understanding tasks, particularly those involving human motion, synthetic data generation often suffers from uncanny features, diminishing its effectiveness for training. Tasks such as sign language translation, gesture recognition, and human motion understanding in autonomous driving have thus been unable to exploit the full potential of synthetic data. This paper proposes a method for generating synthetic human action video data using pose transfer (specifically, controllable 3D Gaussian avatar models). We evaluate this method on the Toyota Smarthome and NTU RGB+D datasets and show that it improves performance in action recognition tasks. Moreover, we demonstrate that the method can effectively scale few-shot datasets, making up for groups underrepresented in the real training data and adding diverse backgrounds. We open-source the method along with RANDOM People, a dataset with videos and avatars of novel human identities for pose transfer crowd-sourced from the internet.
<div id='section'>Paperid: <span id='pid'>1558, <a href='https://arxiv.org/pdf/2505.22111.pdf' target='_blank'>https://arxiv.org/pdf/2505.22111.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Woonho Ko, Jin Bok Park, Il Yong Chun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.22111">Autoregression-free video prediction using diffusion model for mitigating error propagation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing long-term video prediction methods often rely on an autoregressive video prediction mechanism. However, this approach suffers from error propagation, particularly in distant future frames. To address this limitation, this paper proposes the first AutoRegression-Free (ARFree) video prediction framework using diffusion models. Different from an autoregressive video prediction mechanism, ARFree directly predicts any future frame tuples from the context frame tuple. The proposed ARFree consists of two key components: 1) a motion prediction module that predicts a future motion using motion feature extracted from the context frame tuple; 2) a training method that improves motion continuity and contextual consistency between adjacent future frame tuples. Our experiments with two benchmark datasets show that the proposed ARFree video prediction framework outperforms several state-of-the-art video prediction methods.
<div id='section'>Paperid: <span id='pid'>1559, <a href='https://arxiv.org/pdf/2505.21566.pdf' target='_blank'>https://arxiv.org/pdf/2505.21566.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gao Huayu, Huang Tengjiu, Ye Xiaolong, Tsuyoshi Okita
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.21566">Diffusion Model-based Activity Completion for AI Motion Capture from Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>AI-based motion capture is an emerging technology that offers a cost-effective alternative to traditional motion capture systems. However, current AI motion capture methods rely entirely on observed video sequences, similar to conventional motion capture. This means that all human actions must be predefined, and movements outside the observed sequences are not possible. To address this limitation, we aim to apply AI motion capture to virtual humans, where flexible actions beyond the observed sequences are required. We assume that while many action fragments exist in the training data, the transitions between them may be missing. To bridge these gaps, we propose a diffusion-model-based action completion technique that generates complementary human motion sequences, ensuring smooth and continuous movements. By introducing a gate module and a position-time embedding module, our approach achieves competitive results on the Human3.6M dataset. Our experimental results show that (1) MDC-Net outperforms existing methods in ADE, FDE, and MMADE but is slightly less accurate in MMFDE, (2) MDC-Net has a smaller model size (16.84M) compared to HumanMAC (28.40M), and (3) MDC-Net generates more natural and coherent motion sequences. Additionally, we propose a method for extracting sensor data, including acceleration and angular velocity, from human motion sequences.
<div id='section'>Paperid: <span id='pid'>1560, <a href='https://arxiv.org/pdf/2505.21146.pdf' target='_blank'>https://arxiv.org/pdf/2505.21146.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yang Zhao, Yan Zhang, Xubo Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.21146">IKMo: Image-Keyframed Motion Generation with Trajectory-Pose Conditioned Motion Diffusion Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing human motion generation methods with trajectory and pose inputs operate global processing on both modalities, leading to suboptimal outputs. In this paper, we propose IKMo, an image-keyframed motion generation method based on the diffusion model with trajectory and pose being decoupled. The trajectory and pose inputs go through a two-stage conditioning framework. In the first stage, the dedicated optimization module is applied to refine inputs. In the second stage, trajectory and pose are encoded via a Trajectory Encoder and a Pose Encoder in parallel. Then, motion with high spatial and semantic fidelity is guided by a motion ControlNet, which processes the fused trajectory and pose data. Experiment results based on HumanML3D and KIT-ML datasets demonstrate that the proposed method outperforms state-of-the-art on all metrics under trajectory-keyframe constraints. In addition, MLLM-based agents are implemented to pre-process model inputs. Given texts and keyframe images from users, the agents extract motion descriptions, keyframe poses, and trajectories as the optimized inputs into the motion generation model. We conducts a user study with 10 participants. The experiment results prove that the MLLM-based agents pre-processing makes generated motion more in line with users' expectation. We believe that the proposed method improves both the fidelity and controllability of motion generation by the diffusion model.
<div id='section'>Paperid: <span id='pid'>1561, <a href='https://arxiv.org/pdf/2505.17860.pdf' target='_blank'>https://arxiv.org/pdf/2505.17860.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenning Xu, Shiyu Fan, Paul Henderson, Edmond S. L. Ho
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.17860">Multi-Person Interaction Generation from Two-Person Motion Priors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating realistic human motion with high-level controls is a crucial task for social understanding, robotics, and animation. With high-quality MOCAP data becoming more available recently, a wide range of data-driven approaches have been presented. However, modelling multi-person interactions still remains a less explored area. In this paper, we present Graph-driven Interaction Sampling, a method that can generate realistic and diverse multi-person interactions by leveraging existing two-person motion diffusion models as motion priors. Instead of training a new model specific to multi-person interaction synthesis, our key insight is to spatially and temporally separate complex multi-person interactions into a graph structure of two-person interactions, which we name the Pairwise Interaction Graph. We thus decompose the generation task into simultaneous single-person motion generation conditioned on one other's motion. In addition, to reduce artifacts such as interpenetrations of body parts in generated multi-person interactions, we introduce two graph-dependent guidance terms into the diffusion sampling scheme. Unlike previous work, our method can produce various high-quality multi-person interactions without having repetitive individual motions. Extensive experiments demonstrate that our approach consistently outperforms existing methods in reducing artifacts when generating a wide range of two-person and multi-person interactions.
<div id='section'>Paperid: <span id='pid'>1562, <a href='https://arxiv.org/pdf/2505.04961.pdf' target='_blank'>https://arxiv.org/pdf/2505.04961.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyu Zhang, Sergey Bashkirov, Dun Yang, Michael Taylor, Xue Bin Peng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.04961">ADD: Physics-Based Motion Imitation with Adversarial Differential Discriminators</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-objective optimization problems, which require the simultaneous optimization of multiple terms, are prevalent across numerous applications. Existing multi-objective optimization methods often rely on manually tuned aggregation functions to formulate a joint optimization target. The performance of such hand-tuned methods is heavily dependent on careful weight selection, a time-consuming and laborious process. These limitations also arise in the setting of reinforcement-learning-based motion tracking for physically simulated characters, where intricately crafted reward functions are typically used to achieve high-fidelity results. Such solutions not only require domain expertise and significant manual adjustment, but also limit the applicability of the resulting reward function across diverse skills. To bridge this gap, we present a novel adversarial multi-objective optimization technique that is broadly applicable to a range of multi-objective optimization problems, including motion tracking. The proposed adversarial differential discriminator receives a single positive sample, yet is still effective at guiding the optimization process. We demonstrate that our technique can enable characters to closely replicate a variety of acrobatic and agile behaviors, achieving comparable quality to state-of-the-art motion-tracking methods, without relying on manually tuned reward functions. Results are best visualized through https://youtu.be/rz8BYCE9E2w.
<div id='section'>Paperid: <span id='pid'>1563, <a href='https://arxiv.org/pdf/2504.14602.pdf' target='_blank'>https://arxiv.org/pdf/2504.14602.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiwei Li, Bi Zhang, Xiaowei Tan, Wanxin Chen, Zhaoyuan Liu, Juanjuan Zhang, Weiguang Huo, Jian Huang, Lianqing Liu, Xingang Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.14602">K2MUSE: A human lower limb multimodal dataset under diverse conditions for facilitating rehabilitation robotics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The natural interaction and control performance of lower limb rehabilitation robots are closely linked to biomechanical information from various human locomotion activities. Multidimensional human motion data significantly deepen the understanding of the complex mechanisms governing neuromuscular alterations, thereby facilitating the development and application of rehabilitation robots in multifaceted real-world environments. However, currently available lower limb datasets are inadequate for supplying the essential multimodal data and large-scale gait samples necessary for effective data-driven approaches, and they neglect the significant effects of acquisition interference in real applications.To fill this gap, we present the K2MUSE dataset, which includes a comprehensive collection of multimodal data, comprising kinematic, kinetic, amplitude-mode ultrasound (AUS), and surface electromyography (sEMG) measurements. The proposed dataset includes lower limb multimodal data from 30 able-bodied participants walking under different inclines (0$^\circ$, $\pm$5$^\circ$, and $\pm$10$^\circ$), various speeds (0.5 m/s, 1.0 m/s, and 1.5 m/s), and different nonideal acquisition conditions (muscle fatigue, electrode shifts, and inter-day differences). The kinematic and ground reaction force data were collected via a Vicon motion capture system and an instrumented treadmill with embedded force plates, whereas the sEMG and AUS data were synchronously recorded for thirteen muscles on the bilateral lower limbs. This dataset offers a new resource for designing control frameworks for rehabilitation robots and conducting biomechanical analyses of lower limb locomotion. The dataset is available at https://k2muse.github.io/.
<div id='section'>Paperid: <span id='pid'>1564, <a href='https://arxiv.org/pdf/2504.12702.pdf' target='_blank'>https://arxiv.org/pdf/2504.12702.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziqi Wang, Jingyue Zhao, Jichao Yang, Yaohua Wang, Xun Xiao, Yuan Li, Chao Xiao, Lei Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.12702">Embodied Neuromorphic Control Applied on a 7-DOF Robotic Manipulator</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The development of artificial intelligence towards real-time interaction with the environment is a key aspect of embodied intelligence and robotics. Inverse dynamics is a fundamental robotics problem, which maps from joint space to torque space of robotic systems. Traditional methods for solving it rely on direct physical modeling of robots which is difficult or even impossible due to nonlinearity and external disturbance. Recently, data-based model-learning algorithms are adopted to address this issue. However, they often require manual parameter tuning and high computational costs. Neuromorphic computing is inherently suitable to process spatiotemporal features in robot motion control at extremely low costs. However, current research is still in its infancy: existing works control only low-degree-of-freedom systems and lack performance quantification and comparison. In this paper, we propose a neuromorphic control framework to control 7 degree-of-freedom robotic manipulators. We use Spiking Neural Network to leverage the spatiotemporal continuity of the motion data to improve control accuracy, and eliminate manual parameters tuning. We validated the algorithm on two robotic platforms, which reduces torque prediction error by at least 60% and performs a target position tracking task successfully. This work advances embodied neuromorphic control by one step forward from proof of concept to applications in complex real-world tasks.
<div id='section'>Paperid: <span id='pid'>1565, <a href='https://arxiv.org/pdf/2504.11150.pdf' target='_blank'>https://arxiv.org/pdf/2504.11150.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mahir Gulzar, Yar Muhammad, Naveed Muhammad
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.11150">GC-GAT: Multimodal Vehicular Trajectory Prediction using Graph Goal Conditioning and Cross-context Attention</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Predicting future trajectories of surrounding vehicles heavily relies on what contextual information is given to a motion prediction model. The context itself can be static (lanes, regulatory elements, etc) or dynamic (traffic participants). This paper presents a lane graph-based motion prediction model that first predicts graph-based goal proposals and later fuses them with cross attention over multiple contextual elements. We follow the famous encoder-interactor-decoder architecture where the encoder encodes scene context using lightweight Gated Recurrent Units, the interactor applies cross-context attention over encoded scene features and graph goal proposals, and the decoder regresses multimodal trajectories via Laplacian Mixture Density Network from the aggregated encodings. Using cross-attention over graph-based goal proposals gives robust trajectory estimates since the model learns to attend to future goal-relevant scene elements for the intended agent. We evaluate our work on nuScenes motion prediction dataset, achieving state-of-the-art results.
<div id='section'>Paperid: <span id='pid'>1566, <a href='https://arxiv.org/pdf/2504.10676.pdf' target='_blank'>https://arxiv.org/pdf/2504.10676.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhanbo Huang, Xiaoming Liu, Yu Kong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.10676">H-MoRe: Learning Human-centric Motion Representation for Action Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose H-MoRe, a novel pipeline for learning precise human-centric motion representation. Our approach dynamically preserves relevant human motion while filtering out background movement. Notably, unlike previous methods relying on fully supervised learning from synthetic data, H-MoRe learns directly from real-world scenarios in a self-supervised manner, incorporating both human pose and body shape information. Inspired by kinematics, H-MoRe represents absolute and relative movements of each body point in a matrix format that captures nuanced motion details, termed world-local flows. H-MoRe offers refined insights into human motion, which can be integrated seamlessly into various action-related applications. Experimental results demonstrate that H-MoRe brings substantial improvements across various downstream tasks, including gait recognition(CL@R1: +16.01%), action recognition(Acc@1: +8.92%), and video generation(FVD: -67.07%). Additionally, H-MoRe exhibits high inference efficiency (34 fps), making it suitable for most real-time scenarios. Models and code will be released upon publication.
<div id='section'>Paperid: <span id='pid'>1567, <a href='https://arxiv.org/pdf/2504.10275.pdf' target='_blank'>https://arxiv.org/pdf/2504.10275.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Harsh Yadav, Maximilian Schaefer, Kun Zhao, Tobias Meisen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.10275">LMFormer: Lane based Motion Prediction Transformer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion prediction plays an important role in autonomous driving. This study presents LMFormer, a lane-aware transformer network for trajectory prediction tasks. In contrast to previous studies, our work provides a simple mechanism to dynamically prioritize the lanes and shows that such a mechanism introduces explainability into the learning behavior of the network. Additionally, LMFormer uses the lane connection information at intersections, lane merges, and lane splits, in order to learn long-range dependency in lane structure. Moreover, we also address the issue of refining the predicted trajectories and propose an efficient method for iterative refinement through stacked transformer layers. For benchmarking, we evaluate LMFormer on the nuScenes dataset and demonstrate that it achieves SOTA performance across multiple metrics. Furthermore, the Deep Scenario dataset is used to not only illustrate cross-dataset network performance but also the unification capabilities of LMFormer to train on multiple datasets and achieve better performance.
<div id='section'>Paperid: <span id='pid'>1568, <a href='https://arxiv.org/pdf/2504.08206.pdf' target='_blank'>https://arxiv.org/pdf/2504.08206.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lansu Dai, Burak Kantarci
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.08206">Advancing Autonomous Vehicle Safety: A Combined Fault Tree Analysis and Bayesian Network Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper integrates Fault Tree Analysis (FTA) and Bayesian Networks (BN) to assess collision risk and establish Automotive Safety Integrity Level (ASIL) B failure rate targets for critical autonomous vehicle (AV) components. The FTA-BN integration combines the systematic decomposition of failure events provided by FTA with the probabilistic reasoning capabilities of BN, which allow for dynamic updates in failure probabilities, enhancing the adaptability of risk assessment. A fault tree is constructed based on AV subsystem architecture, with collision as the top event, and failure rates are assigned while ensuring the total remains within 100 FIT. Bayesian inference is applied to update posterior probabilities, and the results indicate that perception system failures (46.06 FIT) are the most significant contributor, particularly failures to detect existing objects (PF5) and misclassification (PF6). Mitigation strategies are proposed for sensors, perception, decision-making, and motion control to reduce the collision risk. The FTA-BN integration approach provides dynamic risk quantification, offering system designers refined failure rate targets to improve AV safety.
<div id='section'>Paperid: <span id='pid'>1569, <a href='https://arxiv.org/pdf/2504.08206.pdf' target='_blank'>https://arxiv.org/pdf/2504.08206.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lansu Dai, Burak Kantarci
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.08206">Advancing Autonomous Vehicle Safety: A Combined Fault Tree Analysis and Bayesian Network Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper integrates Fault Tree Analysis (FTA) and Bayesian Networks (BN) to assess collision risk and establish Automotive Safety Integrity Level (ASIL) B failure rate targets for critical autonomous vehicle (AV) components. The FTA-BN integration combines the systematic decomposition of failure events provided by FTA with the probabilistic reasoning capabilities of BN, which allow for dynamic updates in failure probabilities, enhancing the adaptability of risk assessment. A fault tree is constructed based on AV subsystem architecture, with collision as the top event, and failure rates are assigned while ensuring the total remains within 100 FIT. Bayesian inference is applied to update posterior probabilities, and the results indicate that perception system failures (46.06 FIT) are the most significant contributor, particularly failures to detect existing objects (PF5) and misclassification (PF6). Mitigation strategies are proposed for sensors, perception, decision-making, and motion control to reduce the collision risk. The FTA-BN integration approach provides dynamic risk quantification, offering system designers refined failure rate targets to improve AV safety.
<div id='section'>Paperid: <span id='pid'>1570, <a href='https://arxiv.org/pdf/2504.05629.pdf' target='_blank'>https://arxiv.org/pdf/2504.05629.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haodong Huang, Shilong Sun, Zida Zhao, Hailin Huang, Changqing Shen, Wenfu Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.05629">PTRL: Prior Transfer Deep Reinforcement Learning for Legged Robots Locomotion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the field of legged robot motion control, reinforcement learning (RL) holds great promise but faces two major challenges: high computational cost for training individual robots and poor generalization of trained models. To address these problems, this paper proposes a novel framework called Prior Transfer Reinforcement Learning (PTRL), which improves both training efficiency and model transferability across different robots. Drawing inspiration from model transfer techniques in deep learning, PTRL introduces a fine-tuning mechanism that selectively freezes layers of the policy network during transfer, making it the first to apply such a method in RL. The framework consists of three stages: pre-training on a source robot using the Proximal Policy Optimization (PPO) algorithm, transferring the learned policy to a target robot, and fine-tuning with partial network freezing. Extensive experiments on various robot platforms confirm that this approach significantly reduces training time while maintaining or even improving performance. Moreover, the study quantitatively analyzes how the ratio of frozen layers affects transfer results, providing valuable insights into optimizing the process. The experimental outcomes show that PTRL achieves better walking control performance and demonstrates strong generalization and adaptability, offering a promising solution for efficient and scalable RL-based control of legged robots.
<div id='section'>Paperid: <span id='pid'>1571, <a href='https://arxiv.org/pdf/2504.04862.pdf' target='_blank'>https://arxiv.org/pdf/2504.04862.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunxiang Liu, Hongkuo Niu, Jianlin Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.04862">GAMDTP: Dynamic Trajectory Prediction with Graph Attention Mamba Network</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate motion prediction of traffic agents is crucial for the safety and stability of autonomous driving systems. In this paper, we introduce GAMDTP, a novel graph attention-based network tailored for dynamic trajectory prediction. Specifically, we fuse the result of self attention and mamba-ssm through a gate mechanism, leveraging the strengths of both to extract features more efficiently and accurately, in each graph convolution layer. GAMDTP encodes the high-definition map(HD map) data and the agents' historical trajectory coordinates and decodes the network's output to generate the final prediction results. Additionally, recent approaches predominantly focus on dynamically fusing historical forecast results and rely on two-stage frameworks including proposal and refinement. To further enhance the performance of the two-stage frameworks we also design a scoring mechanism to evaluate the prediction quality during the proposal and refinement processes. Experiments on the Argoverse dataset demonstrates that GAMDTP achieves state-of-the-art performance, achieving superior accuracy in dynamic trajectory prediction.
<div id='section'>Paperid: <span id='pid'>1572, <a href='https://arxiv.org/pdf/2503.23171.pdf' target='_blank'>https://arxiv.org/pdf/2503.23171.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shayan Sepahvand, Niloufar Amiri, Farrokh Janabi-Sharifi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.23171">Deep Visual Servoing of an Aerial Robot Using Keypoint Feature Extraction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The problem of image-based visual servoing (IBVS) of an aerial robot using deep-learning-based keypoint detection is addressed in this article. A monocular RGB camera mounted on the platform is utilized to collect the visual data. A convolutional neural network (CNN) is then employed to extract the features serving as the visual data for the servoing task. This paper contributes to the field by circumventing not only the challenge stemming from the need for man-made marker detection in conventional visual servoing techniques, but also enhancing the robustness against undesirable factors including occlusion, varying illumination, clutter, and background changes, thereby broadening the applicability of perception-guided motion control tasks in aerial robots. Additionally, extensive physics-based ROS Gazebo simulations are conducted to assess the effectiveness of this method, in contrast to many existing studies that rely solely on physics-less simulations. A demonstration video is available at https://youtu.be/Dd2Her8Ly-E.
<div id='section'>Paperid: <span id='pid'>1573, <a href='https://arxiv.org/pdf/2503.19984.pdf' target='_blank'>https://arxiv.org/pdf/2503.19984.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ido Rachbuch, Sinwook Park, Yuval Katz, Touvia Miloh, Gilad Yossifon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.19984">Hybrid Magnetically and Electrically Powered Metallo-Dielectric Janus Microrobots: Enhanced Motion Control and Operation Beyond Planar Limits</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study introduces the integration of hybrid magnetic and electric actuation mechanisms to achieve advanced motion capabilities for Janus particle (JP) microrobots. We demonstrate enhanced in-plane motion control through versatile control strategies and present the concepts of interplanar transitions and 2.5-dimensional (2.5D) trajectories, enabled by magnetic levitation and electrostatic trapping. These innovations expand the mobility of JPs into 3D space, allowing dynamic operation beyond the limitations of traditional surface-bound motion. Key functionalities include obstacle crossing, transitions to elevated surfaces, and discrete surface patterning enabling highly localized interventions. Using this set of tools, we also showcase the controlled out-of-plane transport of both synthetic and biological cargo. Together, these advancements lay the groundwork for novel microrobot-related applications in microfluidic systems and biomedical research.
<div id='section'>Paperid: <span id='pid'>1574, <a href='https://arxiv.org/pdf/2503.13090.pdf' target='_blank'>https://arxiv.org/pdf/2503.13090.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>VÃ¡clav TruhlaÅÃ­k, TomÃ¡Å¡ PivoÅka, Michal Kasarda, Libor PÅeuÄil
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.13090">Multi-Platform Teach-and-Repeat Navigation by Visual Place Recognition Based on Deep-Learned Local Features</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Uniform and variable environments still remain a challenge for stable visual localization and mapping in mobile robot navigation. One of the possible approaches suitable for such environments is appearance-based teach-and-repeat navigation, relying on simplified localization and reactive robot motion control - all without a need for standard mapping. This work brings an innovative solution to such a system based on visual place recognition techniques. Here, the major contributions stand in the employment of a new visual place recognition technique, a novel horizontal shift computation approach, and a multi-platform system design for applications across various types of mobile robots. Secondly, a new public dataset for experimental testing of appearance-based navigation methods is introduced. Moreover, the work also provides real-world experimental testing and performance comparison of the introduced navigation system against other state-of-the-art methods. The results confirm that the new system outperforms existing methods in several testing scenarios, is capable of operation indoors and outdoors, and exhibits robustness to day and night scene variations.
<div id='section'>Paperid: <span id='pid'>1575, <a href='https://arxiv.org/pdf/2503.13034.pdf' target='_blank'>https://arxiv.org/pdf/2503.13034.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tinghui Li, Pamuditha Somarathne, Zhanna Sarsenbayeva, Anusha Withana
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.13034">TA-GNN: Physics Inspired Time-Agnostic Graph Neural Network for Finger Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Continuous prediction of finger joint movement using historical joint positions/rotations is vital in a multitude of applications, especially related to virtual reality, computer graphics, robotics, and rehabilitation. However, finger motions are highly articulated with multiple degrees of freedom, making them significantly harder to model and predict. To address this challenge, we propose a physics-inspired time-agnostic graph neural network (TA-GNN) to accurately predict human finger motions. The proposed encoder comprises a kinematic feature extractor to generate filtered velocity and acceleration and a physics-based encoder that follows linear kinematics. The model is designed to be prediction-time-agnostic so that it can seamlessly provide continuous predictions. The graph-based decoder for learning the topological motion between finger joints is designed to address the higher degree articulation of fingers. We show the superiority of our model performance in virtual reality context. This novel approach enhances finger tracking without additional sensors, enabling predictive interactions such as haptic re-targeting and improving predictive rendering quality.
<div id='section'>Paperid: <span id='pid'>1576, <a href='https://arxiv.org/pdf/2503.06050.pdf' target='_blank'>https://arxiv.org/pdf/2503.06050.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alexander Schperberg, Marcel Menner, Stefano Di Cairano
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.06050">Energy-Efficient Motion Planner for Legged Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose an online motion planner for legged robot locomotion with the primary objective of achieving energy efficiency. The conceptual idea is to leverage a placement set of footstep positions based on the robot's body position to determine when and how to execute steps. In particular, the proposed planner uses virtual placement sets beneath the hip joints of the legs and executes a step when the foot is outside of such placement set. Furthermore, we propose a parameter design framework that considers both energy-efficiency and robustness measures to optimize the gait by changing the shape of the placement set along with other parameters, such as step height and swing time, as a function of walking speed. We show that the planner produces trajectories that have a low Cost of Transport (CoT) and high robustness measure, and evaluate our approach against model-free Reinforcement Learning (RL) and motion imitation using biological dog motion priors as the reference. Overall, within low to medium velocity range, we show a 50.4% improvement in CoT and improved robustness over model-free RL, our best performing baseline. Finally, we show ability to handle slippery surfaces, gait transitions, and disturbances in simulation and hardware with the Unitree A1 robot.
<div id='section'>Paperid: <span id='pid'>1577, <a href='https://arxiv.org/pdf/2503.04257.pdf' target='_blank'>https://arxiv.org/pdf/2503.04257.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wonkwang Lee, Jongwon Jeong, Taehong Moon, Hyeon-Jong Kim, Jaehyeon Kim, Gunhee Kim, Byeong-Uk Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.04257">How to Move Your Dragon: Text-to-Motion Synthesis for Large-Vocabulary Objects</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion synthesis for diverse object categories holds great potential for 3D content creation but remains underexplored due to two key challenges: (1) the lack of comprehensive motion datasets that include a wide range of high-quality motions and annotations, and (2) the absence of methods capable of handling heterogeneous skeletal templates from diverse objects. To address these challenges, we contribute the following: First, we augment the Truebones Zoo dataset, a high-quality animal motion dataset covering over 70 species, by annotating it with detailed text descriptions, making it suitable for text-based motion synthesis. Second, we introduce rig augmentation techniques that generate diverse motion data while preserving consistent dynamics, enabling models to adapt to various skeletal configurations. Finally, we redesign existing motion diffusion models to dynamically adapt to arbitrary skeletal templates, enabling motion synthesis for a diverse range of objects with varying structures. Experiments show that our method learns to generate high-fidelity motions from textual descriptions for diverse and even unseen objects, setting a strong foundation for motion synthesis across diverse object categories and skeletal templates. Qualitative results are available at: $\href{https://t2m4lvo.github.io}{https://t2m4lvo.github.io}$.
<div id='section'>Paperid: <span id='pid'>1578, <a href='https://arxiv.org/pdf/2503.00576.pdf' target='_blank'>https://arxiv.org/pdf/2503.00576.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gerard GÃ³mez-Izquierdo, Javier Laplaza, Alberto Sanfeliu, AnaÃ­s Garrell
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.00576">Enhancing Context-Aware Human Motion Prediction for Efficient Robot Handovers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate human motion prediction (HMP) is critical for seamless human-robot collaboration, particularly in handover tasks that require real-time adaptability. Despite the high accuracy of state-of-the-art models, their computational complexity limits practical deployment in real-world robotic applications. In this work, we enhance human motion forecasting for handover tasks by leveraging siMLPe [1], a lightweight yet powerful architecture, and introducing key improvements. Our approach, named IntentMotion incorporates intention-aware conditioning, task-specific loss functions, and a novel intention classifier, significantly improving motion prediction accuracy while maintaining efficiency. Experimental results demonstrate that our method reduces body loss error by over 50%, achieves 200x faster inference, and requires only 3% of the parameters compared to existing state-of-the-art HMP models. These advancements establish our framework as a highly efficient and scalable solution for real-time human-robot interaction.
<div id='section'>Paperid: <span id='pid'>1579, <a href='https://arxiv.org/pdf/2502.20037.pdf' target='_blank'>https://arxiv.org/pdf/2502.20037.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongyu Deng, Tianfan Xue, He Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.20037">FuseGrasp: Radar-Camera Fusion for Robotic Grasping of Transparent Objects</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Transparent objects are prevalent in everyday environments, but their distinct physical properties pose significant challenges for camera-guided robotic arms. Current research is mainly dependent on camera-only approaches, which often falter in suboptimal conditions, such as low-light environments. In response to this challenge, we present FuseGrasp, the first radar-camera fusion system tailored to enhance the transparent objects manipulation. FuseGrasp exploits the weak penetrating property of millimeter-wave (mmWave) signals, which causes transparent materials to appear opaque, and combines it with the precise motion control of a robotic arm to acquire high-quality mmWave radar images of transparent objects. The system employs a carefully designed deep neural network to fuse radar and camera imagery, thereby improving depth completion and elevating the success rate of object grasping. Nevertheless, training FuseGrasp effectively is non-trivial, due to limited radar image datasets for transparent objects. We address this issue utilizing large RGB-D dataset, and propose an effective two-stage training approach: we first pre-train FuseGrasp on a large public RGB-D dataset of transparent objects, then fine-tune it on a self-built small RGB-D-Radar dataset. Furthermore, as a byproduct, FuseGrasp can determine the composition of transparent objects, such as glass or plastic, leveraging the material identification capability of mmWave radar. This identification result facilitates the robotic arm in modulating its grip force appropriately. Extensive testing reveals that FuseGrasp significantly improves the accuracy of depth reconstruction and material identification for transparent objects. Moreover, real-world robotic trials have confirmed that FuseGrasp markedly enhances the handling of transparent items. A video demonstration of FuseGrasp is available at https://youtu.be/MWDqv0sRSok.
<div id='section'>Paperid: <span id='pid'>1580, <a href='https://arxiv.org/pdf/2502.19056.pdf' target='_blank'>https://arxiv.org/pdf/2502.19056.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Iliana Loi, Konstantinos Moustakas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.19056">Fatigue-PINN: Physics-Informed Fatigue-Driven Motion Modulation and Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fatigue modeling is essential for motion synthesis tasks to model human motions under fatigued conditions and biomechanical engineering applications, such as investigating the variations in movement patterns and posture due to fatigue, defining injury risk mitigation and prevention strategies, formulating fatigue minimization schemes and creating improved ergonomic designs. Nevertheless, employing data-driven methods for synthesizing the impact of fatigue on motion, receives little to no attention in the literature. In this work, we present Fatigue-PINN, a deep learning framework based on Physics-Informed Neural Networks, for modeling fatigued human movements, while providing joint-specific fatigue configurations for adaptation and mitigation of motion artifacts on a joint level, resulting in more realistic animations. To account for muscle fatigue, we simulate the fatigue-induced fluctuations in the maximum exerted joint torques by leveraging a PINN adaptation of the Three-Compartment Controller model to exploit physics-domain knowledge for improving accuracy. This model also introduces parametric motion alignment with respect to joint-specific fatigue, hence avoiding sharp frame transitions. Our results indicate that Fatigue-PINN accurately simulates the effects of externally perceived fatigue on open-type human movements being consistent with findings from real-world experimental fatigue studies. Since fatigue is incorporated in torque space, Fatigue-PINN provides an end-to-end encoder-decoder-like architecture, to ensure transforming joint angles to joint torques and vice-versa, thus, being compatible with motion synthesis frameworks operating on joint angles.
<div id='section'>Paperid: <span id='pid'>1581, <a href='https://arxiv.org/pdf/2502.18696.pdf' target='_blank'>https://arxiv.org/pdf/2502.18696.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Christos Papandreou, Michail Mathioudakis, Theodoros Stouraitis, Petros Iatropoulos, Antonios Nikitakis, Stavros Paschalakis, Konstantinos Kyriakopoulos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.18696">Interpretable Data-Driven Ship Dynamics Model: Enhancing Physics-Based Motion Prediction with Parameter Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The deployment of autonomous navigation systems on ships necessitates accurate motion prediction models tailored to individual vessels. Traditional physics-based models, while grounded in hydrodynamic principles, often fail to account for ship-specific behaviors under real-world conditions. Conversely, purely data-driven models offer specificity but lack interpretability and robustness in edge cases. This study proposes a data-driven physics-based model that integrates physics-based equations with data-driven parameter optimization, leveraging the strengths of both approaches to ensure interpretability and adaptability. The model incorporates physics-based components such as 3-DoF dynamics, rudder, and propeller forces, while parameters such as resistance curve and rudder coefficients are optimized using synthetic data. By embedding domain knowledge into the parameter optimization process, the fitted model maintains physical consistency. Validation of the approach is realized with two container ships by comparing, both qualitatively and quantitatively, predictions against ground-truth trajectories. The results demonstrate significant improvements, in predictive accuracy and reliability, of the data-driven physics-based models over baseline physics-based models tuned with traditional marine engineering practices. The fitted models capture ship-specific behaviors in diverse conditions with their predictions being, 51.6% (ship A) and 57.8% (ship B) more accurate, 72.36% (ship A) and 89.67% (ship B) more consistent.
<div id='section'>Paperid: <span id='pid'>1582, <a href='https://arxiv.org/pdf/2502.02817.pdf' target='_blank'>https://arxiv.org/pdf/2502.02817.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Yin, Paritosh Parmar, Daoliang Xu, Yang Zhang, Tianyou Zheng, Weiwei Fu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.02817">A Decade of Action Quality Assessment: Largest Systematic Survey of Trends, Challenges, and Future Directions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Action Quality Assessment (AQA) -- the ability to quantify the quality of human motion, actions, or skill levels and provide feedback -- has far-reaching implications in areas such as low-cost physiotherapy, sports training, and workforce development. As such, it has become a critical field in computer vision & video understanding over the past decade. Significant progress has been made in AQA methodologies, datasets, & applications, yet a pressing need remains for a comprehensive synthesis of this rapidly evolving field. In this paper, we present a thorough survey of the AQA landscape, systematically reviewing over 200 research papers using the preferred reporting items for systematic reviews & meta-analyses (PRISMA) framework. We begin by covering foundational concepts & definitions, then move to general frameworks & performance metrics, & finally discuss the latest advances in methodologies & datasets. This survey provides a detailed analysis of research trends, performance comparisons, challenges, & future directions. Through this work, we aim to offer a valuable resource for both newcomers & experienced researchers, promoting further exploration & progress in AQA. Data are available at https://haoyin116.github.io/Survey_of_AQA/
<div id='section'>Paperid: <span id='pid'>1583, <a href='https://arxiv.org/pdf/2502.02358.pdf' target='_blank'>https://arxiv.org/pdf/2502.02358.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyan Guo, Zeyu Hu, De Wen Soh, Na Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.02358">MotionLab: Unified Human Motion Generation and Editing via the Motion-Condition-Motion Paradigm</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion generation and editing are key components of computer vision. However, current approaches in this field tend to offer isolated solutions tailored to specific tasks, which can be inefficient and impractical for real-world applications. While some efforts have aimed to unify motion-related tasks, these methods simply use different modalities as conditions to guide motion generation. Consequently, they lack editing capabilities, fine-grained control, and fail to facilitate knowledge sharing across tasks. To address these limitations and provide a versatile, unified framework capable of handling both human motion generation and editing, we introduce a novel paradigm: \textbf{Motion-Condition-Motion}, which enables the unified formulation of diverse tasks with three concepts: source motion, condition, and target motion. Based on this paradigm, we propose a unified framework, \textbf{MotionLab}, which incorporates rectified flows to learn the mapping from source motion to target motion, guided by the specified conditions. In MotionLab, we introduce the 1) MotionFlow Transformer to enhance conditional generation and editing without task-specific modules; 2) Aligned Rotational Position Encoding to guarantee the time synchronization between source motion and target motion; 3) Task Specified Instruction Modulation; and 4) Motion Curriculum Learning for effective multi-task learning and knowledge sharing across tasks. Notably, our MotionLab demonstrates promising generalization capabilities and inference efficiency across multiple benchmarks for human motion. Our code and additional video results are available at: https://diouo.github.io/motionlab.github.io/.
<div id='section'>Paperid: <span id='pid'>1584, <a href='https://arxiv.org/pdf/2501.13804.pdf' target='_blank'>https://arxiv.org/pdf/2501.13804.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Michail Mathioudakis, Christos Papandreou, Theodoros Stouraitis, Vicky Margari, Antonios Nikitakis, Stavros Paschalakis, Konstantinos Kyriakopoulos, Kostas J. Spyrou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.13804">Towards Real-World Validation of a Physics-Based Ship Motion Prediction Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The maritime industry aims towards a sustainable future, which requires significant improvements in operational efficiency. Current approaches focus on minimising fuel consumption and emissions through greater autonomy. Efficient and safe autonomous navigation requires high-fidelity ship motion models applicable to real-world conditions. Although physics-based ship motion models can predict ships' motion with sub-second resolution, their validation in real-world conditions is rarely found in the literature. This study presents a physics-based 3D dynamics motion model that is tailored to a container-ship, and compares its predictions against real-world voyages. The model integrates vessel motion over time and accounts for its hydrodynamic behavior under different environmental conditions. The model's predictions are evaluated against real vessel data both visually and using multiple distance measures. Both methodologies demonstrate that the model's predictions align closely with the real-world trajectories of the container-ship.
<div id='section'>Paperid: <span id='pid'>1585, <a href='https://arxiv.org/pdf/2501.08609.pdf' target='_blank'>https://arxiv.org/pdf/2501.08609.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaleab A. Kinfu, Carolina Pacheco, Alice D. Sperry, Deana Crocetti, Bahar TunÃ§genÃ§, Stewart H. Mostofsky, RenÃ© Vidal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.08609">Computerized Assessment of Motor Imitation for Distinguishing Autism in Video (CAMI-2DNet)</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motor imitation impairments are commonly reported in individuals with autism spectrum conditions (ASCs), suggesting that motor imitation could be used as a phenotype for addressing autism heterogeneity. Traditional methods for assessing motor imitation are subjective, labor-intensive, and require extensive human training. Modern Computerized Assessment of Motor Imitation (CAMI) methods, such as CAMI-3D for motion capture data and CAMI-2D for video data, are less subjective. However, they rely on labor-intensive data normalization and cleaning techniques, and human annotations for algorithm training. To address these challenges, we propose CAMI-2DNet, a scalable and interpretable deep learning-based approach to motor imitation assessment in video data, which eliminates the need for data normalization, cleaning and annotation. CAMI-2DNet uses an encoder-decoder architecture to map a video to a motion encoding that is disentangled from nuisance factors such as body shape and camera views. To learn a disentangled representation, we employ synthetic data generated by motion retargeting of virtual characters through the reshuffling of motion, body shape, and camera views, as well as real participant data. To automatically assess how well an individual imitates an actor, we compute a similarity score between their motion encodings, and use it to discriminate individuals with ASCs from neurotypical (NT) individuals. Our comparative analysis demonstrates that CAMI-2DNet has a strong correlation with human scores while outperforming CAMI-2D in discriminating ASC vs NT children. Moreover, CAMI-2DNet performs comparably to CAMI-3D while offering greater practicality by operating directly on video data and without the need for ad-hoc data normalization and human annotations.
<div id='section'>Paperid: <span id='pid'>1586, <a href='https://arxiv.org/pdf/2501.05744.pdf' target='_blank'>https://arxiv.org/pdf/2501.05744.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Loay Rashid, Siddharth Roheda, Amit Unde
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.05744">LLVD: LSTM-based Explicit Motion Modeling in Latent Space for Blind Video Denoising</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video restoration plays a pivotal role in revitalizing degraded video content by rectifying imperfections caused by various degradations introduced during capturing (sensor noise, motion blur, etc.), saving/sharing (compression, resizing, etc.) and editing. This paper introduces a novel algorithm designed for scenarios where noise is introduced during video capture, aiming to enhance the visual quality of videos by reducing unwanted noise artifacts. We propose the Latent space LSTM Video Denoiser (LLVD), an end-to-end blind denoising model. LLVD uniquely combines spatial and temporal feature extraction, employing Long Short Term Memory (LSTM) within the encoded feature domain. This integration of LSTM layers is crucial for maintaining continuity and minimizing flicker in the restored video. Moreover, processing frames in the encoded feature domain significantly reduces computations, resulting in a very lightweight architecture. LLVD's blind nature makes it versatile for real, in-the-wild denoising scenarios where prior information about noise characteristics is not available. Experiments reveal that LLVD demonstrates excellent performance for both synthetic and captured noise. Specifically, LLVD surpasses the current State-Of-The-Art (SOTA) in RAW denoising by 0.3dB, while also achieving a 59\% reduction in computational complexity.
<div id='section'>Paperid: <span id='pid'>1587, <a href='https://arxiv.org/pdf/2412.17344.pdf' target='_blank'>https://arxiv.org/pdf/2412.17344.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Akane Tsuboya, Yu Kono, Tatsuji Takahashi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.17344">Reinforcement Learning with a Focus on Adjusting Policies to Reach Targets</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The objective of a reinforcement learning agent is to discover better actions through exploration. However, typical exploration techniques aim to maximize rewards, often incurring high costs in both exploration and learning processes. We propose a novel deep reinforcement learning method, which prioritizes achieving an aspiration level over maximizing expected return. This method flexibly adjusts the degree of exploration based on the proportion of target achievement. Through experiments on a motion control task and a navigation task, this method achieved returns equal to or greater than other standard methods. The results of the analysis showed two things: our method flexibly adjusts the exploration scope, and it has the potential to enable the agent to adapt to non-stationary environments. These findings indicated that this method may have effectiveness in improving exploration efficiency in practical applications of reinforcement learning.
<div id='section'>Paperid: <span id='pid'>1588, <a href='https://arxiv.org/pdf/2412.14088.pdf' target='_blank'>https://arxiv.org/pdf/2412.14088.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lucas Dal'Col, Miguel Oliveira, VÃ­tor Santos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.14088">Joint Perception and Prediction for Autonomous Driving: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Perception and prediction modules are critical components of autonomous driving systems, enabling vehicles to navigate safely through complex environments. The perception module is responsible for perceiving the environment, including static and dynamic objects, while the prediction module is responsible for predicting the future behavior of these objects. These modules are typically divided into three tasks: object detection, object tracking, and motion prediction. Traditionally, these tasks are developed and optimized independently, with outputs passed sequentially from one to the next. However, this approach has significant limitations: computational resources are not shared across tasks, the lack of joint optimization can amplify errors as they propagate throughout the pipeline, and uncertainty is rarely propagated between modules, resulting in significant information loss. To address these challenges, the joint perception and prediction paradigm has emerged, integrating perception and prediction into a unified model through multi-task learning. This strategy not only overcomes the limitations of previous methods, but also enables the three tasks to have direct access to raw sensor data, allowing richer and more nuanced environmental interpretations. This paper presents the first comprehensive survey of joint perception and prediction for autonomous driving. We propose a taxonomy that categorizes approaches based on input representation, scene context modeling, and output representation, highlighting their contributions and limitations. Additionally, we present a qualitative analysis and quantitative comparison of existing methods. Finally, we discuss future research directions based on identified gaps in the state-of-the-art.
<div id='section'>Paperid: <span id='pid'>1589, <a href='https://arxiv.org/pdf/2412.11360.pdf' target='_blank'>https://arxiv.org/pdf/2412.11360.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ehsan Asali, Prashant Doshi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.11360">Visual IRL for Human-Like Robotic Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a novel method for collaborative robots (cobots) to learn manipulation tasks and perform them in a human-like manner. Our method falls under the learn-from-observation (LfO) paradigm, where robots learn to perform tasks by observing human actions, which facilitates quicker integration into industrial settings compared to programming from scratch. We introduce Visual IRL that uses the RGB-D keypoints in each frame of the observed human task performance directly as state features, which are input to inverse reinforcement learning (IRL). The inversely learned reward function, which maps keypoints to reward values, is transferred from the human to the cobot using a novel neuro-symbolic dynamics model, which maps human kinematics to the cobot arm. This model allows similar end-effector positioning while minimizing joint adjustments, aiming to preserve the natural dynamics of human motion in robotic manipulation. In contrast with previous techniques that focus on end-effector placement only, our method maps multiple joint angles of the human arm to the corresponding cobot joints. Moreover, it uses an inverse kinematics model to then minimally adjust the joint angles, for accurate end-effector positioning. We evaluate the performance of this approach on two different realistic manipulation tasks. The first task is produce processing, which involves picking, inspecting, and placing onions based on whether they are blemished. The second task is liquid pouring, where the robot picks up bottles, pours the contents into designated containers, and disposes of the empty bottles. Our results demonstrate advances in human-like robotic manipulation, leading to more human-robot compatibility in manufacturing applications.
<div id='section'>Paperid: <span id='pid'>1590, <a href='https://arxiv.org/pdf/2412.05277.pdf' target='_blank'>https://arxiv.org/pdf/2412.05277.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hee Jae Kim, Kathakoli Sengupta, Masaki Kuribayashi, Hernisa Kacorri, Eshed Ohn-Bar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.05277">Text to Blind Motion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>People who are blind perceive the world differently than those who are sighted, which can result in distinct motion characteristics. For instance, when crossing at an intersection, blind individuals may have different patterns of movement, such as veering more from a straight path or using touch-based exploration around curbs and obstacles. These behaviors may appear less predictable to motion models embedded in technologies such as autonomous vehicles. Yet, the ability of 3D motion models to capture such behavior has not been previously studied, as existing datasets for 3D human motion currently lack diversity and are biased toward people who are sighted. In this work, we introduce BlindWays, the first multimodal motion benchmark for pedestrians who are blind. We collect 3D motion data using wearable sensors with 11 blind participants navigating eight different routes in a real-world urban setting. Additionally, we provide rich textual descriptions that capture the distinctive movement characteristics of blind pedestrians and their interactions with both the navigation aid (e.g., a white cane or a guide dog) and the environment. We benchmark state-of-the-art 3D human prediction models, finding poor performance with off-the-shelf and pre-training-based methods for our novel task. To contribute toward safer and more reliable systems that can seamlessly reason over diverse human movements in their environments, our text-and-motion benchmark is available at https://blindways.github.io.
<div id='section'>Paperid: <span id='pid'>1591, <a href='https://arxiv.org/pdf/2412.04649.pdf' target='_blank'>https://arxiv.org/pdf/2412.04649.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Simone Borelli, Francesco Giovinazzo, Francesco Grella, Giorgio Cannata
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.04649">Generating Whole-Body Avoidance Motion through Localized Proximity Sensing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a novel control algorithm for robotic manipulators in unstructured environments using proximity sensors partially distributed on the platform. The proposed approach exploits arrays of multi zone Time-of-Flight (ToF) sensors to generate a sparse point cloud representation of the robot surroundings. By employing computational geometry techniques, we fuse the knowledge of robot geometric model with ToFs sensory feedback to generate whole-body motion tasks, allowing to move both sensorized and non-sensorized links in response to unpredictable events such as human motion. In particular, the proposed algorithm computes the pair of closest points between the environment cloud and the robot links, generating a dynamic avoidance motion that is implemented as the highest priority task in a two-level hierarchical architecture. Such a design choice allows the robot to work safely alongside humans even without a complete sensorization over the whole surface. Experimental validation demonstrates the algorithm effectiveness both in static and dynamic scenarios, achieving comparable performances with respect to well established control techniques that aim to move the sensors mounting positions on the robot body. The presented algorithm exploits any arbitrary point on the robot surface to perform avoidance motion, showing improvements in the distance margin up to 100 mm, due to the rendering of virtual avoidance tasks on non-sensorized links.
<div id='section'>Paperid: <span id='pid'>1592, <a href='https://arxiv.org/pdf/2411.18377.pdf' target='_blank'>https://arxiv.org/pdf/2411.18377.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Denys Rozumnyi, Nadine Bertsch, Othman Sbai, Filippo Arcadu, Yuhua Chen, Artsiom Sanakoyeu, Manoj Kumar, Catherine Herold, Robin Kips
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.18377">XR-MBT: Multi-modal Full Body Tracking for XR through Self-Supervision with Learned Depth Point Cloud Registration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tracking the full body motions of users in XR (AR/VR) devices is a fundamental challenge to bring a sense of authentic social presence. Due to the absence of dedicated leg sensors, currently available body tracking methods adopt a synthesis approach to generate plausible motions given a 3-point signal from the head and controller tracking. In order to enable mixed reality features, modern XR devices are capable of estimating depth information of the headset surroundings using available sensors combined with dedicated machine learning models. Such egocentric depth sensing cannot drive the body directly, as it is not registered and is incomplete due to limited field-of-view and body self-occlusions. For the first time, we propose to leverage the available depth sensing signal combined with self-supervision to learn a multi-modal pose estimation model capable of tracking full body motions in real time on XR devices. We demonstrate how current 3-point motion synthesis models can be extended to point cloud modalities using a semantic point cloud encoder network combined with a residual network for multi-modal pose estimation. These modules are trained jointly in a self-supervised way, leveraging a combination of real unregistered point clouds and simulated data obtained from motion capture. We compare our approach against several state-of-the-art systems for XR body tracking and show that our method accurately tracks a diverse range of body motions. XR-MBT tracks legs in XR for the first time, whereas traditional synthesis approaches based on partial body tracking are blind.
<div id='section'>Paperid: <span id='pid'>1593, <a href='https://arxiv.org/pdf/2411.13856.pdf' target='_blank'>https://arxiv.org/pdf/2411.13856.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dexian Ma, Yirong Liu, Wenbo Liu, Bo Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.13856">A Data-Driven Modeling and Motion Control of Heavy-Load Hydraulic Manipulators via Reversible Transformation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work proposes a data-driven modeling and the corresponding hybrid motion control framework for unmanned and automated operation of industrial heavy-load hydraulic manipulator. Rather than the direct use of a neural network black box, we construct a reversible nonlinear model by using multilayer perceptron to approximate dynamics in the physical integrator chain system after reversible transformations. The reversible nonlinear model is trained offline using supervised learning techniques, and the data are obtained from simulations or experiments. Entire hybrid motion control framework consists of the model inversion controller that compensates for the nonlinear dynamics and proportional-derivative controller that enhances the robustness. The stability is proved with Lyapunov theory. Co-simulation and Experiments show the effectiveness of proposed modeling and hybrid control framework. With a commercial 39-ton class hydraulic excavator for motion control tasks, the root mean square error of trajectory tracking error decreases by at least 50\% compared to traditional control methods. In addition, by analyzing the system model, the proposed framework can be rapidly applied to different control plants.
<div id='section'>Paperid: <span id='pid'>1594, <a href='https://arxiv.org/pdf/2411.04151.pdf' target='_blank'>https://arxiv.org/pdf/2411.04151.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kehua Qu, Rui Ding, Jin Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.04151">UnityGraph: Unified Learning of Spatio-temporal features for Multi-person Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-person motion prediction is a complex and emerging field with significant real-world applications. Current state-of-the-art methods typically adopt dual-path networks to separately modeling spatial features and temporal features. However, the uncertain compatibility of the two networks brings a challenge for spatio-temporal features fusion and violate the spatio-temporal coherence and coupling of human motions by nature. To address this issue, we propose a novel graph structure, UnityGraph, which treats spatio-temporal features as a whole, enhancing model coherence and coupling.spatio-temporal features as a whole, enhancing model coherence and coupling. Specifically, UnityGraph is a hypervariate graph based network. The flexibility of the hypergraph allows us to consider the observed motions as graph nodes. We then leverage hyperedges to bridge these nodes for exploring spatio-temporal features. This perspective considers spatio-temporal dynamics unitedly and reformulates multi-person motion prediction into a problem on a single graph. Leveraging the dynamic message passing based on this hypergraph, our model dynamically learns from both types of relations to generate targeted messages that reflect the relevance among nodes. Extensive experiments on several datasets demonstrates that our method achieves state-of-the-art performance, confirming its effectiveness and innovative design.
<div id='section'>Paperid: <span id='pid'>1595, <a href='https://arxiv.org/pdf/2411.03729.pdf' target='_blank'>https://arxiv.org/pdf/2411.03729.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kehua Qu, Rui Ding, Jin Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.03729">Relation Learning and Aggregate-attention for Multi-person Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-person motion prediction is an emerging and intricate task with broad real-world applications. Unlike single person motion prediction, it considers not just the skeleton structures or human trajectories but also the interactions between others. Previous methods use various networks to achieve impressive predictions but often overlook that the joints relations within an individual (intra-relation) and interactions among groups (inter-relation) are distinct types of representations. These methods often lack explicit representation of inter&intra-relations, and inevitably introduce undesired dependencies. To address this issue, we introduce a new collaborative framework for multi-person motion prediction that explicitly modeling these relations:a GCN-based network for intra-relations and a novel reasoning network for inter-relations.Moreover, we propose a novel plug-and-play aggregation module called the Interaction Aggregation Module (IAM), which employs an aggregate-attention mechanism to seamlessly integrate these relations. Experiments indicate that the module can also be applied to other dual-path models. Extensive experiments on the 3DPW, 3DPW-RC, CMU-Mocap, MuPoTS-3D, as well as synthesized datasets Mix1 & Mix2 (9 to 15 persons), demonstrate that our method achieves state-of-the-art performance.
<div id='section'>Paperid: <span id='pid'>1596, <a href='https://arxiv.org/pdf/2410.19606.pdf' target='_blank'>https://arxiv.org/pdf/2410.19606.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kai-Yin Hong, Chieh-Chih Wang, Wen-Chieh Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.19606">Multi-modal Motion Prediction using Temporal Ensembling with Learning-based Aggregation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent years have seen a shift towards learning-based methods for trajectory prediction, with challenges remaining in addressing uncertainty and capturing multi-modal distributions. This paper introduces Temporal Ensembling with Learning-based Aggregation, a meta-algorithm designed to mitigate the issue of missing behaviors in trajectory prediction, which leads to inconsistent predictions across consecutive frames. Unlike conventional model ensembling, temporal ensembling leverages predictions from nearby frames to enhance spatial coverage and prediction diversity. By confirming predictions from multiple frames, temporal ensembling compensates for occasional errors in individual frame predictions. Furthermore, trajectory-level aggregation, often utilized in model ensembling, is insufficient for temporal ensembling due to a lack of consideration of traffic context and its tendency to assign candidate trajectories with incorrect driving behaviors to final predictions. We further emphasize the necessity of learning-based aggregation by utilizing mode queries within a DETR-like architecture for our temporal ensembling, leveraging the characteristics of predictions from nearby frames. Our method, validated on the Argoverse 2 dataset, shows notable improvements: a 4% reduction in minADE, a 5% decrease in minFDE, and a 1.16% reduction in the miss rate compared to the strongest baseline, QCNet, highlighting its efficacy and potential in autonomous driving.
<div id='section'>Paperid: <span id='pid'>1597, <a href='https://arxiv.org/pdf/2410.12023.pdf' target='_blank'>https://arxiv.org/pdf/2410.12023.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mykhaylo Andriluka, Baruch Tabanpour, C. Daniel Freeman, Cristian Sminchisescu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.12023">Learned Neural Physics Simulation for Articulated 3D Human Pose Reconstruction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a novel neural network approach, LARP (Learned Articulated Rigid body Physics), to model the dynamics of articulated human motion with contact. Our goal is to develop a faster and more convenient methodological alternative to traditional physics simulators for use in computer vision tasks such as human motion reconstruction from video. To that end we introduce a training procedure and model components that support the construction of a recurrent neural architecture to accurately simulate articulated rigid body dynamics. Our neural architecture supports features typically found in traditional physics simulators, such as modeling of joint motors, variable dimensions of body parts, contact between body parts and objects, and is an order of magnitude faster than traditional systems when multiple simulations are run in parallel. To demonstrate the value of LARP we use it as a drop-in replacement for a state of the art classical non-differentiable simulator in an existing video-based reconstruction framework and show comparative or better 3D human pose reconstruction accuracy.
<div id='section'>Paperid: <span id='pid'>1598, <a href='https://arxiv.org/pdf/2410.11491.pdf' target='_blank'>https://arxiv.org/pdf/2410.11491.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Niklas Gunnarsson, Jens SjÃ¶lund, Peter Kimstrand, Thomas. B SchÃ¶n
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.11491">Online learning in motion modeling for intra-interventional image sequences</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image monitoring and guidance during medical examinations can aid both diagnosis and treatment. However, the sampling frequency is often too low, which creates a need to estimate the missing images. We present a probabilistic motion model for sequential medical images, with the ability to both estimate motion between acquired images and forecast the motion ahead of time. The core is a low-dimensional temporal process based on a linear Gaussian state-space model with analytically tractable solutions for forecasting, simulation, and imputation of missing samples. The results, from two experiments on publicly available cardiac datasets, show reliable motion estimates and an improved forecasting performance using patient-specific adaptation by online learning.
<div id='section'>Paperid: <span id='pid'>1599, <a href='https://arxiv.org/pdf/2410.11404.pdf' target='_blank'>https://arxiv.org/pdf/2410.11404.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiawei Mo, Yixuan Chen, Rifen Lin, Yongkang Ni, Min Zeng, Xiping Hu, Min Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.11404">MoChat: Joints-Grouped Spatio-Temporal Grounding LLM for Multi-Turn Motion Comprehension and Description</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite continuous advancements in deep learning for understanding human motion, existing models often struggle to accurately identify action timing and specific body parts, typically supporting only single-round interaction. Such limitations in capturing fine-grained motion details reduce their effectiveness in motion understanding tasks. In this paper, we propose MoChat, a multimodal large language model capable of spatio-temporal grounding of human motion and understanding multi-turn dialogue context. To achieve these capabilities, we group the spatial information of each skeleton frame based on human anatomical structure and then apply them with Joints-Grouped Skeleton Encoder, whose outputs are combined with LLM embeddings to create spatio-aware and temporal-aware embeddings separately. Additionally, we develop a pipeline for extracting timestamps from skeleton sequences based on textual annotations, and construct multi-turn dialogues for spatially grounding. Finally, various task instructions are generated for jointly training. Experimental results demonstrate that MoChat achieves state-of-the-art performance across multiple metrics in motion understanding tasks, making it as the first model capable of fine-grained spatio-temporal grounding of human motion.
<div id='section'>Paperid: <span id='pid'>1600, <a href='https://arxiv.org/pdf/2410.08931.pdf' target='_blank'>https://arxiv.org/pdf/2410.08931.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Clayton Leite, Yu Xiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.08931">Enhancing Motion Variation in Text-to-Motion Models via Pose and Video Conditioned Editing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-to-motion models that generate sequences of human poses from textual descriptions are garnering significant attention. However, due to data scarcity, the range of motions these models can produce is still limited. For instance, current text-to-motion models cannot generate a motion of kicking a football with the instep of the foot, since the training data only includes martial arts kicks. We propose a novel method that uses short video clips or images as conditions to modify existing basic motions. In this approach, the model's understanding of a kick serves as the prior, while the video or image of a football kick acts as the posterior, enabling the generation of the desired motion. By incorporating these additional modalities as conditions, our method can create motions not present in the training set, overcoming the limitations of text-motion datasets. A user study with 26 participants demonstrated that our approach produces unseen motions with realism comparable to commonly represented motions in text-motion datasets (e.g., HumanML3D), such as walking, running, squatting, and kicking.
<div id='section'>Paperid: <span id='pid'>1601, <a href='https://arxiv.org/pdf/2409.17790.pdf' target='_blank'>https://arxiv.org/pdf/2409.17790.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Harsh Yadav, Maximilian Schaefer, Kun Zhao, Tobias Meisen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.17790">CASPFormer: Trajectory Prediction from BEV Images with Deformable Attention</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion prediction is an important aspect for Autonomous Driving (AD) and Advance Driver Assistance Systems (ADAS). Current state-of-the-art motion prediction methods rely on High Definition (HD) maps for capturing the surrounding context of the ego vehicle. Such systems lack scalability in real-world deployment as HD maps are expensive to produce and update in real-time. To overcome this issue, we propose Context Aware Scene Prediction Transformer (CASPFormer), which can perform multi-modal motion prediction from rasterized Bird-Eye-View (BEV) images. Our system can be integrated with any upstream perception module that is capable of generating BEV images. Moreover, CASPFormer directly decodes vectorized trajectories without any postprocessing. Trajectories are decoded recurrently using deformable attention, as it is computationally efficient and provides the network with the ability to focus its attention on the important spatial locations of the BEV images. In addition, we also address the issue of mode collapse for generating multiple scene-consistent trajectories by incorporating learnable mode queries. We evaluate our model on the nuScenes dataset and show that it reaches state-of-the-art across multiple metrics
<div id='section'>Paperid: <span id='pid'>1602, <a href='https://arxiv.org/pdf/2409.15904.pdf' target='_blank'>https://arxiv.org/pdf/2409.15904.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chuqiao Li, Julian Chibane, Yannan He, Naama Pearl, Andreas Geiger, Gerard Pons-moll
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.15904">Unimotion: Unifying 3D Human Motion Synthesis and Understanding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce Unimotion, the first unified multi-task human motion model capable of both flexible motion control and frame-level motion understanding. While existing works control avatar motion with global text conditioning, or with fine-grained per frame scripts, none can do both at once. In addition, none of the existing works can output frame-level text paired with the generated poses. In contrast, Unimotion allows to control motion with global text, or local frame-level text, or both at once, providing more flexible control for users. Importantly, Unimotion is the first model which by design outputs local text paired with the generated poses, allowing users to know what motion happens and when, which is necessary for a wide range of applications. We show Unimotion opens up new applications: 1.) Hierarchical control, allowing users to specify motion at different levels of detail, 2.) Obtaining motion text descriptions for existing MoCap data or YouTube videos 3.) Allowing for editability, generating motion from text, and editing the motion via text edits. Moreover, Unimotion attains state-of-the-art results for the frame-level text-to-motion task on the established HumanML3D dataset. The pre-trained model and code are available available on our project page at https://coral79.github.io/uni-motion/.
<div id='section'>Paperid: <span id='pid'>1603, <a href='https://arxiv.org/pdf/2409.15564.pdf' target='_blank'>https://arxiv.org/pdf/2409.15564.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xingrui Gu, Chuyi Jiang, Erte Wang, Qiang Cui, Leimin Tian, Lianlong Wu, Siyang Song, Chuang Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.15564">CauSkelNet: Causal Representation Learning for Human Behaviour Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Traditional machine learning methods for movement recognition often struggle with limited model interpretability and a lack of insight into human movement dynamics. This study introduces a novel representation learning framework based on causal inference to address these challenges. Our two-stage approach combines the Peter-Clark (PC) algorithm and Kullback-Leibler (KL) divergence to identify and quantify causal relationships between human joints. By capturing joint interactions, the proposed causal Graph Convolutional Network (GCN) produces interpretable and robust representations. Experimental results on the EmoPain dataset demonstrate that the causal GCN outperforms traditional GCNs in accuracy, F1 score, and recall, particularly in detecting protective behaviors. This work contributes to advancing human motion analysis and lays a foundation for adaptive and intelligent healthcare solutions.
<div id='section'>Paperid: <span id='pid'>1604, <a href='https://arxiv.org/pdf/2409.12140.pdf' target='_blank'>https://arxiv.org/pdf/2409.12140.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sai Shashank Kalakonda, Shubh Maheshwari, Ravi Kiran Sarvadevabhatla
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.12140">MoRAG -- Multi-Fusion Retrieval Augmented Generation for Human Motion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce MoRAG, a novel multi-part fusion based retrieval-augmented generation strategy for text-based human motion generation. The method enhances motion diffusion models by leveraging additional knowledge obtained through an improved motion retrieval process. By effectively prompting large language models (LLMs), we address spelling errors and rephrasing issues in motion retrieval. Our approach utilizes a multi-part retrieval strategy to improve the generalizability of motion retrieval across the language space. We create diverse samples through the spatial composition of the retrieved motions. Furthermore, by utilizing low-level, part-specific motion information, we can construct motion samples for unseen text descriptions. Our experiments demonstrate that our framework can serve as a plug-and-play module, improving the performance of motion diffusion models. Code, pretrained models and sample videos are available at: https://motion-rag.github.io/
<div id='section'>Paperid: <span id='pid'>1605, <a href='https://arxiv.org/pdf/2409.11920.pdf' target='_blank'>https://arxiv.org/pdf/2409.11920.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lorenzo Mandelli, Stefano Berretti
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.11920">Generation of Complex 3D Human Motion by Temporal and Spatial Composition of Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we address the challenge of generating realistic 3D human motions for action classes that were never seen during the training phase. Our approach involves decomposing complex actions into simpler movements, specifically those observed during training, by leveraging the knowledge of human motion contained in GPTs models. These simpler movements are then combined into a single, realistic animation using the properties of diffusion models. Our claim is that this decomposition and subsequent recombination of simple movements can synthesize an animation that accurately represents the complex input action. This method operates during the inference phase and can be integrated with any pre-trained diffusion model, enabling the synthesis of motion classes not present in the training data. We evaluate our method by dividing two benchmark human motion datasets into basic and complex actions, and then compare its performance against the state-of-the-art.
<div id='section'>Paperid: <span id='pid'>1606, <a href='https://arxiv.org/pdf/2409.11623.pdf' target='_blank'>https://arxiv.org/pdf/2409.11623.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dayuan Tan, Mohamed Younis, Wassila Lalouani, Shuyao Fan, Guozhi Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.11623">A novel pedestrian road crossing simulator for dynamic traffic light scheduling systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The major advances in intelligent transportation systems are pushing societal services toward autonomy where road management is to be more agile in order to cope with changes and continue to yield optimal performance. However, the pedestrian experience is not sufficiently considered. Particularly, signalized intersections are expected to be popular if not dominant in urban settings where pedestrian density is high. This paper presents the design of a novel environment for simulating human motion on signalized crosswalks at a fine-grained level. Such a simulation not only captures typical behavior, but also handles cases where large pedestrian groups cross from both directions. The proposed simulator is instrumental for optimized road configuration management where the pedestrians' quality of experience, for example, waiting time, is factored in. The validation results using field data show that an accuracy of 98.37 percent can be obtained for the estimated crossing time. Other results using synthetic data show that our simulator enables optimized traffic light scheduling that diminishes pedestrians' waiting time without sacrificing vehicular throughput.
<div id='section'>Paperid: <span id='pid'>1607, <a href='https://arxiv.org/pdf/2409.07798.pdf' target='_blank'>https://arxiv.org/pdf/2409.07798.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Liang Feng, Zhixuan Shen, Lihua Wen, Shiyao Li, Ming Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.07798">GateAttentionPose: Enhancing Pose Estimation with Agent Attention and Improved Gated Convolutions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces GateAttentionPose, an innovative approach that enhances the UniRepLKNet architecture for pose estimation tasks. We present two key contributions: the Agent Attention module and the Gate-Enhanced Feedforward Block (GEFB). The Agent Attention module replaces large kernel convolutions, significantly improving computational efficiency while preserving global context modeling. The GEFB augments feature extraction and processing capabilities, particularly in complex scenes. Extensive evaluations on COCO and MPII datasets demonstrate that GateAttentionPose outperforms existing state-of-the-art methods, including the original UniRepLKNet, achieving superior or comparable results with improved efficiency. Our approach offers a robust solution for pose estimation across diverse applications, including autonomous driving, human motion capture, and virtual reality.
<div id='section'>Paperid: <span id='pid'>1608, <a href='https://arxiv.org/pdf/2409.07752.pdf' target='_blank'>https://arxiv.org/pdf/2409.07752.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Liang Feng, Ming Xu, Lihua Wen, Zhixuan Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.07752">GatedUniPose: A Novel Approach for Pose Estimation Combining UniRepLKNet and Gated Convolution</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pose estimation is a crucial task in computer vision, with wide applications in autonomous driving, human motion capture, and virtual reality. However, existing methods still face challenges in achieving high accuracy, particularly in complex scenes. This paper proposes a novel pose estimation method, GatedUniPose, which combines UniRepLKNet and Gated Convolution and introduces the GLACE module for embedding. Additionally, we enhance the feature map concatenation method in the head layer by using DySample upsampling. Compared to existing methods, GatedUniPose excels in handling complex scenes and occlusion challenges. Experimental results on the COCO, MPII, and CrowdPose datasets demonstrate that GatedUniPose achieves significant performance improvements with a relatively small number of parameters, yielding better or comparable results to models with similar or larger parameter sizes.
<div id='section'>Paperid: <span id='pid'>1609, <a href='https://arxiv.org/pdf/2409.07341.pdf' target='_blank'>https://arxiv.org/pdf/2409.07341.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Luo Ji, Runji Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.07341">Online Decision MetaMorphFormer: A Casual Transformer-Based Reinforcement Learning Framework of Universal Embodied Intelligence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Interactive artificial intelligence in the motion control field is an interesting topic, especially when universal knowledge is adaptive to multiple tasks and universal environments. Despite there being increasing efforts in the field of Reinforcement Learning (RL) with the aid of transformers, most of them might be limited by the offline training pipeline, which prohibits exploration and generalization abilities. To address this limitation, we propose the framework of Online Decision MetaMorphFormer (ODM) which aims to achieve self-awareness, environment recognition, and action planning through a unified model architecture. Motivated by cognitive and behavioral psychology, an ODM agent is able to learn from others, recognize the world, and practice itself based on its own experience. ODM can also be applied to any arbitrary agent with a multi-joint body, located in different environments, and trained with different types of tasks using large-scale pre-trained datasets. Through the use of pre-trained datasets, ODM can quickly warm up and learn the necessary knowledge to perform the desired task, while the target environment continues to reinforce the universal policy. Extensive online experiments as well as few-shot and zero-shot environmental tests are used to verify ODM's performance and generalization ability. The results of our study contribute to the study of general artificial intelligence in embodied and cognitive fields. Code, results, and video examples can be found on the website \url{https://rlodm.github.io/odm/}.
<div id='section'>Paperid: <span id='pid'>1610, <a href='https://arxiv.org/pdf/2409.06791.pdf' target='_blank'>https://arxiv.org/pdf/2409.06791.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Michael Adewole, Oluwaseyi Giwa, Favour Nerrise, Martins Osifeko, Ajibola Oyedeji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.06791">Human Motion Synthesis_ A Diffusion Approach for Motion Stitching and In-Betweening</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion generation is an important area of research in many fields. In this work, we tackle the problem of motion stitching and in-betweening. Current methods either require manual efforts, or are incapable of handling longer sequences. To address these challenges, we propose a diffusion model with a transformer-based denoiser to generate realistic human motion. Our method demonstrated strong performance in generating in-betweening sequences, transforming a variable number of input poses into smooth and realistic motion sequences consisting of 75 frames at 15 fps, resulting in a total duration of 5 seconds. We present the performance evaluation of our method using quantitative metrics such as Frechet Inception Distance (FID), Diversity, and Multimodality, along with visual assessments of the generated outputs.
<div id='section'>Paperid: <span id='pid'>1611, <a href='https://arxiv.org/pdf/2409.03421.pdf' target='_blank'>https://arxiv.org/pdf/2409.03421.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiong Yang, Hao Ren, Dong Guo, Zhengrong Ling, Tieshan Zhang, Gen Li, Yifeng Tang, Haoxiang Zhao, Jiale Wang, Hongyuan Chang, Jia Dong, Yajing Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.03421">F3T: A soft tactile unit with 3D force and temperature mathematical decoupling ability for robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The human skin exhibits remarkable capability to perceive contact forces and environmental temperatures, providing intricate information essential for nuanced manipulation. Despite recent advancements in soft tactile sensors, a significant challenge remains in accurately decoupling signals - specifically, separating force from directional orientation and temperature - resulting in fail to meet the advanced application requirements of robots. This research proposes a multi-layered soft sensor unit (F3T) designed to achieve isolated measurements and mathematical decoupling of normal pressure, omnidirectional tangential forces, and temperature. We developed a circular coaxial magnetic film featuring a floating-mountain multi-layer capacitor, facilitating the physical decoupling of normal and tangential forces in all directions. Additionally, we incorporated an ion gel-based temperature sensing film atop the tactile sensor. This sensor is resilient to external pressure and deformation, enabling it to measure temperature and, crucially, eliminate capacitor errors induced by environmental temperature changes. This innovative design allows for the decoupled measurement of multiple signals, paving the way for advancements in higher-level robot motion control, autonomous decision-making, and task planning.
<div id='section'>Paperid: <span id='pid'>1612, <a href='https://arxiv.org/pdf/2409.01591.pdf' target='_blank'>https://arxiv.org/pdf/2409.01591.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sohan Anisetty, James Hays
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.01591">Dynamic Motion Synthesis: Masked Audio-Text Conditioned Spatio-Temporal Transformers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Our research presents a novel motion generation framework designed to produce whole-body motion sequences conditioned on multiple modalities simultaneously, specifically text and audio inputs. Leveraging Vector Quantized Variational Autoencoders (VQVAEs) for motion discretization and a bidirectional Masked Language Modeling (MLM) strategy for efficient token prediction, our approach achieves improved processing efficiency and coherence in the generated motions. By integrating spatial attention mechanisms and a token critic we ensure consistency and naturalness in the generated motions. This framework expands the possibilities of motion generation, addressing the limitations of existing approaches and opening avenues for multimodal motion synthesis.
<div id='section'>Paperid: <span id='pid'>1613, <a href='https://arxiv.org/pdf/2409.01241.pdf' target='_blank'>https://arxiv.org/pdf/2409.01241.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sorin Grigorescu, Mihai Zaha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.01241">CyberCortex.AI: An AI-based Operating System for Autonomous Robotics and Complex Automation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The underlying framework for controlling autonomous robots and complex automation applications are Operating Systems (OS) capable of scheduling perception-and-control tasks, as well as providing real-time data communication to other robotic peers and remote cloud computers. In this paper, we introduce CyberCortex AI, a robotics OS designed to enable heterogeneous AI-based robotics and complex automation applications. CyberCortex AI is a decentralized distributed OS which enables robots to talk to each other, as well as to High Performance Computers (HPC) in the cloud. Sensory and control data from the robots is streamed towards HPC systems with the purpose of training AI algorithms, which are afterwards deployed on the robots. Each functionality of a robot (e.g. sensory data acquisition, path planning, motion control, etc.) is executed within a so-called DataBlock of Filters shared through the internet, where each filter is computed either locally on the robot itself, or remotely on a different robotic system. The data is stored and accessed via a so-called Temporal Addressable Memory (TAM), which acts as a gateway between each filter's input and output. CyberCortex AI has two main components: i) the CyberCortex AI inference system, which is a real-time implementation of the DataBlock running on the robots' embedded hardware, and ii) the CyberCortex AI dojo, which runs on an HPC computer in the cloud, and it is used to design, train and deploy AI algorithms. We present a quantitative and qualitative performance analysis of the proposed approach using two collaborative robotics applications: i) a forest fires prevention system based on an Unitree A1 legged robot and an Anafi Parrot 4K drone, as well as ii) an autonomous driving system which uses CyberCortex AI for collaborative perception and motion control.
<div id='section'>Paperid: <span id='pid'>1614, <a href='https://arxiv.org/pdf/2409.00449.pdf' target='_blank'>https://arxiv.org/pdf/2409.00449.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Longyun Liao, Rong Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.00449">ActionPose: Pretraining 3D Human Pose Estimation with the Dark Knowledge of Action</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>2D-to-3D human pose lifting is an ill-posed problem due to depth ambiguity and occlusion. Existing methods relying on spatial and temporal consistency alone are insufficient to resolve these problems because they lack semantic information of the motions. To overcome this, we propose ActionPose, a framework that leverages action knowledge by aligning motion embeddings with text embeddings of fine-grained action labels. ActionPose operates in two stages: pretraining and fine-tuning. In the pretraining stage, the model learns to recognize actions and reconstruct 3D poses from masked and noisy 2D poses. During the fine-tuning stage, the model is further refined using real-world 3D human pose estimation datasets without action labels. Additionally, our framework incorporates masked body parts and masked time windows in motion modeling to mitigate the effects of ambiguous boundaries between actions in both temporal and spatial domains. Experiments demonstrate the effectiveness of ActionPose, achieving state-of-the-art performance in 3D pose estimation on public datasets, including Human3.6M and MPI-INF-3DHP. Specifically, ActionPose achieves an MPJPE of 36.7mm on Human3.6M with detected 2D poses as input and 15.5mm on MPI-INF-3DHP with ground-truth 2D poses as input.
<div id='section'>Paperid: <span id='pid'>1615, <a href='https://arxiv.org/pdf/2408.14885.pdf' target='_blank'>https://arxiv.org/pdf/2408.14885.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sven Goblirsch, Marcel Weinmann, Johannes Betz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.14885">Three-Dimensional Vehicle Dynamics State Estimation for High-Speed Race Cars under varying Signal Quality</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work aims to present a three-dimensional vehicle dynamics state estimation under varying signal quality. Few researchers have investigated the impact of three-dimensional road geometries on the state estimation and, thus, neglect road inclination and banking. Especially considering high velocities and accelerations, the literature does not address these effects. Therefore, we compare two- and three-dimensional state estimation schemes to outline the impact of road geometries. We use an Extended Kalman Filter with a point-mass motion model and extend it by an additional formulation of reference angles. Furthermore, virtual velocity measurements significantly improve the estimation of road angles and the vehicle's side slip angle. We highlight the importance of steady estimations for vehicle motion control algorithms and demonstrate the challenges of degraded signal quality and Global Navigation Satellite System dropouts. The proposed adaptive covariance facilitates a smooth estimation and enables stable controller behavior. The developed state estimation has been deployed on a high-speed autonomous race car at various racetracks. Our findings indicate that our approach outperforms state-of-the-art vehicle dynamics state estimators and an industry-grade Inertial Navigation System. Further studies are needed to investigate the performance under varying track conditions and on other vehicle types.
<div id='section'>Paperid: <span id='pid'>1616, <a href='https://arxiv.org/pdf/2408.09409.pdf' target='_blank'>https://arxiv.org/pdf/2408.09409.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chen Long-fei, Subramanian Ramamoorthy, Robert B Fisher
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.09409">OPPH: A Vision-Based Operator for Measuring Body Movements for Personal Healthcare</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-based motion estimation methods show promise in accurately and unobtrusively estimating human body motion for healthcare purposes. However, these methods are not specifically designed for healthcare purposes and face challenges in real-world applications. Human pose estimation methods often lack the accuracy needed for detecting fine-grained, subtle body movements, while optical flow-based methods struggle with poor lighting conditions and unseen real-world data. These issues result in human body motion estimation errors, particularly during critical medical situations where the body is motionless, such as during unconsciousness. To address these challenges and improve the accuracy of human body motion estimation for healthcare purposes, we propose the OPPH operator designed to enhance current vision-based motion estimation methods. This operator, which considers human body movement and noise properties, functions as a multi-stage filter. Results tested on two real-world and one synthetic human motion dataset demonstrate that the operator effectively removes real-world noise, significantly enhances the detection of motionless states, maintains the accuracy of estimating active body movements, and maintains long-term body movement trends. This method could be beneficial for analyzing both critical medical events and chronic medical conditions.
<div id='section'>Paperid: <span id='pid'>1617, <a href='https://arxiv.org/pdf/2408.05940.pdf' target='_blank'>https://arxiv.org/pdf/2408.05940.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Eunsoo Im, Changhyun Jee, Jung Kwon Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.05940">Spb3DTracker: A Robust LiDAR-Based Person Tracker for Noisy Environment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Person detection and tracking (PDT) has seen significant advancements with 2D camera-based systems in the autonomous vehicle field, leading to widespread adoption of these algorithms. However, growing privacy concerns have recently emerged as a major issue, prompting a shift towards LiDAR-based PDT as a viable alternative. Within this domain, "Tracking-by-Detection" (TBD) has become a prominent methodology. Despite its effectiveness, LiDAR-based PDT has not yet achieved the same level of performance as camera-based PDT. This paper examines key components of the LiDAR-based PDT framework, including detection post-processing, data association, motion modeling, and lifecycle management. Building upon these insights, we introduce SpbTrack, a robust person tracker designed for diverse environments. Our method achieves superior performance on noisy datasets and state-of-the-art results on KITTI Dataset benchmarks and custom office indoor dataset among LiDAR-based trackers.
<div id='section'>Paperid: <span id='pid'>1618, <a href='https://arxiv.org/pdf/2408.02855.pdf' target='_blank'>https://arxiv.org/pdf/2408.02855.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aleksa Marusic, Louis Annabi, Sao Msi Nguyen, Adriana Tapus
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.02855">Analyzing Data Efficiency and Performance of Machine Learning Algorithms for Assessing Low Back Pain Physical Rehabilitation Exercises</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Analyzing human motion is an active research area, with various applications. In this work, we focus on human motion analysis in the context of physical rehabilitation using a robot coach system. Computer-aided assessment of physical rehabilitation entails evaluation of patient performance in completing prescribed rehabilitation exercises, based on processing movement data captured with a sensory system, such as RGB and RGB-D cameras. As 2D and 3D human pose estimation from RGB images had made impressive improvements, we aim to compare the assessment of physical rehabilitation exercises using movement data obtained from both RGB-D camera (Microsoft Kinect) and estimation from RGB videos (OpenPose and BlazePose algorithms). A Gaussian Mixture Model (GMM) is employed from position (and orientation) features, with performance metrics defined based on the log-likelihood values from GMM. The evaluation is performed on a medical database of clinical patients carrying out low back-pain rehabilitation exercises, previously coached by robot Poppy.
<div id='section'>Paperid: <span id='pid'>1619, <a href='https://arxiv.org/pdf/2407.18140.pdf' target='_blank'>https://arxiv.org/pdf/2407.18140.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alexandre Girard, Jean-SÃ©bastien Plante
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.18140">Influence Vectors Control for Robots Using Cellular-like Binary Actuators</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robots using cellular-like redundant binary actuators could outmatch electric-gearmotor robotic systems in terms of reliability, force-to-weight ratio and cost. This paper presents a robust fault tolerant control scheme that is designed to meet the control challenges encountered by such robots, i.e., discrete actuator inputs, complex system modeling and cross-coupling between actuators. In the proposed scheme, a desired vectorial system output, such as a position or a force, is commanded by recruiting actuators based on their influence vectors on the output. No analytical model of the system is needed; influence vectors are identified experimentally by sequentially activating each actuator. For position control tasks, the controller uses a probabilistic approach and a genetic algorithm to determine an optimal combination of actuators to recruit. For motion control tasks, the controller uses a sliding mode approach and independent recruiting decision for each actuator. Experimental results on a four degrees of freedom binary manipulator with twenty actuators confirm the method's effectiveness, and its ability to tolerate massive perturbations and numerous actuator failures.
<div id='section'>Paperid: <span id='pid'>1620, <a href='https://arxiv.org/pdf/2407.16788.pdf' target='_blank'>https://arxiv.org/pdf/2407.16788.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Su Li, Wang Liang, Jianye Wang, Ziheng Zhang, Lei Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.16788">Occlusion-Aware 3D Motion Interpretation for Abnormal Behavior Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Estimating abnormal posture based on 3D pose is vital in human pose analysis, yet it presents challenges, especially when reconstructing 3D human poses from monocular datasets with occlusions. Accurate reconstructions enable the restoration of 3D movements, which assist in the extraction of semantic details necessary for analyzing abnormal behaviors. However, most existing methods depend on predefined key points as a basis for estimating the coordinates of occluded joints, where variations in data quality have adversely affected the performance of these models. In this paper, we present OAD2D, which discriminates against motion abnormalities based on reconstructing 3D coordinates of mesh vertices and human joints from monocular videos. The OAD2D employs optical flow to capture motion prior information in video streams, enriching the information on occluded human movements and ensuring temporal-spatial alignment of poses. Moreover, we reformulate the abnormal posture estimation by coupling it with Motion to Text (M2T) model in which, the VQVAE is employed to quantize motion features. This approach maps motion tokens to text tokens, allowing for a semantically interpretable analysis of motion, and enhancing the generalization of abnormal posture detection boosted by Language model. Our approach demonstrates the robustness of abnormal behavior detection against severe and self-occlusions, as it reconstructs human motion trajectories in global coordinates to effectively mitigate occlusion issues. Our method, validated using the Human3.6M, 3DPW, and NTU RGB+D datasets, achieves a high $F_1-$Score of 0.94 on the NTU RGB+D dataset for medical condition detection. And we will release all of our code and data.
<div id='section'>Paperid: <span id='pid'>1621, <a href='https://arxiv.org/pdf/2407.05376.pdf' target='_blank'>https://arxiv.org/pdf/2407.05376.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiayu Guo, Mingyue Feng, Pengfei Zhu, Chengjun Li, Jian Pu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.05376">Rethinking Closed-loop Planning Framework for Imitation-based Model Integrating Prediction and Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, the integration of prediction and planning through neural networks has received substantial attention. Despite extensive studies on it, there is a noticeable gap in understanding the operation of such models within a closed-loop planning setting. To bridge this gap, we propose a novel closed-loop planning framework compatible with neural networks engaged in joint prediction and planning. The framework contains two running modes, namely planning and safety monitoring, wherein the neural network performs Motion Prediction and Planning (MPP) and Conditional Motion Prediction (CMP) correspondingly without altering architecture. We evaluate the efficacy of our framework using the nuPlan dataset and its simulator, conducting closed-loop experiments across diverse scenarios. The results demonstrate that the proposed framework ensures the feasibility and local stability of the planning process while maintaining safety with CMP safety monitoring. Compared to other learning-based methods, our approach achieves substantial improvement.
<div id='section'>Paperid: <span id='pid'>1622, <a href='https://arxiv.org/pdf/2407.00738.pdf' target='_blank'>https://arxiv.org/pdf/2407.00738.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Momir AdÅ¾emoviÄ, Predrag TadiÄ, Andrija PetroviÄ, Mladen NikoliÄ
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.00738">Engineering an Efficient Object Tracker for Non-Linear Motion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The goal of multi-object tracking is to detect and track all objects in a scene while maintaining unique identifiers for each, by associating their bounding boxes across video frames. This association relies on matching motion and appearance patterns of detected objects. This task is especially hard in case of scenarios involving dynamic and non-linear motion patterns. In this paper, we introduce DeepMoveSORT, a novel, carefully engineered multi-object tracker designed specifically for such scenarios. In addition to standard methods of appearance-based association, we improve motion-based association by employing deep learnable filters (instead of the most commonly used Kalman filter) and a rich set of newly proposed heuristics. Our improvements to motion-based association methods are severalfold. First, we propose a new transformer-based filter architecture, TransFilter, which uses an object's motion history for both motion prediction and noise filtering. We further enhance the filter's performance by careful handling of its motion history and accounting for camera motion. Second, we propose a set of heuristics that exploit cues from the position, shape, and confidence of detected bounding boxes to improve association performance. Our experimental evaluation demonstrates that DeepMoveSORT outperforms existing trackers in scenarios featuring non-linear motion, surpassing state-of-the-art results on three such datasets. We also perform a thorough ablation study to evaluate the contributions of different tracker components which we proposed. Based on our study, we conclude that using a learnable filter instead of the Kalman filter, along with appearance-based association is key to achieving strong general tracking performance.
<div id='section'>Paperid: <span id='pid'>1623, <a href='https://arxiv.org/pdf/2406.19680.pdf' target='_blank'>https://arxiv.org/pdf/2406.19680.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuang Zhang, Jiaxi Gu, Li-Wen Wang, Han Wang, Junqi Cheng, Yuefeng Zhu, Fangyuan Zou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.19680">MimicMotion: High-Quality Human Motion Video Generation with Confidence-aware Pose Guidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, generative artificial intelligence has achieved significant advancements in the field of image generation, spawning a variety of applications. However, video generation still faces considerable challenges in various aspects, such as controllability, video length, and richness of details, which hinder the application and popularization of this technology. In this work, we propose a controllable video generation framework, dubbed MimicMotion, which can generate high-quality videos of arbitrary length mimicking specific motion guidance. Compared with previous methods, our approach has several highlights. Firstly, we introduce confidence-aware pose guidance that ensures high frame quality and temporal smoothness. Secondly, we introduce regional loss amplification based on pose confidence, which significantly reduces image distortion. Lastly, for generating long and smooth videos, we propose a progressive latent fusion strategy. By this means, we can produce videos of arbitrary length with acceptable resource consumption. With extensive experiments and user studies, MimicMotion demonstrates significant improvements over previous approaches in various aspects. Detailed results and comparisons are available on our project page: https://tencent.github.io/MimicMotion .
<div id='section'>Paperid: <span id='pid'>1624, <a href='https://arxiv.org/pdf/2406.07169.pdf' target='_blank'>https://arxiv.org/pdf/2406.07169.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mirgahney Mohamed, Harry Jake Cunningham, Marc P. Deisenroth, Lourdes Agapito
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.07169">RecMoDiffuse: Recurrent Flow Diffusion for Human Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion generation has paramount importance in computer animation. It is a challenging generative temporal modelling task due to the vast possibilities of human motion, high human sensitivity to motion coherence and the difficulty of accurately generating fine-grained motions. Recently, diffusion methods have been proposed for human motion generation due to their high sample quality and expressiveness. However, generated sequences still suffer from motion incoherence, and are limited to short duration, and simpler motion and take considerable time during inference. To address these limitations, we propose \textit{RecMoDiffuse: Recurrent Flow Diffusion}, a new recurrent diffusion formulation for temporal modelling. Unlike previous work, which applies diffusion to the whole sequence without any temporal dependency, an approach that inherently makes temporal consistency hard to achieve. Our method explicitly enforces temporal constraints with the means of normalizing flow models in the diffusion process and thereby extends diffusion to the temporal dimension. We demonstrate the effectiveness of RecMoDiffuse in the temporal modelling of human motion. Our experiments show that RecMoDiffuse achieves comparable results with state-of-the-art methods while generating coherent motion sequences and reducing the computational overhead in the inference stage.
<div id='section'>Paperid: <span id='pid'>1625, <a href='https://arxiv.org/pdf/2405.12616.pdf' target='_blank'>https://arxiv.org/pdf/2405.12616.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Till Hielscher, Lukas Heuer, Frederik Wulle, Luigi Palmieri
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.12616">Towards Using Fast Embedded Model Predictive Control for Human-Aware Predictive Robot Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Predictive planning is a key capability for robots to efficiently and safely navigate populated environments. Particularly in densely crowded scenes, with uncertain human motion predictions, predictive path planning, and control can become expensive to compute in real time due to the curse of dimensionality. With the goal of achieving pro-active and legible robot motion in shared environments, in this paper we present HuMAN-MPC, a computationally efficient algorithm for Human Motion Aware Navigation using fast embedded Model Predictive Control. The approach consists of a novel model predictive control (MPC) formulation that leverages a fast state-of-the-art optimization backend based on a sequential quadratic programming real-time iteration scheme while also providing feasibility monitoring. Our experiments, in simulation and on a fully integrated ROS-based platform, show that the approach achieves great scalability with fast computation times without penalizing path quality and efficiency of the resulting avoidance behavior.
<div id='section'>Paperid: <span id='pid'>1626, <a href='https://arxiv.org/pdf/2405.12408.pdf' target='_blank'>https://arxiv.org/pdf/2405.12408.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinhao Liu, Jun Yang, Jianliang Mao, Tianqi Zhu, Qihang Xie, Yimeng Li, Xiangyu Wang, Shihua Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.12408">Flexible Active Safety Motion Control for Robotic Obstacle Avoidance: A CBF-Guided MPC Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A flexible active safety motion (FASM) control approach is proposed for the avoidance of dynamic obstacles and the reference tracking in robot manipulators. The distinctive feature of the proposed method lies in its utilization of control barrier functions (CBF) to design flexible CBF-guided safety criteria (CBFSC) with dynamically optimized decay rates, thereby offering flexibility and active safety for robot manipulators in dynamic environments. First, discrete-time CBFs are employed to formulate the novel flexible CBFSC with dynamic decay rates for robot manipulators. Following that, the model predictive control (MPC) philosophy is applied, integrating flexible CBFSC as safety constraints into the receding-horizon optimization problem. Significantly, the decay rates of the designed CBFSC are incorporated as decision variables in the optimization problem, facilitating the dynamic enhancement of flexibility during the obstacle avoidance process. In particular, a novel cost function that integrates a penalty term is designed to dynamically adjust the safety margins of the CBFSC. Finally, experiments are conducted in various scenarios using a Universal Robots 5 (UR5) manipulator to validate the effectiveness of the proposed approach.
<div id='section'>Paperid: <span id='pid'>1627, <a href='https://arxiv.org/pdf/2405.06989.pdf' target='_blank'>https://arxiv.org/pdf/2405.06989.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shubham Singh, Anoop Jain
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.06989">Mobius Transformation-Based Circular Motion Control for Unicycle Robots in Nonconcentric Circular Geofences</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Nonuniform motion constraints are ubiquitous in robotic applications. Geofencing control is one such paradigm where the motion of a robot must be constrained within a predefined boundary. This paper addresses the problem of stabilizing a unicycle robot around a desired circular orbit while confining its motion within a nonconcentric external circular boundary. Our solution approach relies on the concept of the so-called Mobius transformation that, under certain practical conditions, maps two nonconcentric circles to a pair of concentric circles, and hence, results in uniform spatial motion constraints. The choice of such a Mobius transformation is governed by the roots of a quadratic equation in the post-design analysis that decides how the regions enclosed by the two circles are mapped onto the two planes. We show that the problem can be formulated either as a trajectory-constraining problem or an obstacle-avoidance problem in the transformed plane, depending on these roots. Exploiting the idea of the barrier Lyapunov function, we propose a unique control law that solves both these contrasting problems in the transformed plane and renders a solution to the original problem in the actual plane. By relating parameters of two planes under Mobius transformation and its inverse map, we further establish a connection between the control laws in two planes and determine the control law to be applied in the actual plane. Simulation and experimental results are provided to illustrate the key theoretical developments.
<div id='section'>Paperid: <span id='pid'>1628, <a href='https://arxiv.org/pdf/2405.04804.pdf' target='_blank'>https://arxiv.org/pdf/2405.04804.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yin Li, Rajalakshmi Nandakumar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.04804">WixUp: A General Data Augmentation Framework for Wireless Perception in Tracking of Humans</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in wireless perception technologies, including mmWave, WiFi, and acoustics, have expanded their application in human motion tracking and health monitoring. They are promising alternatives to traditional camera-based perception systems, thanks to their efficacy under diverse conditions or occlusions, and enhanced privacy. However, the integration of deep learning within this field introduces new challenges such as the need for extensive training data and poor model generalization, especially with sparse and noisy wireless point clouds. As a remedy, data augmentation is one solution well-explored in other deep learning fields, but they are not directly applicable to the unique characteristics of wireless signals. This motivates us to propose a custom data augmentation framework, WixUp, tailored for wireless perception. Moreover, we aim to make it a general framework supporting various datasets, model architectures, sensing modalities, and tasks; while previous wireless data augmentation or generative simulations do not exhibit this generalizability, only limited to certain use cases. More specifically, WixUp can reverse-transform lossy coordinates into dense range profiles using Gaussian mixture and probability tricks, making it capable of in-depth data diversity enhancement; and its mixing-based method enables unsupervised domain adaptation via self-training, allowing training of the model with no labels from new users or environments in practice. In summary, our extensive evaluation experiments show that WixUp provides consistent performance improvement across various scenarios and outperforms the baselines.
<div id='section'>Paperid: <span id='pid'>1629, <a href='https://arxiv.org/pdf/2404.17837.pdf' target='_blank'>https://arxiv.org/pdf/2404.17837.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiming Bao, Xu Zhao, Dahong Qian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.17837">Hybrid 3D Human Pose Estimation with Monocular Video and Sparse IMUs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Temporal 3D human pose estimation from monocular videos is a challenging task in human-centered computer vision due to the depth ambiguity of 2D-to-3D lifting. To improve accuracy and address occlusion issues, inertial sensor has been introduced to provide complementary source of information. However, it remains challenging to integrate heterogeneous sensor data for producing physically rational 3D human poses. In this paper, we propose a novel framework, Real-time Optimization and Fusion (RTOF), to address this issue. We first incorporate sparse inertial orientations into a parametric human skeleton to refine 3D poses in kinematics. The poses are then optimized by energy functions built on both visual and inertial observations to reduce the temporal jitters. Our framework outputs smooth and biomechanically plausible human motion. Comprehensive experiments with ablation studies demonstrate its rationality and efficiency. On Total Capture dataset, the pose estimation error is significantly decreased compared to the baseline method.
<div id='section'>Paperid: <span id='pid'>1630, <a href='https://arxiv.org/pdf/2404.15371.pdf' target='_blank'>https://arxiv.org/pdf/2404.15371.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aman Kumar, Mark Litterick, Samuele Candido
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.15371">Efficient Verification of a RADAR SoC Using Formal and Simulation-Based Methods</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As the demand for Internet of Things (IoT) and Human-to-Machine Interaction (HMI) increases, modern System-on-Chips (SoCs) offering such solutions are becoming increasingly complex. This intricate design poses significant challenges for verification, particularly when time-to-market is a crucial factor for consumer electronics products. This paper presents a case study based on our work to verify a complex Radio Detection And Ranging (RADAR) based SoC that performs on-chip sensing of human motion with millimetre accuracy. We leverage both formal and simulation-based methods to complement each other and achieve verification sign-off with high confidence. While employing a requirements-driven flow approach, we demonstrate the use of different verification methods to cater to multiple requirements and highlight our know-how from the project. Additionally, we used Machine Learning (ML) based methods, specifically the Xcelium ML tool from Cadence, to improve verification throughput.
<div id='section'>Paperid: <span id='pid'>1631, <a href='https://arxiv.org/pdf/2404.11576.pdf' target='_blank'>https://arxiv.org/pdf/2404.11576.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fei Cui, Jiaojiao Fang, Xiaojiang Wu, Zelong Lai, Mengke Yang, Menghan Jia, Guizhong Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.11576">State-space Decomposition Model for Video Prediction Considering Long-term Motion Trend</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Stochastic video prediction enables the consideration of uncertainty in future motion, thereby providing a better reflection of the dynamic nature of the environment. Stochastic video prediction methods based on image auto-regressive recurrent models need to feed their predictions back into the latent space. Conversely, the state-space models, which decouple frame synthesis and temporal prediction, proves to be more efficient. However, inferring long-term temporal information about motion and generalizing to dynamic scenarios under non-stationary assumptions remains an unresolved challenge. In this paper, we propose a state-space decomposition stochastic video prediction model that decomposes the overall video frame generation into deterministic appearance prediction and stochastic motion prediction. Through adaptive decomposition, the model's generalization capability to dynamic scenarios is enhanced. In the context of motion prediction, obtaining a prior on the long-term trend of future motion is crucial. Thus, in the stochastic motion prediction branch, we infer the long-term motion trend from conditional frames to guide the generation of future frames that exhibit high consistency with the conditional frames. Experimental results demonstrate that our model outperforms baselines on multiple datasets.
<div id='section'>Paperid: <span id='pid'>1632, <a href='https://arxiv.org/pdf/2404.02263.pdf' target='_blank'>https://arxiv.org/pdf/2404.02263.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Youshaa Murhij, Dmitry Yudin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.02263">OFMPNet: Deep End-to-End Model for Occupancy and Flow Prediction in Urban Environment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The task of motion prediction is pivotal for autonomous driving systems, providing crucial data to choose a vehicle behavior strategy within its surroundings. Existing motion prediction techniques primarily focus on predicting the future trajectory of each agent in the scene individually, utilizing its past trajectory data. In this paper, we introduce an end-to-end neural network methodology designed to predict the future behaviors of all dynamic objects in the environment. This approach leverages the occupancy map and the scene's motion flow. We are investigatin various alternatives for constructing a deep encoder-decoder model called OFMPNet. This model uses a sequence of bird's-eye-view road images, occupancy grid, and prior motion flow as input data. The encoder of the model can incorporate transformer, attention-based, or convolutional units. The decoder considers the use of both convolutional modules and recurrent blocks. Additionally, we propose a novel time-weighted motion flow loss, whose application has shown a substantial decrease in end-point error. Our approach has achieved state-of-the-art results on the Waymo Occupancy and Flow Prediction benchmark, with a Soft IoU of 52.1% and an AUC of 76.75% on Flow-Grounded Occupancy.
<div id='section'>Paperid: <span id='pid'>1633, <a href='https://arxiv.org/pdf/2404.00163.pdf' target='_blank'>https://arxiv.org/pdf/2404.00163.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi-Heng Cao, Vincent Bourbonne, FranÃ§ois Lucia, Ulrike Schick, Julien Bert, Vincent Jaouen, Dimitris Visvikis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.00163">CT respiratory motion synthesis using joint supervised and adversarial learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Objective: Four-dimensional computed tomography (4DCT) imaging consists in reconstructing a CT acquisition into multiple phases to track internal organ and tumor motion. It is commonly used in radiotherapy treatment planning to establish planning target volumes. However, 4DCT increases protocol complexity, may not align with patient breathing during treatment, and lead to higher radiation delivery. Approach: In this study, we propose a deep synthesis method to generate pseudo respiratory CT phases from static images for motion-aware treatment planning. The model produces patient-specific deformation vector fields (DVFs) by conditioning synthesis on external patient surface-based estimation, mimicking respiratory monitoring devices. A key methodological contribution is to encourage DVF realism through supervised DVF training while using an adversarial term jointly not only on the warped image but also on the magnitude of the DVF itself. This way, we avoid excessive smoothness typically obtained through deep unsupervised learning, and encourage correlations with the respiratory amplitude. Main results: Performance is evaluated using real 4DCT acquisitions with smaller tumor volumes than previously reported. Results demonstrate for the first time that the generated pseudo-respiratory CT phases can capture organ and tumor motion with similar accuracy to repeated 4DCT scans of the same patient. Mean inter-scans tumor center-of-mass distances and Dice similarity coefficients were $1.97$mm and $0.63$, respectively, for real 4DCT phases and $2.35$mm and $0.71$ for synthetic phases, and compares favorably to a state-of-the-art technique (RMSim).
<div id='section'>Paperid: <span id='pid'>1634, <a href='https://arxiv.org/pdf/2403.16374.pdf' target='_blank'>https://arxiv.org/pdf/2403.16374.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yinke Dong, Haifeng Yuan, Hongkun Liu, Wei Jing, Fangzhen Li, Hongmin Liu, Bin Fan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.16374">ProIn: Learning to Predict Trajectory Based on Progressive Interactions for Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate motion prediction of pedestrians, cyclists, and other surrounding vehicles (all called agents) is very important for autonomous driving. Most existing works capture map information through an one-stage interaction with map by vector-based attention, to provide map constraints for social interaction and multi-modal differentiation. However, these methods have to encode all required map rules into the focal agent's feature, so as to retain all possible intentions' paths while at the meantime to adapt to potential social interaction. In this work, a progressive interaction network is proposed to enable the agent's feature to progressively focus on relevant maps, in order to better learn agents' feature representation capturing the relevant map constraints. The network progressively encode the complex influence of map constraints into the agent's feature through graph convolutions at the following three stages: after historical trajectory encoder, after social interaction, and after multi-modal differentiation. In addition, a weight allocation mechanism is proposed for multi-modal training, so that each mode can obtain learning opportunities from a single-mode ground truth. Experiments have validated the superiority of progressive interactions to the existing one-stage interaction, and demonstrate the effectiveness of each component. Encouraging results were obtained in the challenging benchmarks.
<div id='section'>Paperid: <span id='pid'>1635, <a href='https://arxiv.org/pdf/2403.12214.pdf' target='_blank'>https://arxiv.org/pdf/2403.12214.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gerry Chen, Tristan Al-Haddad, Frank Dellaert, Seth Hutchinson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.12214">Architectural-Scale Artistic Brush Painting with a Hybrid Cable Robot</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robot art presents an opportunity to both showcase and advance state-of-the-art robotics through the challenging task of creating art. Creating large-scale artworks in particular engages the public in a way that small-scale works cannot, and the distinct qualities of brush strokes contribute to an organic and human-like quality. Combining the large scale of murals with the strokes of the brush medium presents an especially impactful result, but also introduces unique challenges in maintaining precise, dextrous motion control of the brush across such a large workspace. In this work, we present the first robot to our knowledge that can paint architectural-scale murals with a brush. We create a hybrid robot consisting of a cable-driven parallel robot and 4 degree of freedom (DoF) serial manipulator to paint a 27m by 3.7m mural on windows spanning 2-stories of a building. We discuss our approach to achieving both the scale and accuracy required for brush-painting a mural through a combination of novel mechanical design elements, coordinated planning and control, and on-site calibration algorithms with experimental validations.
<div id='section'>Paperid: <span id='pid'>1636, <a href='https://arxiv.org/pdf/2403.11304.pdf' target='_blank'>https://arxiv.org/pdf/2403.11304.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Steffen Hagedorn, Marcel Milich, Alexandru P. Condurache
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.11304">Pioneering SE(2)-Equivariant Trajectory Planning for Automated Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Planning the trajectory of the controlled ego vehicle is a key challenge in automated driving. As for human drivers, predicting the motions of surrounding vehicles is important to plan the own actions. Recent motion prediction methods utilize equivariant neural networks to exploit geometric symmetries in the scene. However, no existing method combines motion prediction and trajectory planning in a joint step while guaranteeing equivariance under roto-translations of the input space. We address this gap by proposing a lightweight equivariant planning model that generates multi-modal joint predictions for all vehicles and selects one mode as the ego plan. The equivariant network design improves sample efficiency, guarantees output stability, and reduces model parameters. We further propose equivariant route attraction to guide the ego vehicle along a high-level route provided by an off-the-shelf GPS navigation system. This module creates a momentum from embedded vehicle positions toward the route in latent space while keeping the equivariance property. Route attraction enables goal-oriented behavior without forcing the vehicle to stick to the exact route. We conduct experiments on the challenging nuScenes dataset to investigate the capability of our planner. The results show that the planned trajectory is stable under roto-translations of the input scene which demonstrates the equivariance of our model. Despite using only a small split of the dataset for training, our method improves L2 distance at 3 s by 20.6 % and surpasses the state of the art.
<div id='section'>Paperid: <span id='pid'>1637, <a href='https://arxiv.org/pdf/2403.06994.pdf' target='_blank'>https://arxiv.org/pdf/2403.06994.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zeyuan Qu, Tiange Huang, Yuxin Ji, Yongjun Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.06994">Physics Sensor Based Deep Learning Fall Detection System</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fall detection based on embedded sensor is a practical and popular research direction in recent years. In terms of a specific application: fall detection methods based upon physics sensors such as [gyroscope and accelerator] have been exploited using traditional hand crafted features and feed them in machine learning models like Markov chain or just threshold based classification methods. In this paper, we build a complete system named TSFallDetect including data receiving device based on embedded sensor, mobile deep-learning model deploying platform, and a simple server, which will be used to gather models and data for future expansion. On the other hand, we exploit the sequential deep-learning methods to address this falling motion prediction problem based on data collected by inertial and film pressure sensors. We make a empirical study based on existing datasets and our datasets collected from our system separately, which shows that the deep-learning model has more potential advantage than other traditional methods, and we proposed a new deep-learning model based on the time series data to predict the fall, and it may be superior to other sequential models in this particular field.
<div id='section'>Paperid: <span id='pid'>1638, <a href='https://arxiv.org/pdf/2403.06164.pdf' target='_blank'>https://arxiv.org/pdf/2403.06164.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>PaweÅ A. Pierzchlewicz, Caio O. da Silva, R. James Cotton, Fabian H. Sinz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.06164">Platypose: Calibrated Zero-Shot Multi-Hypothesis 3D Human Motion Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Single camera 3D pose estimation is an ill-defined problem due to inherent ambiguities from depth, occlusion or keypoint noise. Multi-hypothesis pose estimation accounts for this uncertainty by providing multiple 3D poses consistent with the 2D measurements. Current research has predominantly concentrated on generating multiple hypotheses for single frame static pose estimation or single hypothesis motion estimation. In this study we focus on the new task of multi-hypothesis motion estimation. Multi-hypothesis motion estimation is not simply multi-hypothesis pose estimation applied to multiple frames, which would ignore temporal correlation across frames. Instead, it requires distributions which are capable of generating temporally consistent samples, which is significantly more challenging than multi-hypothesis pose estimation or single-hypothesis motion estimation. To this end, we introduce Platypose, a framework that uses a diffusion model pretrained on 3D human motion sequences for zero-shot 3D pose sequence estimation. Platypose outperforms baseline methods on multiple hypotheses for motion estimation. Additionally, Platypose also achieves state-of-the-art calibration and competitive joint error when tested on static poses from Human3.6M, MPI-INF-3DHP and 3DPW. Finally, because it is zero-shot, our method generalizes flexibly to different settings such as multi-camera inference.
<div id='section'>Paperid: <span id='pid'>1639, <a href='https://arxiv.org/pdf/2403.04954.pdf' target='_blank'>https://arxiv.org/pdf/2403.04954.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Edgar Medina, Leyong Loh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.04954">Fooling Neural Networks for Motion Forecasting via Adversarial Attacks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion prediction is still an open problem, which is extremely important for autonomous driving and safety applications. Although there are great advances in this area, the widely studied topic of adversarial attacks has not been applied to multi-regression models such as GCNs and MLP-based architectures in human motion prediction. This work intends to reduce this gap using extensive quantitative and qualitative experiments in state-of-the-art architectures similar to the initial stages of adversarial attacks in image classification. The results suggest that models are susceptible to attacks even on low levels of perturbation. We also show experiments with 3D transformations that affect the model performance, in particular, we show that most models are sensitive to simple rotations and translations which do not alter joint distances. We conclude that similar to earlier CNN models, motion forecasting tasks are susceptible to small perturbations and simple 3D transformations.
<div id='section'>Paperid: <span id='pid'>1640, <a href='https://arxiv.org/pdf/2403.00613.pdf' target='_blank'>https://arxiv.org/pdf/2403.00613.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vishnu Veeraraghavan, Kyle Hunte, Jingang Yi, Kaiyan Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.00613">Complete and Near-Optimal Robotic Crack Coverage and Filling in Civil Infrastructure</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a simultaneous sensor-based inspection and footprint coverage (SIFC) planning and control design with applications to autonomous robotic crack mapping and filling. The main challenge of the SIFC problem lies in the coupling of complete sensing (for mapping) and robotic footprint (for filling) coverage tasks. Initially, we assume known target information (e.g., cracks) and employ classic cell decomposition methods to achieve complete sensing coverage of the workspace and complete robotic footprint coverage using the least-cost route. Subsequently, we generalize the algorithm to handle unknown target information, allowing the robot to scan and incrementally construct the target map online while conducting robotic footprint coverage. The online polynomial-time SIFC planning algorithm minimizes the total robot traveling distance, guarantees complete sensing coverage of the entire workspace, and achieves near-optimal robotic footprint coverage, as demonstrated through experiments. For the demonstrated application, we design coordinated nozzle motion control with the planned robot trajectory to efficiently fill all cracks within the robot's footprint. Experimental results illustrate the algorithm's design, performance, and comparisons. The SIFC algorithm offers a high-efficiency motion planning solution for various robotic applications requiring simultaneous sensing and actuation coverage.
<div id='section'>Paperid: <span id='pid'>1641, <a href='https://arxiv.org/pdf/2402.13172.pdf' target='_blank'>https://arxiv.org/pdf/2402.13172.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhi-Yi Lin, Bofan Lyu, Judith Cueto Fernandez, Eline van der Kruk, Ajay Seth, Xucong Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.13172">3D Kinematics Estimation from Video with a Biomechanical Model and Synthetic Training Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate 3D kinematics estimation of human body is crucial in various applications for human health and mobility, such as rehabilitation, injury prevention, and diagnosis, as it helps to understand the biomechanical loading experienced during movement. Conventional marker-based motion capture is expensive in terms of financial investment, time, and the expertise required. Moreover, due to the scarcity of datasets with accurate annotations, existing markerless motion capture methods suffer from challenges including unreliable 2D keypoint detection, limited anatomic accuracy, and low generalization capability. In this work, we propose a novel biomechanics-aware network that directly outputs 3D kinematics from two input views with consideration of biomechanical prior and spatio-temporal information. To train the model, we create synthetic dataset ODAH with accurate kinematics annotations generated by aligning the body mesh from the SMPL-X model and a full-body OpenSim skeletal model. Our extensive experiments demonstrate that the proposed approach, only trained on synthetic data, outperforms previous state-of-the-art methods when evaluated across multiple datasets, revealing a promising direction for enhancing video-based human motion capture
<div id='section'>Paperid: <span id='pid'>1642, <a href='https://arxiv.org/pdf/2402.05115.pdf' target='_blank'>https://arxiv.org/pdf/2402.05115.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Louis Annabi, Ziqi Ma, Sao Mai Nguyen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.05115">Unsupervised Motion Retargeting for Human-Robot Imitation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This early-stage research work aims to improve online human-robot imitation by translating sequences of joint positions from the domain of human motions to a domain of motions achievable by a given robot, thus constrained by its embodiment. Leveraging the generalization capabilities of deep learning methods, we address this problem by proposing an encoder-decoder neural network model performing domain-to-domain translation. In order to train such a model, one could use pairs of associated robot and human motions. Though, such paired data is extremely rare in practice, and tedious to collect. Therefore, we turn towards deep learning methods for unpaired domain-to-domain translation, that we adapt in order to perform human-robot imitation.
<div id='section'>Paperid: <span id='pid'>1643, <a href='https://arxiv.org/pdf/2401.12919.pdf' target='_blank'>https://arxiv.org/pdf/2401.12919.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sara GarcÃ­a-de-Villa, David Casillas-PÃ©rez, Ana JimÃ©nez-MartÃ­n, Juan JesÃºs GarcÃ­a-DomÃ­nguez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.12919">Inertial Sensors for Human Motion Analysis: A Comprehensive Review</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Inertial motion analysis is having a growing interest during the last decades due to its advantages over classical optical systems. The technological solution based on inertial measurement units allows the measurement of movements in daily living environments, such as in everyday life, which is key for a realistic assessment and understanding of movements. This is why research in this field is still developing and different approaches are proposed. This presents a systematic review of the different proposals for inertial motion analysis found in the literature. The search strategy has been carried out on eight different platforms, including journal articles and conference proceedings, which are written in English and published until August 2022. The results are analyzed in terms of the publishers, the sensors used, the applications, the monitored units, the algorithms of use, the participants of the studies, and the validation systems employed. In addition, we delve deeply into the machine learning techniques proposed in recent years and in the approaches to reduce the estimation error. In this way, we show an overview of the research carried out in this field, going into more detail in recent years, and providing some research directions for future work
<div id='section'>Paperid: <span id='pid'>1644, <a href='https://arxiv.org/pdf/2401.11783.pdf' target='_blank'>https://arxiv.org/pdf/2401.11783.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Feiyu Yao, Zongkai Wu, Li Yi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.11783">Full-Body Motion Reconstruction with Sparse Sensing from Graph Perspective</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Estimating 3D full-body pose from sparse sensor data is a pivotal technique employed for the reconstruction of realistic human motions in Augmented Reality and Virtual Reality. However, translating sparse sensor signals into comprehensive human motion remains a challenge since the sparsely distributed sensors in common VR systems fail to capture the motion of full human body. In this paper, we use well-designed Body Pose Graph (BPG) to represent the human body and translate the challenge into a prediction problem of graph missing nodes. Then, we propose a novel full-body motion reconstruction framework based on BPG. To establish BPG, nodes are initially endowed with features extracted from sparse sensor signals. Features from identifiable joint nodes across diverse sensors are amalgamated and processed from both temporal and spatial perspectives. Temporal dynamics are captured using the Temporal Pyramid Structure, while spatial relations in joint movements inform the spatial attributes. The resultant features serve as the foundational elements of the BPG nodes. To further refine the BPG, node features are updated through a graph neural network that incorporates edge reflecting varying joint relations. Our method's effectiveness is evidenced by the attained state-of-the-art performance, particularly in lower body motion, outperforming other baseline methods. Additionally, an ablation study validates the efficacy of each module in our proposed framework.
<div id='section'>Paperid: <span id='pid'>1645, <a href='https://arxiv.org/pdf/2312.12634.pdf' target='_blank'>https://arxiv.org/pdf/2312.12634.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Payam Jome Yazdian, Rachel Lagasse, Hamid Mohammadi, Eric Liu, Li Cheng, Angelica Lim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.12634">MotionScript: Natural Language Descriptions for Expressive 3D Human Motions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce MotionScript, a novel framework for generating highly detailed, natural language descriptions of 3D human motions. Unlike existing motion datasets that rely on broad action labels or generic captions, MotionScript provides fine-grained, structured descriptions that capture the full complexity of human movement including expressive actions (e.g., emotions, stylistic walking) and interactions beyond standard motion capture datasets. MotionScript serves as both a descriptive tool and a training resource for text-to-motion models, enabling the synthesis of highly realistic and diverse human motions from text. By augmenting motion datasets with MotionScript captions, we demonstrate significant improvements in out-of-distribution motion generation, allowing large language models (LLMs) to generate motions that extend beyond existing data. Additionally, MotionScript opens new applications in animation, virtual human simulation, and robotics, providing an interpretable bridge between intuitive descriptions and motion synthesis. To the best of our knowledge, this is the first attempt to systematically translate 3D motion into structured natural language without requiring training data.
<div id='section'>Paperid: <span id='pid'>1646, <a href='https://arxiv.org/pdf/2312.10309.pdf' target='_blank'>https://arxiv.org/pdf/2312.10309.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxin Chen, Yifan Yin, Julian Brown, Kevin Wang, Yi Wang, Ziyi Wang, Russell H. Taylor, Yixuan Wu, Emad M. Boctor
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.10309">Enabling Mammography with Co-Robotic Ultrasound</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Ultrasound (US) imaging is a vital adjunct to mammography in breast cancer screening and diagnosis, but its reliance on hand-held transducers often lacks repeatability and heavily depends on sonographers' skills. Integrating US systems from different vendors further complicates clinical standards and workflows. This research introduces a co-robotic US platform for repeatable, accurate, and vendor-independent breast US image acquisition. The platform can autonomously perform 3D volume scans or swiftly acquire real-time 2D images of suspicious lesions. Utilizing a Universal Robot UR5 with an RGB camera, a force sensor, and an L7-4 linear array transducer, the system achieves autonomous navigation, motion control, and image acquisition. The calibrations, including camera-mammogram, robot-camera, and robot-US, were rigorously conducted and validated. Governed by a PID force control, the robot-held transducer maintains a constant contact force with the compression plate during the scan for safety and patient comfort. The framework was validated on a lesion-mimicking phantom. Our results indicate that the developed co-robotic US platform promises to enhance the precision and repeatability of breast cancer screening and diagnosis. Additionally, the platform offers straightforward integration into most mammographic devices to ensure vendor-independence.
<div id='section'>Paperid: <span id='pid'>1647, <a href='https://arxiv.org/pdf/2312.05144.pdf' target='_blank'>https://arxiv.org/pdf/2312.05144.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Daniil S. Antonenko, Stepan Konev, Yuriy Biktairov, Boris Yangel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.05144">Kraken: enabling joint trajectory prediction by utilizing Mode Transformer and Greedy Mode Processing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate and reliable motion prediction is essential for safe urban autonomy. The most prominent motion prediction approaches are based on modeling the distribution of possible future trajectories of each actor in autonomous system's vicinity. These "independent" marginal predictions might be accurate enough to properly describe casual driving situations where the prediction target is not likely to interact with other actors. They are, however, inadequate for modeling interactive situations where the actors' future trajectories are likely to intersect. To mitigate this issue we propose Kraken -- a real-time trajectory prediction model capable of approximating pairwise interactions between the actors as well as producing accurate marginal predictions. Kraken relies on a simple Greedy Mode Processing technique allowing it to convert a factorized prediction for a pair of agents into a physically-plausible joint prediction. It also utilizes the Mode Transformer module to increase the diversity of predicted trajectories and make the joint prediction more informative. We evaluate Kraken on Waymo Motion Prediction challenge where it held the first place in the Interaction leaderboard and the second place in the Motion leaderboard in October 2021.
<div id='section'>Paperid: <span id='pid'>1648, <a href='https://arxiv.org/pdf/2312.02409.pdf' target='_blank'>https://arxiv.org/pdf/2312.02409.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiqian Gan, Hao Xiao, Yizhe Zhao, Ethan Zhang, Zhe Huang, Xin Ye, Lingting Ge
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.02409">MGTR: Multi-Granular Transformer for Motion Prediction with LiDAR</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion prediction has been an essential component of autonomous driving systems since it handles highly uncertain and complex scenarios involving moving agents of different types. In this paper, we propose a Multi-Granular TRansformer (MGTR) framework, an encoder-decoder network that exploits context features in different granularities for different kinds of traffic agents. To further enhance MGTR's capabilities, we leverage LiDAR point cloud data by incorporating LiDAR semantic features from an off-the-shelf LiDAR feature extractor. We evaluate MGTR on Waymo Open Dataset motion prediction benchmark and show that the proposed method achieved state-of-the-art performance, ranking 1st on its leaderboard (https://waymo.com/open/challenges/2023/motion-prediction/).
<div id='section'>Paperid: <span id='pid'>1649, <a href='https://arxiv.org/pdf/2311.16097.pdf' target='_blank'>https://arxiv.org/pdf/2311.16097.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Christian Diller, Angela Dai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.16097">CG-HOI: Contact-Guided 3D Human-Object Interaction Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose CG-HOI, the first method to address the task of generating dynamic 3D human-object interactions (HOIs) from text. We model the motion of both human and object in an interdependent fashion, as semantically rich human motion rarely happens in isolation without any interactions. Our key insight is that explicitly modeling contact between the human body surface and object geometry can be used as strong proxy guidance, both during training and inference. Using this guidance to bridge human and object motion enables generating more realistic and physically plausible interaction sequences, where the human body and corresponding object move in a coherent manner. Our method first learns to model human motion, object motion, and contact in a joint diffusion process, inter-correlated through cross-attention. We then leverage this learned contact for guidance during inference to synthesize realistic and coherent HOIs. Extensive evaluation shows that our joint contact-based human-object interaction approach generates realistic and physically plausible sequences, and we show two applications highlighting the capabilities of our method. Conditioned on a given object trajectory, we can generate the corresponding human motion without re-training, demonstrating strong human-object interdependency learning. Our approach is also flexible, and can be applied to static real-world 3D scene scans.
<div id='section'>Paperid: <span id='pid'>1650, <a href='https://arxiv.org/pdf/2311.02502.pdf' target='_blank'>https://arxiv.org/pdf/2311.02502.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohamed Younes, Ewa Kijak, Richard Kulpa, Simon Malinowski, Franck Multon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.02502">MAAIP: Multi-Agent Adversarial Interaction Priors for imitation from fighting demonstrations for physics-based characters</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Simulating realistic interaction and motions for physics-based characters is of great interest for interactive applications, and automatic secondary character animation in the movie and video game industries. Recent works in reinforcement learning have proposed impressive results for single character simulation, especially the ones that use imitation learning based techniques. However, imitating multiple characters interactions and motions requires to also model their interactions. In this paper, we propose a novel Multi-Agent Generative Adversarial Imitation Learning based approach that generalizes the idea of motion imitation for one character to deal with both the interaction and the motions of the multiple physics-based characters. Two unstructured datasets are given as inputs: 1) a single-actor dataset containing motions of a single actor performing a set of motions linked to a specific application, and 2) an interaction dataset containing a few examples of interactions between multiple actors. Based on these datasets, our system trains control policies allowing each character to imitate the interactive skills associated with each actor, while preserving the intrinsic style. This approach has been tested on two different fighting styles, boxing and full-body martial art, to demonstrate the ability of the method to imitate different styles.
<div id='section'>Paperid: <span id='pid'>1651, <a href='https://arxiv.org/pdf/2310.19153.pdf' target='_blank'>https://arxiv.org/pdf/2310.19153.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fayez H. Alruwaili, Michael P. Clancy, Marzieh S. Saeedi-Hosseiny, Jacob A. Logar, Charalampos Papachristou, Christopher Haydel, Javad Parvizi, Iulian I. Iordachita, Mohammad H. Abedin-Nasab
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.19153">Design and Experimental Evaluation of a Haptic Robot-Assisted System for Femur Fracture Surgery</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the face of challenges encountered during femur fracture surgery, such as the high rates of malalignment and X-ray exposure to operating personnel, robot-assisted surgery has emerged as an alternative to conventional state-of-the-art surgical methods. This paper introduces the development of Robossis, a haptic system for robot-assisted femur fracture surgery. Robossis comprises a 7-DOF haptic controller and a 6-DOF surgical robot. A unilateral control architecture is developed to address the kinematic mismatch and the motion transfer between the haptic controller and the Robossis surgical robot. A real-time motion control pipeline is designed to address the motion transfer and evaluated through experimental testing. The analysis illustrates that the Robossis surgical robot can adhere to the desired trajectory from the haptic controller with an average translational error of 0.32 mm and a rotational error of 0.07 deg. Additionally, a haptic rendering pipeline is developed to resolve the kinematic mismatch by constraining the haptic controller (user hand) movement within the permissible joint limits of the Robossis surgical robot. Lastly, in a cadaveric lab test, the Robossis system assisted surgeons during a mock femur fracture surgery. The result shows that Robossis can provide an intuitive solution for surgeons to perform femur fracture surgery.
<div id='section'>Paperid: <span id='pid'>1652, <a href='https://arxiv.org/pdf/2310.13922.pdf' target='_blank'>https://arxiv.org/pdf/2310.13922.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuping Wang, Jier Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.13922">Equivariant Map and Agent Geometry for Autonomous Driving Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In autonomous driving, deep learning enabled motion prediction is a popular topic. A critical gap in traditional motion prediction methodologies lies in ensuring equivariance under Euclidean geometric transformations and maintaining invariant interaction relationships. This research introduces a groundbreaking solution by employing EqMotion, a theoretically geometric equivariant and interaction invariant motion prediction model for particles and humans, plus integrating agent-equivariant high-definition (HD) map features for context aware motion prediction in autonomous driving. The use of EqMotion as backbone marks a significant departure from existing methods by rigorously ensuring motion equivariance and interaction invariance. Equivariance here implies that an output motion must be equally transformed under the same Euclidean transformation as an input motion, while interaction invariance preserves the manner in which agents interact despite transformations. These properties make the network robust to arbitrary Euclidean transformations and contribute to more accurate prediction. In addition, we introduce an equivariant method to process the HD map to enrich the spatial understanding of the network while preserving the overall network equivariance property. By applying these technologies, our model is able to achieve high prediction accuracy while maintain a lightweight design and efficient data utilization.
<div id='section'>Paperid: <span id='pid'>1653, <a href='https://arxiv.org/pdf/2310.06226.pdf' target='_blank'>https://arxiv.org/pdf/2310.06226.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>K. Niranjan Kumar, Irfan Essa, Sehoon Ha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.06226">Words into Action: Learning Diverse Humanoid Robot Behaviors using Language Guided Iterative Motion Refinement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humanoid robots are well suited for human habitats due to their morphological similarity, but developing controllers for them is a challenging task that involves multiple sub-problems, such as control, planning and perception. In this paper, we introduce a method to simplify controller design by enabling users to train and fine-tune robot control policies using natural language commands. We first learn a neural network policy that generates behaviors given a natural language command, such as "walk forward", by combining Large Language Models (LLMs), motion retargeting, and motion imitation. Based on the synthesized motion, we iteratively fine-tune by updating the text prompt and querying LLMs to find the best checkpoint associated with the closest motion in history. We validate our approach using a simulated Digit humanoid robot and demonstrate learning of diverse motions, such as walking, hopping, and kicking, without the burden of complex reward engineering. In addition, we show that our iterative refinement enables us to learn 3x times faster than a naive formulation that learns from scratch.
<div id='section'>Paperid: <span id='pid'>1654, <a href='https://arxiv.org/pdf/2309.16534.pdf' target='_blank'>https://arxiv.org/pdf/2309.16534.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ari Seff, Brian Cera, Dian Chen, Mason Ng, Aurick Zhou, Nigamaa Nayakanti, Khaled S. Refaat, Rami Al-Rfou, Benjamin Sapp
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.16534">MotionLM: Multi-Agent Motion Forecasting as Language Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reliable forecasting of the future behavior of road agents is a critical component to safe planning in autonomous vehicles. Here, we represent continuous trajectories as sequences of discrete motion tokens and cast multi-agent motion prediction as a language modeling task over this domain. Our model, MotionLM, provides several advantages: First, it does not require anchors or explicit latent variable optimization to learn multimodal distributions. Instead, we leverage a single standard language modeling objective, maximizing the average log probability over sequence tokens. Second, our approach bypasses post-hoc interaction heuristics where individual agent trajectory generation is conducted prior to interactive scoring. Instead, MotionLM produces joint distributions over interactive agent futures in a single autoregressive decoding process. In addition, the model's sequential factorization enables temporally causal conditional rollouts. The proposed approach establishes new state-of-the-art performance for multi-agent motion prediction on the Waymo Open Motion Dataset, ranking 1st on the interactive challenge leaderboard.
<div id='section'>Paperid: <span id='pid'>1655, <a href='https://arxiv.org/pdf/2309.15054.pdf' target='_blank'>https://arxiv.org/pdf/2309.15054.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mollik Nayyar, Alan Wagner
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.15054">Near Real-Time Position Tracking for Robot-Guided Evacuation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>During the evacuation of a building, the rapid and accurate tracking of human evacuees can be used by a guide robot to increase the effectiveness of the evacuation [1],[2]. This paper introduces a near real-time human position tracking solution tailored for evacuation robots. Using a pose detector, our system first identifies human joints in the camera frame in near real-time and then translates the position of these pixels into real-world coordinates via a simple calibration process. We run multiple trials of the system in action in an indoor lab environment and show that the system can achieve an accuracy of 0.55 meters when compared to ground truth. The system can also achieve an average of 3 frames per second (FPS) which was sufficient for our study on robot-guided human evacuation. The potential of our approach extends beyond mere tracking, paving the way for evacuee motion prediction, allowing the robot to proactively respond to human movements during an evacuation.
<div id='section'>Paperid: <span id='pid'>1656, <a href='https://arxiv.org/pdf/2309.10248.pdf' target='_blank'>https://arxiv.org/pdf/2309.10248.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jordan Voas, Yili Wang, Qixing Huang, Raymond Mooney
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.10248">What is the Best Automated Metric for Text to Motion Generation?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>There is growing interest in generating skeleton-based human motions from natural language descriptions. While most efforts have focused on developing better neural architectures for this task, there has been no significant work on determining the proper evaluation metric. Human evaluation is the ultimate accuracy measure for this task, and automated metrics should correlate well with human quality judgments. Since descriptions are compatible with many motions, determining the right metric is critical for evaluating and designing effective generative models. This paper systematically studies which metrics best align with human evaluations and proposes new metrics that align even better. Our findings indicate that none of the metrics currently used for this task show even a moderate correlation with human judgments on a sample level. However, for assessing average model performance, commonly used metrics such as R-Precision and less-used coordinate errors show strong correlations. Additionally, several recently developed metrics are not recommended due to their low correlation compared to alternatives. We also introduce a novel metric based on a multimodal BERT-like model, MoBERT, which offers strongly human-correlated sample-level evaluations while maintaining near-perfect model-level correlation. Our results demonstrate that this new metric exhibits extensive benefits over all current alternatives.
<div id='section'>Paperid: <span id='pid'>1657, <a href='https://arxiv.org/pdf/2308.12006.pdf' target='_blank'>https://arxiv.org/pdf/2308.12006.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yujun Ma, Benjia Zhou, Ruili Wang, Pichao Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.12006">Multi-stage Factorized Spatio-Temporal Representation for RGB-D Action and Gesture Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>RGB-D action and gesture recognition remain an interesting topic in human-centered scene understanding, primarily due to the multiple granularities and large variation in human motion. Although many RGB-D based action and gesture recognition approaches have demonstrated remarkable results by utilizing highly integrated spatio-temporal representations across multiple modalities (i.e., RGB and depth data), they still encounter several challenges. Firstly, vanilla 3D convolution makes it hard to capture fine-grained motion differences between local clips under different modalities. Secondly, the intricate nature of highly integrated spatio-temporal modeling can lead to optimization difficulties. Thirdly, duplicate and unnecessary information can add complexity and complicate entangled spatio-temporal modeling. To address the above issues, we propose an innovative heuristic architecture called Multi-stage Factorized Spatio-Temporal (MFST) for RGB-D action and gesture recognition. The proposed MFST model comprises a 3D Central Difference Convolution Stem (CDC-Stem) module and multiple factorized spatio-temporal stages. The CDC-Stem enriches fine-grained temporal perception, and the multiple hierarchical spatio-temporal stages construct dimension-independent higher-order semantic primitives. Specifically, the CDC-Stem module captures bottom-level spatio-temporal features and passes them successively to the following spatio-temporal factored stages to capture the hierarchical spatial and temporal features through the Multi- Scale Convolution and Transformer (MSC-Trans) hybrid block and Weight-shared Multi-Scale Transformer (WMS-Trans) block. The seamless integration of these innovative designs results in a robust spatio-temporal representation that outperforms state-of-the-art approaches on RGB-D action and gesture recognition datasets.
<div id='section'>Paperid: <span id='pid'>1658, <a href='https://arxiv.org/pdf/2308.07783.pdf' target='_blank'>https://arxiv.org/pdf/2308.07783.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohammad Baradaran, Robert Bergevin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.07783">Future Video Prediction from a Single Frame for Video Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video anomaly detection (VAD) is an important but challenging task in computer vision. The main challenge rises due to the rarity of training samples to model all anomaly cases. Hence, semi-supervised anomaly detection methods have gotten more attention, since they focus on modeling normals and they detect anomalies by measuring the deviations from normal patterns. Despite impressive advances of these methods in modeling normal motion and appearance, long-term motion modeling has not been effectively explored so far. Inspired by the abilities of the future frame prediction proxy-task, we introduce the task of future video prediction from a single frame, as a novel proxy-task for video anomaly detection. This proxy-task alleviates the challenges of previous methods in learning longer motion patterns. Moreover, we replace the initial and future raw frames with their corresponding semantic segmentation map, which not only makes the method aware of object class but also makes the prediction task less complex for the model. Extensive experiments on the benchmark datasets (ShanghaiTech, UCSD-Ped1, and UCSD-Ped2) show the effectiveness of the method and the superiority of its performance compared to SOTA prediction-based VAD methods.
<div id='section'>Paperid: <span id='pid'>1659, <a href='https://arxiv.org/pdf/2307.12056.pdf' target='_blank'>https://arxiv.org/pdf/2307.12056.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yihang Ding, Xiaoyu Ji, Lixian Zhang, Yifei Dong, Tong Wu, Chengzhe Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.12056">Chat-PM: A Class of Composite Hybrid Aerial/Terrestrial Precise Manipulator</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper concentrates on the development of Chat-PM, a class of composite hybrid aerial/terrestrial manipulator, in concern with composite configuration design, dynamics modeling, motion control and force estimation. Compared with existing aerial or terrestrial mobile manipulators, Chat-PM demonstrates advantages in terms of reachability, energy efficiency and manipulation precision. To achieve precise manipulation in terrestrial mode, the dynamics is analyzed with consideration of surface contact, based on which a cascaded controller is designed with compensation for the interference force and torque from the arm. Benefiting from the kinematic constraints caused by the surface contact, the position deviation and the vehicle vibration are effectively decreased, resulting in higher control precision of the end gripper. For manipulation on surfaces with unknown inclination angles, the moving horizon estimation (MHE) is exploited to obtain the precise estimations of force and inclination angle, which are used in the control loop to compensate for the effect of the unknown surface. Real-world experiments are performed to evaluate the superiority of the developed manipulator and the proposed controllers.
<div id='section'>Paperid: <span id='pid'>1660, <a href='https://arxiv.org/pdf/2307.09936.pdf' target='_blank'>https://arxiv.org/pdf/2307.09936.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pedro Gomes, Silvia Rossi, Laura Toni
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.09936">AGAR: Attention Graph-RNN for Adaptative Motion Prediction of Point Clouds of Deformable Objects</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper focuses on motion prediction for point cloud sequences in the challenging case of deformable 3D objects, such as human body motion. First, we investigate the challenges caused by deformable shapes and complex motions present in this type of representation, with the ultimate goal of understanding the technical limitations of state-of-the-art models. From this understanding, we propose an improved architecture for point cloud prediction of deformable 3D objects. Specifically, to handle deformable shapes, we propose a graph-based approach that learns and exploits the spatial structure of point clouds to extract more representative features. Then we propose a module able to combine the learned features in an adaptative manner according to the point cloud movements. The proposed adaptative module controls the composition of local and global motions for each point, enabling the network to model complex motions in deformable 3D objects more effectively. We tested the proposed method on the following datasets: MNIST moving digits, the Mixamo human bodies motions, JPEG and CWIPC-SXR real-world dynamic bodies. Simulation results demonstrate that our method outperforms the current baseline methods given its improved ability to model complex movements as well as preserve point cloud shape. Furthermore, we demonstrate the generalizability of the proposed framework for dynamic feature learning, by testing the framework for action recognition on the MSRAction3D dataset and achieving results on-par with state-of-the-art methods
<div id='section'>Paperid: <span id='pid'>1661, <a href='https://arxiv.org/pdf/2306.05197.pdf' target='_blank'>https://arxiv.org/pdf/2306.05197.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shohei Fujii, Quang-Cuong Pham
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.05197">Time-Optimal Path Tracking with ISO Safety Guarantees</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>One way of ensuring operator's safety during human-robot collaboration is through Speed and Separation Monitoring (SSM), as defined in ISO standard ISO/TS 15066. In general, it is impossible to avoid all human-robot collisions: consider for instance the case when the robot does not move at all, a human operator can still collide with it by hitting it of her own voluntary motion. In the SSM framework, it is possible however to minimize harm by requiring this: \emph{if} a collision ever occurs, then the robot must be in a \emph{stationary state} (all links have zero velocity) at the time instant of the collision. In this paper, we propose a time-optimal control policy based on Time-Optimal Path Parameterization (TOPP) to guarantee such a behavior. Specifically, we show that: for any robot motion that is strictly faster than the motion recommended by our policy, there exists a human motion that results in a collision with the robot in a non-stationary state. Correlatively, we show, in simulation, that our policy is strictly less conservative than state-of-the-art safe robot control methods. Additionally, we propose a parallelization method to reduce the computation time of our pre-computation phase (down to 0.5 sec, practically), which enables the whole pipeline (including the pre-computation) to be executed at runtime, nearly in real-time. Finally, we demonstrate the application of our method in a scenario: time-optimal, safe control of a 6-dof industrial robot.
<div id='section'>Paperid: <span id='pid'>1662, <a href='https://arxiv.org/pdf/2306.02742.pdf' target='_blank'>https://arxiv.org/pdf/2306.02742.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinyu Jia, Jun Yang, Kaixin Lu, Yongping Pan, Haoyong Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.02742">Enhanced Robust Motion Control based on Unknown System Dynamics Estimator for Robot Manipulators</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To achieve high-accuracy manipulation in the presence of unknown disturbances, we propose two novel efficient and robust motion control schemes for high-dimensional robot manipulators. Both controllers incorporate an unknown system dynamics estimator (USDE) to estimate disturbances without requiring acceleration signals and the inverse of inertia matrix. Then, based on the USDE framework, an adaptive-gain controller and a super-twisting sliding mode controller are designed to speed up the convergence of tracking errors and strengthen anti-perturbation ability. The former aims to enhance feedback portions through error-driven control gains, while the latter exploits finite-time convergence of discontinuous switching terms. We analyze the boundedness of control signals and the stability of the closed-loop system in theory, and conduct real hardware experiments on a robot manipulator with seven degrees of freedom (DoF). Experimental results verify the effectiveness and improved performance of the proposed controllers, and also show the feasibility of implementation on high-dimensional robots.
<div id='section'>Paperid: <span id='pid'>1663, <a href='https://arxiv.org/pdf/2305.08190.pdf' target='_blank'>https://arxiv.org/pdf/2305.08190.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunong Wu, Thomas Gilles, Bogdan Stanciulescu, Fabien Moutarde
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.08190">TSGN: Temporal Scene Graph Neural Networks with Projected Vectorized Representation for Multi-Agent Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Predicting future motions of nearby agents is essential for an autonomous vehicle to take safe and effective actions. In this paper, we propose TSGN, a framework using Temporal Scene Graph Neural Networks with projected vectorized representations for multi-agent trajectory prediction. Projected vectorized representation models the traffic scene as a graph which is constructed by a set of vectors. These vectors represent agents, road network, and their spatial relative relationships. All relative features under this representation are both translationand rotation-invariant. Based on this representation, TSGN captures the spatial-temporal features across agents, road network, interactions among them, and temporal dependencies of temporal traffic scenes. TSGN can predict multimodal future trajectories for all agents simultaneously, plausibly, and accurately. Meanwhile, we propose a Hierarchical Lane Transformer for capturing interactions between agents and road network, which filters the surrounding road network and only keeps the most probable lane segments which could have an impact on the future behavior of the target agent. Without sacrificing the prediction performance, this greatly reduces the computational burden. Experiments show TSGN achieves state-of-the-art performance on the Argoverse motion forecasting benchmar.
<div id='section'>Paperid: <span id='pid'>1664, <a href='https://arxiv.org/pdf/2304.13678.pdf' target='_blank'>https://arxiv.org/pdf/2304.13678.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kai Armstrong, Lei Zhang, Yan Wen, Alexander P. Willmott, Paul Lee, Xujioing Ye
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.13678">A marker-less human motion analysis system for motion-based biomarker discovery in knee disorders</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years the NHS has been having increased difficulty seeing all low-risk patients, this includes but not limited to suspected osteoarthritis (OA) patients. To help address the increased waiting lists and shortages of staff, we propose a novel method of automated biomarker identification for diagnosis of knee disorders and the monitoring of treatment progression. The proposed method allows for the measurement and analysis of biomechanics and analyse their clinical significance, in both a cheap and sensitive alternative to the currently available commercial alternatives. These methods and results validate the capabilities of standard RGB cameras in clinical environments to capture motion and show that when compared to alternatives such as depth cameras there is a comparable accuracy in the clinical environment. Biomarker identification using Principal Component Analysis (PCA) allows the reduction of the dimensionality to produce the most representative features from motion data, these new biomarkers can then be used to assess the success of treatment and track the progress of rehabilitation. This was validated by applying these techniques on a case study utilising the exploratory use of local anaesthetic applied on knee pain, this allows these new representative biomarkers to be validated as statistically significant (p-value < 0.05).
<div id='section'>Paperid: <span id='pid'>1665, <a href='https://arxiv.org/pdf/2304.11764.pdf' target='_blank'>https://arxiv.org/pdf/2304.11764.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vinicius Trentin, Chenxu Ma, Jorge Villagra, Zaid Al-Ars
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.11764">Learning-enabled multi-modal motion prediction in urban environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion prediction is a key factor towards the full deployment of autonomous vehicles. It is fundamental in order to assure safety while navigating through highly interactive complex scenarios. In this work, the framework IAMP (Interaction- Aware Motion Prediction), producing multi-modal probabilistic outputs from the integration of a Dynamic Bayesian Network and Markov Chains, is extended with a learning-based approach. The integration of a machine learning model tackles the limitations of the ruled-based mechanism since it can better adapt to different driving styles and driving situations. The method here introduced generates context-dependent acceleration distributions used in a Markov-chain-based motion prediction. This hybrid approach results in better evaluation metrics when compared with the baseline in the four
<div id='section'>Paperid: <span id='pid'>1666, <a href='https://arxiv.org/pdf/2304.05345.pdf' target='_blank'>https://arxiv.org/pdf/2304.05345.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alireza Rahimpour, Navid Fallahinia, Devesh Upadhyay, Justin Miller
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.05345">FIR-based Future Trajectory Prediction in Nighttime Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The performance of the current collision avoidance systems in Autonomous Vehicles (AV) and Advanced Driver Assistance Systems (ADAS) can be drastically affected by low light and adverse weather conditions. Collisions with large animals such as deer in low light cause significant cost and damage every year. In this paper, we propose the first AI-based method for future trajectory prediction of large animals and mitigating the risk of collision with them in low light. In order to minimize false collision warnings, in our multi-step framework, first, the large animal is accurately detected and a preliminary risk level is predicted for it and low-risk animals are discarded. In the next stage, a multi-stream CONV-LSTM-based encoder-decoder framework is designed to predict the future trajectory of the potentially high-risk animals. The proposed model uses camera motion prediction as well as the local and global context of the scene to generate accurate predictions. Furthermore, this paper introduces a new dataset of FIR videos for large animal detection and risk estimation in real nighttime driving scenarios. Our experiments show promising results of the proposed framework in adverse conditions. Our code is available online.
<div id='section'>Paperid: <span id='pid'>1667, <a href='https://arxiv.org/pdf/2304.03771.pdf' target='_blank'>https://arxiv.org/pdf/2304.03771.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Brenda Elizabeth Olivas-Padilla, Alina Glushkova, Sotiris Manitsaris
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.03771">Motion Capture Benchmark of Real Industrial Tasks and Traditional Crafts for Human Movement Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human movement analysis is a key area of research in robotics, biomechanics, and data science. It encompasses tracking, posture estimation, and movement synthesis. While numerous methodologies have evolved over time, a systematic and quantitative evaluation of these approaches using verifiable ground truth data of three-dimensional human movement is still required to define the current state of the art. This paper presents seven datasets recorded using inertial-based motion capture. The datasets contain professional gestures carried out by industrial operators and skilled craftsmen performed in real conditions in-situ. The datasets were created with the intention of being used for research in human motion modeling, analysis, and generation. The protocols for data collection are described in detail, and a preliminary analysis of the collected data is provided as a benchmark. The Gesture Operational Model, a hybrid stochastic-biomechanical approach based on kinematic descriptors, is utilized to model the dynamics of the experts' movements and create mathematical representations of their motion trajectories for analysis and quantifying their body dexterity. The models allowed accurate the generation of human professional poses and an intuitive description of how body joints cooperate and change over time through the performance of the task.
<div id='section'>Paperid: <span id='pid'>1668, <a href='https://arxiv.org/pdf/2304.03459.pdf' target='_blank'>https://arxiv.org/pdf/2304.03459.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Henglai Wei, Guangyuan Li, Yang Lu, Hui Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.03459">Integrated motion control and energy management of series hybrid electric vehicles: A multi-objective MPC approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper considers the integrated motion control and energy management problems of the series hybrid electric vehicles (SHEV) with constraints. We propose a multi-objective model predictive control (MOMPC)-based energy management approach, which is embedded with the motion control to guarantee driving comfort. In addition, due to the slow response of the engine, it may cause excessive batter power when HEVs work in different conditions (e.g., uphill or sudden acceleration) with a certain request power; this implies the discharge current is too large. A battery current constraint is designed and incorporated into the MOMPC optimization problem and hence avoids the extra high charge-discharge current. This prevents potential safety hazards and extends the battery's life. Finally, numerical experiments are performed to verify the proposed approach.
<div id='section'>Paperid: <span id='pid'>1669, <a href='https://arxiv.org/pdf/2304.03326.pdf' target='_blank'>https://arxiv.org/pdf/2304.03326.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kartik Krishna, Steven L. Brunton, Zhuoyuan Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.03326">Finite Time Lyapunov Exponent Analysis of Model Predictive Control and Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Finite-time Lyapunov exponents (FTLEs) provide a powerful approach to compute time-varying analogs of invariant manifolds in unsteady fluid flow fields. These manifolds are useful to visualize the transport mechanisms of passive tracers advecting with the flow. However, many vehicles and mobile sensors are not passive, but are instead actuated according to some intelligent trajectory planning or control law; for example, model predictive control and reinforcement learning are often used to design energy-efficient trajectories in a dynamically changing background flow. In this work, we investigate the use of FTLE on such controlled agents to gain insight into optimal transport routes for navigation in known unsteady flows. We find that these controlled FTLE (cFTLE) coherent structures separate the flow field into different regions with similar costs of transport to the goal location. These separatrices are functions of the planning algorithm's hyper-parameters, such as the optimization time horizon and the cost of actuation. Computing the invariant sets and manifolds of active agent dynamics in dynamic flow fields is useful in the context of robust motion control, hyperparameter tuning, and determining safe and collision-free trajectories for autonomous systems. Moreover, these cFTLE structures provide insight into effective deployment locations for mobile agents with actuation and energy constraints to traverse the ocean or atmosphere.
<div id='section'>Paperid: <span id='pid'>1670, <a href='https://arxiv.org/pdf/2303.11765.pdf' target='_blank'>https://arxiv.org/pdf/2303.11765.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Achu Wilson, Helen Jiang, Wenzhao Lian, Wenzhen Yuan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.11765">Cable Routing and Assembly using Tactile-driven Motion Primitives</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Manipulating cables is challenging for robots because of the infinite degrees of freedom of the cables and frequent occlusion by the gripper and the environment. These challenges are further complicated by the dexterous nature of the operations required for cable routing and assembly, such as weaving and inserting, hampering common solutions with vision-only sensing. In this paper, we propose to integrate tactile-guided low-level motion control with high-level vision-based task parsing for a challenging task: cable routing and assembly on a reconfigurable task board. Specifically, we build a library of tactile-guided motion primitives using a fingertip GelSight sensor, where each primitive reliably accomplishes an operation such as cable following and weaving. The overall task is inferred via visual perception given a goal configuration image, and then used to generate the primitive sequence. Experiments demonstrate the effectiveness of individual tactile-guided primitives and the integrated end-to-end solution, significantly outperforming the method without tactile sensing. Our reconfigurable task setup and proposed baselines provide a benchmark for future research in cable manipulation. More details and video are presented in \url{https://helennn.github.io/cable-manip/}
<div id='section'>Paperid: <span id='pid'>1671, <a href='https://arxiv.org/pdf/2303.06277.pdf' target='_blank'>https://arxiv.org/pdf/2303.06277.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Avinash Ajit Nargund, Misha Sra
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.06277">SPOTR: Spatio-temporal Pose Transformers for Human Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D human motion prediction is a research area of high significance and a challenge in computer vision. It is useful for the design of many applications including robotics and autonomous driving. Traditionally, autogregressive models have been used to predict human motion. However, these models have high computation needs and error accumulation that make it difficult to use them for realtime applications. In this paper, we present a non-autogressive model for human motion prediction. We focus on learning spatio-temporal representations non-autoregressively for generation of plausible future motions. We propose a novel architecture that leverages the recently proposed Transformers. Human motion involves complex spatio-temporal dynamics with joints affecting the position and rotation of each other even though they are not connected directly. The proposed model extracts these dynamics using both convolutions and the self-attention mechanism. Using specialized spatial and temporal self-attention to augment the features extracted through convolution allows our model to generate spatio-temporally coherent predictions in parallel independent of the activity. Our contributions are threefold: (i) we frame human motion prediction as a sequence-to-sequence problem and propose a non-autoregressive Transformer to forecast a sequence of poses in parallel; (ii) our method is activity agnostic; (iii) we show that despite its simplicity, our approach is able to make accurate predictions, achieving better or comparable results compared to the state-of-the-art on two public datasets, with far fewer parameters and much faster inference.
<div id='section'>Paperid: <span id='pid'>1672, <a href='https://arxiv.org/pdf/2303.04569.pdf' target='_blank'>https://arxiv.org/pdf/2303.04569.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Janine Matschek, Johanna Bethge, Rolf Findeisen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.04569">Safe Machine-Learning-supported Model Predictive Force and Motion Control in Robotics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Many robotic tasks, such as human-robot interactions or the handling of fragile objects, require tight control and limitation of appearing forces and moments alongside sensible motion control to achieve safe yet high-performance operation. We propose a learning-supported model predictive force and motion control scheme that provides stochastic safety guarantees while adapting to changing situations. Gaussian processes are used to learn the uncertain relations that map the robot's states to the forces and moments. The model predictive controller uses these Gaussian process models to achieve precise motion and force control under stochastic constraint satisfaction. As the uncertainty only occurs in the static model parts -- the output equations -- a computationally efficient stochastic MPC formulation is used. Analysis of recursive feasibility of the optimal control problem and convergence of the closed loop system for the static uncertainty case are given. Chance constraint formulation and back-offs are constructed based on the variance of the Gaussian process to guarantee safe operation. The approach is illustrated on a lightweight robot in simulations and experiments.
<div id='section'>Paperid: <span id='pid'>1673, <a href='https://arxiv.org/pdf/2301.13545.pdf' target='_blank'>https://arxiv.org/pdf/2301.13545.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Daniel Grimm, Philip SchÃ¶rner, Moritz DreÃler, J. -Marius ZÃ¶llner
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.13545">Holistic Graph-based Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion prediction for automated vehicles in complex environments is a difficult task that is to be mastered when automated vehicles are to be used in arbitrary situations. Many factors influence the future motion of traffic participants starting with traffic rules and reaching from the interaction between each other to personal habits of human drivers. Therefore we present a novel approach for a graph-based prediction based on a heterogeneous holistic graph representation that combines temporal information, properties and relations between traffic participants as well as relations with static elements like the road network. The information are encoded through different types of nodes and edges that both are enriched with arbitrary features. We evaluated the approach on the INTERACTION and the Argoverse dataset and conducted an informative ablation study to demonstrate the benefit of different types of information for the motion prediction quality.
<div id='section'>Paperid: <span id='pid'>1674, <a href='https://arxiv.org/pdf/2301.10134.pdf' target='_blank'>https://arxiv.org/pdf/2301.10134.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Baptiste Chopin, Hao Tang, Mohamed Daoudi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.10134">Bipartite Graph Diffusion Model for Human Interaction Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The generation of natural human motion interactions is a hot topic in computer vision and computer animation. It is a challenging task due to the diversity of possible human motion interactions. Diffusion models, which have already shown remarkable generative capabilities in other domains, are a good candidate for this task. In this paper, we introduce a novel bipartite graph diffusion method (BiGraphDiff) to generate human motion interactions between two persons. Specifically, bipartite node sets are constructed to model the inherent geometric constraints between skeleton nodes during interactions. The interaction graph diffusion model is transformer-based, combining some state-of-the-art motion methods. We show that the proposed achieves new state-of-the-art results on leading benchmarks for the human interaction generation task.
<div id='section'>Paperid: <span id='pid'>1675, <a href='https://arxiv.org/pdf/2301.00939.pdf' target='_blank'>https://arxiv.org/pdf/2301.00939.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Emre Sariyildiz, Rahim Mutlu, Jon Roberts, Chin-Hsing Kuo, Barkan Ugurlu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.00939">Design and Control of a Novel Variable Stiffness Series Elastic Actuator</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper expounds the design and control of a new Variable Stiffness Series Elastic Actuator (VSSEA). It is established by employing a modular mechanical design approach that allows us to effectively optimise the stiffness modulation characteristics and power density of the actuator. The proposed VSSEA possesses the following features: i) no limitation in the work-range of output link, ii) a wide range of stiffness modulation (~20Nm/rad to ~1KNm/rad), iii) low-energy-cost stiffness modulation at equilibrium and non-equilibrium positions, iv) compact design and high torque density (~36Nm/kg), and v) high-speed stiffness modulation (~3000Nm/rad/s). Such features can help boost the safety and performance of many advanced robotic systems, e.g., a cobot that physically interacts with unstructured environments and an exoskeleton that provides physical assistance to human users. These features can also enable us to utilise variable stiffness property to attain various regulation and trajectory tracking control tasks only by employing conventional controllers, eliminating the need for synthesising complex motion control systems in compliant actuation. To this end, it is experimentally demonstrated that the proposed VSSEA is capable of precisely tracking desired position and force control references through the use of conventional Proportional-Integral-Derivative (PID) controllers.
<div id='section'>Paperid: <span id='pid'>1676, <a href='https://arxiv.org/pdf/2212.12760.pdf' target='_blank'>https://arxiv.org/pdf/2212.12760.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sina Saadati, Mohammadreza Razzazi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.12760">Agent-based Modeling and Simulation of Human Muscle For Development of Human Gait Analyzer Application</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite the fact that only a small portion of muscles are affected in motion disease and disorders, medical therapies do not distinguish between healthy and unhealthy muscles. In this paper, a method is devised in order to calculate the neural stimuli of the lower body during gait cycle and check if any group of muscles are not acting properly. For this reason, an agent-based model of human muscle is proposed. The agent is able to convert neural stimuli to force generated by the muscle and vice versa. It can be used in many researches including medical education and research and prosthesis development. Then, Boots algorithm is designed based on a biomechanical model of human lower body to do a reverse dynamics of human motion by computing the forces generated by each muscle group. Using the agent-driven model of human muscle and boots algorithm, a user-friendly application is developed which can calculate the number of neural stimuli received by each muscle during gait cycle. The application can be used by clinical experts to distinguish between healthy and unhealthy muscles.
<div id='section'>Paperid: <span id='pid'>1677, <a href='https://arxiv.org/pdf/2212.01055.pdf' target='_blank'>https://arxiv.org/pdf/2212.01055.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Erik GÃ¤rtner, Luke Metz, Mykhaylo Andriluka, C. Daniel Freeman, Cristian Sminchisescu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.01055">Transformer-Based Learned Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a new approach to learned optimization where we represent the computation of an optimizer's update step using a neural network. The parameters of the optimizer are then learned by training on a set of optimization tasks with the objective to perform minimization efficiently. Our innovation is a new neural network architecture, Optimus, for the learned optimizer inspired by the classic BFGS algorithm. As in BFGS, we estimate a preconditioning matrix as a sum of rank-one updates but use a Transformer-based neural network to predict these updates jointly with the step length and direction. In contrast to several recent learned optimization-based approaches, our formulation allows for conditioning across the dimensions of the parameter space of the target problem while remaining applicable to optimization tasks of variable dimensionality without retraining. We demonstrate the advantages of our approach on a benchmark composed of objective functions traditionally used for the evaluation of optimization algorithms, as well as on the real world-task of physics-based visual reconstruction of articulated 3d human motion.
<div id='section'>Paperid: <span id='pid'>1678, <a href='https://arxiv.org/pdf/2211.15603.pdf' target='_blank'>https://arxiv.org/pdf/2211.15603.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sai Shashank Kalakonda, Shubh Maheshwari, Ravi Kiran Sarvadevabhatla
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.15603">Action-GPT: Leveraging Large-scale Language Models for Improved and Generalized Action Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce Action-GPT, a plug-and-play framework for incorporating Large Language Models (LLMs) into text-based action generation models. Action phrases in current motion capture datasets contain minimal and to-the-point information. By carefully crafting prompts for LLMs, we generate richer and fine-grained descriptions of the action. We show that utilizing these detailed descriptions instead of the original action phrases leads to better alignment of text and motion spaces. We introduce a generic approach compatible with stochastic (e.g. VAE-based) and deterministic (e.g. MotionCLIP) text-to-motion models. In addition, the approach enables multiple text descriptions to be utilized. Our experiments show (i) noticeable qualitative and quantitative improvement in the quality of synthesized motions, (ii) benefits of utilizing multiple LLM-generated descriptions, (iii) suitability of the prompt function, and (iv) zero-shot generation capabilities of the proposed approach. Project page: https://actiongpt.github.io
<div id='section'>Paperid: <span id='pid'>1679, <a href='https://arxiv.org/pdf/2211.14309.pdf' target='_blank'>https://arxiv.org/pdf/2211.14309.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Christian Diller, Thomas Funkhouser, Angela Dai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.14309">FutureHuman3D: Forecasting Complex Long-Term 3D Human Behavior from Video Observations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a generative approach to forecast long-term future human behavior in 3D, requiring only weak supervision from readily available 2D human action data. This is a fundamental task enabling many downstream applications. The required ground-truth data is hard to capture in 3D (mocap suits, expensive setups) but easy to acquire in 2D (simple RGB cameras). Thus, we design our method to only require 2D RGB data at inference time while being able to generate 3D human motion sequences. We use a differentiable 2D projection scheme in an autoregressive manner for weak supervision, and an adversarial loss for 3D regularization. Our method predicts long and complex human behavior sequences (e.g., cooking, assembly) consisting of multiple sub-actions. We tackle this in a semantically hierarchical manner, jointly predicting high-level coarse action labels together with their low-level fine-grained realizations as characteristic 3D human poses. We observe that these two action representations are coupled in nature, and joint prediction benefits both action and pose forecasting. Our experiments demonstrate the complementary nature of joint action and 3D pose prediction: our joint approach outperforms each task treated individually, enables robust longer-term sequence prediction, and improves over alternative approaches to forecast actions and characteristic 3D poses.
<div id='section'>Paperid: <span id='pid'>1680, <a href='https://arxiv.org/pdf/2211.05231.pdf' target='_blank'>https://arxiv.org/pdf/2211.05231.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Joachim Ott, Shih-Chii Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.05231">Biologically-Inspired Continual Learning of Human Motion Sequences</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work proposes a model for continual learning on tasks involving temporal sequences, specifically, human motions. It improves on a recently proposed brain-inspired replay model (BI-R) by building a biologically-inspired conditional temporal variational autoencoder (BI-CTVAE), which instantiates a latent mixture-of-Gaussians for class representation. We investigate a novel continual-learning-to-generate (CL2Gen) scenario where the model generates motion sequences of different classes. The generative accuracy of the model is tested over a set of tasks. The final classification accuracy of BI-CTVAE on a human motion dataset after sequentially learning all action classes is 78%, which is 63% higher than using no-replay, and only 5.4% lower than a state-of-the-art offline trained GRU model.
<div id='section'>Paperid: <span id='pid'>1681, <a href='https://arxiv.org/pdf/2210.07697.pdf' target='_blank'>https://arxiv.org/pdf/2210.07697.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohammad Baradaran, Robert Bergevin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.07697">Multi-Task Learning based Video Anomaly Detection with Attention</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-task learning based video anomaly detection methods combine multiple proxy tasks in different branches to detect video anomalies in different situations. Most existing methods either do not combine complementary tasks to effectively cover all motion patterns, or the class of the objects is not explicitly considered. To address the aforementioned shortcomings, we propose a novel multi-task learning based method that combines complementary proxy tasks to better consider the motion and appearance features. We combine the semantic segmentation and future frame prediction tasks in a single branch to learn the object class and consistent motion patterns, and to detect respective anomalies simultaneously. In the second branch, we added several attention mechanisms to detect motion anomalies with attention to object parts, the direction of motion, and the distance of the objects from the camera. Our qualitative results show that the proposed method considers the object class effectively and learns motion with attention to the aforementioned important factors which results in a precise motion modeling and a better motion anomaly detection. Additionally, quantitative results show the superiority of our method compared with state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>1682, <a href='https://arxiv.org/pdf/2210.01199.pdf' target='_blank'>https://arxiv.org/pdf/2210.01199.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kensuke Nakamura, Somil Bansal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.01199">Online Update of Safety Assurances Using Confidence-Based Predictions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robots such as autonomous vehicles and assistive manipulators are increasingly operating in dynamic environments and close physical proximity to people. In such scenarios, the robot can leverage a human motion predictor to predict their future states and plan safe and efficient trajectories. However, no model is ever perfect -- when the observed human behavior deviates from the model predictions, the robot might plan unsafe maneuvers. Recent works have explored maintaining a confidence parameter in the human model to overcome this challenge, wherein the predicted human actions are tempered online based on the likelihood of the observed human action under the prediction model. This has opened up a new research challenge, i.e., \textit{how to compute the future human states online as the confidence parameter changes?} In this work, we propose a Hamilton-Jacobi (HJ) reachability-based approach to overcome this challenge. Treating the confidence parameter as a virtual state in the system, we compute a parameter-conditioned forward reachable tube (FRT) that provides the future human states as a function of the confidence parameter. Online, as the confidence parameter changes, we can simply query the corresponding FRT, and use it to update the robot plan. Computing parameter-conditioned FRT corresponds to an (offline) high-dimensional reachability problem, which we solve by leveraging recent advances in data-driven reachability analysis. Overall, our framework enables online maintenance and updates of safety assurances in human-robot interaction scenarios, even when the human prediction model is incorrect. We demonstrate our approach in several safety-critical autonomous driving scenarios, involving a state-of-the-art deep learning-based prediction model.
<div id='section'>Paperid: <span id='pid'>1683, <a href='https://arxiv.org/pdf/2209.09508.pdf' target='_blank'>https://arxiv.org/pdf/2209.09508.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Garen Haddeler, Hari P. Palanivelu, Yung Chuen Ng, Fabien Colonnier, Albertus H. Adiwahono, Zhibin Li, Chee-Meng Chew, Meng Yee Michael Chuah
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.09508">Real-time Digital Double Framework to Predict Collapsible Terrains for Legged Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Inspired by the digital twinning systems, a novel real-time digital double framework is developed to enhance robot perception of the terrain conditions. Based on the very same physical model and motion control, this work exploits the use of such simulated digital double synchronized with a real robot to capture and extract discrepancy information between the two systems, which provides high dimensional cues in multiple physical quantities to represent differences between the modelled and the real world. Soft, non-rigid terrains cause common failures in legged locomotion, whereby visual perception solely is insufficient in estimating such physical properties of terrains. We used digital double to develop the estimation of the collapsibility, which addressed this issue through physical interactions during dynamic walking. The discrepancy in sensory measurements between the real robot and its digital double are used as input of a learning-based algorithm for terrain collapsibility analysis. Although trained only in simulation, the learned model can perform collapsibility estimation successfully in both simulation and real world. Our evaluation of results showed the generalization to different scenarios and the advantages of the digital double to reliably detect nuances in ground conditions.
<div id='section'>Paperid: <span id='pid'>1684, <a href='https://arxiv.org/pdf/2209.03261.pdf' target='_blank'>https://arxiv.org/pdf/2209.03261.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tao Huang, Zhe Chen, Wang Gao, Zhenfeng Xue, Yong Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.03261">Cooperative trajectory planning algorithm of USV-UAV with hull dynamic constraints</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Efficient trajectory generation in complex dynamic environments remains an open problem in the unmanned surface vehicle (USV). The perception of the USV is usually interfered with by the swing of the hull and the ambient weather, making it challenging to plan the optimal USV trajectories. In this paper, a cooperative trajectory planning algorithm for the coupled USV-UAV system is proposed to ensure that USV can execute a safe and smooth path in the process of autonomous advance in multi-obstacle maps. Specifically, the unmanned aerial vehicle (UAV) plays the role of a flight sensor, providing real-time global map and obstacle information with a lightweight semantic segmentation network and 3D projection transformation. And then, an initial obstacle avoidance trajectory is generated by a graph-based search method. Concerning the unique under-actuated kinematic characteristics of the USV, a numerical optimization method based on hull dynamic constraints is introduced to make the trajectory easier to be tracked for motion control. Finally, a motion control method based on NMPC with the lowest energy consumption constraint during execution is proposed. Experimental results verify the effectiveness of the whole system, and the generated trajectory is locally optimal for USV with considerable tracking accuracy.
<div id='section'>Paperid: <span id='pid'>1685, <a href='https://arxiv.org/pdf/2207.06718.pdf' target='_blank'>https://arxiv.org/pdf/2207.06718.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Honghao Lv, Zhibo Pang, Ming Xiao, Geng Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2207.06718">Hardware-in-the-Loop Simulation for Evaluating Communication Impacts on the Wireless-Network-Controlled Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>More and more robot automation applications have changed to wireless communication, and network performance has a growing impact on robotic systems. This study proposes a hardware-in-the-loop (HiL) simulation methodology for connecting the simulated robot platform to real network devices. This project seeks to provide robotic engineers and researchers with the capability to experiment without heavily modifying the original controller and get more realistic test results that correlate with actual network conditions. We deployed this HiL simulation system in two common cases for wireless-network-controlled robotic applications: (1) safe multi-robot coordination for mobile robots, and (2) human-motion-based teleoperation for manipulators. The HiL simulation system is deployed and tested under various network conditions in all circumstances. The experiment results are analyzed and compared with the previous simulation methods, demonstrating that the proposed HiL simulation methodology can identify a more reliable communication impact on robot systems.
<div id='section'>Paperid: <span id='pid'>1686, <a href='https://arxiv.org/pdf/2203.12232.pdf' target='_blank'>https://arxiv.org/pdf/2203.12232.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yue Cao, Zhen Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2203.12232">Enhanced Contour Tracking: a Time-Varying Internal Model Principle-Based Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Contour tracking plays a crucial role in multi-axis motion control systems, and it requires both multi-axial contouring as well as standard servo performance in each axis. Among the existing contouring control methods, the cross coupled control (CCC) lacks of an asymptotical tracking performance for general contours, and the task coordinate frame (TCF) control usually leads to system nonlinearity, and by design is not well-suited for multi-axis contour tracking. Here we propose a novel time-varying internal model principle-based contouring control (TV-IMCC) methodology to enhance contour tracking performance with both axial and contour error reduction. The proposed TV-IMCC is twofold, including an extended position domain framework with master-slave structures for contour regulation, and a time-varying internal model principle-based controller for each axial tracking precision improvement. Specifically, a novel signal conversion algorithm is proposed with the extended position domain framework, hence the original n-axis contouring problem can be decoupled into (n-1) two-axis master-slave tracking problems in the position domain, and the class of contour candidates can be extended as well. With this, the time-varying internal model principle-based control method is proposed to deal with the time-varying dynamics in the axial systems resulted from the transformation between the time and position domains. Furthermore, the stability analysis is provided for the closed-loop system of the TV-IMCC. Various simulation and experimental results validate the TV-IMCC with enhanced contour tracking performance compared with the existing methods. Moreover, there is no strict requirement on the precision of the master axis, therefore a potential application of the TV-IMCC is multi-axis macro-micro motion systems.
<div id='section'>Paperid: <span id='pid'>1687, <a href='https://arxiv.org/pdf/2111.13907.pdf' target='_blank'>https://arxiv.org/pdf/2111.13907.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nefeli Andreou, Andreas Aristidou, Yiorgos Chrysanthou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2111.13907">Pose Representations for Deep Skeletal Animation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Data-driven character animation techniques rely on the existence of a properly established model of motion, capable of describing its rich context. However, commonly used motion representations often fail to accurately encode the full articulation of motion, or present artifacts. In this work, we address the fundamental problem of finding a robust pose representation for motion modeling, suitable for deep character animation, one that can better constrain poses and faithfully capture nuances correlated with skeletal characteristics. Our representation is based on dual quaternions, the mathematical abstractions with well-defined operations, which simultaneously encode rotational and positional orientation, enabling a hierarchy-aware encoding, centered around the root. We demonstrate that our representation overcomes common motion artifacts, and assess its performance compared to other popular representations. We conduct an ablation study to evaluate the impact of various losses that can be incorporated during learning. Leveraging the fact that our representation implicitly encodes skeletal motion attributes, we train a network on a dataset comprising of skeletons with different proportions, without the need to retarget them first to a universal skeleton, which causes subtle motion elements to be missed. We show that smooth and natural poses can be achieved, paving the way for fascinating applications.
<div id='section'>Paperid: <span id='pid'>1688, <a href='https://arxiv.org/pdf/2510.08482.pdf' target='_blank'>https://arxiv.org/pdf/2510.08482.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Onur Keleş, Aslı Özyürek, Gerardo Ortega, Kadir Gökgö, Esam Ghaleb
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.08482">The Visual Iconicity Challenge: Evaluating Vision-Language Models on Sign Language Form-Meaning Mapping</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Iconicity, the resemblance between linguistic form and meaning, is pervasive in signed languages, offering a natural testbed for visual grounding. For vision-language models (VLMs), the challenge is to recover such essential mappings from dynamic human motion rather than static context. We introduce the \textit{Visual Iconicity Challenge}, a novel video-based benchmark that adapts psycholinguistic measures to evaluate VLMs on three tasks: (i) phonological sign-form prediction (e.g., handshape, location), (ii) transparency (inferring meaning from visual form), and (iii) graded iconicity ratings. We assess $13$ state-of-the-art VLMs in zero- and few-shot settings on Sign Language of the Netherlands and compare them to human baselines. On \textit{phonological form prediction}, VLMs recover some handshape and location detail but remain below human performance; on \textit{transparency}, they are far from human baselines; and only top models correlate moderately with human \textit{iconicity ratings}. Interestingly, \textit{models with stronger phonological form prediction correlate better with human iconicity judgment}, indicating shared sensitivity to visually grounded structure. Our findings validate these diagnostic tasks and motivate human-centric signals and embodied learning methods for modelling iconicity and improving visual grounding in multimodal models.
<div id='section'>Paperid: <span id='pid'>1689, <a href='https://arxiv.org/pdf/2510.08406.pdf' target='_blank'>https://arxiv.org/pdf/2510.08406.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Filip Bečanović, Kosta Jovanović, Vincent Bonnet
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.08406">Reliability of Single-Level Equality-Constrained Inverse Optimal Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Inverse optimal control (IOC) allows the retrieval of optimal cost function weights, or behavioral parameters, from human motion. The literature on IOC uses methods that are either based on a slow bilevel process or a fast but noise-sensitive minimization of optimality condition violation. Assuming equality-constrained optimal control models of human motion, this article presents a faster but robust approach to solving IOC using a single-level reformulation of the bilevel method and yields equivalent results. Through numerical experiments in simulation, we analyze the robustness to noise of the proposed single-level reformulation to the bilevel IOC formulation with a human-like planar reaching task that is used across recent studies. The approach shows resilience to very large levels of noise and reduces the computation time of the IOC on this task by a factor of 15 when compared to a classical bilevel implementation.
<div id='section'>Paperid: <span id='pid'>1690, <a href='https://arxiv.org/pdf/2510.06251.pdf' target='_blank'>https://arxiv.org/pdf/2510.06251.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ieva Bagdonaviciute, Vibhav Vineet
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.06251">Does Physics Knowledge Emerge in Frontier Models?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Leading Vision-Language Models (VLMs) show strong results in visual perception and general reasoning, but their ability to understand and predict physical dynamics remains unclear. We benchmark six frontier VLMs on three physical simulation datasets - CLEVRER, Physion, and Physion++ - where the evaluation tasks test whether a model can predict outcomes or hypothesize about alternative situations. To probe deeper, we design diagnostic subtests that isolate perception (objects, colors, occluders) from physics reasoning (motion prediction, spatial relations). Intuitively, stronger diagnostic performance should support higher evaluation accuracy. Yet our analysis reveals weak correlations: models that excel at perception or physics reasoning do not consistently perform better on predictive or counterfactual evaluation. This counterintuitive gap exposes a central limitation of current VLMs: perceptual and physics skills remain fragmented and fail to combine into causal understanding, underscoring the need for architectures that bind perception and reasoning more tightly.
<div id='section'>Paperid: <span id='pid'>1691, <a href='https://arxiv.org/pdf/2510.05957.pdf' target='_blank'>https://arxiv.org/pdf/2510.05957.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vaughn Gzenda, Robin Chhabra
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.05957">Learning to Crawl: Latent Model-Based Reinforcement Learning for Soft Robotic Adaptive Locomotion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Soft robotic crawlers are mobile robots that utilize soft body deformability and compliance to achieve locomotion through surface contact. Designing control strategies for such systems is challenging due to model inaccuracies, sensor noise, and the need to discover locomotor gaits. In this work, we present a model-based reinforcement learning (MB-RL) framework in which latent dynamics inferred from onboard sensors serve as a predictive model that guides an actor-critic algorithm to optimize locomotor policies. We evaluate the framework on a minimal crawler model in simulation using inertial measurement units and time-of-flight sensors as observations. The learned latent dynamics enable short-horizon motion prediction while the actor-critic discovers effective locomotor policies. This approach highlights the potential of latent-dynamics MB-RL for enabling embodied soft robotic adaptive locomotion based solely on noisy sensor feedback.
<div id='section'>Paperid: <span id='pid'>1692, <a href='https://arxiv.org/pdf/2510.03496.pdf' target='_blank'>https://arxiv.org/pdf/2510.03496.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vadivelan Murugesan, Rajasundaram Mathiazhagan, Sanjana Joshi, Aliasghar Arab
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03496">Digital-Twin Evaluation for Proactive Human-Robot Collision Avoidance via Prediction-Guided A-RRT*</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human-robot collaboration requires precise prediction of human motion over extended horizons to enable proactive collision avoidance. Unlike existing planners that rely solely on kinodynamic models, we present a prediction-driven safe planning framework that leverages granular, joint-by-joint human motion forecasting validated in a physics-based digital twin. A capsule-based artificial potential field (APF) converts these granular predictions into collision risk metrics, triggering an Adaptive RRT* (A-RRT*) planner when thresholds are exceeded. The depth camera is used to extract 3D skeletal poses and a convolutional neural network-bidirectional long short-term memory (CNN-BiLSTM) model to predict individual joint trajectories ahead of time. A digital twin model integrates real-time human posture prediction placed in front of a simulated robot to evaluate motions and physical contacts. The proposed method enables validation of planned trajectories ahead of time and bridging potential latency gaps in updating planned trajectories in real-time. In 50 trials, our method achieved 100% proactive avoidance with > 250 mm clearance and sub-2 s replanning, demonstrating superior precision and reliability compared to existing kinematic-only planners through the integration of predictive human modeling with digital twin validation.
<div id='section'>Paperid: <span id='pid'>1693, <a href='https://arxiv.org/pdf/2509.20927.pdf' target='_blank'>https://arxiv.org/pdf/2509.20927.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Akihisa Watanabe, Jiawei Ren, Li Siyao, Yichen Peng, Erwin Wu, Edgar Simo-Serra
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20927">SimDiff: Simulator-constrained Diffusion Model for Physically Plausible Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating physically plausible human motion is crucial for applications such as character animation and virtual reality. Existing approaches often incorporate a simulator-based motion projection layer to the diffusion process to enforce physical plausibility. However, such methods are computationally expensive due to the sequential nature of the simulator, which prevents parallelization. We show that simulator-based motion projection can be interpreted as a form of guidance, either classifier-based or classifier-free, within the diffusion process. Building on this insight, we propose SimDiff, a Simulator-constrained Diffusion Model that integrates environment parameters (e.g., gravity, wind) directly into the denoising process. By conditioning on these parameters, SimDiff generates physically plausible motions efficiently, without repeated simulator calls at inference, and also provides fine-grained control over different physical coefficients. Moreover, SimDiff successfully generalizes to unseen combinations of environmental parameters, demonstrating compositional generalization.
<div id='section'>Paperid: <span id='pid'>1694, <a href='https://arxiv.org/pdf/2509.07990.pdf' target='_blank'>https://arxiv.org/pdf/2509.07990.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Charan Gajjala Chenchu, Kinam Kim, Gao Lu, Zia Ud Din
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.07990">Signals vs. Videos: Advancing Motion Intention Recognition for Human-Robot Collaboration in Construction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human-robot collaboration (HRC) in the construction industry depends on precise and prompt recognition of human motion intentions and actions by robots to maximize safety and workflow efficiency. There is a research gap in comparing data modalities, specifically signals and videos, for motion intention recognition. To address this, the study leverages deep learning to assess two different modalities in recognizing workers' motion intention at the early stage of movement in drywall installation tasks. The Convolutional Neural Network - Long Short-Term Memory (CNN-LSTM) model utilizing surface electromyography (sEMG) data achieved an accuracy of around 87% with an average time of 0.04 seconds to perform prediction on a sample input. Meanwhile, the pre-trained Video Swin Transformer combined with transfer learning harnessed video sequences as input to recognize motion intention and attained an accuracy of 94% but with a longer average time of 0.15 seconds for a similar prediction. This study emphasizes the unique strengths and trade-offs of both data formats, directing their systematic deployments to enhance HRC in real-world construction projects.
<div id='section'>Paperid: <span id='pid'>1695, <a href='https://arxiv.org/pdf/2509.04984.pdf' target='_blank'>https://arxiv.org/pdf/2509.04984.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Koji Matsuno, Chien Chern Cheah
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.04984">Lyapunov-Based Deep Learning Control for Robots with Unknown Jacobian</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning, with its exceptional learning capabilities and flexibility, has been widely applied in various applications. However, its black-box nature poses a significant challenge in real-time robotic applications, particularly in robot control, where trustworthiness and robustness are critical in ensuring safety. In robot motion control, it is essential to analyze and ensure system stability, necessitating the establishment of methodologies that address this need. This paper aims to develop a theoretical framework for end-to-end deep learning control that can be integrated into existing robot control theories. The proposed control algorithm leverages a modular learning approach to update the weights of all layers in real time, ensuring system stability based on Lyapunov-like analysis. Experimental results on industrial robots are presented to illustrate the performance of the proposed deep learning controller. The proposed method offers an effective solution to the black-box problem in deep learning, demonstrating the possibility of deploying real-time deep learning strategies for robot kinematic control in a stable manner. This achievement provides a critical foundation for future advancements in deep learning based real-time robotic applications.
<div id='section'>Paperid: <span id='pid'>1696, <a href='https://arxiv.org/pdf/2509.03238.pdf' target='_blank'>https://arxiv.org/pdf/2509.03238.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Martin Goubej, Lauria Clarke, Martin Hrabačka, David Tolar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03238">Vibration Damping in Underactuated Cable-suspended Artwork -- Flying Belt Motion Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a comprehensive refurbishment of the interactive robotic art installation Standards and Double Standards by Rafael Lozano-Hemmer. The installation features an array of belts suspended from the ceiling, each actuated by stepper motors and dynamically oriented by a vision-based tracking system that follows the movements of exhibition visitors. The original system was limited by oscillatory dynamics, resulting in torsional and pendulum-like vibrations that constrained rotational speed and reduced interactive responsiveness. To address these challenges, the refurbishment involved significant upgrades to both hardware and motion control algorithms. A detailed mathematical model of the flying belt system was developed to accurately capture its dynamic behavior, providing a foundation for advanced control design. An input shaping method, formulated as a convex optimization problem, was implemented to effectively suppress vibrations, enabling smoother and faster belt movements. Experimental results demonstrate substantial improvements in system performance and audience interaction. This work exemplifies the integration of robotics, control engineering, and interactive art, offering new solutions to technical challenges in real-time motion control and vibration damping for large-scale kinetic installations.
<div id='section'>Paperid: <span id='pid'>1697, <a href='https://arxiv.org/pdf/2509.00767.pdf' target='_blank'>https://arxiv.org/pdf/2509.00767.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yangsong Zhang, Abdul Ahad Butt, Gül Varol, Ivan Laptev
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.00767">InterPose: Learning to Generate Human-Object Interactions from Large-Scale Web Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion generation has shown great advances thanks to the recent diffusion models trained on large-scale motion capture data. Most of existing works, however, currently target animation of isolated people in empty scenes. Meanwhile, synthesizing realistic human-object interactions in complex 3D scenes remains a critical challenge in computer graphics and robotics. One obstacle towards generating versatile high-fidelity human-object interactions is the lack of large-scale datasets with diverse object manipulations. Indeed, existing motion capture data is typically restricted to single people and manipulations of limited sets of objects. To address this issue, we propose an automatic motion extraction pipeline and use it to collect interaction-rich human motions. Our new dataset InterPose contains 73.8K sequences of 3D human motions and corresponding text captions automatically obtained from 45.8K videos with human-object interactions. We perform extensive experiments and demonstrate InterPose to bring significant improvements to state-of-the-art methods for human motion generation. Moreover, using InterPose we develop an LLM-based agent enabling zero-shot animation of people interacting with diverse objects and scenes.
<div id='section'>Paperid: <span id='pid'>1698, <a href='https://arxiv.org/pdf/2508.20740.pdf' target='_blank'>https://arxiv.org/pdf/2508.20740.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuki Tanaka, Seiichiro Katsura
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.20740">Non-expert to Expert Motion Translation Using Generative Adversarial Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Decreasing skilled workers is a very serious problem in the world. To deal with this problem, the skill transfer from experts to robots has been researched. These methods which teach robots by human motion are called imitation learning. Experts' skills generally appear in not only position data, but also force data. Thus, position and force data need to be saved and reproduced. To realize this, a lot of research has been conducted in the framework of a motion-copying system. Recent research uses machine learning methods to generate motion commands. However, most of them could not change tasks by following human intention. Some of them can change tasks by conditional training, but the labels are limited. Thus, we propose the flexible motion translation method by using Generative Adversarial Networks. The proposed method enables users to teach robots tasks by inputting data, and skills by a trained model. We evaluated the proposed system with a 3-DOF calligraphy robot.
<div id='section'>Paperid: <span id='pid'>1699, <a href='https://arxiv.org/pdf/2508.06547.pdf' target='_blank'>https://arxiv.org/pdf/2508.06547.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Heran Wu, Zirun Zhou, Jingfeng Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.06547">A tutorial note on collecting simulated data for vision-language-action models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Traditional robotic systems typically decompose intelligence into independent modules for computer vision, natural language processing, and motion control. Vision-Language-Action (VLA) models fundamentally transform this approach by employing a single neural network that can simultaneously process visual observations, understand human instructions, and directly output robot actions -- all within a unified framework. However, these systems are highly dependent on high-quality training datasets that can capture the complex relationships between visual observations, language instructions, and robotic actions. This tutorial reviews three representative systems: the PyBullet simulation framework for flexible customized data generation, the LIBERO benchmark suite for standardized task definition and evaluation, and the RT-X dataset collection for large-scale multi-robot data acquisition. We demonstrated dataset generation approaches in PyBullet simulation and customized data collection within LIBERO, and provide an overview of the characteristics and roles of the RT-X dataset for large-scale multi-robot data acquisition.
<div id='section'>Paperid: <span id='pid'>1700, <a href='https://arxiv.org/pdf/2508.05091.pdf' target='_blank'>https://arxiv.org/pdf/2508.05091.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingxuan He, Busheng Su, Finn Wong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05091">PoseGen: In-Context LoRA Finetuning for Pose-Controllable Long Human Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating long, temporally coherent videos with precise control over subject identity and motion is a formidable challenge for current diffusion models, which often suffer from identity drift and are limited to short clips. We introduce PoseGen, a novel framework that generates arbitrarily long videos of a specific subject from a single reference image and a driving pose sequence. Our core innovation is an in-context LoRA finetuning strategy that injects subject appearance at the token level for identity preservation, while simultaneously conditioning on pose information at the channel level for fine-grained motion control. To overcome duration limits, PoseGen pioneers an interleaved segment generation method that seamlessly stitches video clips together, using a shared KV cache mechanism and a specialized transition process to ensure background consistency and temporal smoothness. Trained on a remarkably small 33-hour video dataset, extensive experiments show that PoseGen significantly outperforms state-of-the-art methods in identity fidelity, pose accuracy, and its unique ability to produce coherent, artifact-free videos of unlimited duration.
<div id='section'>Paperid: <span id='pid'>1701, <a href='https://arxiv.org/pdf/2508.04847.pdf' target='_blank'>https://arxiv.org/pdf/2508.04847.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Md Zahidul Hasan, A. Ben Hamza, Nizar Bouguila
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.04847">LuKAN: A Kolmogorov-Arnold Network Framework for 3D Human Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The goal of 3D human motion prediction is to forecast future 3D poses of the human body based on historical motion data. Existing methods often face limitations in achieving a balance between prediction accuracy and computational efficiency. In this paper, we present LuKAN, an effective model based on Kolmogorov-Arnold Networks (KANs) with Lucas polynomial activations. Our model first applies the discrete wavelet transform to encode temporal information in the input motion sequence. Then, a spatial projection layer is used to capture inter-joint dependencies, ensuring structural consistency of the human body. At the core of LuKAN is the Temporal Dependency Learner, which employs a KAN layer parameterized by Lucas polynomials for efficient function approximation. These polynomials provide computational efficiency and an enhanced capability to handle oscillatory behaviors. Finally, the inverse discrete wavelet transform reconstructs motion sequences in the time domain, generating temporally coherent predictions. Extensive experiments on three benchmark datasets demonstrate the competitive performance of our model compared to strong baselines, as evidenced by both quantitative and qualitative evaluations. Moreover, its compact architecture coupled with the linear recurrence of Lucas polynomials, ensures computational efficiency.
<div id='section'>Paperid: <span id='pid'>1702, <a href='https://arxiv.org/pdf/2507.23350.pdf' target='_blank'>https://arxiv.org/pdf/2507.23350.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mahmoud Ghorab, Matthias Lorenzen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.23350">Multi-Waypoint Path Planning and Motion Control for Non-holonomic Mobile Robots in Agricultural Applications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>There is a growing demand for autonomous mobile robots capable of navigating unstructured agricultural environments. Tasks such as weed control in meadows require efficient path planning through an unordered set of coordinates while minimizing travel distance and adhering to curvature constraints to prevent soil damage and protect vegetation. This paper presents an integrated navigation framework combining a global path planner based on the Dubins Traveling Salesman Problem (DTSP) with a Nonlinear Model Predictive Control (NMPC) strategy for local path planning and control. The DTSP generates a minimum-length, curvature-constrained path that efficiently visits all targets, while the NMPC leverages this path to compute control signals to accurately reach each waypoint. The system's performance was validated through comparative simulation analysis on real-world field datasets, demonstrating that the coupled DTSP-based planner produced smoother and shorter paths, with a reduction of about 16% in the provided scenario, compared to decoupled methods. Based thereon, the NMPC controller effectively steered the robot to the desired waypoints, while locally optimizing the trajectory and ensuring adherence to constraints. These findings demonstrate the potential of the proposed framework for efficient autonomous navigation in agricultural environments.
<div id='section'>Paperid: <span id='pid'>1703, <a href='https://arxiv.org/pdf/2507.19119.pdf' target='_blank'>https://arxiv.org/pdf/2507.19119.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanghong Liu, Xingping Dong, Ming Li, Weixing Zhang, Yidong Lou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.19119">PatchTraj: Unified Time-Frequency Representation Learning via Dynamic Patches for Trajectory Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pedestrian trajectory prediction is crucial for autonomous driving and robotics. While existing point-based and grid-based methods expose two main limitations: insufficiently modeling human motion dynamics, as they fail to balance local motion details with long-range spatiotemporal dependencies, and the time representations lack interaction with their frequency components in jointly modeling trajectory sequences. To address these challenges, we propose PatchTraj, a dynamic patch-based framework that integrates time-frequency joint modeling for trajectory prediction. Specifically, we decompose the trajectory into raw time sequences and frequency components, and employ dynamic patch partitioning to perform multi-scale segmentation, capturing hierarchical motion patterns. Each patch undergoes adaptive embedding with scale-aware feature extraction, followed by hierarchical feature aggregation to model both fine-grained and long-range dependencies. The outputs of the two branches are further enhanced via cross-modal attention, facilitating complementary fusion of temporal and spectral cues. The resulting enhanced embeddings exhibit strong expressive power, enabling accurate predictions even when using a vanilla Transformer architecture. Extensive experiments on ETH-UCY, SDD, NBA, and JRDB datasets demonstrate that our method achieves state-of-the-art performance. Notably, on the egocentric JRDB dataset, PatchTraj attains significant relative improvements of 26.7% in ADE and 17.4% in FDE, underscoring its substantial potential in embodied intelligence.
<div id='section'>Paperid: <span id='pid'>1704, <a href='https://arxiv.org/pdf/2507.18979.pdf' target='_blank'>https://arxiv.org/pdf/2507.18979.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Deokjin Lee, Junho Song, Alireza Karimi, Sehoon Oh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.18979">Frequency Response Data-Driven Disturbance Observer Design for Flexible Joint Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion control of flexible joint robots (FJR) is challenged by inherent flexibility and configuration-dependent variations in system dynamics. While disturbance observers (DOB) can enhance system robustness, their performance is often limited by the elasticity of the joints and the variations in system parameters, which leads to a conservative design of the DOB. This paper presents a novel frequency response function (FRF)-based optimization method aimed at improving DOB performance, even in the presence of flexibility and system variability. The proposed method maximizes control bandwidth and effectively suppresses vibrations, thus enhancing overall system performance. Closed-loop stability is rigorously proven using the Nyquist stability criterion. Experimental validation on a FJR demonstrates that the proposed approach significantly improves robustness and motion performance, even under conditions of joint flexibility and system variation.
<div id='section'>Paperid: <span id='pid'>1705, <a href='https://arxiv.org/pdf/2507.18052.pdf' target='_blank'>https://arxiv.org/pdf/2507.18052.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>David Sinclair, Ademyemi Ademola, Babis Koniaris, Kenny Mitchell
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.18052">DanceGraph: A Complementary Architecture for Synchronous Dancing Online</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>DanceGraph is an architecture for synchronized online dancing overcoming the latency of networked body pose sharing. We break down this challenge by developing a real-time bandwidth-efficient architecture to minimize lag and reduce the timeframe of required motion prediction for synchronization with the music's rhythm. In addition, we show an interactive method for the parameterized stylization of dance motions for rhythmic dance using online dance correctives.
<div id='section'>Paperid: <span id='pid'>1706, <a href='https://arxiv.org/pdf/2507.13179.pdf' target='_blank'>https://arxiv.org/pdf/2507.13179.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyu Zhong, BjÃ¶rn Landfeldt, GÃ¼nter Alce, Hector A Caltenco
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.13179">Predictability-Aware Motion Prediction for Edge XR via High-Order Error-State Kalman Filtering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As 6G networks are developed and defined, offloading of XR applications is emerging as one of the strong new use cases. The reduced 6G latency coupled with edge processing infrastructure will for the first time provide a realistic offloading scenario in cellular networks where several computationally intensive functions, including rendering, can migrate from the user device and into the network. A key advantage of doing so is the lowering of the battery needs in the user devices and the possibility to design new devices with smaller form factors. However, offloading introduces increased delays compared to local execution, primarily due to network transmission latency and queuing delays at edge servers, especially under multi-user concurrency. Despite the computational power of edge platforms, the resulting motion-to-photon (MTP) latency negatively impacts user experience. To mitigate this, motion prediction has been proposed to offset delays. Existing approaches build on either deep learning or Kalman filtering. Deep learning techniques face scalability limitations at the resource-constrained edge, as their computational expense intensifies with increasing user concurrency, while Kalman filtering suffers from poor handling of complex movements and fragility to packet loss inherent in 6G's high-frequency radio interfaces. In this work, we introduce a context-aware error-state Kalman filter (ESKF) prediction framework, which forecasts the user's head motion trajectory to compensate for MTP latency in remote XR. By integrating a motion classifier that categorizes head motions based on their predictability, our algorithm demonstrates reduced prediction error across different motion classes. Our findings demonstrate that the optimized ESKF not only surpasses traditional Kalman filters in positional and orientational accuracy but also exhibits enhanced robustness and resilience to packet loss.
<div id='section'>Paperid: <span id='pid'>1707, <a href='https://arxiv.org/pdf/2507.12138.pdf' target='_blank'>https://arxiv.org/pdf/2507.12138.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Michal Heker, Sefy Kararlitsky, David Tolpin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.12138">Neural Human Pose Prior</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a principled, data-driven approach for modeling a neural prior over human body poses using normalizing flows. Unlike heuristic or low-expressivity alternatives, our method leverages RealNVP to learn a flexible density over poses represented in the 6D rotation format. We address the challenge of modeling distributions on the manifold of valid 6D rotations by inverting the Gram-Schmidt process during training, enabling stable learning while preserving downstream compatibility with rotation-based frameworks. Our architecture and training pipeline are framework-agnostic and easily reproducible. We demonstrate the effectiveness of the learned prior through both qualitative and quantitative evaluations, and we analyze its impact via ablation studies. This work provides a sound probabilistic foundation for integrating pose priors into human motion capture and reconstruction pipelines.
<div id='section'>Paperid: <span id='pid'>1708, <a href='https://arxiv.org/pdf/2507.06530.pdf' target='_blank'>https://arxiv.org/pdf/2507.06530.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kazi Mahathir Rahman, Naveed Imtiaz Nafis, Md. Farhan Sadik, Mohammad Al Rafi, Mehedi Hasan Shahed
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06530">Speak2Sign3D: A Multi-modal Pipeline for English Speech to American Sign Language Animation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Helping deaf and hard-of-hearing people communicate more easily is the main goal of Automatic Sign Language Translation. Although most past research has focused on turning sign language into text, doing the reverse, turning spoken English into sign language animations, has been largely overlooked. That's because it involves multiple steps, such as understanding speech, translating it into sign-friendly grammar, and generating natural human motion. In this work, we introduce a complete pipeline that converts English speech into smooth, realistic 3D sign language animations. Our system starts with Whisper to translate spoken English into text. Then, we use a MarianMT machine translation model to translate that text into American Sign Language (ASL) gloss, a simplified version of sign language that captures meaning without grammar. This model performs well, reaching BLEU scores of 0.7714 and 0.8923. To make the gloss translation more accurate, we also use word embeddings such as Word2Vec and FastText to understand word meanings. Finally, we animate the translated gloss using a 3D keypoint-based motion system trained on Sign3D-WLASL, a dataset we created by extracting body, hand, and face key points from real ASL videos in the WLASL dataset. To support the gloss translation stage, we also built a new dataset called BookGlossCorpus-CG, which turns everyday English sentences from the BookCorpus dataset into ASL gloss using grammar rules. Our system stitches everything together by smoothly interpolating between signs to create natural, continuous animations. Unlike previous works like How2Sign and Phoenix-2014T that focus on recognition or use only one type of data, our pipeline brings together audio, text, and motion in a single framework that goes all the way from spoken English to lifelike 3D sign language animation.
<div id='section'>Paperid: <span id='pid'>1709, <a href='https://arxiv.org/pdf/2507.05867.pdf' target='_blank'>https://arxiv.org/pdf/2507.05867.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nikita Savin, Elena Ambrosovskaya, Dmitry Romaev, Anton Proskurnikov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.05867">Assessing Linear Control Strategies for Zero-Speed Fin Roll Damping</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Roll stabilization is a critical aspect of ship motion control, particularly for vessels operating in low-speed or zero-speed conditions, where traditional hydrodynamic fins lose their effectiveness. In this paper, we consider a roll damping system, developed by Navis JSC, based on two actively controlled zero-speed fins. Unlike conventional fin stabilizers, zero-speed fins employ a drag-based mechanism and active oscillations to generate stabilizing forces even when the vessel is stationary. We propose a simple linear control architecture that, however, accounts for nonlinear drag forces and actuator limitations. Simulation results on a high-fidelity vessel model used for HIL testing demonstrate the effectiveness of the proposed approach.
<div id='section'>Paperid: <span id='pid'>1710, <a href='https://arxiv.org/pdf/2507.01737.pdf' target='_blank'>https://arxiv.org/pdf/2507.01737.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lin Wu, Zhixiang Chen, Jianglin Lan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.01737">HOI-Dyn: Learning Interaction Dynamics for Human-Object Motion Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating realistic 3D human-object interactions (HOIs) remains a challenging task due to the difficulty of modeling detailed interaction dynamics. Existing methods treat human and object motions independently, resulting in physically implausible and causally inconsistent behaviors. In this work, we present HOI-Dyn, a novel framework that formulates HOI generation as a driver-responder system, where human actions drive object responses. At the core of our method is a lightweight transformer-based interaction dynamics model that explicitly predicts how objects should react to human motion. To further enforce consistency, we introduce a residual-based dynamics loss that mitigates the impact of dynamics prediction errors and prevents misleading optimization signals. The dynamics model is used only during training, preserving inference efficiency. Through extensive qualitative and quantitative experiments, we demonstrate that our approach not only enhances the quality of HOI generation but also establishes a feasible metric for evaluating the quality of generated interactions.
<div id='section'>Paperid: <span id='pid'>1711, <a href='https://arxiv.org/pdf/2507.00676.pdf' target='_blank'>https://arxiv.org/pdf/2507.00676.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Edward Effendy, Kuan-Wei Tseng, Rei Kawakami
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.00676">A Unified Transformer-Based Framework with Pretraining For Whole Body Grasping Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accepted in the ICIP 2025
  We present a novel transformer-based framework for whole-body grasping that addresses both pose generation and motion infilling, enabling realistic and stable object interactions. Our pipeline comprises three stages: Grasp Pose Generation for full-body grasp generation, Temporal Infilling for smooth motion continuity, and a LiftUp Transformer that refines downsampled joints back to high-resolution markers. To overcome the scarcity of hand-object interaction data, we introduce a data-efficient Generalized Pretraining stage on large, diverse motion datasets, yielding robust spatio-temporal representations transferable to grasping tasks. Experiments on the GRAB dataset show that our method outperforms state-of-the-art baselines in terms of coherence, stability, and visual realism. The modular design also supports easy adaptation to other human-motion applications.
<div id='section'>Paperid: <span id='pid'>1712, <a href='https://arxiv.org/pdf/2506.17996.pdf' target='_blank'>https://arxiv.org/pdf/2506.17996.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>David Tolpin, Sefy Kagarlitsky
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.17996">Fast Neural Inverse Kinematics on Human Body Motions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Markerless motion capture enables the tracking of human motion without requiring physical markers or suits, offering increased flexibility and reduced costs compared to traditional systems. However, these advantages often come at the expense of higher computational demands and slower inference, limiting their applicability in real-time scenarios. In this technical report, we present a fast and reliable neural inverse kinematics framework designed for real-time capture of human body motions from 3D keypoints. We describe the network architecture, training methodology, and inference procedure in detail. Our framework is evaluated both qualitatively and quantitatively, and we support key design decisions through ablation studies.
<div id='section'>Paperid: <span id='pid'>1713, <a href='https://arxiv.org/pdf/2506.14563.pdf' target='_blank'>https://arxiv.org/pdf/2506.14563.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jesse St. Amand, Leonardo Gizzi, Martin A. Giese
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.14563">Single-Example Learning in a Mixture of GPDMs with Latent Geometries</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present the Gaussian process dynamical mixture model (GPDMM) and show its utility in single-example learning of human motion data. The Gaussian process dynamical model (GPDM) is a form of the Gaussian process latent variable model (GPLVM), but optimized with a hidden Markov model dynamical prior. The GPDMM combines multiple GPDMs in a probabilistic mixture-of-experts framework, utilizing embedded geometric features to allow for diverse sequences to be encoded in a single latent space, enabling the categorization and generation of each sequence class. GPDMs and our mixture model are particularly advantageous in addressing the challenges of modeling human movement in scenarios where data is limited and model interpretability is vital, such as in patient-specific medical applications like prosthesis control. We score the GPDMM on classification accuracy and generative ability in single-example learning, showcase model variations, and benchmark it against LSTMs, VAEs, and transformers.
<div id='section'>Paperid: <span id='pid'>1714, <a href='https://arxiv.org/pdf/2506.14428.pdf' target='_blank'>https://arxiv.org/pdf/2506.14428.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruihao Xi, Xuekuan Wang, Yongcheng Li, Shuhua Li, Zichen Wang, Yiwei Wang, Feng Wei, Cairong Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.14428">Toward Rich Video Human-Motion2D Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating realistic and controllable human motions, particularly those involving rich multi-character interactions, remains a significant challenge due to data scarcity and the complexities of modeling inter-personal dynamics. To address these limitations, we first introduce a new large-scale rich video human motion 2D dataset (Motion2D-Video-150K) comprising 150,000 video sequences. Motion2D-Video-150K features a balanced distribution of diverse single-character and, crucially, double-character interactive actions, each paired with detailed textual descriptions. Building upon this dataset, we propose a novel diffusion-based rich video human motion2D generation (RVHM2D) model. RVHM2D incorporates an enhanced textual conditioning mechanism utilizing either dual text encoders (CLIP-L/B) or T5-XXL with both global and local features. We devise a two-stage training strategy: the model is first trained with a standard diffusion objective, and then fine-tuned using reinforcement learning with an FID-based reward to further enhance motion realism and text alignment. Extensive experiments demonstrate that RVHM2D achieves leading performance on the Motion2D-Video-150K benchmark in generating both single and interactive double-character scenarios.
<div id='section'>Paperid: <span id='pid'>1715, <a href='https://arxiv.org/pdf/2506.09836.pdf' target='_blank'>https://arxiv.org/pdf/2506.09836.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junli Deng, Ping Shi, Qipei Li, Jinyang Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.09836">DynaSplat: Dynamic-Static Gaussian Splatting with Hierarchical Motion Decomposition for Scene Reconstruction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reconstructing intricate, ever-changing environments remains a central ambition in computer vision, yet existing solutions often crumble before the complexity of real-world dynamics. We present DynaSplat, an approach that extends Gaussian Splatting to dynamic scenes by integrating dynamic-static separation and hierarchical motion modeling. First, we classify scene elements as static or dynamic through a novel fusion of deformation offset statistics and 2D motion flow consistency, refining our spatial representation to focus precisely where motion matters. We then introduce a hierarchical motion modeling strategy that captures both coarse global transformations and fine-grained local movements, enabling accurate handling of intricate, non-rigid motions. Finally, we integrate physically-based opacity estimation to ensure visually coherent reconstructions, even under challenging occlusions and perspective shifts. Extensive experiments on challenging datasets reveal that DynaSplat not only surpasses state-of-the-art alternatives in accuracy and realism but also provides a more intuitive, compact, and efficient route to dynamic scene reconstruction.
<div id='section'>Paperid: <span id='pid'>1716, <a href='https://arxiv.org/pdf/2506.06318.pdf' target='_blank'>https://arxiv.org/pdf/2506.06318.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Feiyang Pan, Shenghe Zheng, Chunyan Yin, Guangbin Dou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.06318">MoE-Gyro: Self-Supervised Over-Range Reconstruction and Denoising for MEMS Gyroscopes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>MEMS gyroscopes play a critical role in inertial navigation and motion control applications but typically suffer from a fundamental trade-off between measurement range and noise performance. Existing hardware-based solutions aimed at mitigating this issue introduce additional complexity, cost, and scalability challenges. Deep-learning methods primarily focus on noise reduction and typically require precisely aligned ground-truth signals, making them difficult to deploy in practical scenarios and leaving the fundamental trade-off unresolved. To address these challenges, we introduce Mixture of Experts for MEMS Gyroscopes (MoE-Gyro), a novel self-supervised framework specifically designed for simultaneous over-range signal reconstruction and noise suppression. MoE-Gyro employs two experts: an Over-Range Reconstruction Expert (ORE), featuring a Gaussian-Decay Attention mechanism for reconstructing saturated segments; and a Denoise Expert (DE), utilizing dual-branch complementary masking combined with FFT-guided augmentation for robust noise reduction. A lightweight gating module dynamically routes input segments to the appropriate expert. Furthermore, existing evaluation lack a comprehensive standard for assessing multi-dimensional signal enhancement. To bridge this gap, we introduce IMU Signal Enhancement Benchmark (ISEBench), an open-source benchmarking platform comprising the GyroPeak-100 dataset and a unified evaluation of IMU signal enhancement methods. We evaluate MoE-Gyro using our proposed ISEBench, demonstrating that our framework significantly extends the measurable range from 450 deg/s to 1500 deg/s, reduces Bias Instability by 98.4%, and achieves state-of-the-art performance, effectively addressing the long-standing trade-off in inertial sensing.
<div id='section'>Paperid: <span id='pid'>1717, <a href='https://arxiv.org/pdf/2506.04680.pdf' target='_blank'>https://arxiv.org/pdf/2506.04680.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ping-Kong Huang, Chien-Wu Lan, Chin-Tien Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.04680">Application of SDRE to Achieve Gait Control in a Bipedal Robot for Knee-Type Exoskeleton Testing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Exoskeletons are widely used in rehabilitation and industrial applications to assist human motion. However, direct human testing poses risks due to possible exoskeleton malfunctions and inconsistent movement replication. To provide a safer and more repeatable testing environment, this study employs a bipedal robot platform to reproduce human gait, allowing for controlled exoskeleton evaluations. A control strategy based on the State-Dependent Riccati Equation (SDRE) is formulated to achieve optimal torque control for accurate gait replication. The bipedal robot dynamics are represented using double pendulum model, where SDRE-optimized control inputs minimize deviations from human motion trajectories. To align with motor behavior constraints, a parameterized control method is introduced to simplify the control process while effectively replicating human gait. The proposed approach initially adopts a ramping trapezoidal velocity model, which is then adapted into a piecewise linear velocity-time representation through motor command overwriting. This modification enables finer control over gait phase transitions while ensuring compatibility with motor dynamics. The corresponding cost function optimizes the control parameters to minimize errors in joint angles, velocities, and torques relative to SDRE control result. By structuring velocity transitions in accordance with motor limitations, the method reduce the computational load associated with real-time control. Experimental results verify the feasibility of the proposed parameterized control method in reproducing human gait. The bipedal robot platform provides a reliable and repeatable testing mechanism for knee-type exoskeletons, offering insights into exoskeleton performance under controlled conditions.
<div id='section'>Paperid: <span id='pid'>1718, <a href='https://arxiv.org/pdf/2506.03084.pdf' target='_blank'>https://arxiv.org/pdf/2506.03084.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zizhao Wu, Yingying Sun, Yiming Chen, Xiaoling Gu, Ruyu Liu, Jiazhou Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.03084">InterMamba: Efficient Human-Human Interaction Generation with Adaptive Spatio-Temporal Mamba</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human-human interaction generation has garnered significant attention in motion synthesis due to its vital role in understanding humans as social beings. However, existing methods typically rely on transformer-based architectures, which often face challenges related to scalability and efficiency. To address these issues, we propose a novel, efficient human-human interaction generation method based on the Mamba framework, designed to meet the demands of effectively capturing long-sequence dependencies while providing real-time feedback. Specifically, we introduce an adaptive spatio-temporal Mamba framework that utilizes two parallel SSM branches with an adaptive mechanism to integrate the spatial and temporal features of motion sequences. To further enhance the model's ability to capture dependencies within individual motion sequences and the interactions between different individual sequences, we develop two key modules: the self-adaptive spatio-temporal Mamba module and the cross-adaptive spatio-temporal Mamba module, enabling efficient feature learning. Extensive experiments demonstrate that our method achieves state-of-the-art results on two interaction datasets with remarkable quality and efficiency. Compared to the baseline method InterGen, our approach not only improves accuracy but also requires a minimal parameter size of just 66M ,only 36% of InterGen's, while achieving an average inference speed of 0.57 seconds, which is 46% of InterGen's execution time.
<div id='section'>Paperid: <span id='pid'>1719, <a href='https://arxiv.org/pdf/2505.20370.pdf' target='_blank'>https://arxiv.org/pdf/2505.20370.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Martine Dyring Hansen, Elena Celledoni, Benjamin Kwanen Tapley
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.20370">Learning mechanical systems from real-world data using discrete forced Lagrangian dynamics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a data-driven method for learning the equations of motion of mechanical systems directly from position measurements, without requiring access to velocity data. This is particularly relevant in system identification tasks where only positional information is available, such as motion capture, pixel data or low-resolution tracking. Our approach takes advantage of the discrete Lagrange-d'Alembert principle and the forced discrete Euler-Lagrange equations to construct a physically grounded model of the system's dynamics. We decompose the dynamics into conservative and non-conservative components, which are learned separately using feed-forward neural networks. In the absence of external forces, our method reduces to a variational discretization of the action principle naturally preserving the symplectic structure of the underlying Hamiltonian system. We validate our approach on a variety of synthetic and real-world datasets, demonstrating its effectiveness compared to baseline methods. In particular, we apply our model to (1) measured human motion data and (2) latent embeddings obtained via an autoencoder trained on image sequences. We demonstrate that we can faithfully reconstruct and separate both the conservative and forced dynamics, yielding interpretable and physically consistent predictions.
<div id='section'>Paperid: <span id='pid'>1720, <a href='https://arxiv.org/pdf/2505.19976.pdf' target='_blank'>https://arxiv.org/pdf/2505.19976.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Naoki Agata, Takeo Igarashi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.19976">MAMM: Motion Control via Metric-Aligning Motion Matching</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a novel method for controlling a motion sequence using an arbitrary temporal control sequence using temporal alignment. Temporal alignment of motion has gained significant attention owing to its applications in motion control and retargeting. Traditional methods rely on either learned or hand-craft cross-domain mappings between frames in the original and control domains, which often require large, paired, or annotated datasets and time-consuming training. Our approach, named Metric-Aligning Motion Matching, achieves alignment by solely considering within-domain distances. It computes distances among patches in each domain and seeks a matching that optimally aligns the two within-domain distances. This framework allows for the alignment of a motion sequence to various types of control sequences, including sketches, labels, audio, and another motion sequence, all without the need for manually defined mappings or training with annotated data. We demonstrate the effectiveness of our approach through applications in efficient motion control, showcasing its potential in practical scenarios.
<div id='section'>Paperid: <span id='pid'>1721, <a href='https://arxiv.org/pdf/2505.14866.pdf' target='_blank'>https://arxiv.org/pdf/2505.14866.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nisarga Nilavadi, Andrey Rudenko, Timm Linder
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.14866">UPTor: Unified 3D Human Pose Dynamics and Trajectory Prediction for Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a unified approach to forecast the dynamics of human keypoints along with the motion trajectory based on a short sequence of input poses. While many studies address either full-body pose prediction or motion trajectory prediction, only a few attempt to merge them. We propose a motion transformation technique to simultaneously predict full-body pose and trajectory key-points in a global coordinate frame. We utilize an off-the-shelf 3D human pose estimation module, a graph attention network to encode the skeleton structure, and a compact, non-autoregressive transformer suitable for real-time motion prediction for human-robot interaction and human-aware navigation. We introduce a human navigation dataset ``DARKO'' with specific focus on navigational activities that are relevant for human-aware mobile robot navigation. We perform extensive evaluation on Human3.6M, CMU-Mocap, and our DARKO dataset. In comparison to prior work, we show that our approach is compact, real-time, and accurate in predicting human navigation motion across all datasets. Result animations, our dataset, and code will be available at https://nisarganc.github.io/UPTor-page/
<div id='section'>Paperid: <span id='pid'>1722, <a href='https://arxiv.org/pdf/2505.14858.pdf' target='_blank'>https://arxiv.org/pdf/2505.14858.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fernando Coutinho, Nicolas Lizarralde, Fernando Lizarralde
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.14858">Coordinated motion control of a wire arc additive manufacturing robotic system for multi-directional building parts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work investigates the manufacturing of complex shapes parts with wire arc additive manufacturing (WAAM). In order to guarantee the integrity and quality of each deposited layer that composes the final piece, the deposition process is usually carried out in a flat position. However, for complex geometry parts with non-flat surfaces, this strategy causes unsupported overhangs and staircase effect, which contribute to a poor surface finishing. Generally, the build direction is not constant for every deposited section or layer in complex geometry parts. As a result, there is an additional concern to ensure the build direction is aligned with gravity, thus improving the quality of the final part. This paper proposes an algorithm to control the torch motion with respect to a deposition substrate as well as the torch orientation with respect to an inertial frame. The control scheme is based on task augmentation applied to an extended kinematic chain composed by two robots, which constitutes a coordinated control problem, and allows the deposition trajectory to be planned with respect to the deposition substrate coordinate frame while aligning each layer buildup direction with gravity (or any other direction defined for an inertial frame). Parts with complex geometry aspects have been produced in a WAAM cell composed by two robots (a manipulator with a welding torch and a positioning table holding the workpiece) in order to validate the proposed approach.
<div id='section'>Paperid: <span id='pid'>1723, <a href='https://arxiv.org/pdf/2505.12327.pdf' target='_blank'>https://arxiv.org/pdf/2505.12327.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Albert Zhao, Stefano Soatto
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.12327">Robust Planning for Autonomous Driving via Mixed Adversarial Diffusion Predictions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We describe a robust planning method for autonomous driving that mixes normal and adversarial agent predictions output by a diffusion model trained for motion prediction. We first train a diffusion model to learn an unbiased distribution of normal agent behaviors. We then generate a distribution of adversarial predictions by biasing the diffusion model at test time to generate predictions that are likely to collide with a candidate plan. We score plans using expected cost with respect to a mixture distribution of normal and adversarial predictions, leading to a planner that is robust against adversarial behaviors but not overly conservative when agents behave normally. Unlike current approaches, we do not use risk measures that over-weight adversarial behaviors while placing little to no weight on low-cost normal behaviors or use hard safety constraints that may not be appropriate for all driving scenarios. We show the effectiveness of our method on single-agent and multi-agent jaywalking scenarios as well as a red light violation scenario.
<div id='section'>Paperid: <span id='pid'>1724, <a href='https://arxiv.org/pdf/2505.09379.pdf' target='_blank'>https://arxiv.org/pdf/2505.09379.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ali Rida Sahili, Najett Neji, Hedi Tabia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.09379">Text-driven Motion Generation: Overview, Challenges and Directions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-driven motion generation offers a powerful and intuitive way to create human movements directly from natural language. By removing the need for predefined motion inputs, it provides a flexible and accessible approach to controlling animated characters. This makes it especially useful in areas like virtual reality, gaming, human-computer interaction, and robotics. In this review, we first revisit the traditional perspective on motion synthesis, where models focused on predicting future poses from observed initial sequences, often conditioned on action labels. We then provide a comprehensive and structured survey of modern text-to-motion generation approaches, categorizing them from two complementary perspectives: (i) architectural, dividing methods into VAE-based, diffusion-based, and hybrid models; and (ii) motion representation, distinguishing between discrete and continuous motion generation strategies. In addition, we explore the most widely used datasets, evaluation methods, and recent benchmarks that have shaped progress in this area. With this survey, we aim to capture where the field currently stands, bring attention to its key challenges and limitations, and highlight promising directions for future exploration. We hope this work offers a valuable starting point for researchers and practitioners working to push the boundaries of language-driven human motion synthesis.
<div id='section'>Paperid: <span id='pid'>1725, <a href='https://arxiv.org/pdf/2505.04660.pdf' target='_blank'>https://arxiv.org/pdf/2505.04660.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sana Alamgeer, Yasine Souissi, Anne H. H. Ngu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.04660">AI-Generated Fall Data: Assessing LLMs and Diffusion Model for Wearable Fall Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Training fall detection systems is challenging due to the scarcity of real-world fall data, particularly from elderly individuals. To address this, we explore the potential of Large Language Models (LLMs) for generating synthetic fall data. This study evaluates text-to-motion (T2M, SATO, ParCo) and text-to-text models (GPT4o, GPT4, Gemini) in simulating realistic fall scenarios. We generate synthetic datasets and integrate them with four real-world baseline datasets to assess their impact on fall detection performance using a Long Short-Term Memory (LSTM) model. Additionally, we compare LLM-generated synthetic data with a diffusion-based method to evaluate their alignment with real accelerometer distributions. Results indicate that dataset characteristics significantly influence the effectiveness of synthetic data, with LLM-generated data performing best in low-frequency settings (e.g., 20Hz) while showing instability in high-frequency datasets (e.g., 200Hz). While text-to-motion models produce more realistic biomechanical data than text-to-text models, their impact on fall detection varies. Diffusion-based synthetic data demonstrates the closest alignment to real data but does not consistently enhance model performance. An ablation study further confirms that the effectiveness of synthetic data depends on sensor placement and fall representation. These findings provide insights into optimizing synthetic data generation for fall detection models.
<div id='section'>Paperid: <span id='pid'>1726, <a href='https://arxiv.org/pdf/2505.04596.pdf' target='_blank'>https://arxiv.org/pdf/2505.04596.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohammad Merati, David CastaÃ±Ã³n
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.04596">Dynamic Network Flow Optimization for Task Scheduling in PTZ Camera Surveillance Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a novel approach for optimizing the scheduling and control of Pan-Tilt-Zoom (PTZ) cameras in dynamic surveillance environments. The proposed method integrates Kalman filters for motion prediction with a dynamic network flow model to enhance real-time video capture efficiency. By assigning Kalman filters to tracked objects, the system predicts future locations, enabling precise scheduling of camera tasks. This prediction-driven approach is formulated as a network flow optimization, ensuring scalability and adaptability to various surveillance scenarios. To further reduce redundant monitoring, we also incorporate group-tracking nodes, allowing multiple objects to be captured within a single camera focus when appropriate. In addition, a value-based system is introduced to prioritize camera actions, focusing on the timely capture of critical events. By adjusting the decay rates of these values over time, the system ensures prompt responses to tasks with imminent deadlines. Extensive simulations demonstrate that this approach improves coverage, reduces average wait times, and minimizes missed events compared to traditional master-slave camera systems. Overall, our method significantly enhances the efficiency, scalability, and effectiveness of surveillance systems, particularly in dynamic and crowded environments.
<div id='section'>Paperid: <span id='pid'>1727, <a href='https://arxiv.org/pdf/2505.01752.pdf' target='_blank'>https://arxiv.org/pdf/2505.01752.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Longze Zheng, Qinghe Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.01752">NMPCB: A Lightweight and Safety-Critical Motion Control Framework for Ackermann Mobile Robot</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In multi-obstacle environments, real-time performance and safety in robot motion control have long been challenging issues, as conventional methods often struggle to balance the two. In this paper, we propose a novel motion control framework composed of a Neural network-based path planner and a Model Predictive Control (MPC) controller based on control Barrier function (NMPCB) . The planner predicts the next target point through a lightweight neural network and generates a reference trajectory for the controller. In the design of the controller, we introduce the dual problem of control barrier function (CBF) as the obstacle avoidance constraint, enabling it to ensure robot motion safety while significantly reducing computation time. The controller directly outputs control commands to the robot by tracking the reference trajectory. This framework achieves a balance between real-time performance and safety. We validate the feasibility of the framework through numerical simulations and real-world experiments.
<div id='section'>Paperid: <span id='pid'>1728, <a href='https://arxiv.org/pdf/2504.08280.pdf' target='_blank'>https://arxiv.org/pdf/2504.08280.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiong Li, Shulei Liu, Xingning Chen, Yisong Wu, Dong Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.08280">PNE-SGAN: Probabilistic NDT-Enhanced Semantic Graph Attention Network for LiDAR Loop Closure Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>LiDAR loop closure detection (LCD) is crucial for consistent Simultaneous Localization and Mapping (SLAM) but faces challenges in robustness and accuracy. Existing methods, including semantic graph approaches, often suffer from coarse geometric representations and lack temporal robustness against noise, dynamics, and viewpoint changes. We introduce PNE-SGAN, a Probabilistic NDT-Enhanced Semantic Graph Attention Network, to overcome these limitations. PNE-SGAN enhances semantic graphs by using Normal Distributions Transform (NDT) covariance matrices as rich, discriminative geometric node features, processed via a Graph Attention Network (GAT). Crucially, it integrates graph similarity scores into a probabilistic temporal filtering framework (modeled as an HMM/Bayes filter), incorporating uncertain odometry for motion modeling and utilizing forward-backward smoothing to effectively handle ambiguities. Evaluations on challenging KITTI sequences (00 and 08) demonstrate state-of-the-art performance, achieving Average Precision of 96.2\% and 95.1\%, respectively. PNE-SGAN significantly outperforms existing methods, particularly in difficult bidirectional loop scenarios where others falter. By synergizing detailed NDT geometry with principled probabilistic temporal reasoning, PNE-SGAN offers a highly accurate and robust solution for LiDAR LCD, enhancing SLAM reliability in complex, large-scale environments.
<div id='section'>Paperid: <span id='pid'>1729, <a href='https://arxiv.org/pdf/2504.01338.pdf' target='_blank'>https://arxiv.org/pdf/2504.01338.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Manolo Canales Cuba, VinÃ­cius do Carmo MelÃ­cio, JoÃ£o Paulo Gois
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.01338">FlowMotion: Target-Predictive Conditional Flow Matching for Jitter-Reduced Text-Driven Human Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Achieving high-fidelity and temporally smooth 3D human motion generation remains a challenge, particularly within resource-constrained environments. We introduce FlowMotion, a novel method leveraging Conditional Flow Matching (CFM). FlowMotion incorporates a training objective within CFM that focuses on more accurately predicting target motion in 3D human motion generation, resulting in enhanced generation fidelity and temporal smoothness while maintaining the fast synthesis times characteristic of flow-matching-based methods. FlowMotion achieves state-of-the-art jitter performance, achieving the best jitter in the KIT dataset and the second-best jitter in the HumanML3D dataset, and a competitive FID value in both datasets. This combination provides robust and natural motion sequences, offering a promising equilibrium between generation quality and temporal naturalness.
<div id='section'>Paperid: <span id='pid'>1730, <a href='https://arxiv.org/pdf/2504.01024.pdf' target='_blank'>https://arxiv.org/pdf/2504.01024.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yufei He, Xucong Zhang, Arno H. A. Stienen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.01024">Gaze-Guided 3D Hand Motion Prediction for Detecting Intent in Egocentric Grasping Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human intention detection with hand motion prediction is critical to drive the upper-extremity assistive robots in neurorehabilitation applications. However, the traditional methods relying on physiological signal measurement are restrictive and often lack environmental context. We propose a novel approach that predicts future sequences of both hand poses and joint positions. This method integrates gaze information, historical hand motion sequences, and environmental object data, adapting dynamically to the assistive needs of the patient without prior knowledge of the intended object for grasping. Specifically, we use a vector-quantized variational autoencoder for robust hand pose encoding with an autoregressive generative transformer for effective hand motion sequence prediction. We demonstrate the usability of these novel techniques in a pilot study with healthy subjects. To train and evaluate the proposed method, we collect a dataset consisting of various types of grasp actions on different objects from multiple subjects. Through extensive experiments, we demonstrate that the proposed method can successfully predict sequential hand movement. Especially, the gaze information shows significant enhancements in prediction capabilities, particularly with fewer input frames, highlighting the potential of the proposed method for real-world applications.
<div id='section'>Paperid: <span id='pid'>1731, <a href='https://arxiv.org/pdf/2503.20237.pdf' target='_blank'>https://arxiv.org/pdf/2503.20237.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vineela Reddy Pippera Badguna, Aliasghar Arab, Durga Avinash Kodavalla
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.20237">A Virtual Fencing Framework for Safe and Efficient Collaborative Robotics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Collaborative robots (cobots) increasingly operate alongside humans, demanding robust real-time safeguarding. Current safety standards (e.g., ISO 10218, ANSI/RIA 15.06, ISO/TS 15066) require risk assessments but offer limited guidance for real-time responses. We propose a virtual fencing approach that detects and predicts human motion, ensuring safe cobot operation. Safety and performance tradeoffs are modeled as an optimization problem and solved via sequential quadratic programming. Experimental validation shows that our method minimizes operational pauses while maintaining safety, providing a modular solution for human-robot collaboration.
<div id='section'>Paperid: <span id='pid'>1732, <a href='https://arxiv.org/pdf/2503.18386.pdf' target='_blank'>https://arxiv.org/pdf/2503.18386.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sicong Feng, Jielong Yang, Li Peng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.18386">Resource-Efficient Motion Control for Video Generation via Dynamic Mask Guidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in diffusion models bring new vitality to visual content creation. However, current text-to-video generation models still face significant challenges such as high training costs, substantial data requirements, and difficulties in maintaining consistency between given text and motion of the foreground object. To address these challenges, we propose mask-guided video generation, which can control video generation through mask motion sequences, while requiring limited training data. Our model enhances existing architectures by incorporating foreground masks for precise text-position matching and motion trajectory control. Through mask motion sequences, we guide the video generation process to maintain consistent foreground objects throughout the sequence. Additionally, through a first-frame sharing strategy and autoregressive extension approach, we achieve more stable and longer video generation. Extensive qualitative and quantitative experiments demonstrate that this approach excels in various video generation tasks, such as video editing and generating artistic videos, outperforming previous methods in terms of consistency and quality. Our generated results can be viewed in the supplementary materials.
<div id='section'>Paperid: <span id='pid'>1733, <a href='https://arxiv.org/pdf/2503.17458.pdf' target='_blank'>https://arxiv.org/pdf/2503.17458.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jean C. Pereira, Valter J. S. Leite, Guilherme V. Raffo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.17458">Stabilizing NMPC Approaches for Underactuated Mechanical Systems on the SE(3) Manifold</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper addresses the motion control problem for underactuated mechanical systems with full attitude control and one translational force input to manage the six degrees of freedom involved in the three-dimensional Euclidean space. These systems are often classified as second-order nonholonomic due to their completely nonintegrable acceleration constraints. To tackle this complex control problem, we propose two nonlinear model predictive control (NMPC) schemes that ensure closed-loop stability and recursive feasibility without terminal conditions. The system dynamics are modeled on the SE(3) manifold for a globally and unique description of rigid body configurations. One NMPC scheme also aims to reduce mission time as an economic criterion. The controllers' effectiveness is validated through numerical experiments on a quadrotor UAV.
<div id='section'>Paperid: <span id='pid'>1734, <a href='https://arxiv.org/pdf/2503.15225.pdf' target='_blank'>https://arxiv.org/pdf/2503.15225.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Angelo Di Porzio, Marco Coraggio
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.15225">A Personalized Data-Driven Generative Model of Human Motion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The deployment of autonomous virtual avatars (in extended reality) and robots in human group activities - such as rehabilitation therapy, sports, and manufacturing - is expected to increase as these technologies become more pervasive. Designing cognitive architectures and control strategies to drive these agents requires realistic models of human motion. However, existing models only provide simplified descriptions of human motor behavior. In this work, we propose a fully data-driven approach, based on Long Short-Term Memory neural networks, to generate original motion that captures the unique characteristics of specific individuals. We validate the architecture using real data of scalar oscillatory motion. Extensive analyses show that our model effectively replicates the velocity distribution and amplitude envelopes of the individual it was trained on, remaining different from other individuals, and outperforming state-of-the-art models in terms of similarity to human data.
<div id='section'>Paperid: <span id='pid'>1735, <a href='https://arxiv.org/pdf/2503.12628.pdf' target='_blank'>https://arxiv.org/pdf/2503.12628.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ritik Batra, Narjes Pourjafarian, Samantha Chang, Margaret Tsai, Jacob Revelo, Cindy Hsin-Liu Kao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.12628">texTENG: Fabricating Wearable Textile-Based Triboelectric Nanogenerators</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, there has been a surge of interest in sustainable energy sources, particularly for wearable computing. Triboelectric nanogenerators (TENGs) have shown promise in converting human motion into electric power. Textile-based TENGs, valued for their flexibility and breathability, offer an ideal form factor for wearables. However, uptake in maker communities has been slow due to commercially unavailable materials, complex fabrication processes, and structures incompatible with human motion. This paper introduces texTENG, a textile-based framework simplifying the fabrication of power harvesting and self-powered sensing applications. By leveraging accessible materials and familiar tools, texTENG bridges the gap between advanced TENG research and wearable applications. We explore a design menu for creating multidimensional TENG structures using braiding, weaving, and knitting. Technical evaluations and example applications highlight the performance and feasibility of these designs, offering DIY-friendly pathways for fabricating textile-based TENGs and promoting sustainable prototyping practices within the HCI and maker communities.
<div id='section'>Paperid: <span id='pid'>1736, <a href='https://arxiv.org/pdf/2503.11072.pdf' target='_blank'>https://arxiv.org/pdf/2503.11072.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haotian Tan, Yuan-Hua Ni
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.11072">A High-Speed Time-Optimal Trajectory Generation Strategy via a Two-layer Planning Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion planning and trajectory generation are crucial technologies in various domains including the control of Unmanned Aerial Vehicles, manipulators, and rockets. However, optimization-based real-time motion planning becomes increasingly challenging due to the problem's probable non-convexity and the inherent limitations of non-linear programming algorithms. Highly nonlinear dynamics, obstacle avoidance constraints, and non-convex inputs can exacerbate these difficulties. In order to enhance the robustness and reduce the computational burden, this paper proposes a two-layer trajectory generating algorithm for intelligent ground vehicles with convex optimization methods, aiming to provide real-time guarantees for trajectory optimization and to improve the calculate speed of motion prediction. Our approach involves breaking down the original problem into small horizon-based planning cycles with fixed final times, referred to as planning cycles. Each planning cycle is then solved within a series of restricted convex sets constructed by some customized search algorithms incrementally. We rigorously establish these advantages through mathematical analysis under moderate assumptions and comprehensive experimental validations. For linear vehicle models, comparative experiments with general sequential convex programming algorithms demonstrate the superior performance of our proposed method, particularly in terms of the computational efficiency in dynamic maps and the reduced final time.
<div id='section'>Paperid: <span id='pid'>1737, <a href='https://arxiv.org/pdf/2503.10488.pdf' target='_blank'>https://arxiv.org/pdf/2503.10488.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Evgeniia Vu, Andrei Boiarov, Dmitry Vetrov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.10488">Streaming Generation of Co-Speech Gestures via Accelerated Rolling Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating co-speech gestures in real time requires both temporal coherence and efficient sampling. We introduce Accelerated Rolling Diffusion, a novel framework for streaming gesture generation that extends rolling diffusion models with structured progressive noise scheduling, enabling seamless long-sequence motion synthesis while preserving realism and diversity. We further propose Rolling Diffusion Ladder Acceleration (RDLA), a new approach that restructures the noise schedule into a stepwise ladder, allowing multiple frames to be denoised simultaneously. This significantly improves sampling efficiency while maintaining motion consistency, achieving up to a 2x speedup with high visual fidelity and temporal coherence. We evaluate our approach on ZEGGS and BEAT, strong benchmarks for real-world applicability. Our framework is universally applicable to any diffusion-based gesture generation model, transforming it into a streaming approach. Applied to three state-of-the-art methods, it consistently outperforms them, demonstrating its effectiveness as a generalizable and efficient solution for real-time, high-fidelity co-speech gesture synthesis.
<div id='section'>Paperid: <span id='pid'>1738, <a href='https://arxiv.org/pdf/2503.06897.pdf' target='_blank'>https://arxiv.org/pdf/2503.06897.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xingzu Zhan, Chen Xie, Honghang Chen, Haoran Sun, Xiaochun Mai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.06897">Multi-granular body modeling with Redundancy-Free Spatiotemporal Fusion for Text-Driven Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-to-motion generation sits at the intersection of multimodal learning and computer graphics and is gaining momentum because it can simplify content creation for games, animation, robotics and virtual reality. Most current methods stack spatial and temporal features in a straightforward way, which adds redundancy and still misses subtle joint-level cues. We introduce HiSTF Mamba, a framework with three parts: Dual-Spatial Mamba, Bi-Temporal Mamba and a Dynamic Spatiotemporal Fusion Module (DSFM). The Dual-Spatial module runs part-based and whole-body models in parallel, capturing both overall coordination and fine-grained joint motion. The Bi-Temporal module scans sequences forward and backward to encode short-term details and long-term dependencies. DSFM removes redundant temporal information, extracts complementary cues and fuses them with spatial features to build a richer spatiotemporal representation. Experiments on the HumanML3D benchmark show that HiSTF Mamba performs well across several metrics, achieving high fidelity and tight semantic alignment between text and motion.
<div id='section'>Paperid: <span id='pid'>1739, <a href='https://arxiv.org/pdf/2503.06119.pdf' target='_blank'>https://arxiv.org/pdf/2503.06119.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shinichi Tanaka, Zhao Wang, Yoichi Kato, Jun Ohya
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.06119">Unlocking Pretrained LLMs for Motion-Related Multimodal Generation: A Fine-Tuning Approach to Unify Diffusion and Next-Token Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose a unified framework that leverages a single pretrained LLM for Motion-related Multimodal Generation, referred to as MoMug. MoMug integrates diffusion-based continuous motion generation with the model's inherent autoregressive discrete text prediction capabilities by fine-tuning a pretrained LLM. This enables seamless switching between continuous motion output and discrete text token prediction within a single model architecture, effectively combining the strengths of both diffusion- and LLM-based approaches. Experimental results show that, compared to the most recent LLM-based baseline, MoMug improves FID by 38% and mean accuracy across seven metrics by 16.61% on the text-to-motion task. Additionally, it improves mean accuracy across eight metrics by 8.44% on the text-to-motion task. To the best of our knowledge, this is the first approach to integrate diffusion- and LLM-based generation within a single model for motion-related multimodal tasks while maintaining low training costs. This establishes a foundation for future advancements in motion-related generation, paving the way for high-quality yet cost-efficient motion synthesis.
<div id='section'>Paperid: <span id='pid'>1740, <a href='https://arxiv.org/pdf/2503.02869.pdf' target='_blank'>https://arxiv.org/pdf/2503.02869.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>M. van der Hulst, R. A. GonzÃ¡lez, K. Classens, P. Tacx, N. Dirkx, J. van de Wijdeven, T. Oomen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.02869">Frequency domain identification for multivariable motion control systems: Applied to a prototype wafer stage</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multivariable parametric models are essential for optimizing the performance of high-tech systems. The main objective of this paper is to develop an identification strategy that provides accurate parametric models for complex multivariable systems. To achieve this, an additive model structure is adopted, offering advantages over traditional black-box model structures when considering physical systems. The introduced method minimizes a weighted least-squares criterion and uses an iterative linear regression algorithm to solve the estimation problem, achieving local optimality upon convergence. Experimental validation is conducted on a prototype wafer-stage system, featuring a large number of spatially distributed actuators and sensors and exhibiting complex flexible dynamic behavior, to evaluate performance and demonstrate the effectiveness of the proposed method.
<div id='section'>Paperid: <span id='pid'>1741, <a href='https://arxiv.org/pdf/2503.00602.pdf' target='_blank'>https://arxiv.org/pdf/2503.00602.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Zhang, Xiaoyu Shi, Tongyang Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.00602">Zero-Power Backscatter Sensing and Communication Proof-of-Concept</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we present an experimental setup to evaluate the performance of a radio frequency identification (RFID)-based integrated sensing and communication (ISAC) system. We focus on both the communication and sensing capabilities of the system. Our experiments evaluate the system's performance in various channel fading scenarios and with different substrate materials, including wood, plastic, wall, and glass. Additionally, we utilize radio tomographic imaging (RTI) to detect human motion by analyzing received signal strength indicator (RSSI) data. Our results demonstrate the impact of different materials and environments on RSSI and highlight the potential of RFID-based systems for effective sensing and communication in diverse applications.
<div id='section'>Paperid: <span id='pid'>1742, <a href='https://arxiv.org/pdf/2502.21207.pdf' target='_blank'>https://arxiv.org/pdf/2502.21207.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>ThÃ©o Cheynel, Thomas Rossi, Baptiste Bellot-Gurlet, Damien Rohmer, Marie-Paule Cani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.21207">ReConForM : Real-time Contact-aware Motion Retargeting for more Diverse Character Morphologies</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Preserving semantics, in particular in terms of contacts, is a key challenge when retargeting motion between characters of different morphologies. Our solution relies on a low-dimensional embedding of the character's mesh, based on rigged key vertices that are automatically transferred from the source to the target. Motion descriptors are extracted from the trajectories of these key vertices, providing an embedding that contains combined semantic information about both shape and pose. A novel, adaptive algorithm is then used to automatically select and weight the most relevant features over time, enabling us to efficiently optimize the target motion until it conforms to these constraints, so as to preserve the semantics of the source motion. Our solution allows extensions to several novel use-cases where morphology and mesh contacts were previously overlooked, such as multi-character retargeting and motion transfer on uneven terrains. As our results show, our method is able to achieve real-time retargeting onto a wide variety of characters. Extensive experiments and comparison with state-of-the-art methods using several relevant metrics demonstrate improved results, both in terms of motion smoothness and contact accuracy.
<div id='section'>Paperid: <span id='pid'>1743, <a href='https://arxiv.org/pdf/2502.20824.pdf' target='_blank'>https://arxiv.org/pdf/2502.20824.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fadeel Sher Khan, Joshua Ebenezer, Hamid Sheikh, Seok-Jun Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.20824">MFSR-GAN: Multi-Frame Super-Resolution with Handheld Motion Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Smartphone cameras have become ubiquitous imaging tools, yet their small sensors and compact optics often limit spatial resolution and introduce distortions. Combining information from multiple low-resolution (LR) frames to produce a high-resolution (HR) image has been explored to overcome the inherent limitations of smartphone cameras. Despite the promise of multi-frame super-resolution (MFSR), current approaches are hindered by datasets that fail to capture the characteristic noise and motion patterns found in real-world handheld burst images. In this work, we address this gap by introducing a novel synthetic data engine that uses multi-exposure static images to synthesize LR-HR training pairs while preserving sensor-specific noise characteristics and image motion found during handheld burst photography. We also propose MFSR-GAN: a multi-scale RAW-to-RGB network for MFSR. Compared to prior approaches, MFSR-GAN emphasizes a "base frame" throughout its architecture to mitigate artifacts. Experimental results on both synthetic and real data demonstrates that MFSR-GAN trained with our synthetic engine yields sharper, more realistic reconstructions than existing methods for real-world MFSR.
<div id='section'>Paperid: <span id='pid'>1744, <a href='https://arxiv.org/pdf/2502.16134.pdf' target='_blank'>https://arxiv.org/pdf/2502.16134.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Huaiqu Feng, Guoyang Zhao, Cheng Liu, Yongwei Wang, Jun Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.16134">Motion-Coupled Mapping Algorithm for Hybrid Rice Canopy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a motion-coupled mapping algorithm for contour mapping of hybrid rice canopies, specifically designed for Agricultural Unmanned Ground Vehicles (Agri-UGV) navigating complex and unknown rice fields. Precise canopy mapping is essential for Agri-UGVs to plan efficient routes and avoid protected zones. The motion control of Agri-UGVs, tasked with impurity removal and other operations, depends heavily on accurate estimation of rice canopy height and structure. To achieve this, the proposed algorithm integrates real-time RGB-D sensor data with kinematic and inertial measurements, enabling efficient mapping and proprioceptive localization. The algorithm produces grid-based elevation maps that reflect the probabilistic distribution of canopy contours, accounting for motion-induced uncertainties. It is implemented on a high-clearance Agri-UGV platform and tested in various environments, including both controlled and dynamic rice field settings. This approach significantly enhances the mapping accuracy and operational reliability of Agri-UGVs, contributing to more efficient autonomous agricultural operations.
<div id='section'>Paperid: <span id='pid'>1745, <a href='https://arxiv.org/pdf/2502.15956.pdf' target='_blank'>https://arxiv.org/pdf/2502.15956.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Canxuan Gang, Yiran Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.15956">Human Motion Prediction, Reconstruction, and Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This report reviews recent advancements in human motion prediction, reconstruction, and generation. Human motion prediction focuses on forecasting future poses and movements from historical data, addressing challenges like nonlinear dynamics, occlusions, and motion style variations. Reconstruction aims to recover accurate 3D human body movements from visual inputs, often leveraging transformer-based architectures, diffusion models, and physical consistency losses to handle noise and complex poses. Motion generation synthesizes realistic and diverse motions from action labels, textual descriptions, or environmental constraints, with applications in robotics, gaming, and virtual avatars. Additionally, text-to-motion generation and human-object interaction modeling have gained attention, enabling fine-grained and context-aware motion synthesis for augmented reality and robotics. This review highlights key methodologies, datasets, challenges, and future research directions driving progress in these fields.
<div id='section'>Paperid: <span id='pid'>1746, <a href='https://arxiv.org/pdf/2502.00665.pdf' target='_blank'>https://arxiv.org/pdf/2502.00665.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fuxi Ling, Hongye Liu, Guoqiang Huang, Jing Li, Hong Wu, Zhihao Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.00665">Cross-Modal Synergies: Unveiling the Potential of Motion-Aware Fusion Networks in Handling Dynamic and Static ReID Scenarios</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Navigating the complexities of person re-identification (ReID) in varied surveillance scenarios, particularly when occlusions occur, poses significant challenges. We introduce an innovative Motion-Aware Fusion (MOTAR-FUSE) network that utilizes motion cues derived from static imagery to significantly enhance ReID capabilities. This network incorporates a dual-input visual adapter capable of processing both images and videos, thereby facilitating more effective feature extraction. A unique aspect of our approach is the integration of a motion consistency task, which empowers the motion-aware transformer to adeptly capture the dynamics of human motion. This technique substantially improves the recognition of features in scenarios where occlusions are prevalent, thereby advancing the ReID process. Our comprehensive evaluations across multiple ReID benchmarks, including holistic, occluded, and video-based scenarios, demonstrate that our MOTAR-FUSE network achieves superior performance compared to existing approaches.
<div id='section'>Paperid: <span id='pid'>1747, <a href='https://arxiv.org/pdf/2502.00215.pdf' target='_blank'>https://arxiv.org/pdf/2502.00215.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fabio Spada, Purnanand Elango, BehÃ§et AÃ§Ä±kmeÅe
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.00215">Impulsive Relative Motion Control with Continuous-Time Constraint Satisfaction for Cislunar Space Missions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent investments in cislunar applications open new frontiers for space missions within highly nonlinear dynamical regimes. In this paper, we propose a method based on Sequential Convex Programming (SCP) to loiter around a given target with impulsive actuation while satisfying path constraints continuously over the finite time-horizon, i.e., independently of the number of nodes in which domain is discretized. Location, timing, magnitude, and direction of a fixed number of impulses are optimized in a model predictive framework, exploiting the exact nonlinear dynamics of non-stationary orbital regimes. The proposed approach is first validated on a relative orbiting problem with respect to a selenocentric near rectilinear halo orbit. The approach is then compared to a formulation with path constraints imposed only at nodes and with mesh refined to ensure complete satisfaction of path constraints over the continuous-time horizon. CPU time per iteration of 400 ms for the refined-mesh approach reduce to 5.5 ms for the proposed approach.
<div id='section'>Paperid: <span id='pid'>1748, <a href='https://arxiv.org/pdf/2501.18729.pdf' target='_blank'>https://arxiv.org/pdf/2501.18729.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anthony Richardson, Felix Putze
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.18729">Motion Diffusion Autoencoders: Enabling Attribute Manipulation in Human Motion Demonstrated on Karate Techniques</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Attribute manipulation deals with the problem of changing individual attributes of a data point or a time series, while leaving all other aspects unaffected. This work focuses on the domain of human motion, more precisely karate movement patterns. To the best of our knowledge, it presents the first success at manipulating attributes of human motion data. One of the key requirements for achieving attribute manipulation on human motion is a suitable pose representation. Therefore, we design a novel continuous, rotation-based pose representation that enables the disentanglement of the human skeleton and the motion trajectory, while still allowing an accurate reconstruction of the original anatomy. The core idea of the manipulation approach is to use a transformer encoder for discovering high-level semantics, and a diffusion probabilistic model for modeling the remaining stochastic variations. We show that the embedding space obtained from the transformer encoder is semantically meaningful and linear. This enables the manipulation of high-level attributes, by discovering their linear direction of change in the semantic embedding space and moving the embedding along said direction. All code and data is made publicly available.
<div id='section'>Paperid: <span id='pid'>1749, <a href='https://arxiv.org/pdf/2501.15058.pdf' target='_blank'>https://arxiv.org/pdf/2501.15058.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Jiang, Yixing Chen, Xingyang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.15058">KETA: Kinematic-Phrases-Enhanced Text-to-Motion Generation via Fine-grained Alignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion synthesis plays a vital role in various fields of artificial intelligence. Among the various conditions of motion generation, text can describe motion details elaborately and is easy to acquire, making text-to-motion(T2M) generation important. State-of-the-art T2M techniques mainly leverage diffusion models to generate motions with text prompts as guidance, tackling the many-to-many nature of T2M tasks. However, existing T2M approaches face challenges, given the gap between the natural language domain and the physical domain, making it difficult to generate motions fully consistent with the texts.
  We leverage kinematic phrases(KP), an intermediate representation that bridges these two modalities, to solve this. Our proposed method, KETA, decomposes the given text into several decomposed texts via a language model. It trains an aligner to align decomposed texts with the KP segments extracted from the generated motions. Thus, it's possible to restrict the behaviors for diffusion-based T2M models. During the training stage, we deploy the text-KP alignment loss as an auxiliary goal to supervise the models. During the inference stage, we refine our generated motions for multiple rounds in our decoder structure, where we compute the text-KP distance as the guidance signal in each new round. Experiments demonstrate that KETA achieves up to 1.19x, 2.34x better R precision and FID value on both backbones of the base model, motion diffusion model. Compared to a wide range of T2M generation models. KETA achieves either the best or the second-best performance.
<div id='section'>Paperid: <span id='pid'>1750, <a href='https://arxiv.org/pdf/2501.04793.pdf' target='_blank'>https://arxiv.org/pdf/2501.04793.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Caner OdabaÅ, Ãmer MorgÃ¼l
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.04793">A Novel Observer Design for LuGre Friction Estimation and Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dynamic components of the friction may directly impact the stability and performance of the motion control systems. The LuGre model is a prevalent friction model utilized to express this dynamic behavior. Since the LuGre model is very comprehensive, friction compensation based on it might be challenging. Inspired by this, we develop a novel observer to estimate and compensate for LuGre friction. Furthermore, we present a Lyapunov stability analysis to show that observer dynamics are asymptotically stable under certain conditions. Compared to its counterparts, the proposed observer constitutes a simple and standalone scheme that can be utilized with arbitrary control inputs in a straightforward way. As a primary difference, the presented observer estimates velocity and uses the velocity error to estimate friction in addition to control input. The extensive simulations revealed that the introduced observer enhances position and velocity tracking performance in the presence of friction.
<div id='section'>Paperid: <span id='pid'>1751, <a href='https://arxiv.org/pdf/2412.15819.pdf' target='_blank'>https://arxiv.org/pdf/2412.15819.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cheng Wang, Ziyang Feng, Pin Zhang, Manjiang Cao, Yiming Yuan, Tengfei Chang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.15819">Robustness-enhanced Myoelectric Control with GAN-based Open-set Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Electromyography (EMG) signals are widely used in human motion recognition and medical rehabilitation, yet their variability and susceptibility to noise significantly limit the reliability of myoelectric control systems. Existing recognition algorithms often fail to handle unfamiliar actions effectively, leading to system instability and errors. This paper proposes a novel framework based on Generative Adversarial Networks (GANs) to enhance the robustness and usability of myoelectric control systems by enabling open-set recognition. The method incorporates a GAN-based discriminator to identify and reject unknown actions, maintaining system stability by preventing misclassifications. Experimental evaluations on publicly available and self-collected datasets demonstrate a recognition accuracy of 97.6\% for known actions and a 23.6\% improvement in Active Error Rate (AER) after rejecting unknown actions. The proposed approach is computationally efficient and suitable for deployment on edge devices, making it practical for real-world applications.
<div id='section'>Paperid: <span id='pid'>1752, <a href='https://arxiv.org/pdf/2412.04820.pdf' target='_blank'>https://arxiv.org/pdf/2412.04820.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Charles Dietzel, Patrick J. Martin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.04820">Assessing Similarity Measures for the Evaluation of Human-Robot Motion Correspondence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>One key area of research in Human-Robot Interaction is solving the human-robot correspondence problem, which asks how a robot can learn to reproduce a human motion demonstration when the human and robot have different dynamics and kinematic structures. Evaluating these correspondence problem solutions often requires the use of qualitative surveys that can be time consuming to design and administer. Additionally, qualitative survey results vary depending on the population of survey participants. In this paper, we propose the use of heterogeneous time-series similarity measures as a quantitative evaluation metric for evaluating motion correspondence to complement these qualitative surveys. To assess the suitability of these measures, we develop a behavioral cloning-based motion correspondence model, and evaluate it with a qualitative survey as well as quantitative measures. By comparing the resulting similarity scores with the human survey results, we identify Gromov Dynamic Time Warping as a promising quantitative measure for evaluating motion correspondence.
<div id='section'>Paperid: <span id='pid'>1753, <a href='https://arxiv.org/pdf/2412.04097.pdf' target='_blank'>https://arxiv.org/pdf/2412.04097.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Meenakshi Gupta, Mingyuan Lei, Tat-Jen Cham, Hwee Kuan Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.04097">D-LORD for Motion Stylization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces a novel framework named D-LORD (Double Latent Optimization for Representation Disentanglement), which is designed for motion stylization (motion style transfer and motion retargeting). The primary objective of this framework is to separate the class and content information from a given motion sequence using a data-driven latent optimization approach. Here, class refers to person-specific style, such as a particular emotion or an individual's identity, while content relates to the style-agnostic aspect of an action, such as walking or jumping, as universally understood concepts. The key advantage of D-LORD is its ability to perform style transfer without needing paired motion data. Instead, it utilizes class and content labels during the latent optimization process. By disentangling the representation, the framework enables the transformation of one motion sequences style to another's style using Adaptive Instance Normalization. The proposed D-LORD framework is designed with a focus on generalization, allowing it to handle different class and content labels for various applications. Additionally, it can generate diverse motion sequences when specific class and content labels are provided. The framework's efficacy is demonstrated through experimentation on three datasets: the CMU XIA dataset for motion style transfer, the MHAD dataset, and the RRIS Ability dataset for motion retargeting. Notably, this paper presents the first generalized framework for motion style transfer and motion retargeting, showcasing its potential contributions in this area.
<div id='section'>Paperid: <span id='pid'>1754, <a href='https://arxiv.org/pdf/2412.03299.pdf' target='_blank'>https://arxiv.org/pdf/2412.03299.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sam A. Scivier, Tarje Nissen-Meyer, Paula Koelemeijer, AtÄ±lÄ±m GÃ¼neÅ Baydin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.03299">Gaussian Processes for Probabilistic Estimates of Earthquake Ground Shaking: A 1-D Proof-of-Concept</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Estimates of seismic wave speeds in the Earth (seismic velocity models) are key input parameters to earthquake simulations for ground motion prediction. Owing to the non-uniqueness of the seismic inverse problem, typically many velocity models exist for any given region. The arbitrary choice of which velocity model to use in earthquake simulations impacts ground motion predictions. However, current hazard analysis methods do not account for this source of uncertainty. We present a proof-of-concept ground motion prediction workflow for incorporating uncertainties arising from inconsistencies between existing seismic velocity models. Our analysis is based on the probabilistic fusion of overlapping seismic velocity models using scalable Gaussian process (GP) regression. Specifically, we fit a GP to two synthetic 1-D velocity profiles simultaneously, and show that the predictive uncertainty accounts for the differences between the models. We subsequently draw velocity model samples from the predictive distribution and estimate peak ground displacement using acoustic wave propagation through the velocity models. The resulting distribution of possible ground motion amplitudes is much wider than would be predicted by simulating shaking using only the two input velocity models. This proof-of-concept illustrates the importance of probabilistic methods for physics-based seismic hazard analysis.
<div id='section'>Paperid: <span id='pid'>1755, <a href='https://arxiv.org/pdf/2411.18745.pdf' target='_blank'>https://arxiv.org/pdf/2411.18745.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zheyan Zhang, Diego Klabjan, Renee CB Manworren
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.18745">DiffMVR: Diffusion-based Automated Multi-Guidance Video Restoration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we address a challenge in video inpainting: reconstructing occluded regions in dynamic, real-world scenarios. Motivated by the need for continuous human motion monitoring in healthcare settings, where facial features are frequently obscured, we propose a diffusion-based video-level inpainting model, DiffMVR. Our approach introduces a dynamic dual-guided image prompting system, leveraging adaptive reference frames to guide the inpainting process. This enables the model to capture both fine-grained details and smooth transitions between video frames, offering precise control over inpainting direction and significantly improving restoration accuracy in challenging, dynamic environments. DiffMVR represents a significant advancement in the field of diffusion-based inpainting, with practical implications for real-time applications in various dynamic settings.
<div id='section'>Paperid: <span id='pid'>1756, <a href='https://arxiv.org/pdf/2411.17745.pdf' target='_blank'>https://arxiv.org/pdf/2411.17745.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiarui Song, Yingbo Sun, Qing Dong, Xuewu Ji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.17745">A Parameter Adaptive Trajectory Tracking and Motion Control Framework for Autonomous Vehicle</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper studies the trajectory tracking and motion control problems for autonomous vehicles (AVs). A parameter adaptive control framework for AVs is proposed to enhance tracking accuracy and yaw stability. While establishing linear quadratic regulator (LQR) and three robust controllers, the control framework addresses trajectory tracking and motion control in a modular fashion, without introducing complexity into each controller. The robust performance has been guaranteed in three robust controllers by considering the parameter uncertainties, mismatch of unmodeled subsystem as well as external disturbance, comprehensively. Also, the dynamic characteristics of uncertain parameters are identified by Recursive Least Squares (RLS) algorithm, while the boundaries of three robust factors are determined through combining Gaussian Process Regression (GPR) and Bayesian optimization machine learning methods, reducing the conservatism of the controller. Sufficient conditions for closed-loop stability under the diverse robust factors are provided by the Lyapunov method analytically. The simulation results on MATLAB/Simulink and Carsim joint platform demonstrate that the proposed methodology considerably improves tracking accuracy, driving stability, and robust performance, guaranteeing the feasibility and capability of driving in extreme scenarios.
<div id='section'>Paperid: <span id='pid'>1757, <a href='https://arxiv.org/pdf/2411.16802.pdf' target='_blank'>https://arxiv.org/pdf/2411.16802.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Omar El Assal, Carlos M. Mateo, Sebastien Ciron, David Fofi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.16802">Leveraging Foundation Models To learn the shape of semi-fluid deformable objects</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>One of the difficulties imposed on the manipulation of deformable objects is their characterization and the detection of representative keypoints for the purpose of manipulation. A keen interest was manifested by researchers in the last decade to characterize and manipulate deformable objects of non-fluid nature, such as clothes and ropes. Even though several propositions were made in the regard of object characterization, however researchers were always confronted with the need of pixel-level information of the object through images to extract relevant information. This usually is accomplished by means of segmentation networks trained on manually labeled data for this purpose. In this paper, we address the subject of characterizing weld pool to define stable features that serve as information for further motion control objectives. We achieve this by employing different pipelines. The first one consists of characterizing fluid deformable objects through the use of a generative model that is trained using a teacher-student framework. And in the second one we leverage foundation models by using them as teachers to characterize the object in the image, without the need of any pre-training and any dataset. The performance of knowledge distillation from foundation models into a smaller generative model shows prominent results in the characterization of deformable objects. The student network was capable of learning to retrieve the keypoitns of the object with an error of 13.4 pixels. And the teacher was evaluated based on its capacities to retrieve pixel level information represented by the object mask, with a mean Intersection Over Union (mIoU) of 75.26%.
<div id='section'>Paperid: <span id='pid'>1758, <a href='https://arxiv.org/pdf/2411.05734.pdf' target='_blank'>https://arxiv.org/pdf/2411.05734.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Agamdeep Singh, Sujit PB, Mayank Vatsa
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.05734">Poze: Sports Technique Feedback under Data Constraints</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Access to expert coaching is essential for developing technique in sports, yet economic barriers often place it out of reach for many enthusiasts. To bridge this gap, we introduce Poze, an innovative video processing framework that provides feedback on human motion, emulating the insights of a professional coach. Poze combines pose estimation with sequence comparison and is optimized to function effectively with minimal data. Poze surpasses state-of-the-art vision-language models in video question-answering frameworks, achieving 70% and 196% increase in accuracy over GPT4V and LLaVAv1.6 7b, respectively.
<div id='section'>Paperid: <span id='pid'>1759, <a href='https://arxiv.org/pdf/2410.17741.pdf' target='_blank'>https://arxiv.org/pdf/2410.17741.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zexu Huang, Sarah Monazam Erfani, Siying Lu, Mingming Gong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.17741">Efficient Neural Implicit Representation for 3D Human Reconstruction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>High-fidelity digital human representations are increasingly in demand in the digital world, particularly for interactive telepresence, AR/VR, 3D graphics, and the rapidly evolving metaverse. Even though they work well in small spaces, conventional methods for reconstructing 3D human motion frequently require the use of expensive hardware and have high processing costs. This study presents HumanAvatar, an innovative approach that efficiently reconstructs precise human avatars from monocular video sources. At the core of our methodology, we integrate the pre-trained HuMoR, a model celebrated for its proficiency in human motion estimation. This is adeptly fused with the cutting-edge neural radiance field technology, Instant-NGP, and the state-of-the-art articulated model, Fast-SNARF, to enhance the reconstruction fidelity and speed. By combining these two technologies, a system is created that can render quickly and effectively while also providing estimation of human pose parameters that are unmatched in accuracy. We have enhanced our system with an advanced posture-sensitive space reduction technique, which optimally balances rendering quality with computational efficiency. In our detailed experimental analysis using both artificial and real-world monocular videos, we establish the advanced performance of our approach. HumanAvatar consistently equals or surpasses contemporary leading-edge reconstruction techniques in quality. Furthermore, it achieves these complex reconstructions in minutes, a fraction of the time typically required by existing methods. Our models achieve a training speed that is 110X faster than that of State-of-The-Art (SoTA) NeRF-based models. Our technique performs noticeably better than SoTA dynamic human NeRF methods if given an identical runtime limit. HumanAvatar can provide effective visuals after only 30 seconds of training.
<div id='section'>Paperid: <span id='pid'>1760, <a href='https://arxiv.org/pdf/2410.16623.pdf' target='_blank'>https://arxiv.org/pdf/2410.16623.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sudarshan Harithas, Srinath Sridhar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.16623">MotionGlot: A Multi-Embodied Motion Generation Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces MotionGlot, a model that can generate motion across multiple embodiments with different action dimensions, such as quadruped robots and human bodies. By leveraging the well-established training procedures commonly used in large language models (LLMs), we introduce an instruction-tuning template specifically designed for motionrelated tasks. Our approach demonstrates that the principles underlying LLM training can be successfully adapted to learn a wide range of motion generation tasks across multiple embodiments with different action dimensions. We demonstrate the various abilities of MotionGlot on a set of 6 tasks and report an average improvement of 35.3% across tasks. Additionally, we contribute two new datasets: (1) a dataset of expert-controlled quadruped locomotion with approximately 48,000 trajectories paired with direction-based text annotations, and (2) a dataset of over 23,000 situational text prompts for human motion generation tasks. Finally, we conduct hardware experiments to validate the capabilities of our system in real-world applications.
<div id='section'>Paperid: <span id='pid'>1761, <a href='https://arxiv.org/pdf/2409.19647.pdf' target='_blank'>https://arxiv.org/pdf/2409.19647.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shiming Fang, Kaiyan Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.19647">Fine-Tuning Hybrid Physics-Informed Neural Networks for Vehicle Dynamics Model Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate dynamic modeling is critical for autonomous racing vehicles, especially during high-speed and agile maneuvers where precise motion prediction is essential for safety. Traditional parameter estimation methods face limitations such as reliance on initial guesses, labor-intensive fitting procedures, and complex testing setups. On the other hand, purely data-driven machine learning methods struggle to capture inherent physical constraints and typically require large datasets for optimal performance. To address these challenges, this paper introduces the Fine-Tuning Hybrid Dynamics (FTHD) method, which integrates supervised and unsupervised Physics-Informed Neural Networks (PINNs), combining physics-based modeling with data-driven techniques. FTHD fine-tunes a pre-trained Deep Dynamics Model (DDM) using a smaller training dataset, delivering superior performance compared to state-of-the-art methods such as the Deep Pacejka Model (DPM) and outperforming the original DDM. Furthermore, an Extended Kalman Filter (EKF) is embedded within FTHD (EKF-FTHD) to effectively manage noisy real-world data, ensuring accurate denoising while preserving the vehicle's essential physical characteristics. The proposed FTHD framework is validated through scaled simulations using the BayesRace Physics-based Simulator and full-scale real-world experiments from the Indy Autonomous Challenge. Results demonstrate that the hybrid approach significantly improves parameter estimation accuracy, even with reduced data, and outperforms existing models. EKF-FTHD enhances robustness by denoising real-world data while maintaining physical insights, representing a notable advancement in vehicle dynamics modeling for high-speed autonomous racing.
<div id='section'>Paperid: <span id='pid'>1762, <a href='https://arxiv.org/pdf/2409.13251.pdf' target='_blank'>https://arxiv.org/pdf/2409.13251.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingdian Liu, Yilin Liu, Gurunandan Krishnan, Karl S Bayer, Bing Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.13251">T2M-X: Learning Expressive Text-to-Motion Generation from Partially Annotated Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The generation of humanoid animation from text prompts can profoundly impact animation production and AR/VR experiences. However, existing methods only generate body motion data, excluding facial expressions and hand movements. This limitation, primarily due to a lack of a comprehensive whole-body motion dataset, inhibits their readiness for production use. Recent attempts to create such a dataset have resulted in either motion inconsistency among different body parts in the artificially augmented data or lower quality in the data extracted from RGB videos. In this work, we propose T2M-X, a two-stage method that learns expressive text-to-motion generation from partially annotated data. T2M-X trains three separate Vector Quantized Variational AutoEncoders (VQ-VAEs) for body, hand, and face on respective high-quality data sources to ensure high-quality motion outputs, and a Multi-indexing Generative Pretrained Transformer (GPT) model with motion consistency loss for motion generation and coordination among different body parts. Our results show significant improvements over the baselines both quantitatively and qualitatively, demonstrating its robustness against the dataset limitations.
<div id='section'>Paperid: <span id='pid'>1763, <a href='https://arxiv.org/pdf/2409.13208.pdf' target='_blank'>https://arxiv.org/pdf/2409.13208.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiyana Figuera, Soogeun Park, Hyemin Ahn
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.13208">Redefining Data Pairing for Motion Retargeting Leveraging a Human Body Prior</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose MR HuBo(Motion Retargeting leveraging a HUman BOdy prior), a cost-effective and convenient method to collect high-quality upper body paired <robot, human> pose data, which is essential for data-driven motion retargeting methods. Unlike existing approaches which collect <robot, human> pose data by converting human MoCap poses into robot poses, our method goes in reverse. We first sample diverse random robot poses, and then convert them into human poses. However, since random robot poses can result in extreme and infeasible human poses, we propose an additional technique to sort out extreme poses by exploiting a human body prior trained from a large amount of human pose data. Our data collection method can be used for any humanoid robots, if one designs or optimizes the system's hyperparameters which include a size scale factor and the joint angle ranges for sampling. In addition to this data collection method, we also present a two-stage motion retargeting neural network that can be trained via supervised learning on a large amount of paired data. Compared to other learning-based methods trained via unsupervised learning, we found that our deep neural network trained with ample high-quality paired data achieved notable performance. Our experiments also show that our data filtering method yields better retargeting results than training the model with raw and noisy data. Our code and video results are available on https://sites.google.com/view/mr-hubo/
<div id='section'>Paperid: <span id='pid'>1764, <a href='https://arxiv.org/pdf/2409.09326.pdf' target='_blank'>https://arxiv.org/pdf/2409.09326.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Deng Junli, Luo Yihao, Yang Xueting, Li Siyou, Wang Wei, Guo Jinyang, Shi Ping
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.09326">LawDNet: Enhanced Audio-Driven Lip Synthesis via Local Affine Warping Deformation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the domain of photorealistic avatar generation, the fidelity of audio-driven lip motion synthesis is essential for realistic virtual interactions. Existing methods face two key challenges: a lack of vivacity due to limited diversity in generated lip poses and noticeable anamorphose motions caused by poor temporal coherence. To address these issues, we propose LawDNet, a novel deep-learning architecture enhancing lip synthesis through a Local Affine Warping Deformation mechanism. This mechanism models the intricate lip movements in response to the audio input by controllable non-linear warping fields. These fields consist of local affine transformations focused on abstract keypoints within deep feature maps, offering a novel universal paradigm for feature warping in networks. Additionally, LawDNet incorporates a dual-stream discriminator for improved frame-to-frame continuity and employs face normalization techniques to handle pose and scene variations. Extensive evaluations demonstrate LawDNet's superior robustness and lip movement dynamism performance compared to previous methods. The advancements presented in this paper, including the methodologies, training data, source codes, and pre-trained models, will be made accessible to the research community.
<div id='section'>Paperid: <span id='pid'>1765, <a href='https://arxiv.org/pdf/2408.04106.pdf' target='_blank'>https://arxiv.org/pdf/2408.04106.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sagar Ojha, Karl Leodler, Lou Barbieri, TseHuai Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.04106">Force-Motion Control For A Six Degree-Of-Freedom Robotic Manipulator</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a unified algorithm for motion and force control for a six degree-of-freedom spatial manipulator. The motion-force controller performs trajectory tracking, maneuvering the manipulator's end-effector through desired position, orientations and rates. When contacting an obstacle or target object, the force module of the controller restricts the manipulator movements with a novel force exertion method, which prevents damage to the manipulator, the end-effector, and the objects during the contact or collision. The core strategy presented in this paper is to design the linear acceleration for the end-effector which ensures both trajectory tracking and restriction of any contact force at the end-effector. The design of the controller is validated through numerical simulations and digital twin validation.
<div id='section'>Paperid: <span id='pid'>1766, <a href='https://arxiv.org/pdf/2406.18031.pdf' target='_blank'>https://arxiv.org/pdf/2406.18031.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Juan David Adarve, Robert Mahony
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.18031">Real-time Structure Flow</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This article introduces the structure flow field; a flow field that can provide high-speed robo-centric motion information for motion control of highly dynamic robotic devices and autonomous vehicles. Structure flow is the angular 3D velocity of the scene at a given pixel. We show that structure flow posses an elegant evolution model in the form of a Partial Differential Equation (PDE) that enables us to create dense flow predictions forward in time. We exploit this structure to design a predictor-update algorithm to compute structure flow in real time using image and depth measurements. The prediction stage takes the previous estimate of the structure flow and propagates it forward in time using a numerical implementation of the structure flow PDE. The predicted flow is then updated using new image and depth data. The algorithm runs up to 600 Hz on a Desktop GPU machine for 512x512 images with flow values up to 8 pixels. We provide ground truth validation on high-speed synthetic image sequences as well as results on real-life video on driving scenarios.
<div id='section'>Paperid: <span id='pid'>1767, <a href='https://arxiv.org/pdf/2406.13419.pdf' target='_blank'>https://arxiv.org/pdf/2406.13419.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yide Liu, Xiyan Liu, Dongqi Wang, Wei Yang, shaoxing Qu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.13419">An eight-neuron network for quadruped locomotion with hip-knee joint control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The gait generator, which is capable of producing rhythmic signals for coordinating multiple joints, is an essential component in the quadruped robot locomotion control framework. The biological counterpart of the gait generator is the Central Pattern Generator (abbreviated as CPG), a small neural network consisting of interacting neurons. Inspired by this architecture, researchers have designed artificial neural networks composed of simulated neurons or oscillator equations. Despite the widespread application of these designed CPGs in various robot locomotion controls, some issues remain unaddressed, including: (1) Simplistic network designs often overlook the symmetry between signal and network structure, resulting in fewer gait patterns than those found in nature. (2) Due to minimal architectural consideration, quadruped control CPGs typically consist of only four neurons, which restricts the network's direct control to leg phases rather than joint coordination. (3) Gait changes are achieved by varying the neuron couplings or the assignment between neurons and legs, rather than through external stimulation. We apply symmetry theory to design an eight-neuron network, composed of Stein neuronal models, capable of achieving five gaits and coordinated control of the hip-knee joints. We validate the signal stability of this network as a gait generator through numerical simulations, which reveal various results and patterns encountered during gait transitions using neuronal stimulation. Based on these findings, we have developed several successful gait transition strategies through neuronal stimulations. Using a commercial quadruped robot model, we demonstrate the usability and feasibility of this network by implementing motion control and gait transitions.
<div id='section'>Paperid: <span id='pid'>1768, <a href='https://arxiv.org/pdf/2405.09109.pdf' target='_blank'>https://arxiv.org/pdf/2405.09109.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Stanley Mugisha, Vamsi Krishna Guda, Christine Chevallereau, Damien Chablat, Matteo Zoppi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.09109">Motion Prediction with Gaussian Processes for Safe Human-Robot Interaction in Virtual Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humans use collaborative robots as tools for accomplishing various tasks. The interaction between humans and robots happens in tight shared workspaces. However, these machines must be safe to operate alongside humans to minimize the risk of accidental collisions. Ensuring safety imposes many constraints, such as reduced torque and velocity limits during operation, thus increasing the time to accomplish many tasks. However, for applications such as using collaborative robots as haptic interfaces with intermittent contacts for virtual reality applications, speed limitations result in poor user experiences. This research aims to improve the efficiency of a collaborative robot while improving the safety of the human user. We used Gaussian process models to predict human hand motion and developed strategies for human intention detection based on hand motion and gaze to improve the time for the robot and human security in a virtual environment. We then studied the effect of prediction. Results from comparisons show that the prediction models improved the robot time by 3\% and safety by 17\%. When used alongside gaze, prediction with Gaussian process models resulted in an improvement of the robot time by 2\% and the safety by 13\%.
<div id='section'>Paperid: <span id='pid'>1769, <a href='https://arxiv.org/pdf/2405.06778.pdf' target='_blank'>https://arxiv.org/pdf/2405.06778.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kebing Xue, Hyewon Seo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.06778">Shape Conditioned Human Motion Generation with Diffusion Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion synthesis is an important task in computer graphics and computer vision. While focusing on various conditioning signals such as text, action class, or audio to guide the generation process, most existing methods utilize skeleton-based pose representation, requiring additional skinning to produce renderable meshes. Given that human motion is a complex interplay of bones, joints, and muscles, considering solely the skeleton for generation may neglect their inherent interdependency, which can limit the variability and precision of the generated results. To address this issue, we propose a Shape-conditioned Motion Diffusion model (SMD), which enables the generation of motion sequences directly in mesh format, conditioned on a specified target mesh. In SMD, the input meshes are transformed into spectral coefficients using graph Laplacian, to efficiently represent meshes. Subsequently, we propose a Spectral-Temporal Autoencoder (STAE) to leverage cross-temporal dependencies within the spectral domain. Extensive experimental evaluations show that SMD not only produces vivid and realistic motions but also achieves competitive performance in text-to-motion and action-to-motion tasks when compared to state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>1770, <a href='https://arxiv.org/pdf/2405.05428.pdf' target='_blank'>https://arxiv.org/pdf/2405.05428.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Thomas Carr, Depeng Xu, Aidong Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.05428">Adversary-Guided Motion Retargeting for Skeleton Anonymization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Skeleton-based motion visualization is a rising field in computer vision, especially in the case of virtual reality (VR). With further advancements in human-pose estimation and skeleton extracting sensors, more and more applications that utilize skeleton data have come about. These skeletons may appear to be anonymous but they contain embedded personally identifiable information (PII). In this paper we present a new anonymization technique that is based on motion retargeting, utilizing adversary classifiers to further remove PII embedded in the skeleton. Motion retargeting is effective in anonymization as it transfers the movement of the user onto the a dummy skeleton. In doing so, any PII linked to the skeleton will be based on the dummy skeleton instead of the user we are protecting. We propose a Privacy-centric Deep Motion Retargeting model (PMR) which aims to further clear the retargeted skeleton of PII through adversarial learning. In our experiments, PMR achieves motion retargeting utility performance on par with state of the art models while also reducing the performance of privacy attacks.
<div id='section'>Paperid: <span id='pid'>1771, <a href='https://arxiv.org/pdf/2404.13915.pdf' target='_blank'>https://arxiv.org/pdf/2404.13915.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiyuan Lu, Muhammad Hanif, Takumi Shimizu, Takeshi Hatanaka
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.13915">Angle-Aware Coverage with Camera Rotational Motion Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a novel control strategy for drone networks to improve the quality of 3D structures reconstructed from aerial images by drones. Unlike the existing coverage control strategies for this purpose, our proposed approach simultaneously controls both the camera orientation and drone translational motion, enabling more comprehensive perspectives and enhancing the map's overall quality. Subsequently, we present a novel problem formulation, including a new performance function to evaluate the drone positions and camera orientations. We then design a QP-based controller with a control barrier-like function for a constraint on the decay rate of the objective function. The present problem formulation poses a new challenge, requiring significantly greater computational efforts than the case involving only translational motion control. We approach this issue technologically, namely by introducing JAX, utilizing just-in-time (JIT) compilation and Graphical Processing Unit (GPU) acceleration. We finally conduct extensive verifications through simulation in ROS (Robot Operating System) and show the real-time feasibility of the controller and the superiority of the present controller to the conventional method.
<div id='section'>Paperid: <span id='pid'>1772, <a href='https://arxiv.org/pdf/2404.10880.pdf' target='_blank'>https://arxiv.org/pdf/2404.10880.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Arnab Kumar Mondal, Stefano Alletto, Denis Tome
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.10880">HumMUSS: Human Motion Understanding using State Space Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding human motion from video is essential for a range of applications, including pose estimation, mesh recovery and action recognition. While state-of-the-art methods predominantly rely on transformer-based architectures, these approaches have limitations in practical scenarios. Transformers are slower when sequentially predicting on a continuous stream of frames in real-time, and do not generalize to new frame rates. In light of these constraints, we propose a novel attention-free spatiotemporal model for human motion understanding building upon recent advancements in state space models. Our model not only matches the performance of transformer-based models in various motion understanding tasks but also brings added benefits like adaptability to different video frame rates and enhanced training speed when working with longer sequence of keypoints. Moreover, the proposed model supports both offline and real-time applications. For real-time sequential prediction, our model is both memory efficient and several times faster than transformer-based approaches while maintaining their high accuracy.
<div id='section'>Paperid: <span id='pid'>1773, <a href='https://arxiv.org/pdf/2404.01576.pdf' target='_blank'>https://arxiv.org/pdf/2404.01576.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jesudara Omidokun, Darlington Egeonu, Bochen Jia, Liang Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.01576">Leveraging Digital Perceptual Technologies for Remote Perception and Analysis of Human Biomechanical Processes: A Contactless Approach for Workload and Joint Force Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study presents an innovative computer vision framework designed to analyze human movements in industrial settings, aiming to enhance biomechanical analysis by integrating seamlessly with existing software. Through a combination of advanced imaging and modeling techniques, the framework allows for comprehensive scrutiny of human motion, providing valuable insights into kinematic patterns and kinetic data. Utilizing Convolutional Neural Networks (CNNs), Direct Linear Transform (DLT), and Long Short-Term Memory (LSTM) networks, the methodology accurately detects key body points, reconstructs 3D landmarks, and generates detailed 3D body meshes. Extensive evaluations across various movements validate the framework's effectiveness, demonstrating comparable results to traditional marker-based models with minor differences in joint angle estimations and precise estimations of weight and height. Statistical analyses consistently support the framework's reliability, with joint angle estimations showing less than a 5-degree difference for hip flexion, elbow flexion, and knee angle methods. Additionally, weight estimation exhibits an average error of less than 6 % for weight and less than 2 % for height when compared to ground-truth values from 10 subjects. The integration of the Biomech-57 landmark skeleton template further enhances the robustness and reinforces the framework's credibility. This framework shows significant promise for meticulous biomechanical analysis in industrial contexts, eliminating the need for cumbersome markers and extending its utility to diverse research domains, including the study of specific exoskeleton devices' impact on facilitating the prompt return of injured workers to their tasks.
<div id='section'>Paperid: <span id='pid'>1774, <a href='https://arxiv.org/pdf/2403.15444.pdf' target='_blank'>https://arxiv.org/pdf/2403.15444.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Abhi Kamboj, Minh Do
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.15444">A Survey of IMU Based Cross-Modal Transfer Learning in Human Activity Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite living in a multi-sensory world, most AI models are limited to textual and visual understanding of human motion and behavior. In fact, full situational awareness of human motion could best be understood through a combination of sensors. In this survey we investigate how knowledge can be transferred and utilized amongst modalities for Human Activity/Action Recognition (HAR), i.e. cross-modality transfer learning. We motivate the importance and potential of IMU data and its applicability in cross-modality learning as well as the importance of studying the HAR problem. We categorize HAR related tasks by time and abstractness and then compare various types of multimodal HAR datasets. We also distinguish and expound on many related but inconsistently used terms in the literature, such as transfer learning, domain adaptation, representation learning, sensor fusion, and multimodal learning, and describe how cross-modal learning fits with all these concepts. We then review the literature in IMU-based cross-modal transfer for HAR. The two main approaches for cross-modal transfer are instance-based transfer, where instances of one modality are mapped to another (e.g. knowledge is transferred in the input space), or feature-based transfer, where the model relates the modalities in an intermediate latent space (e.g. knowledge is transferred in the feature space). Finally, we discuss future research directions and applications in cross-modal HAR.
<div id='section'>Paperid: <span id='pid'>1775, <a href='https://arxiv.org/pdf/2403.13905.pdf' target='_blank'>https://arxiv.org/pdf/2403.13905.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anegi James, Efstathios Bakolas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.13905">Motion Prediction of Multi-agent systems with Multi-view clustering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a method for future motion prediction of multi-agent systems by including group formation information and future intent. Formation of groups depends on a physics-based clustering method that follows the agglomerative hierarchical clustering algorithm. We identify clusters that incorporate the minimum cost-to-go function of a relevant optimal control problem as a metric for clustering between the groups among agents, where groups with similar associated costs are assumed to be likely to move together. The cost metric accounts for proximity to other agents as well as the intended goal of each agent. An unscented Kalman filter based approach is used to update the established clusters as well as add new clusters when new information is obtained. Our approach is verified through non-trivial numerical simulations implementing the proposed algorithm on different datasets pertaining to a variety of scenarios and agents.
<div id='section'>Paperid: <span id='pid'>1776, <a href='https://arxiv.org/pdf/2403.06569.pdf' target='_blank'>https://arxiv.org/pdf/2403.06569.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sharmita Dey, Sarath R. Nair
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.06569">Enhancing Joint Motion Prediction for Individuals with Limb Loss Through Model Reprogramming</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Mobility impairment caused by limb loss is a significant challenge faced by millions of individuals worldwide. The development of advanced assistive technologies, such as prosthetic devices, has the potential to greatly improve the quality of life for amputee patients. A critical component in the design of such technologies is the accurate prediction of reference joint motion for the missing limb. However, this task is hindered by the scarcity of joint motion data available for amputee patients, in contrast to the substantial quantity of data from able-bodied subjects. To overcome this, we leverage deep learning's reprogramming property to repurpose well-trained models for a new goal without altering the model parameters. With only data-level manipulation, we adapt models originally designed for able-bodied people to forecast joint motion in amputees. The findings in this study have significant implications for advancing assistive tech and amputee mobility.
<div id='section'>Paperid: <span id='pid'>1777, <a href='https://arxiv.org/pdf/2401.05018.pdf' target='_blank'>https://arxiv.org/pdf/2401.05018.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sarmad Idrees, Jongeun Choi, Seokman Sohn
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.05018">AdvMT: Adversarial Motion Transformer for Long-term Human Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To achieve seamless collaboration between robots and humans in a shared environment, accurately predicting future human movements is essential. Human motion prediction has traditionally been approached as a sequence prediction problem, leveraging historical human motion data to estimate future poses. Beginning with vanilla recurrent networks, the research community has investigated a variety of methods for learning human motion dynamics, encompassing graph-based and generative approaches. Despite these efforts, achieving accurate long-term predictions continues to be a significant challenge. In this regard, we present the Adversarial Motion Transformer (AdvMT), a novel model that integrates a transformer-based motion encoder and a temporal continuity discriminator. This combination effectively captures spatial and temporal dependencies simultaneously within frames. With adversarial training, our method effectively reduces the unwanted artifacts in predictions, thereby ensuring the learning of more realistic and fluid human motions. The evaluation results indicate that AdvMT greatly enhances the accuracy of long-term predictions while also delivering robust short-term predictions
<div id='section'>Paperid: <span id='pid'>1778, <a href='https://arxiv.org/pdf/2401.02620.pdf' target='_blank'>https://arxiv.org/pdf/2401.02620.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Song Bai, Jie Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.02620">Progress and Prospects in 3D Generative AI: A Technical Overview including 3D human</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While AI-generated text and 2D images continue to expand its territory, 3D generation has gradually emerged as a trend that cannot be ignored. Since the year 2023 an abundant amount of research papers has emerged in the domain of 3D generation. This growth encompasses not just the creation of 3D objects, but also the rapid development of 3D character and motion generation. Several key factors contribute to this progress. The enhanced fidelity in stable diffusion, coupled with control methods that ensure multi-view consistency, and realistic human models like SMPL-X, contribute synergistically to the production of 3D models with remarkable consistency and near-realistic appearances. The advancements in neural network-based 3D storing and rendering models, such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS), have accelerated the efficiency and realism of neural rendered models. Furthermore, the multimodality capabilities of large language models have enabled language inputs to transcend into human motion outputs. This paper aims to provide a comprehensive overview and summary of the relevant papers published mostly during the latter half year of 2023. It will begin by discussing the AI generated object models in 3D, followed by the generated 3D human models, and finally, the generated 3D human motions, culminating in a conclusive summary and a vision for the future.
<div id='section'>Paperid: <span id='pid'>1779, <a href='https://arxiv.org/pdf/2312.07340.pdf' target='_blank'>https://arxiv.org/pdf/2312.07340.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yusen Feng, Xiyan Xu, Libin Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.07340">MuscleVAE: Model-Based Controllers of Muscle-Actuated Characters</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we present a simulation and control framework for generating biomechanically plausible motion for muscle-actuated characters. We incorporate a fatigue dynamics model, the 3CC-r model, into the widely-adopted Hill-type muscle model to simulate the development and recovery of fatigue in muscles, which creates a natural evolution of motion style caused by the accumulation of fatigue from prolonged activities. To address the challenging problem of controlling a musculoskeletal system with high degrees of freedom, we propose a novel muscle-space control strategy based on PD control. Our simulation and control framework facilitates the training of a generative model for muscle-based motion control, which we refer to as MuscleVAE. By leveraging the variational autoencoders (VAEs), MuscleVAE is capable of learning a rich and flexible latent representation of skills from a large unstructured motion dataset, encoding not only motion features but also muscle control and fatigue properties. We demonstrate that the MuscleVAE model can be efficiently trained using a model-based approach, resulting in the production of high-fidelity motions and enabling a variety of downstream tasks.
<div id='section'>Paperid: <span id='pid'>1780, <a href='https://arxiv.org/pdf/2312.06184.pdf' target='_blank'>https://arxiv.org/pdf/2312.06184.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tenghao Deng, Yan Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.06184">Recent Advances in Deterministic Human Motion Prediction: A Review</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, with the continuous advancement of deep learning and the emergence of large-scale human motion datasets, human motion prediction technology has gradually gained prominence in various fields such as human-computer interaction, autonomous driving, sports analysis, and personnel tracking. This article introduces common model architectures in this domain along with their respective advantages and disadvantages. It also systematically summarizes recent research innovations, focusing on in-depth discussions of relevant papers in these areas, thereby highlighting forward-looking insights into the field's development. Furthermore, this paper provides a comprehensive overview of existing methods, commonly used datasets, and evaluation metrics in this field. Finally, it discusses some of the current limitations in the field and proposes potential future research directions to address these challenges and promote further advancements in human motion prediction.
<div id='section'>Paperid: <span id='pid'>1781, <a href='https://arxiv.org/pdf/2312.04572.pdf' target='_blank'>https://arxiv.org/pdf/2312.04572.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Feifan Yu, Wenyuan Cong, Xinmin Chen, Yue Lin, Jiqiang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.04572">Harnessing LSTM for Nonlinear Ship Deck Motion Prediction in UAV Autonomous Landing amidst High Sea States</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous landing of UAVs in high sea states requires the UAV to land exclusively during the ship deck's "rest period," coinciding with minimal movement. Given this scenario, determining the ship's "rest period" based on its movement patterns becomes a fundamental prerequisite for addressing this challenge. This study employs the Long Short-Term Memory (LSTM) neural network to predict the ship's motion across three dimensions: longi-tudinal, transverse, and vertical waves. In the absence of actual ship data under high sea states, this paper employs a composite sine wave model to simulate ship deck motion. Through this approach, a highly accurate model is established, exhibiting promising outcomes within various stochastic sine wave combination models.
<div id='section'>Paperid: <span id='pid'>1782, <a href='https://arxiv.org/pdf/2312.03528.pdf' target='_blank'>https://arxiv.org/pdf/2312.03528.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maria Priisalu, Ted Kronvall, Cristian Sminchisescu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.03528">Personalized Pose Forecasting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human pose forecasting is the task of predicting articulated human motion given past human motion. There exists a number of popular benchmarks that evaluate an array of different models performing human pose forecasting. These benchmarks do not reflect that a human interacting system, such as a delivery robot, observes and plans for the motion of the same individual over an extended period of time. Every individual has unique and distinct movement patterns. This is however not reflected in existing benchmarks that evaluate a model's ability to predict an average human's motion rather than a particular individual's. We reformulate the human motion forecasting problem and present a model-agnostic personalization method. Motion forecasting personalization can be performed efficiently online by utilizing a low-parametric time-series analysis model that personalizes neural network pose predictions.
<div id='section'>Paperid: <span id='pid'>1783, <a href='https://arxiv.org/pdf/2311.11662.pdf' target='_blank'>https://arxiv.org/pdf/2311.11662.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sushovan Chanda, Amogh Tiwari, Lokender Tiwari, Brojeshwar Bhowmick, Avinash Sharma, Hrishav Barua
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.11662">Enhanced Spatio-Temporal Context for Temporally Consistent Robust 3D Human Motion Recovery from Monocular Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recovering temporally consistent 3D human body pose, shape and motion from a monocular video is a challenging task due to (self-)occlusions, poor lighting conditions, complex articulated body poses, depth ambiguity, and limited availability of annotated data. Further, doing a simple perframe estimation is insufficient as it leads to jittery and implausible results. In this paper, we propose a novel method for temporally consistent motion estimation from a monocular video. Instead of using generic ResNet-like features, our method uses a body-aware feature representation and an independent per-frame pose and camera initialization over a temporal window followed by a novel spatio-temporal feature aggregation by using a combination of self-similarity and self-attention over the body-aware features and the perframe initialization. Together, they yield enhanced spatiotemporal context for every frame by considering remaining past and future frames. These features are used to predict the pose and shape parameters of the human body model, which are further refined using an LSTM. Experimental results on the publicly available benchmark data show that our method attains significantly lower acceleration error and outperforms the existing state-of-the-art methods over all key quantitative evaluation metrics, including complex scenarios like partial occlusion, complex poses and even relatively low illumination.
<div id='section'>Paperid: <span id='pid'>1784, <a href='https://arxiv.org/pdf/2311.10582.pdf' target='_blank'>https://arxiv.org/pdf/2311.10582.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Oscar Gil, Alberto Sanfeliu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.10582">Human motion trajectory prediction using the Social Force Model for real-time and low computational cost applications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion trajectory prediction is a very important functionality for human-robot collaboration, specifically in accompanying, guiding, or approaching tasks, but also in social robotics, self-driving vehicles, or security systems. In this paper, a novel trajectory prediction model, Social Force Generative Adversarial Network (SoFGAN), is proposed. SoFGAN uses a Generative Adversarial Network (GAN) and Social Force Model (SFM) to generate different plausible people trajectories reducing collisions in a scene. Furthermore, a Conditional Variational Autoencoder (CVAE) module is added to emphasize the destination learning. We show that our method is more accurate in making predictions in UCY or BIWI datasets than most of the current state-of-the-art models and also reduces collisions in comparison to other approaches. Through real-life experiments, we demonstrate that the model can be used in real-time without GPU's to perform good quality predictions with a low computational cost.
<div id='section'>Paperid: <span id='pid'>1785, <a href='https://arxiv.org/pdf/2311.04383.pdf' target='_blank'>https://arxiv.org/pdf/2311.04383.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuke Yan, Dan Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.04383">Active Collision Avoidance System for E-Scooters in Pedestrian Environment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the dense fabric of urban areas, electric scooters have rapidly become a preferred mode of transportation. As they cater to modern mobility demands, they present significant safety challenges, especially when interacting with pedestrians. In general, e-scooters are suggested to be ridden in bike lanes/sidewalks or share the road with cars at the maximum speed of about 15-20 mph, which is more flexible and much faster than pedestrians and bicyclists. Accurate prediction of pedestrian movement, coupled with assistant motion control of scooters, is essential in minimizing collision risks and seamlessly integrating scooters in areas dense with pedestrians. Addressing these safety concerns, our research introduces a novel e-Scooter collision avoidance system (eCAS) with a method for predicting pedestrian trajectories, employing an advanced LSTM network integrated with a state refinement module. This proactive model is designed to ensure unobstructed movement in areas with substantial pedestrian traffic without collisions. Results are validated on two public datasets, ETH and UCY, providing encouraging outcomes. Our model demonstrated proficiency in anticipating pedestrian paths and augmented scooter path planning, allowing for heightened adaptability in densely populated locales. This study shows the potential of melding pedestrian trajectory prediction with scooter motion planning. With the ubiquity of electric scooters in urban environments, such advancements have become crucial to safeguard all participants in urban transit.
<div id='section'>Paperid: <span id='pid'>1786, <a href='https://arxiv.org/pdf/2310.20034.pdf' target='_blank'>https://arxiv.org/pdf/2310.20034.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Moritz A. Graule, Volkan Isler
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.20034">GG-LLM: Geometrically Grounding Large Language Models for Zero-shot Human Activity Forecasting in Human-Aware Task Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A robot in a human-centric environment needs to account for the human's intent and future motion in its task and motion planning to ensure safe and effective operation. This requires symbolic reasoning about probable future actions and the ability to tie these actions to specific locations in the physical environment. While one can train behavioral models capable of predicting human motion from past activities, this approach requires large amounts of data to achieve acceptable long-horizon predictions. More importantly, the resulting models are constrained to specific data formats and modalities. Moreover, connecting predictions from such models to the environment at hand to ensure the applicability of these predictions is an unsolved problem. We present a system that utilizes a Large Language Model (LLM) to infer a human's next actions from a range of modalities without fine-tuning. A novel aspect of our system that is critical to robotics applications is that it links the predicted actions to specific locations in a semantic map of the environment. Our method leverages the fact that LLMs, trained on a vast corpus of text describing typical human behaviors, encode substantial world knowledge, including probable sequences of human actions and activities. We demonstrate how these localized activity predictions can be incorporated in a human-aware task planner for an assistive robot to reduce the occurrences of undesirable human-robot interactions by 29.2% on average.
<div id='section'>Paperid: <span id='pid'>1787, <a href='https://arxiv.org/pdf/2310.18206.pdf' target='_blank'>https://arxiv.org/pdf/2310.18206.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pablo RamÃ³n, Cristian Romero, Javier Tapia, Miguel A. Otaduy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.18206">FLSH -- Friendly Library for the Simulation of Humans</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Computer models of humans are ubiquitous throughout computer animation and computer vision. However, these models rarely represent the dynamics of human motion, as this requires adding a complex layer that solves body motion in response to external interactions and according to the laws of physics. FLSH is a library that facilitates this task for researchers and developers who are not interested in the nuisances of physics simulation, but want to easily integrate dynamic humans in their applications. FLSH provides easy access to three flavors of body physics, with different features and computational complexity: skeletal dynamics, full soft-tissue dynamics, and reduced-order modeling of soft-tissue dynamics. In all three cases, the simulation models are built on top of the pseudo-standard SMPL parametric body model.
<div id='section'>Paperid: <span id='pid'>1788, <a href='https://arxiv.org/pdf/2310.12409.pdf' target='_blank'>https://arxiv.org/pdf/2310.12409.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinseong Park, Yong-Sik Shin, Sanghyun Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.12409">Object-Aware Impedance Control for Human-Robot Collaborative Task with Online Object Parameter Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Physical human-robot interactions (pHRIs) can improve robot autonomy and reduce physical demands on humans. In this paper, we consider a collaborative task with a considerably long object and no prior knowledge of the object's parameters. An integrated control framework with an online object parameter estimator and a Cartesian object-aware impedance controller is proposed to realize complicated scenarios. During the transportation task, the object parameters are estimated online while a robot and human lift an object. The perturbation motion is incorporated into the null space of the desired trajectory to enhance the estimator accuracy. An object-aware impedance controller is designed using the real-time estimation results to effectively transmit the intended human motion to the robot through the object. Experimental demonstrations of collaborative tasks, including object transportation and assembly tasks, are implemented to show the effectiveness of our proposed method.
<div id='section'>Paperid: <span id='pid'>1789, <a href='https://arxiv.org/pdf/2310.09519.pdf' target='_blank'>https://arxiv.org/pdf/2310.09519.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zirui Wan, Saeid Sanei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.09519">Crowd Modeling and Control via Cooperative Adaptive Filtering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces a crowd modeling and motion control approach that employs diffusion adaptation within an adaptive network. In the network, nodes collaboratively address specific estimation problems while simultaneously moving as agents governed by certain motion control mechanisms. Our research delves into the behaviors of agents when they encounter spatial constraints. Within this framework, agents pursue several objectives, such as target tracking, coherent motion, and obstacle evasion. Throughout their navigation, they demonstrate a nature of self-organization and self-adjustment that drives them to maintain certain social distances with each other, and adaptively adjust their behaviors in response to the environmental changes. Our findings suggest a promising approach to mitigate the spread of viral pandemics and averting stampedes.
<div id='section'>Paperid: <span id='pid'>1790, <a href='https://arxiv.org/pdf/2310.04232.pdf' target='_blank'>https://arxiv.org/pdf/2310.04232.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kay Scheerer, Thomas Michalke, Juergen Mathes
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.04232">The WayHome: Long-term Motion Prediction on Dynamically Scaled</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>One of the key challenges for autonomous vehicles is the ability to accurately predict the motion of other objects in the surrounding environment, such as pedestrians or other vehicles. In this contribution, a novel motion forecasting approach for autonomous vehicles is developed, inspired by the work of Gilles et al. [1]. We predict multiple heatmaps with a neuralnetwork-based model for every traffic participant in the vicinity of the autonomous vehicle; with one heatmap per timestep. The heatmaps are used as input to a novel sampling algorithm that extracts coordinates corresponding to the most likely future positions. We experiment with different encoders and decoders, as well as a comparison of two loss functions. Additionally, a new grid-scaling technique is introduced, showing further improved performance. Overall, our approach improves stateof-the-art miss rate performance for the function-relevant prediction interval of 3 seconds while being competitive in longer prediction intervals (up to eight seconds). The evaluation is done on the public 2022 Waymo motion challenge.
<div id='section'>Paperid: <span id='pid'>1791, <a href='https://arxiv.org/pdf/2310.03375.pdf' target='_blank'>https://arxiv.org/pdf/2310.03375.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haitao Yu, Deheng Zhang, Peiyuan Xie, Tianyi Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.03375">Point-Based Radiance Fields for Controllable Human Motion Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper proposes a novel controllable human motion synthesis method for fine-level deformation based on static point-based radiance fields. Although previous editable neural radiance field methods can generate impressive results on novel-view synthesis and allow naive deformation, few algorithms can achieve complex 3D human editing such as forward kinematics. Our method exploits the explicit point cloud to train the static 3D scene and apply the deformation by encoding the point cloud translation using a deformation MLP. To make sure the rendering result is consistent with the canonical space training, we estimate the local rotation using SVD and interpolate the per-point rotation to the query view direction of the pre-trained radiance field. Extensive experiments show that our approach can significantly outperform the state-of-the-art on fine-level complex deformation which can be generalized to other 3D characters besides humans.
<div id='section'>Paperid: <span id='pid'>1792, <a href='https://arxiv.org/pdf/2309.10199.pdf' target='_blank'>https://arxiv.org/pdf/2309.10199.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Carlos R. de Cos, JosÃ© Ãngel Acosta
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.10199">Unified Force and Motion Adaptive-Integral Control of Flexible Robot Manipulators</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, an adaptive nonlinear strategy for the motion and force control of flexible manipulators is proposed. The approach provides robust motion control until contact is detected when force control is then available--without any control switch--, and vice versa. This self-tuning in mixed contact/non-contact scenarios is possible thanks to the unified formulation of force and motion control, including an integral transpose-based inverse kinematics and adaptive-update laws for the flexible manipulator link and contact stiffnesses. Global boundedness of all signals and asymptotic stability of force and position are guaranteed through Lyapunov analysis. The control strategy and its implementation has been validated using a low-cost basic microcontroller and a manipulator with 3 flexible joints and 4 actuators. Complete experimental results are provided in a realistic mixed contact scenario, demonstrating very-low computational demand with inexpensive force sensors.
<div id='section'>Paperid: <span id='pid'>1793, <a href='https://arxiv.org/pdf/2309.09237.pdf' target='_blank'>https://arxiv.org/pdf/2309.09237.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianchen Shen, Irene Di Giulio, Matthew Howard
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.09237">Human Movement Forecasting with Loose Clothing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion prediction and trajectory forecasting are essential in human motion analysis. Nowadays, sensors can be seamlessly integrated into clothing using cutting-edge electronic textile (e-textile) technology, allowing long-term recording of human movements outside the laboratory. Motivated by the recent findings that clothing-attached sensors can achieve higher activity recognition accuracy than body-attached sensors. This work investigates the performance of human motion prediction using clothing-attached sensors compared with body-attached sensors. It reports experiments in which statistical models learnt from the movement of loose clothing are used to predict motion patterns of the body of robotically simulated and real human behaviours. Counterintuitively, the results show that fabric-attached sensors can have better motion prediction performance than rigid-attached sensors. Specifically, The fabric-attached sensor can improve the accuracy up to 40% and requires up to 80% less duration of the past trajectory to achieve high prediction accuracy (i.e., 95%) compared to the rigid-attached sensor.
<div id='section'>Paperid: <span id='pid'>1794, <a href='https://arxiv.org/pdf/2309.09021.pdf' target='_blank'>https://arxiv.org/pdf/2309.09021.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Honghui Wang, Weiming Zhi, Gustavo Batista, Rohitash Chandra
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.09021">Pedestrian Trajectory Prediction Using Dynamics-based Deep Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pedestrian trajectory prediction plays an important role in autonomous driving systems and robotics. Recent work utilizing prominent deep learning models for pedestrian motion prediction makes limited a priori assumptions about human movements, resulting in a lack of explainability and explicit constraints enforced on predicted trajectories. We present a dynamics-based deep learning framework with a novel asymptotically stable dynamical system integrated into a Transformer-based model. We use an asymptotically stable dynamical system to model human goal-targeted motion by enforcing the human walking trajectory, which converges to a predicted goal position, and to provide the Transformer model with prior knowledge and explainability. Our framework features the Transformer model that works with a goal estimator and dynamical system to learn features from pedestrian motion history. The results show that our framework outperforms prominent models using five benchmark human motion datasets.
<div id='section'>Paperid: <span id='pid'>1795, <a href='https://arxiv.org/pdf/2309.08988.pdf' target='_blank'>https://arxiv.org/pdf/2309.08988.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Diego Navarro-Cabrera, Niceto R. Luque, Eduardo Ros
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.08988">Multi-objective tuning for torque PD controllers of cobots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Collaborative robotics is a new and challenging field in the realm of motion control and human-robot interaction. The safety measures needed for a reliable interaction between the robot and its environment hinder the use of classical control methods, pushing researchers to try new techniques such as machine learning (ML). In this context, reinforcement learning has been adopted as the primary way to create intelligent controllers for collaborative robots, however supervised learning shows great promise in the hope of developing data-driven model based ML controllers in a faster and safer way. In this work we study several aspects of the methodology needed to create a dataset to be used to learn the dynamics of a robot. For this we tune several PD controllers to several trajectories, using a multi-objective genetic algorithm (GA) which takes into account not only their accuracy, but also their safety. We demonstrate the need to tune the controllers individually to each trajectory and empirically explore the best population size for the GA and how the speed of the trajectory affects the tuning and the dynamics of the robot.
<div id='section'>Paperid: <span id='pid'>1796, <a href='https://arxiv.org/pdf/2309.07550.pdf' target='_blank'>https://arxiv.org/pdf/2309.07550.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jayjun Lee, Adam J. Spiers
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.07550">Naturalistic Robot Arm Trajectory Generation via Representation Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The integration of manipulator robots in household environments suggests a need for more predictable and human-like robot motion. This holds especially true for wheelchair-mounted assistive robots that can support the independence of people with paralysis. One method of generating naturalistic motion trajectories is via the imitation of human demonstrators. This paper explores a self-supervised imitation learning method using an autoregressive spatio-temporal graph neural network for an assistive drinking task. We address learning from diverse human motion trajectory data that were captured via wearable IMU sensors on a human arm as the action-free task demonstrations. Observed arm motion data from several participants is used to generate natural and functional drinking motion trajectories for a UR5e robot arm.
<div id='section'>Paperid: <span id='pid'>1797, <a href='https://arxiv.org/pdf/2309.06621.pdf' target='_blank'>https://arxiv.org/pdf/2309.06621.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vittorio Giammarino, Alberto Giammarino, Matthew Pearce
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.06621">A Reinforcement Learning Approach for Robotic Unloading from Visual Observations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we focus on a robotic unloading problem from visual observations, where robots are required to autonomously unload stacks of parcels using RGB-D images as their primary input source. While supervised and imitation learning have accomplished good results in these types of tasks, they heavily rely on labeled data, which are challenging to obtain in realistic scenarios. Our study aims to develop a sample efficient controller framework that can learn unloading tasks without the need for labeled data during the learning process. To tackle this challenge, we propose a hierarchical controller structure that combines a high-level decision-making module with classical motion control. The high-level module is trained using Deep Reinforcement Learning (DRL), wherein we incorporate a safety bias mechanism and design a reward function tailored to this task. Our experiments demonstrate that both these elements play a crucial role in achieving improved learning performance. Furthermore, to ensure reproducibility and establish a benchmark for future research, we provide free access to our code and simulation.
<div id='section'>Paperid: <span id='pid'>1798, <a href='https://arxiv.org/pdf/2309.05972.pdf' target='_blank'>https://arxiv.org/pdf/2309.05972.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tetsuya Abe, Ryusuke Sagawa, Ko Ayusawa, Wataru Takano
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.05972">Self-supervised Extraction of Human Motion Structures via Frame-wise Discrete Features</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The present paper proposes an encoder-decoder model for extracting the structures of human motions represented by frame-wise discrete features in a self-supervised manner. In the proposed method, features are extracted as codes in a motion codebook without the use of human knowledge, and the relationship between these codes can be visualized on a graph. Since the codes are expected to be temporally sparse compared to the captured frame rate and can be shared by multiple sequences, the proposed network model also addresses the need for training constraints. Specifically, the model consists of self-attention layers and a vector clustering block. The attention layers contribute to finding sparse keyframes and discrete features as motion codes, which are then extracted by vector clustering. The constraints are realized as training losses so that the same motion codes can be as contiguous as possible and can be shared by multiple sequences. In addition, we propose the use of causal self-attention as a method by which to calculate attention for long sequences consisting of numerous frames. In our experiments, the sparse structures of motion codes were used to compile a graph that facilitates visualization of the relationship between the codes and the differences between sequences. We then evaluated the effectiveness of the extracted motion codes by applying them to multiple recognition tasks and found that performance levels comparable to task-optimized methods could be achieved by linear probing.
<div id='section'>Paperid: <span id='pid'>1799, <a href='https://arxiv.org/pdf/2308.08753.pdf' target='_blank'>https://arxiv.org/pdf/2308.08753.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lubing Zhou, Xiaoli Meng, Yiluan Guo, Jiong Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.08753">BOTT: Box Only Transformer Tracker for 3D Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tracking 3D objects is an important task in autonomous driving. Classical Kalman Filtering based methods are still the most popular solutions. However, these methods require handcrafted designs in motion modeling and can not benefit from the growing data amounts. In this paper, Box Only Transformer Tracker (BOTT) is proposed to learn to link 3D boxes of the same object from the different frames, by taking all the 3D boxes in a time window as input. Specifically, transformer self-attention is applied to exchange information between all the boxes to learn global-informative box embeddings. The similarity between these learned embeddings can be used to link the boxes of the same object. BOTT can be used for both online and offline tracking modes seamlessly. Its simplicity enables us to significantly reduce engineering efforts required by traditional Kalman Filtering based methods. Experiments show BOTT achieves competitive performance on two largest 3D MOT benchmarks: 69.9 and 66.7 AMOTA on nuScenes validation and test splits, respectively, 56.45 and 59.57 MOTA L2 on Waymo Open Dataset validation and test splits, respectively. This work suggests that tracking 3D objects by learning features directly from 3D boxes using transformers is a simple yet effective way.
<div id='section'>Paperid: <span id='pid'>1800, <a href='https://arxiv.org/pdf/2308.07751.pdf' target='_blank'>https://arxiv.org/pdf/2308.07751.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maximilian SchÃ¤fer, Kun Zhao, Anton Kummert
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.07751">CASPNet++: Joint Multi-Agent Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The prediction of road users' future motion is a critical task in supporting advanced driver-assistance systems (ADAS). It plays an even more crucial role for autonomous driving (AD) in enabling the planning and execution of safe driving maneuvers. Based on our previous work, Context-Aware Scene Prediction Network (CASPNet), an improved system, CASPNet++, is proposed. In this work, we focus on further enhancing the interaction modeling and scene understanding to support the joint prediction of all road users in a scene using spatiotemporal grids to model future occupancy. Moreover, an instance-based output head is introduced to provide multi-modal trajectories for agents of interest. In extensive quantitative and qualitative analysis, we demonstrate the scalability of CASPNet++ in utilizing and fusing diverse environmental input sources such as HD maps, Radar detection, and Lidar segmentation. Tested on the urban-focused prediction dataset nuScenes, CASPNet++ reaches state-of-the-art performance. The model has been deployed in a testing vehicle, running in real-time with moderate computational resources.
<div id='section'>Paperid: <span id='pid'>1801, <a href='https://arxiv.org/pdf/2308.02397.pdf' target='_blank'>https://arxiv.org/pdf/2308.02397.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Iris FÃ¼rst-Walter, Antonio Nappi, Tanja Harbaum, JÃ¼rgen Becker
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.02397">Design Space Exploration on Efficient and Accurate Human Pose Estimation from Sparse IMU-Sensing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human Pose Estimation (HPE) to assess human motion in sports, rehabilitation or work safety requires accurate sensing without compromising the sensitive underlying personal data. Therefore, local processing is necessary and the limited energy budget in such systems can be addressed by Inertial Measurement Units (IMU) instead of common camera sensing. The central trade-off between accuracy and efficient use of hardware resources is rarely discussed in research. We address this trade-off by a simulative Design Space Exploration (DSE) of a varying quantity and positioning of IMU-sensors. First, we generate IMU-data from a publicly available body model dataset for different sensor configurations and train a deep learning model with this data. Additionally, we propose a combined metric to assess the accuracy-resource trade-off. We used the DSE as a tool to evaluate sensor configurations and identify beneficial ones for a specific use case. Exemplary, for a system with equal importance of accuracy and resources, we identify an optimal sensor configuration of 4 sensors with a mesh error of 6.03 cm, increasing the accuracy by 32.7% and reducing the hardware effort by two sensors compared to state of the art. Our work can be used to design health applications with well-suited sensor positioning and attention to data privacy and resource-awareness.
<div id='section'>Paperid: <span id='pid'>1802, <a href='https://arxiv.org/pdf/2306.14941.pdf' target='_blank'>https://arxiv.org/pdf/2306.14941.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vidyaa Krishnan Nivash, Ahmed H. Qureshi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.14941">SIMMF: Semantics-aware Interactive Multiagent Motion Forecasting for Autonomous Vehicle Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous vehicles require motion forecasting of their surrounding multiagents (pedestrians and vehicles) to make optimal decisions for navigation. The existing methods focus on techniques to utilize the positions and velocities of these agents and fail to capture semantic information from the scene. Moreover, to mitigate the increase in computational complexity associated with the number of agents in the scene, some works leverage Euclidean distance to prune far-away agents. However, distance-based metric alone is insufficient to select relevant agents and accurately perform their predictions. To resolve these issues, we propose the Semantics-aware Interactive Multiagent Motion Forecasting (SIMMF) method to capture semantics along with spatial information and optimally select relevant agents for motion prediction. Specifically, we achieve this by implementing a semantic-aware selection of relevant agents from the scene and passing them through an attention mechanism to extract global encodings. These encodings along with agents' local information, are passed through an encoder to obtain time-dependent latent variables for a motion policy predicting the future trajectories. Our results show that the proposed approach outperforms state-of-the-art baselines and provides more accurate and scene-consistent predictions.
<div id='section'>Paperid: <span id='pid'>1803, <a href='https://arxiv.org/pdf/2306.11868.pdf' target='_blank'>https://arxiv.org/pdf/2306.11868.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Wang, Tiebiao Zhao, Fan Yi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.11868">Multiverse Transformer: 1st Place Solution for Waymo Open Sim Agents Challenge 2023</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This technical report presents our 1st place solution for the Waymo Open Sim Agents Challenge (WOSAC) 2023. Our proposed MultiVerse Transformer for Agent simulation (MVTA) effectively leverages transformer-based motion prediction approaches, and is tailored for closed-loop simulation of agents. In order to produce simulations with a high degree of realism, we design novel training and sampling methods, and implement a receding horizon prediction mechanism. In addition, we introduce a variable-length history aggregation method to mitigate the compounding error that can arise during closed-loop autoregressive execution. On the WOSAC, our MVTA and its enhanced version MVTE reach a realism meta-metric of 0.5091 and 0.5168, respectively, outperforming all the other methods on the leaderboard.
<div id='section'>Paperid: <span id='pid'>1804, <a href='https://arxiv.org/pdf/2306.08861.pdf' target='_blank'>https://arxiv.org/pdf/2306.08861.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Makito Kobayashi, Chen-Chieh Liao, Keito Inoue, Sentaro Yojima, Masafumi Takahashi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.08861">Motion Capture Dataset for Practical Use of AI-based Motion Editing and Stylization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we proposed a new style-diverse dataset for the domain of motion style transfer. The motion dataset uses an industrial-standard human bone structure and thus is industry-ready to be plugged into 3D characters for many projects. We claim the challenges in motion style transfer and encourage future work in this domain by releasing the proposed motion dataset both to the public and the market. We conduct a comprehensive study on motion style transfer in the experiment using the state-of-the-art method, and the results show the proposed dataset's validity for the motion style transfer task.
<div id='section'>Paperid: <span id='pid'>1805, <a href='https://arxiv.org/pdf/2306.08164.pdf' target='_blank'>https://arxiv.org/pdf/2306.08164.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pierre Puchaud, Benjamin Michaud, MickaÃ«l Begon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.08164">The interplay of fatigue dynamics and task achievement using optimal control predictive simulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Predictive simulation of human motion could provide insight into optimal techniques. In repetitive or long-duration tasks, these simulations must predict fatigue-induced adaptation. However, most studies minimize cost function terms related to actuator activations, assuming it minimizes fatigue. An additional modeling layer is needed to consider the previous use of muscles to reveal adaptive strategies to the decreased force production capability. Here, we propose interfacing Xia's three-compartment fatigue dynamics model with rigid-body dynamics. A stabilization invariant was added to Xia's model. We simulated the maximum repetition of dumbbell biceps curls as an optimal control problem (OCP) using direct multiple shooting. We explored three cost functions (minimizing minimum torque, fatigue, or both) and two OCP formulations (full-horizon and sliding-horizon approaches). We found that Xia's model modified with the stabilization invariant (10 or 5) was adapted to direct multiple shooting. Sliding-horizon OCPs achieved 20 to 21 repetitions. The kinematic strategy slowly deviated from a plausible dumbbell lifting task to a swinging strategy as fatigue onset increasingly compromised the ability to keep the arm vertical. In full-horizon OCPs, the latter kinematic strategy was used over the whole motion, resulting in 32 repetitions. We showed that sliding-horizon OCPs revealed a reactive strategy to fatigue when only torque was included in the cost function, whereas an anticipatory strategy was revealed when the fatigue term was included in the cost function. Overall, the proposed approach has the potential to be a valuable tool in optimizing performance and helping reduce fatigue-related injuries in a variety of fields.
<div id='section'>Paperid: <span id='pid'>1806, <a href='https://arxiv.org/pdf/2306.08063.pdf' target='_blank'>https://arxiv.org/pdf/2306.08063.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gaurav Bhardwaj, Soham Dasgupta, N. Sukavanam, R. Balasubramanian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.08063">Soft Soil Gait Planning and Control for Biped Robot using Deep Deterministic Policy Gradient Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Biped robots have plenty of benefits over wheeled, quadruped, or hexapod robots due to their ability to behave like human beings in tough and non-flat environments. Deformable terrain is another challenge for biped robots as it has to deal with sinkage and maintain stability without falling. In this study, we are proposing a Deep Deterministic Policy Gradient (DDPG) approach for motion control of a flat-foot biped robot walking on deformable terrain. We have considered a 7-link biped robot for our proposed approach. For soft soil terrain modeling, we have considered triangular Mesh to describe its geometry, where mesh parameters determine the softness of soil. All simulations have been performed on PyChrono, which can handle soft soil environments.
<div id='section'>Paperid: <span id='pid'>1807, <a href='https://arxiv.org/pdf/2306.02309.pdf' target='_blank'>https://arxiv.org/pdf/2306.02309.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>X. Jin, Daniel W. C. Ho, Y. Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.02309">Synchronization of multiple rigid body systems: a survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The multi-agent system has been a hot topic in the past few decades owing to its lower cost, higher robustness, and higher flexibility. As a particular multi-agent system, the multiple rigid body system received a growing interest for its wide applications in transportation, aerospace, and ocean exploration. Due to the non-Euclidean configuration space of attitudes and the inherent nonlinearity of the dynamics of rigid body systems, synchronization of multiple rigid body systems is quite challenging. This paper aims to present an overview of the recent progress in synchronization of multiple rigid body systems from the view of two fundamental problems. The first problem focuses on attitude synchronization, while the second one focuses on cooperative motion control in that rotation and translation dynamics are coupled. Finally, a summary and future directions are given in the conclusion.
<div id='section'>Paperid: <span id='pid'>1808, <a href='https://arxiv.org/pdf/2305.18897.pdf' target='_blank'>https://arxiv.org/pdf/2305.18897.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lucas Mourot, Ludovic Hoyet, FranÃ§ois Le Clerc, Pierre Hellier
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.18897">HuMoT: Human Motion Representation using Topology-Agnostic Transformers for Character Animation Retargeting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion retargeting is the long-standing problem in character animation that consists in transferring and adapting the motion of a source character to another target character. A typical application is the creation of motion sequences from off-the-shelf motions by transferring them onto new characters. Motion retargeting is also promising to increase interoperability of existing animation systems and motion databases, as they often differ in the structure of the skeleton(s) considered. Moreover, since the goal of motion retargeting is to abstract and transfer motion dynamics, effective solutions might provide expressive and powerful human motion models in which operations such as cleaning or editing are easier. In this article, we present a novel neural network architecture for retargeting that extracts an abstract representation of human motion agnostic to skeleton topology and morphology. Based on transformers, our model is able to encode and decode motion sequences with variable morphology and topology -- extending the current scope of retargeting -- while supporting skeleton topologies not seen during the training phase. More specifically, our model is structured as an autoencoder, and encoding and decoding are separately conditioned on skeleton templates to extract and control morphology and topology. Beyond motion retargeting, our model has many applications since our abstract representation is a convenient space to embed motion data from different sources. It may potentially be benefical to a number of data-driven methods, allowing them to combine scarce specialised motion datasets (e.g. with style or contact annotations) and larger general motion datasets, for improved performance and generalisation ability. Moreover, we show that our model can be useful for other applications beyond retargeting, including motion denoising and joint upsampling.
<div id='section'>Paperid: <span id='pid'>1809, <a href='https://arxiv.org/pdf/2305.18743.pdf' target='_blank'>https://arxiv.org/pdf/2305.18743.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenshuo Chen, Xiang Zhou, Zhengdi Yu, Weixi Gu, Kai Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.18743">Decomposed Human Motion Prior for Video Pose Estimation via Adversarial Training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Estimating human pose from video is a task that receives considerable attention due to its applicability in numerous 3D fields. The complexity of prior knowledge of human body movements poses a challenge to neural network models in the task of regressing keypoints. In this paper, we address this problem by incorporating motion prior in an adversarial way. Different from previous methods, we propose to decompose holistic motion prior to joint motion prior, making it easier for neural networks to learn from prior knowledge thereby boosting the performance on the task. We also utilize a novel regularization loss to balance accuracy and smoothness introduced by motion prior. Our method achieves 9\% lower PA-MPJPE and 29\% lower acceleration error than previous methods tested on 3DPW. The estimator proves its robustness by achieving impressive performance on in-the-wild dataset.
<div id='section'>Paperid: <span id='pid'>1810, <a href='https://arxiv.org/pdf/2305.08247.pdf' target='_blank'>https://arxiv.org/pdf/2305.08247.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaowen Tao, Pengxiang Meng, Bing Zhu, Jian Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.08247">A Fast and Robust Camera-IMU Online Calibration Method For Localization System</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous driving has spurred the development of sensor fusion techniques, which combine data from multiple sensors to improve system performance. In particular, localization system based on sensor fusion , such as Visual Simultaneous Localization and Mapping (VSLAM), is an important component in environment perception, and is the basis of decision-making and motion control for intelligent vehicles. The accuracy of extrinsic calibration parameters between camera and IMU has significant effect on the positioning precision when performing VSLAM system. Currently, existing methods are time-consuming using complex optimization methods and sensitive to noise and outliers due to off-calibration, which can negatively impact system performance. To address these problems, this paper presents a fast and robust camera-IMU online calibration method based space coordinate transformation constraints and SVD (singular Value Decomposition) tricks. First, constraint equations are constructed based on equality of rotation and transformation matrices between camera frames and IMU coordinates at different moments. Secondly, the external parameters of the camera-IMU are solved using quaternion transformation and SVD techniques. Finally, the proposed method is validated using ROS platform, where images from the camera and velocity, acceleration, and angular velocity data from the IMU are recorded in a ROS bag file. The results showed that the proposed method can achieve robust and reliable camera-IMU online calibration parameters results with less tune consuming and less uncertainty.
<div id='section'>Paperid: <span id='pid'>1811, <a href='https://arxiv.org/pdf/2305.00038.pdf' target='_blank'>https://arxiv.org/pdf/2305.00038.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Elizabeth M. Jacobs, Fani Deligianni., Frank Pollick
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.00038">Threat Perception Modulation by Capturing Emotion, Motor and Empathetic System Responses: A Systematic Review</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The fight or flight phenomena is of evolutionary origin and responsible for the type of defensive behaviours enacted, when in the face of threat. This review attempts to draw the link between fear and aggression as behavioural motivations for fight or flight defensive behaviours. Hence, this review intends to examine whether fight or flight behavioural responses are the result of fear and aggression. Furthermore, this review investigates whether human biological motion captures the affective states associated with the fight or flight phenomenon. This review also aims to investigate how threat informed emotion and motor systems have the potential to result in empathetic appraisal modulation. This is of interest to this systematic review, as empathetic modulation is crucial to prosocial drive, which has the potential to increase the inclination of alleviating the perceived threat of another. Hence, this review investigates the role of affective computing in capturing the potential outcome of empathy from threat perception. To gain a comprehensive understanding of the affective states and biological motion evoked from threat scenarios, affective computing methods used to capture these behavioural responses are discussed.
  A systematic review using Google Scholar and Web of Science was conducted as of 2023, and findings were supplemented by bibliographies of key articles. A total of 22 studies were analysed from initial web searches to explore the topics of empathy, threat perception, fight or flight, fear, aggression, and human motion. Relationships between affective states (fear, aggression) and corresponding motor defensive behaviours (fight or flight) were examined within threat scenarios, and whether existing affective computing methods are succinct in capturing these responses, identifying the varying consensus in the literature, challenges, and limitations of existing research.
<div id='section'>Paperid: <span id='pid'>1812, <a href='https://arxiv.org/pdf/2304.08379.pdf' target='_blank'>https://arxiv.org/pdf/2304.08379.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>AntÃ³nio Amorim, Diana GuimarÃ£es, Tiago MendonÃ§a, Pedro Neto, Paulo Costa, AntÃ³nio Paulo Moreira
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.08379">Robust human position estimation in cooperative robotic cells</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robots are increasingly present in our lives, sharing the workspace and tasks with human co-workers. However, existing interfaces for human-robot interaction / cooperation (HRI/C) have limited levels of intuitiveness to use and safety is a major concern when humans and robots share the same workspace. Many times, this is due to the lack of a reliable estimation of the human pose in space which is the primary input to calculate the human-robot minimum distance (required for safety and collision avoidance) and HRI/C featuring machine learning algorithms classifying human behaviours / gestures. Each sensor type has its own characteristics resulting in problems such as occlusions (vision) and drift (inertial) when used in an isolated fashion. In this paper, it is proposed a combined system that merges the human tracking provided by a 3D vision sensor with the pose estimation provided by a set of inertial measurement units (IMUs) placed in human body limbs. The IMUs compensate the gaps in occluded areas to have tracking continuity. To mitigate the lingering effects of the IMU offset we propose a continuous online calculation of the offset value. Experimental tests were designed to simulate human motion in a human-robot collaborative environment where the robot moves away to avoid unexpected collisions with de human. Results indicate that our approach is able to capture the human\textsc's position, for example the forearm, with a precision in the millimetre range and robustness to occlusions.
<div id='section'>Paperid: <span id='pid'>1813, <a href='https://arxiv.org/pdf/2304.06121.pdf' target='_blank'>https://arxiv.org/pdf/2304.06121.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Abduallah Mohamed, Jundi Liu, Linda Ng Boyle, Christian Claudel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.06121">FollowMe: Vehicle Behaviour Prediction in Autonomous Vehicle Settings</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>An ego vehicle following a virtual lead vehicle planned route is an essential component when autonomous and non-autonomous vehicles interact. Yet, there is a question about the driver's ability to follow the planned lead vehicle route. Thus, predicting the trajectory of the ego vehicle route given a lead vehicle route is of interest. We introduce a new dataset, the FollowMe dataset, which offers a motion and behavior prediction problem by answering the latter question of the driver's ability to follow a lead vehicle. We also introduce a deep spatio-temporal graph model FollowMe-STGCNN as a baseline for the dataset. In our experiments and analysis, we show the design benefits of FollowMe-STGCNN in capturing the interactions that lie within the dataset. We contrast the performance of FollowMe-STGCNN with prior motion prediction models showing the need to have a different design mechanism to address the lead vehicle following settings.
<div id='section'>Paperid: <span id='pid'>1814, <a href='https://arxiv.org/pdf/2304.04956.pdf' target='_blank'>https://arxiv.org/pdf/2304.04956.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongwei Ren, Yuhong Shi, Kewei Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.04956">Multi-Graph Convolution Network for Pose Forecasting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, there has been a growing interest in predicting human motion, which involves forecasting future body poses based on observed pose sequences. This task is complex due to modeling spatial and temporal relationships. The most commonly used models for this task are autoregressive models, such as recurrent neural networks (RNNs) or variants, and Transformer Networks. However, RNNs have several drawbacks, such as vanishing or exploding gradients. Other researchers have attempted to solve the communication problem in the spatial dimension by integrating Graph Convolutional Networks (GCN) and Long Short-Term Memory (LSTM) models. These works deal with temporal and spatial information separately, which limits the effectiveness. To fix this problem, we propose a novel approach called the multi-graph convolution network (MGCN) for 3D human pose forecasting. This model simultaneously captures spatial and temporal information by introducing an augmented graph for pose sequences. Multiple frames give multiple parts, joined together in a single graph instance. Furthermore, we also explore the influence of natural structure and sequence-aware attention to our model. In our experimental evaluation of the large-scale benchmark datasets, Human3.6M, AMSS and 3DPW, MGCN outperforms the state-of-the-art in pose prediction.
<div id='section'>Paperid: <span id='pid'>1815, <a href='https://arxiv.org/pdf/2304.04879.pdf' target='_blank'>https://arxiv.org/pdf/2304.04879.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jing Qin, Biyun Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.04879">Human Motion Detection Based on Dual-Graph and Weighted Nuclear Norm Regularizations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion detection has been widely used in many applications, such as surveillance and robotics. Due to the presence of the static background, a motion video can be decomposed into a low-rank background and a sparse foreground. Many regularization techniques that preserve low-rankness of matrices can therefore be imposed on the background. In the meanwhile, geometry-based regularizations, such as graph regularizations, can be imposed on the foreground. Recently, weighted regularization techniques including the weighted nuclear norm regularization have been proposed in the image processing community to promote adaptive sparsity while achieving efficient performance. In this paper, we propose a robust dual graph regularized moving object detection model based on a novel weighted nuclear norm regularization and spatiotemporal graph Laplacians. Numerical experiments on realistic human motion data sets have demonstrated the effectiveness and robustness of this approach in separating moving objects from background, and the enormous potential in robotic applications.
<div id='section'>Paperid: <span id='pid'>1816, <a href='https://arxiv.org/pdf/2304.02061.pdf' target='_blank'>https://arxiv.org/pdf/2304.02061.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aymen Mir, Xavier Puig, Angjoo Kanazawa, Gerard Pons-Moll
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.02061">Generating Continual Human Motion in Diverse 3D Scenes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a method to synthesize animator guided human motion across 3D scenes. Given a set of sparse (3 or 4) joint locations (such as the location of a person's hand and two feet) and a seed motion sequence in a 3D scene, our method generates a plausible motion sequence starting from the seed motion while satisfying the constraints imposed by the provided keypoints. We decompose the continual motion synthesis problem into walking along paths and transitioning in and out of the actions specified by the keypoints, which enables long generation of motions that satisfy scene constraints without explicitly incorporating scene information. Our method is trained only using scene agnostic mocap data. As a result, our approach is deployable across 3D scenes with various geometries. For achieving plausible continual motion synthesis without drift, our key contribution is to generate motion in a goal-centric canonical coordinate frame where the next immediate target is situated at the origin. Our model can generate long sequences of diverse actions such as grabbing, sitting and leaning chained together in arbitrary order, demonstrated on scenes of varying geometry: HPS, Replica, Matterport, ScanNet and scenes represented using NeRFs. Several experiments demonstrate that our method outperforms existing methods that navigate paths in 3D scenes. For more results we urge the reader to watch our supplementary video available at: https://www.youtube.com/watch?v=0wZgsdyCT4A&t=1s
<div id='section'>Paperid: <span id='pid'>1817, <a href='https://arxiv.org/pdf/2303.16468.pdf' target='_blank'>https://arxiv.org/pdf/2303.16468.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sahan Wijethunga, Shehan Kaushalya Senavirathna, Kavishka Dissanayake, Janith Bandara Senanayake, Eranda Somathilake, Upekha Hansanie Delay, Roshan Indika Godaliyadda, Mervyn Parakrama Ekanayake, Janaka Wijayakulasooriya
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.16468">IMU-based Modularized Wearable Device for Human Motion Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion analysis is used in many different fields and applications. Currently, existing systems either focus on one single limb or one single class of movements. Many proposed systems are designed to be used in an indoor controlled environment and must possess good technical know-how to operate. To improve mobility, a less restrictive, modularized, and simple Inertial Measurement units based system is proposed that can be worn separately and combined. This allows the user to measure singular limb movements separately and also monitor whole body movements over a prolonged period at any given time while not restricted to a controlled environment. For proper analysis, data is conditioned and pre-processed through possible five stages namely power-based, clustering index-based, Kalman filtering, distance-measure-based, and PCA-based dimension reduction. Different combinations of the above stages are analyzed using machine learning algorithms for selected case studies namely hand gesture recognition and environment and shoe parameter-based walking pattern analysis to validate the performance capability of the proposed wearable device and multi-stage algorithms. The results of the case studies show that distance-measure-based and PCA-based dimension reduction will significantly improve human motion identification accuracy. This is further improved with the introduction of the Kalman filter. An LSTM neural network is proposed as an alternate classifier and the results indicate that it is a robust classifier for human motion recognition. As the results indicate, the proposed wearable device architecture and multi-stage algorithms are cable of distinguishing between subtle human limb movements making it a viable tool for human motion analysis.
<div id='section'>Paperid: <span id='pid'>1818, <a href='https://arxiv.org/pdf/2303.14255.pdf' target='_blank'>https://arxiv.org/pdf/2303.14255.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>James Mullen, Dinesh Manocha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.14255">PACE: Data-Driven Virtual Agent Interaction in Dense and Cluttered Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present PACE, a novel method for modifying motion-captured virtual agents to interact with and move throughout dense, cluttered 3D scenes. Our approach changes a given motion sequence of a virtual agent as needed to adjust to the obstacles and objects in the environment. We first take the individual frames of the motion sequence most important for modeling interactions with the scene and pair them with the relevant scene geometry, obstacles, and semantics such that interactions in the agents motion match the affordances of the scene (e.g., standing on a floor or sitting in a chair). We then optimize the motion of the human by directly altering the high-DOF pose at each frame in the motion to better account for the unique geometric constraints of the scene. Our formulation uses novel loss functions that maintain a realistic flow and natural-looking motion. We compare our method with prior motion generating techniques and highlight the benefits of our method with a perceptual study and physical plausibility metrics. Human raters preferred our method over the prior approaches. Specifically, they preferred our method 57.1% of the time versus the state-of-the-art method using existing motions, and 81.0% of the time versus a state-of-the-art motion synthesis method. Additionally, our method performs significantly higher on established physical plausibility and interaction metrics. Specifically, we outperform competing methods by over 1.2% in terms of the non-collision metric and by over 18% in terms of the contact metric. We have integrated our interactive system with Microsoft HoloLens and demonstrate its benefits in real-world indoor scenes. Our project website is available at https://gamma.umd.edu/pace/.
<div id='section'>Paperid: <span id='pid'>1819, <a href='https://arxiv.org/pdf/2303.12071.pdf' target='_blank'>https://arxiv.org/pdf/2303.12071.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xishun Wang, Tong Su, Fang Da, Xiaodong Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.12071">ProphNet: Efficient Agent-Centric Motion Forecasting with Anchor-Informed Proposals</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion forecasting is a key module in an autonomous driving system. Due to the heterogeneous nature of multi-sourced input, multimodality in agent behavior, and low latency required by onboard deployment, this task is notoriously challenging. To cope with these difficulties, this paper proposes a novel agent-centric model with anchor-informed proposals for efficient multimodal motion prediction. We design a modality-agnostic strategy to concisely encode the complex input in a unified manner. We generate diverse proposals, fused with anchors bearing goal-oriented scene context, to induce multimodal prediction that covers a wide range of future trajectories. Our network architecture is highly uniform and succinct, leading to an efficient model amenable for real-world driving deployment. Experiments reveal that our agent-centric network compares favorably with the state-of-the-art methods in prediction accuracy, while achieving scene-centric level inference latency.
<div id='section'>Paperid: <span id='pid'>1820, <a href='https://arxiv.org/pdf/2303.08444.pdf' target='_blank'>https://arxiv.org/pdf/2303.08444.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Huilan Luo, Zehua Zeng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.08444">Real-time Multi-Object Tracking Based on Bi-directional Matching</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, anchor-free object detection models combined with matching algorithms are used to achieve real-time muti-object tracking and also ensure high tracking accuracy. However, there are still great challenges in multi-object tracking. For example, when most part of a target is occluded or the target just disappears from images temporarily, it often leads to tracking interruptions for most of the existing tracking algorithms. Therefore, this study offers a bi-directional matching algorithm for multi-object tracking that makes advantage of bi-directional motion prediction information to improve occlusion handling. A stranded area is used in the matching algorithm to temporarily store the objects that fail to be tracked. When objects recover from occlusions, our method will first try to match them with objects in the stranded area to avoid erroneously generating new identities, thus forming a more continuous trajectory. Experiments show that our approach can improve the multi-object tracking performance in the presence of occlusions. In addition, this study provides an attentional up-sampling module that not only assures tracking accuracy but also accelerates training speed. In the MOT17 challenge, the proposed algorithm achieves 63.4% MOTA, 55.3% IDF1, and 20.1 FPS tracking speed.
<div id='section'>Paperid: <span id='pid'>1821, <a href='https://arxiv.org/pdf/2302.01161.pdf' target='_blank'>https://arxiv.org/pdf/2302.01161.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Max Winkelmann, Constantin Vasconi, Steffen MÃ¼ller
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.01161">Vectorized Scenario Description and Motion Prediction for Scenario-Based Testing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automated vehicles (AVs) are tested in diverse scenarios, typically specified by parameters such as velocities, distances, or curve radii. To describe scenarios uniformly independent of such parameters, this paper proposes a vectorized scenario description defined by the road geometry and vehicles' trajectories. Data of this form are generated for three scenarios, merged, and used to train the motion prediction model VectorNet, allowing to predict an AV's trajectory for unseen scenarios. Predicting scenario evaluation metrics, VectorNet partially achieves lower errors than regression models that separately process the three scenarios' data. However, for comprehensive generalization, sufficient variance in the training data must be ensured. Thus, contrary to existing methods, our proposed method can merge diverse scenarios' data and exploit spatial and temporal nuances in the vectorized scenario description. As a result, data from specified test scenarios and real-world scenarios can be compared and combined for (predictive) analyses and scenario selection.
<div id='section'>Paperid: <span id='pid'>1822, <a href='https://arxiv.org/pdf/2212.11771.pdf' target='_blank'>https://arxiv.org/pdf/2212.11771.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rafael Rego Drumond, Lukas Brinkmeyer, Lars Schmidt-Thieme
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.11771">Few-shot human motion prediction for heterogeneous sensors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion prediction is a complex task as it involves forecasting variables over time on a graph of connected sensors. This is especially true in the case of few-shot learning, where we strive to forecast motion sequences for previously unseen actions based on only a few examples. Despite this, almost all related approaches for few-shot motion prediction do not incorporate the underlying graph, while it is a common component in classical motion prediction. Furthermore, state-of-the-art methods for few-shot motion prediction are restricted to motion tasks with a fixed output space meaning these tasks are all limited to the same sensor graph. In this work, we propose to extend recent works on few-shot time-series forecasting with heterogeneous attributes with graph neural networks to introduce the first few-shot motion approach that explicitly incorporates the spatial graph while also generalizing across motion tasks with heterogeneous sensors. In our experiments on motion tasks with heterogeneous sensors, we demonstrate significant performance improvements with lifts from 10.4% up to 39.3% compared to best state-of-the-art models. Moreover, we show that our model can perform on par with the best approach so far when evaluating on tasks with a fixed output space while maintaining two magnitudes fewer parameters.
<div id='section'>Paperid: <span id='pid'>1823, <a href='https://arxiv.org/pdf/2211.16197.pdf' target='_blank'>https://arxiv.org/pdf/2211.16197.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Luke Rowe, Martin Ethier, Eli-Henry Dykhne, Krzysztof Czarnecki
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.16197">FJMP: Factorized Joint Multi-Agent Motion Prediction over Learned Directed Acyclic Interaction Graphs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Predicting the future motion of road agents is a critical task in an autonomous driving pipeline. In this work, we address the problem of generating a set of scene-level, or joint, future trajectory predictions in multi-agent driving scenarios. To this end, we propose FJMP, a Factorized Joint Motion Prediction framework for multi-agent interactive driving scenarios. FJMP models the future scene interaction dynamics as a sparse directed interaction graph, where edges denote explicit interactions between agents. We then prune the graph into a directed acyclic graph (DAG) and decompose the joint prediction task into a sequence of marginal and conditional predictions according to the partial ordering of the DAG, where joint future trajectories are decoded using a directed acyclic graph neural network (DAGNN). We conduct experiments on the INTERACTION and Argoverse 2 datasets and demonstrate that FJMP produces more accurate and scene-consistent joint trajectory predictions than non-factorized approaches, especially on the most interactive and kinematically interesting agents. FJMP ranks 1st on the multi-agent test leaderboard of the INTERACTION dataset.
<div id='section'>Paperid: <span id='pid'>1824, <a href='https://arxiv.org/pdf/2210.04366.pdf' target='_blank'>https://arxiv.org/pdf/2210.04366.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Patrick Perrine, Trevor Kirkby
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.04366">KP-RNN: A Deep Learning Pipeline for Human Motion Prediction and Synthesis of Performance Art</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Digitally synthesizing human motion is an inherently complex process, which can create obstacles in application areas such as virtual reality. We offer a new approach for predicting human motion, KP-RNN, a neural network which can integrate easily with existing image processing and generation pipelines. We utilize a new human motion dataset of performance art, Take The Lead, as well as the motion generation pipeline, the Everybody Dance Now system, to demonstrate the effectiveness of KP-RNN's motion predictions. We have found that our neural network can predict human dance movements effectively, which serves as a baseline result for future works using the Take The Lead dataset. Since KP-RNN can work alongside a system such as Everybody Dance Now, we argue that our approach could inspire new methods for rendering human avatar animation. This work also serves to benefit the visualization of performance art in digital platforms by utilizing accessible neural networks.
<div id='section'>Paperid: <span id='pid'>1825, <a href='https://arxiv.org/pdf/2209.00349.pdf' target='_blank'>https://arxiv.org/pdf/2209.00349.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jihoon Kim, Jiseob Kim, Sungjoon Choi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.00349">FLAME: Free-form Language-based Motion Synthesis & Editing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-based motion generation models are drawing a surge of interest for their potential for automating the motion-making process in the game, animation, or robot industries. In this paper, we propose a diffusion-based motion synthesis and editing model named FLAME. Inspired by the recent successes in diffusion models, we integrate diffusion-based generative models into the motion domain. FLAME can generate high-fidelity motions well aligned with the given text. Also, it can edit the parts of the motion, both frame-wise and joint-wise, without any fine-tuning. FLAME involves a new transformer-based architecture we devise to better handle motion data, which is found to be crucial to manage variable-length motions and well attend to free-form text. In experiments, we show that FLAME achieves state-of-the-art generation performances on three text-motion datasets: HumanML3D, BABEL, and KIT. We also demonstrate that editing capability of FLAME can be extended to other tasks such as motion prediction or motion in-betweening, which have been previously covered by dedicated models.
<div id='section'>Paperid: <span id='pid'>1826, <a href='https://arxiv.org/pdf/2203.02778.pdf' target='_blank'>https://arxiv.org/pdf/2203.02778.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alexander Fabisch, Manuela Uliano, Dennis Marschner, Melvin Laux, Johannes Brust, Marco Controzzi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2203.02778">A Modular Approach to the Embodiment of Hand Motions from Human Demonstrations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Manipulating objects with robotic hands is a complicated task. Not only the fingers of the hand, but also the pose of the robot's end effector need to be coordinated. Using human demonstrations of movements is an intuitive and data-efficient way of guiding the robot's behavior. We propose a modular framework with an automatic embodiment mapping to transfer recorded human hand motions to robotic systems. In this work, we use motion capture to record human motion. We evaluate our approach on eight challenging tasks, in which a robotic hand needs to grasp and manipulate either deformable or small and fragile objects. We test a subset of trajectories in simulation and on a real robot and the overall success rates are aligned.
<div id='section'>Paperid: <span id='pid'>1827, <a href='https://arxiv.org/pdf/2105.06501.pdf' target='_blank'>https://arxiv.org/pdf/2105.06501.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Thiago B. Burghi, Juliano G. Iossaqui, Juan F. Camino
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2105.06501">Kinematic control design for wheeled mobile robots with longitudinal and lateral slip</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The motion control of wheeled mobile robots at high speeds under adverse ground conditions is a difficult task, since the robots' wheels may be subject to different kinds of slip. This work introduces an adaptive kinematic controller that is capable of solving the trajectory tracking problem of a nonholonomic mobile robot under longitudinal and lateral slip. While the controller can effectively compensate for the longitudinal slip, the lateral slip is a more involved problem to deal with, since nonholonomic robots cannot directly produce movement in the lateral direction. To show that the proposed controller is still able to make the mobile robot follow a reference trajectory under lateral and longitudinal time-varying slip, the solutions of the robot's position and orientation error dynamics are shown to be uniformly ultimately bounded. Numerical simulations are presented to illustrate the robot's performance using the proposed adaptive control law.
<div id='section'>Paperid: <span id='pid'>1828, <a href='https://arxiv.org/pdf/2103.00583.pdf' target='_blank'>https://arxiv.org/pdf/2103.00583.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nigora Gafur, Gajanan Kanagalingam, Martin Ruskowski
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2103.00583">Dynamic collision avoidance for multiple robotic manipulators based on a non-cooperative multi-agent game</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A flexible operation of multiple robotic manipulators in a shared workspace requires an online trajectory planning with static and dynamic collision avoidance. In this work, we propose a real-time capable motion control algorithm, based on non-linear model predictive control, which accounts for static and dynamic collision avoidance. The proposed algorithm is formulated as a non-cooperative game, where each robot is considered as an agent. Each agent optimizes its own motion and accounts for the predicted movement of surrounding agents. We propose a novel approach to formulate the dynamic collision constraints. Additionally, we account for deadlocks that might occur in a setup of multiple robotic manipulators. We validate our algorithm on a pick and place scenario for four collaborative robots operating in a common workspace in the simulation environment Gazebo. The robots are controlled by the Robot Operating System (ROS). We demonstrate, that our approach is real-time capable and, due to the distributed nature of the approach, easily scales to an arbitrary number of robot manipulators in a shared workspace.
<div id='section'>Paperid: <span id='pid'>1829, <a href='https://arxiv.org/pdf/2012.04514.pdf' target='_blank'>https://arxiv.org/pdf/2012.04514.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Radu Horaud, Matti Niskanen, Guillaume Dewaele, Edmond Boyer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2012.04514">Human Motion Tracking by Registering an Articulated Surface to 3-D Points and Normals</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We address the problem of human motion tracking by registering a surface to 3-D data. We propose a method that iteratively computes two things: Maximum likelihood estimates for both the kinematic and free-motion parameters of a kinematic human-body representation, as well as probabilities that the data are assigned either to a body part, or to an outlier cluster. We introduce a new metric between observed points and normals on one side, and a parameterized surface on the other side, the latter being defined as a blending over a set of ellipsoids. We claim that this metric is well suited when one deals with either visual-hull or visual-shape observations. We illustrate the method by tracking human motions using sparse visual-shape data (3-D surface points and normals) gathered from imperfect silhouettes.
<div id='section'>Paperid: <span id='pid'>1830, <a href='https://arxiv.org/pdf/2006.06119.pdf' target='_blank'>https://arxiv.org/pdf/2006.06119.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruozi Huang, Huang Hu, Wei Wu, Kei Sawada, Mi Zhang, Daxin Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2006.06119">Dance Revolution: Long-Term Dance Generation with Music via Curriculum Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dancing to music is one of human's innate abilities since ancient times. In machine learning research, however, synthesizing dance movements from music is a challenging problem. Recently, researchers synthesize human motion sequences through autoregressive models like recurrent neural network (RNN). Such an approach often generates short sequences due to an accumulation of prediction errors that are fed back into the neural network. This problem becomes even more severe in the long motion sequence generation. Besides, the consistency between dance and music in terms of style, rhythm and beat is yet to be taken into account during modeling. In this paper, we formalize the music-conditioned dance generation as a sequence-to-sequence learning problem and devise a novel seq2seq architecture to efficiently process long sequences of music features and capture the fine-grained correspondence between music and dance. Furthermore, we propose a novel curriculum learning strategy to alleviate error accumulation of autoregressive models in long motion sequence generation, which gently changes the training process from a fully guided teacher-forcing scheme using the previous ground-truth movements, towards a less guided autoregressive scheme mostly using the generated movements instead. Extensive experiments show that our approach significantly outperforms the existing state-of-the-arts on automatic metrics and human evaluation. We also make a demo video to demonstrate the superior performance of our proposed approach at https://www.youtube.com/watch?v=lmE20MEheZ8.
<div id='section'>Paperid: <span id='pid'>1831, <a href='https://arxiv.org/pdf/2507.16850.pdf' target='_blank'>https://arxiv.org/pdf/2507.16850.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohamed Adjel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.16850">Toward a Real-Time Framework for Accurate Monocular 3D Human Pose Estimation with Geometric Priors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Monocular 3D human pose estimation remains a challenging and ill-posed problem, particularly in real-time settings and unconstrained environments. While direct imageto-3D approaches require large annotated datasets and heavy models, 2D-to-3D lifting offers a more lightweight and flexible alternative-especially when enhanced with prior knowledge. In this work, we propose a framework that combines real-time 2D keypoint detection with geometry-aware 2D-to-3D lifting, explicitly leveraging known camera intrinsics and subject-specific anatomical priors. Our approach builds on recent advances in self-calibration and biomechanically-constrained inverse kinematics to generate large-scale, plausible 2D-3D training pairs from MoCap and synthetic datasets. We discuss how these ingredients can enable fast, personalized, and accurate 3D pose estimation from monocular images without requiring specialized hardware. This proposal aims to foster discussion on bridging data-driven learning and model-based priors to improve accuracy, interpretability, and deployability of 3D human motion capture on edge devices in the wild.
<div id='section'>Paperid: <span id='pid'>1832, <a href='https://arxiv.org/pdf/2506.14287.pdf' target='_blank'>https://arxiv.org/pdf/2506.14287.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanwei Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.14287">Steering Robots with Inference-Time Interactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Imitation learning has driven the development of generalist policies capable of autonomously solving multiple tasks. However, when a pretrained policy makes errors during deployment, there are limited mechanisms for users to correct its behavior. While collecting additional data for finetuning can address such issues, doing so for each downstream use case is inefficient at deployment. My research proposes an alternative: keeping pretrained policies frozen as a fixed skill repertoire while allowing user interactions to guide behavior generation toward user preferences at inference time. By making pretrained policies steerable, users can help correct policy errors when the model struggles to generalize-without needing to finetune the policy. Specifically, I propose (1) inference-time steering, which leverages user interactions to switch between discrete skills, and (2) task and motion imitation, which enables user interactions to edit continuous motions while satisfying task constraints defined by discrete symbolic plans. These frameworks correct misaligned policy predictions without requiring additional training, maximizing the utility of pretrained models while achieving inference-time user objectives.
<div id='section'>Paperid: <span id='pid'>1833, <a href='https://arxiv.org/pdf/2505.16535.pdf' target='_blank'>https://arxiv.org/pdf/2505.16535.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Asrar Alruwayqi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.16535">SHaDe: Compact and Consistent Dynamic 3D Reconstruction via Tri-Plane Deformation and Latent Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a novel framework for dynamic 3D scene reconstruction that integrates three key components: an explicit tri-plane deformation field, a view-conditioned canonical radiance field with spherical harmonics (SH) attention, and a temporally-aware latent diffusion prior. Our method encodes 4D scenes using three orthogonal 2D feature planes that evolve over time, enabling efficient and compact spatiotemporal representation. These features are explicitly warped into a canonical space via a deformation offset field, eliminating the need for MLP-based motion modeling.
  In canonical space, we replace traditional MLP decoders with a structured SH-based rendering head that synthesizes view-dependent color via attention over learned frequency bands improving both interpretability and rendering efficiency. To further enhance fidelity and temporal consistency, we introduce a transformer-guided latent diffusion module that refines the tri-plane and deformation features in a compressed latent space. This generative module denoises scene representations under ambiguous or out-of-distribution (OOD) motion, improving generalization.
  Our model is trained in two stages: the diffusion module is first pre-trained independently, and then fine-tuned jointly with the full pipeline using a combination of image reconstruction, diffusion denoising, and temporal consistency losses. We demonstrate state-of-the-art results on synthetic benchmarks, surpassing recent methods such as HexPlane and 4D Gaussian Splatting in visual quality, temporal coherence, and robustness to sparse-view dynamic inputs.
<div id='section'>Paperid: <span id='pid'>1834, <a href='https://arxiv.org/pdf/2505.12631.pdf' target='_blank'>https://arxiv.org/pdf/2505.12631.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Li Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.12631">Multi-Resolution Haar Network: Enhancing human motion prediction via Haar transform</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The 3D human pose is vital for modern computer vision and computer graphics, and its prediction has drawn attention in recent years. 3D human pose prediction aims at forecasting a human's future motion from the previous sequence. Ignoring that the arbitrariness of human motion sequences has a firm origin in transition in both temporal and spatial axes limits the performance of state-of-the-art methods, leading them to struggle with making precise predictions on complex cases, e.g., arbitrarily posing or greeting. To alleviate this problem, a network called HaarMoDic is proposed in this paper, which utilizes the 2D Haar transform to project joints to higher resolution coordinates where the network can access spatial and temporal information simultaneously. An ablation study proves that the significant contributing module within the HaarModic Network is the Multi-Resolution Haar (MR-Haar) block. Instead of mining in one of two axes or extracting separately, the MR-Haar block projects whole motion sequences to a mixed-up coordinate in higher resolution with 2D Haar Transform, allowing the network to give scope to information from both axes in different resolutions. With the MR-Haar block, the HaarMoDic network can make predictions referring to a broader range of information. Experimental results demonstrate that HaarMoDic surpasses state-of-the-art methods in every testing interval on the Human3.6M dataset in the Mean Per Joint Position Error (MPJPE) metric.
<div id='section'>Paperid: <span id='pid'>1835, <a href='https://arxiv.org/pdf/2504.21316.pdf' target='_blank'>https://arxiv.org/pdf/2504.21316.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Michael Ruderman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.21316">Reduced order asymptotic observer of friction in motion control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>An asymptotic observer of the motion state variables with nonlinear friction [1] benefits from a robust to the noise state-space representation of the dynamic friction force, including pre-sliding transitions, and implements the reduced order Luenberger observation law with only measurable output displacement. The uniform asymptotic stability and convergence analysis of the proposed observer are elaborated by using the Lyapunov function-based stability criteria by Ignatyev and imposing the parametric constraints on the time dependent eigenvalues to be always negative real. A design procedure for assigning a dominant (thus slowest) real pole of the observer system matrix is proposed. A thorough experimental evaluation is given for the proposed observer-based friction compensation, which is performed for positioning and tracking tasks and compared with an optimally tuned PID feedback control.
<div id='section'>Paperid: <span id='pid'>1836, <a href='https://arxiv.org/pdf/2503.18348.pdf' target='_blank'>https://arxiv.org/pdf/2503.18348.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ji-Hong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.18348">MCE-based Direct FTC Method for Dynamic Positioning of Underwater Vehicles with Thruster Redundancy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents an active model-based FTC (fault-tolerant control) method for the dynamic positioning of a class of underwater vehicles with thruster redundancy. Compared to the widely used state and parameter estimation methods, this proposed scheme directly utilizes the vehicle's motion control error (MCE) to construct a residual for detecting thruster faults and failures in the steady state of the control system. In the case of thruster fault identification, the most difficult aspect is that the actual control input with thruster faults is unknown. However, through a detailed and precise analyses of MCE variation trends in the case of thruster faults, highly useful information about this unknown control input can be extracted. This characteristic also serves as the foundation for the novel scheme proposed in this paper. As for control reconfiguration, it is straightforward since the thrust losses can be directly estimated as a result of the identification process. Numerical studies with the real world vehicle model are also carried out to demonstrate the effectiveness of the proposed method.
<div id='section'>Paperid: <span id='pid'>1837, <a href='https://arxiv.org/pdf/2503.04879.pdf' target='_blank'>https://arxiv.org/pdf/2503.04879.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sammy Christen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.04879">Modeling Dynamic Hand-Object Interactions with Applications to Human-Robot Handovers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humans frequently grasp, manipulate, and move objects. Interactive systems assist humans in these tasks, enabling applications in Embodied AI, human-robot interaction, and virtual reality. However, current methods in hand-object synthesis often neglect dynamics and focus on generating static grasps. The first part of this dissertation introduces dynamic grasp synthesis, where a hand grasps and moves an object to a target pose. We approach this task using physical simulation and reinforcement learning. We then extend this to bimanual manipulation and articulated objects, requiring fine-grained coordination between hands. In the second part of this dissertation, we study human-to-robot handovers. We integrate captured human motion into simulation and introduce a student-teacher framework that adapts to human behavior and transfers from sim to real. To overcome data scarcity, we generate synthetic interactions, increasing training diversity by 100x. Our user study finds no difference between policies trained on synthetic vs. real motions.
<div id='section'>Paperid: <span id='pid'>1838, <a href='https://arxiv.org/pdf/2503.02577.pdf' target='_blank'>https://arxiv.org/pdf/2503.02577.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Boseong Jeon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.02577">SPG: Improving Motion Diffusion by Smooth Perturbation Guidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a test-time guidance method to improve the output quality of the human motion diffusion models without requiring additional training. To have negative guidance, Smooth Perturbation Guidance (SPG) builds a weak model by temporally smoothing the motion in the denoising steps. Compared to model-agnostic methods originating from the image generation field, SPG effectively mitigates out-of-distribution issues when perturbing motion diffusion models. In SPG guidance, the nature of motion structure remains intact. This work conducts a comprehensive analysis across distinct model architectures and tasks. Despite its extremely simple implementation and no need for additional training requirements, SPG consistently enhances motion fidelity. Project page can be found at https://spg-blind.vercel.app/
<div id='section'>Paperid: <span id='pid'>1839, <a href='https://arxiv.org/pdf/2502.19364.pdf' target='_blank'>https://arxiv.org/pdf/2502.19364.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ali Ismail-Fawaz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.19364">Deep Learning For Time Series Analysis With Application On Human Motion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Time series data, defined by equally spaced points over time, is essential in fields like medicine, telecommunications, and energy. Analyzing it involves tasks such as classification, clustering, prototyping, and regression. Classification identifies normal vs. abnormal movements in skeleton-based motion sequences, clustering detects stock market behavior patterns, prototyping expands physical therapy datasets, and regression predicts patient recovery. Deep learning has recently gained traction in time series analysis due to its success in other domains. This thesis leverages deep learning to enhance classification with feature engineering, introduce foundation models, and develop a compact yet state-of-the-art architecture. We also address limited labeled data with self-supervised learning. Our contributions apply to real-world tasks, including human motion analysis for action recognition and rehabilitation. We introduce a generative model for human motion data, valuable for cinematic production and gaming. For prototyping, we propose a shape-based synthetic sample generation method to support regression models when data is scarce. Lastly, we critically evaluate discriminative and generative models, identifying limitations in current methodologies and advocating for a robust, standardized evaluation framework. Our experiments on public datasets provide novel insights and methodologies, advancing time series analysis with practical applications.
<div id='section'>Paperid: <span id='pid'>1840, <a href='https://arxiv.org/pdf/2502.00685.pdf' target='_blank'>https://arxiv.org/pdf/2502.00685.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Emre Sariyildiz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.00685">IEEEICM25: "A High-Performance Disturbance Observer"</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper proposes a novel Disturbance Observer, termed the High-Performance Disturbance Observer, which achieves more accurate disturbance estimation compared to the conventional disturbance observer, thereby delivering significant improvements in robustness and performance for motion control systems.
<div id='section'>Paperid: <span id='pid'>1841, <a href='https://arxiv.org/pdf/2502.00683.pdf' target='_blank'>https://arxiv.org/pdf/2502.00683.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Emre Sariyildiz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.00683">IEEEICM25: "Stability of Digital Robust Motion Control Systems with Disturbance Observer"</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, new stability analysis methods are proposed for digital robust motion control systems implemented using a disturbance observer.
<div id='section'>Paperid: <span id='pid'>1842, <a href='https://arxiv.org/pdf/2501.18726.pdf' target='_blank'>https://arxiv.org/pdf/2501.18726.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Canxuan Gang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.18726">Strong and Controllable 3D Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion generation is a significant pursuit in generative computer vision with widespread applications in film-making, video games, AR/VR, and human-robot interaction. Current methods mainly utilize either diffusion-based generative models or autoregressive models for text-to-motion generation. However, they face two significant challenges: (1) The generation process is time-consuming, posing a major obstacle for real-time applications such as gaming, robot manipulation, and other online settings. (2) These methods typically learn a relative motion representation guided by text, making it difficult to generate motion sequences with precise joint-level control. These challenges significantly hinder progress and limit the real-world application of human motion generation techniques. To address this gap, we propose a simple yet effective architecture consisting of two key components. Firstly, we aim to improve hardware efficiency and computational complexity in transformer-based diffusion models for human motion generation. By customizing flash linear attention, we can optimize these models specifically for generating human motion efficiently. Furthermore, we will customize the consistency model in the motion latent space to further accelerate motion generation. Secondly, we introduce Motion ControlNet, which enables more precise joint-level control of human motion compared to previous text-to-motion generation methods. These contributions represent a significant advancement for text-to-motion generation, bringing it closer to real-world applications.
<div id='section'>Paperid: <span id='pid'>1843, <a href='https://arxiv.org/pdf/2501.07026.pdf' target='_blank'>https://arxiv.org/pdf/2501.07026.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Emre Sariyildiz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.07026">IEEE_TIE25: Analysis and Synthesis of DOb-based Robust Motion Controllers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>By employing a unified state-space design framework, this paper proposes a novel systematic analysis and synthesis method that facilitates the implementation of both conventional zero-order (ZO) and high-order (HO) DObs. Furthermore, this design method supports the development of advanced DObs (e.g., the proposed High-Performance (HP) DOb in this paper), enabling more accurate disturbance estimation and, consequently, enhancing the robust stability and performance of motion control systems. Lyapunov direct method is employed in the discrete-time domain to analyse the stability of the proposed digital robust motion controllers. The analysis demonstrates that the proposed DObs are stable in the sense that the estimation error is uniformly ultimately bounded when subjected to bounded disturbances. Additionally, they are proven to be asymptotically stable under specific disturbance conditions, such as constant disturbances for the ZO and HP DObs. Stability constraints on the design parameters of the DObs are analytically derived, providing effective synthesis tools for the implementation of the digital robust motion controllers. The discrete-time analysis facilitates the derivation of more practical design constraints. The proposed analysis and synthesis methods have been rigorously validated through experimental evaluations, confirming their effectiveness.
<div id='section'>Paperid: <span id='pid'>1844, <a href='https://arxiv.org/pdf/2412.11632.pdf' target='_blank'>https://arxiv.org/pdf/2412.11632.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Juncheng Zou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.11632">Multi-Scale Incremental Modeling for Enhanced Human Motion Prediction in Human-Robot Collaboration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate human motion prediction is crucial for safe human-robot collaboration but remains challenging due to the complexity of modeling intricate and variable human movements. This paper presents Parallel Multi-scale Incremental Prediction (PMS), a novel framework that explicitly models incremental motion across multiple spatio-temporal scales to capture subtle joint evolutions and global trajectory shifts. PMS encodes these multi-scale increments using parallel sequence branches, enabling iterative refinement of predictions. A multi-stage training procedure with a full-timeline loss integrates temporal context. Extensive experiments on four datasets demonstrate substantial improvements in continuity, biomechanical consistency, and long-term forecast stability by modeling inter-frame increments. PMS achieves state-of-the-art performance, increasing prediction accuracy by 16.3%-64.2% over previous methods. The proposed multi-scale incremental approach provides a powerful technique for advancing human motion prediction capabilities critical for seamless human-robot interaction.
<div id='section'>Paperid: <span id='pid'>1845, <a href='https://arxiv.org/pdf/2412.07797.pdf' target='_blank'>https://arxiv.org/pdf/2412.07797.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongjie Fu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.07797">Mogo: RQ Hierarchical Causal Transformer for High-Quality 3D Human Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the field of text-to-motion generation, Bert-type Masked Models (MoMask, MMM) currently produce higher-quality outputs compared to GPT-type autoregressive models (T2M-GPT). However, these Bert-type models often lack the streaming output capability required for applications in video game and multimedia environments, a feature inherent to GPT-type models. Additionally, they demonstrate weaker performance in out-of-distribution generation. To surpass the quality of BERT-type models while leveraging a GPT-type structure, without adding extra refinement models that complicate scaling data, we propose a novel architecture, Mogo (Motion Only Generate Once), which generates high-quality lifelike 3D human motions by training a single transformer model. Mogo consists of only two main components: 1) RVQ-VAE, a hierarchical residual vector quantization variational autoencoder, which discretizes continuous motion sequences with high precision; 2) Hierarchical Causal Transformer, responsible for generating the base motion sequences in an autoregressive manner while simultaneously inferring residuals across different layers. Experimental results demonstrate that Mogo can generate continuous and cyclic motion sequences up to 260 frames (13 seconds), surpassing the 196 frames (10 seconds) length limitation of existing datasets like HumanML3D. On the HumanML3D test set, Mogo achieves a FID score of 0.079, outperforming both the GPT-type model T2M-GPT (FID = 0.116), AttT2M (FID = 0.112) and the BERT-type model MMM (FID = 0.080). Furthermore, our model achieves the best quantitative performance in out-of-distribution generation.
<div id='section'>Paperid: <span id='pid'>1846, <a href='https://arxiv.org/pdf/2411.19495.pdf' target='_blank'>https://arxiv.org/pdf/2411.19495.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Michael Ruderman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.19495">Loop Shaping of Hybrid Motion Control with Contact Transition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A standard motion control with feedback of the output displacement cannot handle unforeseen contact with environment without penetrating into the soft, i.e. viscoelastic, materials or even damaging the fragile materials. Robotics and mechatronics with tactile and haptic capabilities, and in particular medical robotics for example, place special demands on the advanced motion control systems that should enable the safe and harmless contact transitions. This paper shows how the basic principles of loop shaping can be easily used to handle sufficiently stiff motion control in such a way that it is extended by sensor-free dynamic reconfiguration upon contact with the environment. A thereupon based hybrid control scheme is proposed. A remarkable feature of the developed approach is that no measurement of the contact force is required and the input signal and the measured output displacement are the only quantities used for design and operation. Experiments on 1-DOF actuator are shown, where the moving tool comes into contact with grapes that are soft and simultaneously penetrable.
<div id='section'>Paperid: <span id='pid'>1847, <a href='https://arxiv.org/pdf/2410.07200.pdf' target='_blank'>https://arxiv.org/pdf/2410.07200.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>SK Hasan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.07200">A Realistic Model Reference Computed Torque Control Strategy for Human Lower Limb Exoskeletons</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Exoskeleton robots have become a promising tool in neurorehabilitation, offering effective physical therapy and recovery monitoring. The success of these therapies relies on precise motion control systems. Although computed torque control based on inverse dynamics provides a robust theoretical foundation, its practical application in rehabilitation is limited by its sensitivity to model accuracy, making it less effective when dealing with unpredictable payloads. To overcome these limitations, this study introduces a novel model reference computed torque controller that accounts for parametric uncertainties while optimizing computational efficiency. A dynamic model of a seven-degree-of-freedom human lower limb exoskeleton is developed, incorporating a realistic joint friction model to accurately reflect the physical behavior of the robot. To reduce computational demands, the control system is split into two loops: a slower loop that predicts joint torque requirements based on input trajectories and robot dynamics, and a faster PID loop that corrects trajectory tracking errors. Coriolis and centrifugal forces are excluded from the model due to their minimal impact on system dynamics relative to their computational cost. Experimental results show high accuracy in trajectory tracking, and statistical analyses confirm the controller's robustness and effectiveness in handling parametric uncertainties. This approach presents a promising advancement for improving the stability and performance of exoskeleton-based neurorehabilitation.
<div id='section'>Paperid: <span id='pid'>1848, <a href='https://arxiv.org/pdf/2409.19686.pdf' target='_blank'>https://arxiv.org/pdf/2409.19686.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xingyu Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.19686">Text-driven Human Motion Generation with Motion Masked Diffusion Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-driven human motion generation is a multimodal task that synthesizes human motion sequences conditioned on natural language. It requires the model to satisfy textual descriptions under varying conditional inputs, while generating plausible and realistic human actions with high diversity. Existing diffusion model-based approaches have outstanding performance in the diversity and multimodality of generation. However, compared to autoregressive methods that train motion encoders before inference, diffusion methods lack in fitting the distribution of human motion features which leads to an unsatisfactory FID score. One insight is that the diffusion model lack the ability to learn the motion relations among spatio-temporal semantics through contextual reasoning. To solve this issue, in this paper, we proposed Motion Masked Diffusion Model \textbf{(MMDM)}, a novel human motion masked mechanism for diffusion model to explicitly enhance its ability to learn the spatio-temporal relationships from contextual joints among motion sequences. Besides, considering the complexity of human motion data with dynamic temporal characteristics and spatial structure, we designed two mask modeling strategies: \textbf{time frames mask} and \textbf{body parts mask}. During training, MMDM masks certain tokens in the motion embedding space. Then, the diffusion decoder is designed to learn the whole motion sequence from masked embedding in each sampling step, this allows the model to recover a complete sequence from incomplete representations. Experiments on HumanML3D and KIT-ML dataset demonstrate that our mask strategy is effective by balancing motion quality and text-motion consistency.
<div id='section'>Paperid: <span id='pid'>1849, <a href='https://arxiv.org/pdf/2409.05295.pdf' target='_blank'>https://arxiv.org/pdf/2409.05295.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Farhad Aghili
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.05295">Adaptive Visual Servoing for On-Orbit Servicing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents an adaptive visual servoing framework for robotic on-orbit servicing (OOS), specifically designed for capturing tumbling satellites. The vision-guided robotic system is capable of selecting optimal control actions in the event of partial or complete vision system failure, particularly in the short term. The autonomous system accounts for physical and operational constraints, executing visual servoing tasks to minimize a cost function. A hierarchical control architecture is developed, integrating a variant of the Iterative Closest Point (ICP) algorithm for image registration, a constrained noise-adaptive Kalman filter, fault detection and recovery logic, and a constrained optimal path planner. The dynamic estimator provides real-time estimates of unknown states and uncertain parameters essential for motion prediction, while ensuring consistency through a set of inequality constraints. It also adjusts the Kalman filter parameters adaptively in response to unexpected vision errors. In the event of vision system faults, a recovery strategy is activated, guided by fault detection logic that monitors the visual feedback via the metric fit error of image registration. The estimated/predicted pose and parameters are subsequently fed into an optimal path planner, which directs the robot's end-effector to the target's grasping point. This process is subject to multiple constraints, including acceleration limits, smooth capture, and line-of-sight maintenance with the target. Experimental results demonstrate that the proposed visual servoing system successfully captured a free-floating object, despite complete occlusion of the vision system.
<div id='section'>Paperid: <span id='pid'>1850, <a href='https://arxiv.org/pdf/2409.02636.pdf' target='_blank'>https://arxiv.org/pdf/2409.02636.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Toshiaki Tsuji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.02636">Mamba as a motion encoder for robotic imitation learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in imitation learning, particularly with the integration of LLM techniques, are set to significantly improve robots' dexterity and adaptability. This paper proposes using Mamba, a state-of-the-art architecture with potential applications in LLMs, for robotic imitation learning, highlighting its ability to function as an encoder that effectively captures contextual information. By reducing the dimensionality of the state space, Mamba operates similarly to an autoencoder. It effectively compresses the sequential information into state variables while preserving the essential temporal dynamics necessary for accurate motion prediction. Experimental results in tasks such as cup placing and case loading demonstrate that despite exhibiting higher estimation errors, Mamba achieves superior success rates compared to Transformers in practical task execution. This performance is attributed to Mamba's structure, which encompasses the state space model. Additionally, the study investigates Mamba's capacity to serve as a real-time motion generator with a limited amount of training data.
<div id='section'>Paperid: <span id='pid'>1851, <a href='https://arxiv.org/pdf/2407.13330.pdf' target='_blank'>https://arxiv.org/pdf/2407.13330.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Arunabh Bora
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.13330">Exploring Robot Trajectory Planning -- A Comparative Analysis of Algorithms And Software Implementations in Dynamic Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Trajectory Planning is a crucial word in Modern & Advanced Robotics. It's a way of generating a smooth and feasible path for the robot to follow over time. The process primarily takes several factors to generate the path, such as velocity, acceleration and jerk. The process deals with how the robot can follow a desired motion path in a suitable environment. This trajectory planning is extensively used in Automobile Industrial Robot, Manipulators, and Mobile Robots. Trajectory planning is a fundamental component of motion control systems. To perform tasks like pick and place operations, assembly, welding, painting, path following, and obstacle avoidance. This paper introduces a comparative analysis of trajectory planning algorithms and their key software elements working strategy in complex and dynamic environments. Adaptability and real-time analysis are the most common problems in trajectory planning. The paper primarily focuses on getting a better understanding of these unpredictable environments.
<div id='section'>Paperid: <span id='pid'>1852, <a href='https://arxiv.org/pdf/2405.01284.pdf' target='_blank'>https://arxiv.org/pdf/2405.01284.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Liu Qiyuan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.01284">Behavior Imitation for Manipulator Control and Grasping with Deep Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The existing Motion Imitation models typically require expert data obtained through MoCap devices, but the vast amount of training data needed is difficult to acquire, necessitating substantial investments of financial resources, manpower, and time. This project combines 3D human pose estimation with reinforcement learning, proposing a novel model that simplifies Motion Imitation into a prediction problem of joint angle values in reinforcement learning. This significantly reduces the reliance on vast amounts of training data, enabling the agent to learn an imitation policy from just a few seconds of video and exhibit strong generalization capabilities. It can quickly apply the learned policy to imitate human arm motions in unfamiliar videos. The model first extracts skeletal motions of human arms from a given video using 3D human pose estimation. These extracted arm motions are then morphologically retargeted onto a robotic manipulator. Subsequently, the retargeted motions are used to generate reference motions. Finally, these reference motions are used to formulate a reinforcement learning problem, enabling the agent to learn a policy for imitating human arm motions. This project excels at imitation tasks and demonstrates robust transferability, accurately imitating human arm motions from other unfamiliar videos. This project provides a lightweight, convenient, efficient, and accurate Motion Imitation model. While simplifying the complex process of Motion Imitation, it achieves notably outstanding performance.
<div id='section'>Paperid: <span id='pid'>1853, <a href='https://arxiv.org/pdf/2401.01644.pdf' target='_blank'>https://arxiv.org/pdf/2401.01644.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hanxiao Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.01644">Motion Control of Interactive Robotic Arms Based on Mixed Reality Development</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Mixed Reality (MR) is constantly evolving to inspire new patterns of robot manipulation for more advanced Human- Robot Interaction under the 4th Industrial Revolution Paradigm. Consider that Mixed Reality aims to connect physical and digital worlds to provide special immersive experiences, it is necessary to establish the information exchange platform and robot control systems within the developed MR scenarios. In this work, we mainly present multiple effective motion control methods applied on different interactive robotic arms (e.g., UR5, UR5e, myCobot) for the Unity-based development of MR applications, including GUI control panel, text input control panel, end-effector object dynamic tracking and ROS-Unity digital-twin connection.
<div id='section'>Paperid: <span id='pid'>1854, <a href='https://arxiv.org/pdf/2312.10628.pdf' target='_blank'>https://arxiv.org/pdf/2312.10628.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Congyi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.10628">T2M-HiFiGPT: Generating High Quality Human Motion from Textual Descriptions with Residual Discrete Representations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this study, we introduce T2M-HiFiGPT, a novel conditional generative framework for synthesizing human motion from textual descriptions. This framework is underpinned by a Residual Vector Quantized Variational AutoEncoder (RVQ-VAE) and a double-tier Generative Pretrained Transformer (GPT) architecture. We demonstrate that our CNN-based RVQ-VAE is capable of producing highly accurate 2D temporal-residual discrete motion representations. Our proposed double-tier GPT structure comprises a temporal GPT and a residual GPT. The temporal GPT efficiently condenses information from previous frames and textual descriptions into a 1D context vector. This vector then serves as a context prompt for the residual GPT, which generates the final residual discrete indices. These indices are subsequently transformed back into motion data by the RVQ-VAE decoder. To mitigate the exposure bias issue, we employ straightforward code corruption techniques for RVQ and a conditional dropout strategy, resulting in enhanced synthesis performance. Remarkably, T2M-HiFiGPT not only simplifies the generative process but also surpasses existing methods in both performance and parameter efficacy, including the latest diffusion-based and GPT-based models. On the HumanML3D and KIT-ML datasets, our framework achieves exceptional results across nearly all primary metrics. We further validate the efficacy of our framework through comprehensive ablation studies on the HumanML3D dataset, examining the contribution of each component. Our findings reveal that RVQ-VAE is more adept at capturing precise 3D human motion with comparable computational demand compared to its VQ-VAE counterparts. As a result, T2M-HiFiGPT enables the generation of human motion with significantly increased accuracy, outperforming recent state-of-the-art approaches such as T2M-GPT and Att-T2M.
<div id='section'>Paperid: <span id='pid'>1855, <a href='https://arxiv.org/pdf/2312.10395.pdf' target='_blank'>https://arxiv.org/pdf/2312.10395.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohamed Sorour
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.10395">RoboPainter -- a conceptual towards robotized interior finishes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>High demand for painters is required nowadays and foreseen in the near future for both developed and developing countries. To satisfy such demand, this paper presents the detailed computer aided design (CAD) model of a fully functional wall painting robot for interior finishes. The RoboPainter is capable of performing full scale wall-ceil painting in addition to decorative wall drawings. The 8 degrees of freedom (DOF) mobile robot structure consists of a 6DOF spray painting arm mounted on a 2DOF differentially driven mobile base. The design presented endows several achievements in terms of total robot mass and painting rate as compared to existing literature. Detailed dynamic model parameters are presented to allow for further enhancement in terms of robot motion control.
<div id='section'>Paperid: <span id='pid'>1856, <a href='https://arxiv.org/pdf/2312.02635.pdf' target='_blank'>https://arxiv.org/pdf/2312.02635.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiawei Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.02635">Multi-rotor Aerial Vehicles in Physical Interactions: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Research on Multi-rotor Aerial Vehicles (MAVs) has experienced remarkable advancements over the past two decades, propelling the field forward at an accelerated pace. Through the implementation of motion control and the integration of specialized mechanisms, researchers have unlocked the potential of MAVs to perform a wide range of tasks in diverse scenarios. Notably, the literature has highlighted the distinctive attributes of MAVs that endow them with a competitive edge in physical interaction when compared to other robotic systems. In this survey, we present a categorization of the various types of physical interactions in which MAVs are involved, supported by comprehensive case studies. We examine the approaches employed by researchers to address different challenges using MAVs and their applications, including the development of different types of controllers to handle uncertainties inherent in these interactions. By conducting a thorough analysis of the strengths and limitations associated with different methodologies, as well as engaging in discussions about potential enhancements, this survey aims to illuminate the path for future research focusing on MAVs with high actuation capabilities.
<div id='section'>Paperid: <span id='pid'>1857, <a href='https://arxiv.org/pdf/2311.05115.pdf' target='_blank'>https://arxiv.org/pdf/2311.05115.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenbo Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.05115">A Survey on Convex Optimization for Guidance and Control of Vehicular Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Guidance and control (G&C) technologies play a central role in the development and operation of vehicular systems. The emergence of computational guidance and control (CG&C) and highly efficient numerical algorithms has opened up the great potential for solving complex constrained G&C problems onboard, enabling higher level of autonomous vehicle operations. In particular, convex-optimization-based G&C has matured significantly over the years and many advances continue to be made, allowing the generation of optimal G&C solutions in real-time for many vehicular systems in aerospace, automotive, and other domains. In this paper, we review recent major advances in convex optimization and convexification techniques for G&C of vehicular systems, focusing primarily on three important application fields: 1) Space vehicles for powered descent guidance, small body landing, rendezvous and proximity operations, orbital transfer, spacecraft reorientation, space robotics and manipulation, spacecraft formation flying, and station keeping; 2) Air vehicles including hypersonic/entry vehicles, missiles and projectiles, launch/ascent vehicles, and low-speed air vehicles; and 3) Motion control and powertrain control of ground vehicles. Throughout the paper, we draw figures that illustrate the basic mission concepts and scenarios and present tables that summarize similarities and distinctions among the key problems, ideas, and approaches. Where available, we provide comparative analyses and reveal correlations between different applications. Finally, we identify open challenges and issues, discuss potential opportunities, and make suggestions for future research directions.
<div id='section'>Paperid: <span id='pid'>1858, <a href='https://arxiv.org/pdf/2308.15345.pdf' target='_blank'>https://arxiv.org/pdf/2308.15345.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingbo Zeng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.15345">IndGIC: Supervised Action Recognition under Low Illumination</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Technologies of human action recognition in the dark are gaining more and more attention as huge demand in surveillance, motion control and human-computer interaction. However, because of limitation in image enhancement method and low-lighting video datasets, e.g. labeling cost, existing methods meet some problems. Some video-based approached are effect and efficient in specific datasets but cannot generalize to most cases while others methods using multiple sensors rely heavily to prior knowledge to deal with noisy nature from video stream. In this paper, we proposes action recognition method using deep multi-input network. Furthermore, we proposed a Independent Gamma Intensity Corretion (Ind-GIC) to enhance poor-illumination video, generating one gamma for one frame to increase enhancement performance. To prove our method is effective, there is some evaluation and comparison between our method and existing methods. Experimental results show that our model achieves high accuracy in on ARID dataset.
<div id='section'>Paperid: <span id='pid'>1859, <a href='https://arxiv.org/pdf/2307.15852.pdf' target='_blank'>https://arxiv.org/pdf/2307.15852.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alexandre Girard
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.15852">Dimensionless Policies based on the Buckingham $Ï$ Theorem: Is This a Good Way to Generalize Numerical Results?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The answer to the question posed in the title is yes if the context (the list of variables defining the motion control problem) is dimensionally similar. This article explores the use of the Buckingham $Ï$ theorem as a tool to encode the control policies of physical systems into a more generic form of knowledge that can be reused in various situations. This approach can be interpreted as enforcing invariance to the scaling of the fundamental units in an algorithm learning a control policy. First, we show, by restating the solution to a motion control problem using dimensionless variables, that (1) the policy mapping involves a reduced number of parameters and (2) control policies generated numerically for a specific system can be transferred exactly to a subset of dimensionally similar systems by scaling the input and output variables appropriately. Those two generic theoretical results are then demonstrated, with numerically generated optimal controllers, for the classic motion control problem of swinging up a torque-limited inverted pendulum and positioning a vehicle in slippery conditions. We also discuss the concept of regime, a region in the space of context variables, that can help to relax the similarity condition. Furthermore, we discuss how applying dimensional scaling of the input and output of a context-specific black-box policy is equivalent to substituting new system parameters in an analytical equation under some conditions, using a linear quadratic regulator (LQR) and a computed torque controller as examples. It remains to be seen how practical this approach can be to generalize policies for more complex high-dimensional problems, but the early results show that it is a promising transfer learning tool for numerical approaches like dynamic programming and reinforcement learning.
<div id='section'>Paperid: <span id='pid'>1860, <a href='https://arxiv.org/pdf/2304.08916.pdf' target='_blank'>https://arxiv.org/pdf/2304.08916.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zeeshan Khan Suri
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.08916">Pose Constraints for Consistent Self-supervised Monocular Depth and Ego-motion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Self-supervised monocular depth estimation approaches suffer not only from scale ambiguity but also infer temporally inconsistent depth maps w.r.t. scale. While disambiguating scale during training is not possible without some kind of ground truth supervision, having scale consistent depth predictions would make it possible to calculate scale once during inference as a post-processing step and use it over-time. With this as a goal, a set of temporal consistency losses that minimize pose inconsistencies over time are introduced. Evaluations show that introducing these constraints not only reduces depth inconsistencies but also improves the baseline performance of depth and ego-motion prediction.
